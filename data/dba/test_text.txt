running sp askbrent expertmode powershell trying run sp askbrent powershell using invoke sqlcmd capture output variable query exec saidba monitoring,brent correct expertmode turns multiple result sets diagnostics wait stats file stats perfmon counters want one result set dont turn expertmode want multiple result sets application case posh cant consume youll need log tables thats parameters come outputdatabasename name database want write must already exist outputschemaname schema tables live like dbo must already exist outputtablename table name top result set diagnostics parameter also requires two table doesnt already exist created outputtablenamefilestats outputtablenameperfmonstats outputtablenamewaitstats rest result sets written table doesnt already exist created totally optional pass none want work differently describe need ill see come way get hope helps comment actually 10,discount brents answer although give good ones training times powershell execute queries return multiple datasets using invoke sqlcmd built currently would two options either net native code system data sqlclient using trusty smo tend chose smo simply event want include server properties something else smo access context sp askbrent went ahead spent minutes building output html report main points interest script handle multiple datasets execute query executewithresults method available following namespaces microsoft sqlserver management smo server microsoft sqlserver management smo database see example using database namespace msdn script using server namespace work either way review msdn article executewithresults return dataset 7,speed things could try adding primary key cspec rowid dont scan every iteration change cte temp table suitable pk see next point add index sourcetable1 match cte clause currently pk scanned meaning sourcetable1 rows scanned every iteration million rows sourcetable2 marketid also index wouldnt worry scanned understand query plans show lot scans poor indexes operations target table indexing appears ok another observation uniqueidentifier varchar bad choices clustered indexes pks wide increasing overhead collection comparisons least edit another observation thanks marian clustered index wide generally every non clustered index points clustered index means huge nc index could probably achieve result reordering 0,smart move smart move depends specific case file location directly affecting url structure youre storing full file addresses database bad say bad move since trouble case somebody move rename directory application built way simply point files directory file access logic dynamic made smart move following reasons database storing need serve lot images per request increase response time application since files going sent synchronously network also add little processing server file storing usually cheaper database storage file storing unless restrictions image access determined user access given group images theres need processing access images kind file database storage possible access files using 0
database redesign opportunity table design use sensor data collection background network approximately sensors data points collect minute intervals data points,think partitioning table big reason indexes giant table even one index generated lot cpu load disk perform index maintenance executing inserts updates deletes wrote earlier post back october table partitioning would big help one excerpt past post partitioning data serve group data logically cohesively class performance searching partition need main consideration long data correctly grouped achieved logical partitioning concentrate search time separating data id possible many rows data may never accessed reads writes major consideration locate ids frequently accessed partition less frequently accessed ids reside one big archive table still accessible index lookup blue moon query read entire post later 5,interesting sensors produce kind data make sense put table amount data see youd worried performance days usual amount time produce graph could two tables main sensor data table stores data little want slack days ago today everything older goes archive table could help reduce size table reports begin generated hopefully majority gb data archive table main table archiving job scheduled run nightly maybe also consider building separate reporting database stores data structure better generating reports tables designed closely match querying maybe pre calculate aggregate values would otherwise take long time generate possible populate main database regular nightly basis course need 5,issue stems fact entity type missing model consider following erd note ive added intersection entity type department classification new entity type position provides information implicit model particular department give set jobs various classifications adding position model explicit entity advantages avoids problem concerned worker potentially assigned departments classifications different companies gives locus predicates might applicable position salary grade etc allows record fact position exists even workers currently position quite possibly useful information note avoid problem position defined department classification different companies ive expanded keys department classification good reasons read length todd everetts answer beware model presumes simplification specifically assumes position recorded 0,kernel paramter configured properly oracle database installation guide 11g release linux part number e24321 configuring kernel parameters linux 0
change data capture know made change tracking made change identified cdc along lines datetime hack tried approach adding suser sname,filed bug closed design http connect microsoft com sqlserver feedback details cdc options capture data username date time etc unfortunately use another technique sql audit trigger get information custom code try correlate cdc data much wrote deficiency also chapter sql server audit change tracking change data capture book sql server mvp deep dives volume im sorry dont better workaround built capabilities cdc arent going meet requirements 7,could add column trigger table populate user insert update delete cdc would store grab username either ui passing username using context info actual session set capturedby castreplacecastcontext info varchar10char0 int capturedby null begin select capturedby code sys dm exec sessions inner join dbo person login name user name session id spid end 6,resolved restarting sql server service unfortunately fortunately initial idea detach attach database thought would actually fix problem read forum somewhere effect regrettably pj mahoney suggested running trace hope see issue find trace reveals anything thanks suggestion least answering question means dont see another suggestion changing database owner thanks 0,varchar work good enough lot western european languages norwegian danish german french dutch etc subject collation issues see varchar vs nvarchar performance nvarchar serious performance implications trivial compared dealing dates mdy vs dmy 0
logical order execution plan sql query one join know logical order execution sql query join group cube rollup select distinct,one way determine logical order joins replace first inner join example left outer join select user branch t1 left join dimcustomer2 t2 t1 branch code t2 branch code inner join customer guarantee t3 t3 customer num t2 customer num let us assume rows t1 matches t2 specifically let us assume three tables t1 t2 t3 branch code branch code customer num customer num two joins two possibilities order executed left join inner join left join evaluates first result nulls t2 columns t1 rows match t1 branch code t2 barnch code t2 customer num null null null null joining result t3 11,im afraid phrase logical execution make much sense query execution definition physical materialization result set think mean logical execution query compilation phase query syntax semantic meaning analyzed query plan prepared implement said semantic meaning joined tables1 query always evaluated left right top bottom select evaluated first join evaluated second c1 c3 join evaluated third c4 c5 verify try reference clause column belongs table included later evaluation sequence fail compile select join c1 c8 known yet join query parsed execution plan perform joins order maintains query semantics manual says order table sources keyword affect result set returned somewhat related joined table 9,able use auto explain turn set auto explain log min duration get plans log statements run session might also want set set auto explain log analyze true youll essentially run everything double real explain analyze non timing performance testing phase output much useful explain plans alone provides plan actually happened 0,microsoft enabling feature mean useful everyone systems using features mean relying information stored msdb cases query store useful articles usage tuning msdb database objects msdb database books online msdb performance tuning geoff hiten importance maintenance msdb tim radney mentioned following optimizing indexes msdb important user databases many times found clients optimizing user databases system databases since msdb database heavily used sql server agent log shipping service broker ssis backup restore processes indexes get highly fragmented ensure index optimization jobs also include system databases least msdb seen index optimizations free several gigabytes space highly fragmented indexes within msdb see query store 0
use truncate drop system work lot stored procedures sql scripts make use temporary tables using tables good practice drop many,truncate drop almost identical behavior speed truncate right drop simply unnecessary note wrote answer sql server perspective assumed would apply equally sybase appears entirely case note first posted answer several highly rated answers including accepted answer made several false claims like truncate logged truncate rolled back truncate faster drop etc thread cleaned rebuttals follow may seem tangential original question leave reference others looking debunk myths couple popular falsehoods pervasive even among experienced dbas may motivated truncate drop pattern myth truncate logged therefore rolled back myth truncate faster drop let rebut falsehoods writing rebuttal sql server perspective everything say equally applicable 131,testing truncate drop vs drop directly shows first approach actually slight increased logging overhead may even mildly counter productive looking individual log records shows truncate drop version almost identical drop version except additional entries operation context allocunitname lop count delta lcx clustered sys sysallocunits clust lop count delta lcx clustered sys sysrowsets clust lop count delta lcx clustered sys sysrscols clst lop count delta lcx clustered sys sysrscols clst lop hobt ddl lcx null null lop modify row lcx clustered sys sysallocunits clust lop hobt ddl lcx null null lop modify row lcx clustered sys sysrowsets clust lop lock xact 52,try sql alter authorization schema yourschemaname dbo go drop user theuseryouwanttodelete go cant drop principal schema owner alter authorzation changes owned schema used yourschemaname obviously substitute owned schema database dbo likewise change ownership whatever principal need environment allow drop previously schema owning user example purposes used theuseryouwanttodelete thatll non owner want drop 0,user logs theyre assigned security token includes information group membership token persists user logs point discarded even make changes group membership ad mean time changes make take effect next time user logs receives new security token reproduce scenario assigning permissions file system example ad behaviour sql server behaviour 0
managing sql server database one terabyte data recently audit database went one terabyte since storage problems management looking options proposal,get space business requirement retain much data come money turn page compression biggest tables indexes test test test log data compresses extremely well compressing amount data going take time see point 8,im adding trubs answer upvote shes right track im adding options well add space stated high amount average free space per page indexes run alter indexreorg statement ample free space id suggest rebuild operation instead since talking running space operation may complete larger tables find average free space per page amount need run sys dm db index physical stats dmv using either sampled detailed mode take purge data batch deletes help reduce excessive logging locking better long term solution would utilize table index partitioning switch truncate last partition quicker data purging enable page row compression stated see estimated space savings 5,think part problem perspective dbas data analysts database developers deal data time application developers concerned make things work send production dont worry much data look like six months long errors deployed data people live results short sighted decisions cause data lose integrity cause duplicate records get inserted try explain data bad dbas ones deal performance problem process worked fine thousand records takes hours records databases harder refactor dbas concerned getting right first time developers see problem refactoring road developers see problem making database act object oriented database people know effective efficient way store get data application developers often deal small 0,please note definitive answer best answer chatting taryn however primary showing different story reporting separate ag syncing without issues dags synchronzing healthy state individual databases ags underlying distributed ag say healthy synchronizing good chance hiccup dmvs ssms dashboards since nothing errorlog suggest replica didnt connect disconnected state unfortunately since issue resolved hard say exactly future occurs someone check sys dm hadr database replica states clusters looking anything isnt healthy shows healthy possible dmv hasnt updated yet unhealthy check errorlog dmvs connectivity issues able connect forwarder global primary dans answer mentions issues could arise database startup though case instance cant read 0
database design new table versus new columns suggested repost stackoverflow currently table need start adding new data columns every record,personally lean toward adding columns existing table new table doesnt really buy anything dont really save much space null values original table dont take space new table needs kind identifier offsets savings anyway queries become complex newcolumn null becomes left outer join single table means row size vary page page shouldnt affect many existing pages especially clustered index monotonically increasing column identity date time 10,wrestling vertical partitioning physical database design technique improve performance physical database design technique applicability depends specific queries trying optimize technique optimize logical standpoint new fields depend upon candidate key entity facts belong first make sure fully understand functional dependence new fields candidate keys verify really facts daily page views deciding partition another table performance optimization done achieves performance goals general vertical partitioning useful query new columns infrequently distinctly columns original table placing columns another table shares pk existing table query directly want new columns get much greater put many rows per page disk new table columns original table wont sitting 29,given information youve provided general normalization goal would probably simply add nullable columns havent given enough information data used know best way model data depending upon really using data may want consider different data model putting data reporting might want looking dimensional model efficient certain types reporting instance time day analysis works well date time dimension split answering analytic questions like popular time day visits campaigns like day campaign see visits per hour single data time column going work well even split relational model lot cases might treat ip address dimension perhaps kind geography data snowflake 4,try improve google foo results terms query planner optimizers job understand best use index means understand better even build one temporary see try check also temporary indexes also hash tables well depending definition temporary index see details 0,load data infile extended inserts distinct advantages load data infile designed mass loading table data single operation along bells whistles perform tings like skipping initial lines skipping specific columns transforming specific columns loading specific columns handling duplicate key issues less overhead needed parsing flip side importing rows instead rows extended insert sensible notice mysqldump designed around extended inserts sake carrying table design along data performs injection hundreds thousands rows per insert load data infile always creates physical dichomoty schema data application point view load data infile also insensitive schema change extended inserts one go back forth good bad ugly using 0,alternative test attribs unpivot view provided jackpdouglas works versions 11g fewer full table scans create replace view test attribs unpivot select id name myrow attr cast decodemyrow1attr12attr23attr34attr4attr5 varchar22000 attr test attribs cross join select level myrow dual connect level final query used unchanged view 0
null computed column considered nullable view table create table dbo realty id int identity11 null rankingbonus int null ranking id,ranking field showing nullable due computed column yes declared null msdn page computed columns states database engine change determination query time database engine automatically determines nullability computed columns based expressions used result expressions considered nullable even nonnullable columns present possible underflows overflows produce null results well use columnproperty function allowsnull property investigate nullability computed column table expression nullable turned nonnullable one specifying isnullcheck expression constant constant nonnull value substituted null result lets see true create table dbo realty id int identity11 null rankingbonus int null ranking id rankingbonus persisted null go exec sp help dbo realty ranking nullable select columnpropertyobject 17,guarantee ranking computed column expression return null circumstances must wrap isnull suitable default value example ranking isnullid rankingbonus persisted null null constraint ensures persisted value null context table session level settings effect table modified however query references expression sql server choice using persisted value settings match computing expression afresh session settings cause overflow return null example sql server must account possibility accessed via view sql server correctly marks column potentially returning null using outermost isnull expression supported way achieve want using coalesce work instance demo create table dbo t1 c1 integer null c2 integer null c3 c1 c2 persisted null 13,first would avoid making round trip database every value example application knows needs new ids make round trips make one stored procedure call increment counter also might better split table multiple ones possible avoid deadlocks altogether deadlocks system several ways accomplish show would use sp getapplock eliminate deadlocks idea work sql server closed source see source code know tested possible cases following describes works ymmv first let us start scenario always get considerable amount deadlocks second shall use sp getapplock eliminate important point stress test solution solution may different need expose high concurrency demonstrate later prerequisites let us set table 0,cant rely insert generate identity values order original string may observe lucky coincidence certainly guaranteed following work dont duplicates declare str varchar255 onetwothreefourfive select value row number order charindex value str string splitonetwothreefourfive order code golf maybe declare char99 onetwothreefourfive select valuen rank overorder charindex value string splitonetwothreefourfive order duplicates becomes lot complex ideas maybe removing duplicates strings sql server solve old problems sql server new string agg string split functions 0
datawarehouse design combined date time dimension vs separate day time dimensions timezones starting design new data warehouse trying design date,date time separate allow aggregates time much easily eg want run query find time period day busy much easily performed using separate time dimension also one timekey decide either gmt est time use fact table need run reports based timezone convert application query 5,follow decided implement datawarehouse support multiple time zones efficient possible chose create table time zones id name etc well time zone bridge table looks like time zone bridge date key utc time key utc timezone id date key local time key local way keep normal date time dimension tables small facts link utc date time keys need report group different time zone join time zone bridge table link local date time keys back date time dimension tables populate time zone bridge table using code invoked ssis since much less complicated tz stuff sqlserver directly 5,charles mentioned ms excel comment pretty much safe assume youre microsoft environment definitely much power know mess database management system youre serious data analysis id say go enterprise databases like oracle sql server mysql db2 etc relational databases good note also non relational databases gaining traction database market since youre probably microsoft environment suggest go sql server ask department enterprise database already place none download express version sql server see link comment youre read limitation express edition express edition free express edition fully functioning production ready version sql server although limited ways storage capacity memory usage etc even run reporting 0,please refer postgres manual copy pgadmin sql string pass via script db connection would use copy prefix enter something like copy tablename need make sure relevant privileges run command case need able log database write access tablename postgres also needs able reach file path home uploads accessible database server postgres user able read file check permissions file directory 0
way programmatically script objects associated given table know sql management studio right click table trigger key script object way programmatically,technique using server management objects dont know way pure sql 5,way get started would following declare tablename varchar50 declare objectid int set tablename name objects want investigate select objectid id sysobjects name tablename select sysobjects name tablename union select sysobjects id select id sysdepends depid objectid sysdepends table tell objects dependent upon another hierarchical may recursively run sysdepends start getting nulls sometimes sysdepends incomplete article suggestions sysobjects table tell stuff objects database type also xtype columns tell item user defined table stored proc trigger etc want sp helptext spit text stored procedure reproduce text encrypted stored procedure full complete solution involve programming something especially encrypted stored procedures triggers involved one 5,important thing keep mind hash merge join options unconditional equality join condition consider simpler version query select c1 customerid customer1 c2 customerid customer2 customer c1 inner join customer c2 c1 firstname c2 firstname c1 birthdate c2 birthdate option hash join merge join possible generate query plan hints disable loop join msg level state line query processor could produce query plan hints defined query resubmit query without specifying hints without using set forceplan logic means theres nothing suitable hashed sort complex join clause without unconditional equality conditions tend get cross join like posted question table rows expect rows scanned good option 0,undo ctrl wrong auto selection appears original entry restored 0
amazon rds mysql vs installing mysql amazon ec2 instance work host webservers amazon ec2 usually used mysql databases installed box,scaling db servers cup tea amazon rds ok use bells whistles come simply want moderate ha backups scaling benefit great deal flip side want scale hardware question rds want scale mysqls capabilities unfortunately question many aspects one would want example know two fields capped across seven7 rds server models max connections innodb buffer pool size note following chart two options model max connections innodb buffer pool size t1 micro 311m m1 small 1125m 097g m1 large 5610m 479g m1 xlarge 11370m 103g m2 xlarge 12975m 671g m2 2xlarge 26100m 488g m2 4xlarge 52350m 123g given super privilege direct access cnf 11,im big aws fan general rds much rolandomysqldba pointed pretty good points advantage see rds compared mysql ec2 ability point click snapshots clones point time recovery nearly sufficient make loss control flexibility certainly dont justify price higher rds sexy ways cant ultimately trust cant ultimately fix cant get moving parts dont like super privilege dont like able tail error log dont like able run top iostat database server see cores drives enjoying load dont like access federated storage engine dont like thought paying hot standyby multi az backup master machine cant even leverage read replica sure perfectly reasonable explanations constraints 20,least intrusive way get working either use following command dba issue command use master go grant view server state user name 0,oracle provides nvl scenario isnull equivalent ms sql server could disguise view make code clearer 0
script permissions schema sql management studio allows create scripts db objects however far couldnt find way correctly script schema user,friend sys database permissions check script permisssions script using refresh prod dev uat server script hand permissions restore run script script db level permissions v2 source http www sqlservercentral com scripts security declare sql varchar2048 sort int declare tmp cursor db context statement select db context sql statements result order holder union select use space1 quotenamedb name sql statements result order holder union select sql statements result order holder union db user creation select db users sql statements result order holder union select exists select name sys database principals name space1 name begin create user space1 quotename name login quotename 12,need script permissions two steps roles objects kin alludes use sys database permissions objects want use sys database principals sys database role members role membership following sql work sql previous versions use sp addrolemember roles executed context database scripting permissions filter specific user necessary add roles select alter role quotenamedpr name add member quotenamedpu name sys database principals dpr join sys database role members drm dpr principal id drm role principal id join sys database principals dpu drm member principal id dpu principal id dpu principal id grant explicit permissions select grant dp permission name collate latin1 general cs name 8,dont want bother installing java compiling already python could try python script https github com thusoy postgres mitm blob master postgres get server cert py use check certificate dates postgres get server cert py example com openssl x509 noout dates full cert text postgres get server cert py example com openssl x509 noout text 0,date time separate allow aggregates time much easily eg want run query find time period day busy much easily performed using separate time dimension also one timekey decide either gmt est time use fact table need run reports based timezone convert application query 0
myisam table locks copy mysql running read disclaimer disclaimer im aware supposed done time consistency tables arent concert im trying,cases could face issues copying myd myis major weakness myisam data changes changes myd files cached os course would times worse mysql windows ill leave windows answer using mysql little good news command flush tables read lock would look myisam tables single lock mysql syntax flush tables db tbdb tb read lock mysql documentation says flush tables tbl name tbl name read lock statement flushes acquires read locks named tables statement first acquires exclusive metadata locks tables waits transactions tables open complete statement flushes tables table cache reopens tables acquires table locks like lock tables read downgrades metadata locks exclusive 4,rolandomysqldba course another valiant answer point question reading problem copy myisam table files frm myd myi gets write transaction yes cant get consistent backup even single myisam table unless type locking prevent writes rolando gave pretty thorough answer options available locking one option people use backing myisam data lvm snapshots see http www lenzg net mylvmbackup great tool assist final recommendation stop using myisam use innodb instead fast non locking physical backups percona xtrabackup comment reading large file isnt instantaneous atomic backup progressing table concurrent updates could change rows backup already read rows backup hasnt reached yet take textbook example 5,different ways easiest way use lastval function return value generated last sequence nextval start transaction insert entity insert t2 eid values lastval commit know name sequence entity table could also use currval function start transaction insert entity insert t2 eid values currvalentity eid seq commit written general way using pg get serial sequence function avoiding hardcode sequence name start transaction insert entity insert t2 eid values currvalpg get serial sequenceentity eid commit details please see manual http www postgresql org docs current static functions sequence html 0,one time another ive worked databases mention unfortunately found doesnt take long syntax deviate flavours anything simplest select insert update delete gets categories suggest quickly get vendor specific ive always pretty much port sql one platform another although going back years amazed different sql server teradata sql even update joins 0
many times cte run cte code many times table people get queried impression called time stored memory queries running seem,sql cte used referenced one statement defined know statements end using statement terminators sql server forgiving allows developers put terminators except special cases complain really good practice microsoft recommends use every statement placed would obvious code parses statements 1st statement starts ctegeneric select person people person dumb select ctegeneric ends 2nd statement starts select ctegeneric ends 3rd statement starts select ctegeneric ends second third statements actually work return error invalid object name ctegeneric soon statement cte made ends lose ability reference kind like valid syntax either another way think ctes ctegeneric select person people person dumb begin select ctegeneric end 4,putting aside syntax correct edit get put memory proper example join get call multiple times loop join expensive cte called multiple times materialize temp way go 7,optimiser takes submitted sql translates set actions called query plan free arrange actions sequence logically equivalent sql may result object mentioned sql accessed many times applies objects cte although commonly observed objects mentioned cte accessed guarantee happen like sql statement data processed cte part scope duration statement data referenced subsequent statement accessed 4,use module signing sign procedures certificate grant required permissions certificate derived user link contains full example 0,convert implicit occurring collation column match parameters collation parameter converted columns collation explain collation coercion rules triggers conversion implicit collation column coercible default parameter parameter converted columns collation explicit different collations collation conflict error would result 0,great explanation richard wanted add links official documentation topics written sql server much concepts remain today understanding avoiding blocking understanding locking sql server edit additions five ways fight blocking video fresh video kendra little published today dba detective troubleshooting locking blocking rodney landrum identify blocking problems sql profiler brad mcgehee well known sql server authors mvps 0
easily show rows different two tables queries imagine two different tables queries supposed return identical data want verify whats easy,handled using except intersect http msdn microsoft com en us library ms188055 aspx first find records table1 table find records table table one select table1 except select table2 union select table2 except select table1 undoubtedly efficient way first quick dirty solution top head also recommend using wildcard suits brevity alternately could use intersect operator exclude results 22,easy accomplish third party tool like data compare client context unit testing stored procedures wrote code code using quoted old article close loopholes testing stored procedures internal static class datasetcomparer internal static bool comparedataset one dataset two ifone tables count two tables count return false forint one tables count ifcomparetablesone tables two tables return false return true private static bool comparetablesdatatable one datatable two ifone rows count two rows count return false forint one rows count ifcomparerowsone rows two rows return false return true private static bool comparerowsdatarow one datarow two ifone itemarray length two itemarray length return false forint 7,dont need join conditions full outer join full outer join pk preserve rows least one difference exists select except select use cross apply select union select unpivot sides joined rows individual rows tableacol1 col2 col3 select dog11 union select cat2786 union select cat12892 tablebcol1 col2 col3 select dog11 union select cat27105 union select lizard83null select ca tablea full outer join tableb col1 col1 col2 col2 unpivot joined rows cross apply select tablea union select tableb ca exclude identical rows exists select except select discard null extended row ca col1 null order ca col1 ca col2 gives col1 col2 col3 tablea 17,heres way show asked select tablea select dbo tablea except select dbo tableb union select tableb select dbo tableb except select dbo tablea order col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 col11 col12 col13 col14 col15 col16 col17 col18 col19 col20 col21 col22 col23 col24 col25 col26 col27 col28 col29 col30 4,found reference recovering disk space section table need reclaim excess disk space occupies need use vacuum full alternatively cluster one table rewriting variants alter table commands rewrite entire new copy table build new indexes 0,error occurred referencing column another table unique good answers already given lennart balazs papp would like explain need unique column parent table said want keep duplicate values column used foreign key possible creating table create reference existing table contains duplicate values create primary key non unique index novalidate option possible lead confusing results let explain situation created table one column id primary key constraint non unique index sql create table t1id number sql create index t1 index t1id sql insert t1 values1 sql insert t1 values1 sql commit sql select id t1 id sql alter table t1 add constraint t1 0,looks like limitation sql servers ability imply predicates change t0 cardcode p2 t2 fathercard p3 t2 cardcode p2 t2 fathercard p3 predicate pushed scan t2 performance much better guaranteed join condition t2 cardcode t0 cardcode two equal change semantics example data added question option hash join hint original version cpu time ms elapsed time ms machine second version cpu time ms elapsed time ms im sure details implied predicate fails appears play part however ed predicate t0 cardcode p2 t2 fathercard p3 able applied scan t2 without problems 0,second query qualify minimal logging minimal logging available second query engine chooses use runtime minimum threshold insert select chooses use bulk load optimizations cost involved setting bulk rowset operation bulk inserting rows would result efficient space utilization done improve situation use one many methods select threshold alternatively might able rewrite source query way boost estimated number rows pages threshold insert select see also geoffs self answer useful information possibly interesting trivia set statistics io reports logical reads target table bulk loading optimizations used 0
sql server r2 query last successful database backup dbs possible query last successful backup date perhaps type backup database sql,try select database name convert smalldatetime maxbackup finish date last backup datediffd maxbackup finish date getdate days since last msdb dbo backupset type group database name 6,yes long history data still msdb database following block code get last backups including full differential log backups select top database name physical device name castdatediffsecond backup start date backup finish date varchar100 seconds timetaken backup start date case type full differential transaction log end backuptype server name recovery model msdb dbo backupset inner join msdb dbo backupmediafamily media set id media set id database name db name remove line database order backup start date desc backup finish date go 7,know possible bad idea database hate cant speak database would hate legacy column contain redundant data change could result conflicting data old column new xref table maintained consistently one another consider developers unfamiliar technical debt could logically corrupt database im hard pressed think reason one remove legacy column relationship also ensure dependent code properly changed 0,char3 natural key short enough using adding surrogate key using joins adds overhead extra bytes per row tinyint phrasing using adds unnecessary opaqueness code complexity experience easier use even 40k writes second billions rows 0
check statistics last executed weve number issues indexes lately dba team attributed statistics run recently made wonder check statistics recently,edit need enclose table name single quotes exec sp autostats tablename original question first find statistics want check second see properties see last updated timestamp may want execute following query select name tablename name statname stats datet object ids stats id lastupdated sys stats join sys tables object id object id type 11,best way get information statistics command dbcc show statistics tablename indexname return information stats updated size density selective histogram shows distribution data determine stats date effective 8,use identity value generating sequence table sequence values take lot overhead cause lot locking blocking trying generate numbers identity exists reason use sql denali comes support sequences efficient identity cant create something efficient moving records one environment another either turn identity insert insert check box ssis 0,arguably even messier end weird decimals stop cares numbers computer deal doesnt matter many fractional digits ugly appear us using decimal values means move item items need select order values average update two select statements one update statement probably done using serializable isolation avoid deadlocks want see integers rather fractions output either calculate integers client application use row number rank functions rdbms includes 0
storage engines work oracle http en wikipedia org wiki database engine mentions database engines aka storage engines storage engines used,oracle like mysql like rdbmss comes built storage engine exchanged another 17,completeness whilst jacks answer technically true possible use data cartridges expand vanilla oracle offerings fact company called coppereye released new indexing method patented available utilizing functionality see old press release patent makes fascinating reading 6,outer join clause outer table turns inner table meaning rows predicate evaluated make effectively clause outer join makes inner join would try fe violationdate fe violationdate fj feeduedate fj feeduedate fe violationdate fe violationdate fj feeduedate fj feeduedate 0,coalesce internally translated case expression isnull internal engine function coalesce ansi standard function isnull sql performance differences arise choice influences execution plan difference raw function speed miniscule 0
help undoing accidental restore sql server yesterday made serious mistake restoring wrong database background may skip section right clicked test,first think best come clean manager issues business notice missing data appreciate blindsided facts hand event things even worse formal background dbaing business dba tasks real production dbas even exist ensured business protected human errors simply put way restore individual rows auto magically tool set available red gate data compare springs mind allow compare data sets build scripts differences upto decide changes good 5,come clean asap sooner better im reading timeline correctly restored full backup production database broke backup chain means wont able get data 30pm 00pm current database holds data 01am 30pm new data since restore would suggest restore database 30pm 00pm data figure sql code based way move data live database may problems identity columns new rows databases probably taken ids imo quicker get people involved build plan fix better good luck 5,see part pg dump manpage password force pg dump prompt password connecting database option never essential since pg dump automatically prompt password server demands password authentication dont use case confusing also need know fact server asks password driven existence password driven server side pg hba conf file need study possibly modify according needs dont forget reload server modifying edit reviewing pg hba conf relevant lines type database user address method local postgres peer local peer host md5 host md5 1st line concerns postgres user irrelevant pg dump command since youre using santa user santa 2nd line concerns connection unix domain 0,put file directory full permission everyone like tmp set read privileges file want import put everyone reading permission importing file could revoke permission 0
arguments favor using elt process etl realized company uses elt extract load transform process instead using etl extract transform load,lots discussions etl vs elt main difference etl vs elt processing happens etl processing data happens etl tool usually record time memory elt processing data happens database engine data end results data achieved methods much depends environment strong database engine good hardware heavy processing elt good busy datawarehouse engine need free processing go etl notice etl tool gives options like etlt transformation etl tool transformation database engine well elt option transformation database engine know databases better set based operations record time etl tools similar question asked supporting etl also nice article comparing etl vs elt favoring elt 13,almost matter semantics lot hot air gets released discussions im really convinced real philosophical depth distinction two level view etl transforming data client side tool finally loading elt implying data transferred sort staging area relatively little change format transformation takes place afterwards fluffy definitions could applied wide variety technical architectures many possible designs either term could used describe im strongly favour architecture transformation business logic built less homogeneous code base ive done lot systems transformation logic quite complex tended use etl tool land data transformation done stored procedures arguably could described etl elt difference merely one semantics tools database centric 10,quantity quality dealing tables size helps think fact table table think segment level collection discrete tables old enough remember rolling partitioning partition views helps tim gormans scaling infinity paper invaluable resource 0,real answer prefix indexes virtually useless referring key teststoretitle storetitle182 since index contains truncated values completely ordered list titles hence easily used order innodb limit bytes max utf8 varchar255 increased complex set steps get later set global innodb file format barracuda set global innodb file per table alter table tbl drop index teststoretitle add indexstoretitle row format dynamic compressed agree limit group join ypercube suggests solution mostly orthogonal one solution probably significantly faster since wont need scan anything 0
cpu utilization affect cost foreign numa access scenario lets assume sql server sockets numa node socket physical cores gb memory,please update question coreinfo sysinternal utility output get better context cpu sockets numa distribution looked overall cpu utilization around percent look socket specific cpu metrics metrics average seems barking wrong tree sql server numa aware much smaller performance penalty cross numa memory access also use query see many numa nodes cpu cores assigned numa select parent node id scheduler id cpu id sys dm os schedulers nolock status nvisible online many numa select countdistinct parent node id sys dm os schedulers status visible online parent node id queries logical reads taking minute normally happens bad query plans generated due outdated 7,hefty question ill outline factors involved given context factors others vary produce interesting result sorry wasnt able make much shorter accumuated cpu ms vs logical io sql server logical memory node alignment physical numa nodes spinlock contention query workspace memory allocation task assignment schedulers relevant data placement buffer pool physical memory placement accumuated cpu ms vs logical io use graphs logical io perfmon terminology buffer pool page lookups cpu utilization often order gauge cpu efficiency workloads look spinlock prone cases sql server accumulates cpu time lots activity page lookups spinlocks plans compiled compiled clr code executed functions performed lot activities 18,etc cnf set mysqld general log file path mysql dir mysql general log edit etc rsyslog conf rhel centos file enable module imfile read path mysql dir mysql general log send remote syslog server respecting interval configured pollinginterval parameter section look like moduleload imfile pollinginterval inputtype imfile file path mysql dir mysql general log statefile statefile mysql general tag mysql general severity warning facility local1 uncoment line workdirectory workdirectory var lib rsyslog place spool files configure send logs remotely remote ip tcp connection remote ip udp connection 0,would approach differently xact abort sledge hammer use refined approach see exception handling nested transactions create procedure usp procedure name begin set nocount declare trancount int set trancount trancount begin try trancount begin transaction else save transaction usp procedure name actual work lbexit trancount commit end try begin catch declare error int message varchar4000 xstate int select error error number message error message xstate xact state xstate rollback xstate trancount rollback xstate trancount rollback transaction usp procedure name raiserror usp procedure name error message end catch end go approach rollback possible work performed inside try block restore state state entering 0
using max text specific smaller type someone reviewing ddl code creating tables suggested saw saw using varchar256 fields text expect,going read like paranoids answer arent storage performance considerations database doesnt control clients clients cant assumed always securely insert user input even database designed used net application uses entity framework encapsulate transactions ensure parameterized queries systematically used know always going case wouldnt know exactly making text fields varcharmax client bobby tables issues stored procedures parameters also varcharmax youre making easier attacker come valid cleverly evil parameter value things clients arent supposed whatever limiting length actually need youre shielding clever attacks im even sure actually called remember reading back youre saying go ahead try give 2gb script run either 8,always use nvarcharmax text columns sql server max data types specified alternative one instead choose correct base type varchar nvarchar specify explicit maximum length appropriate data stored physical storage identical whether column typed varcharn varcharmax concern reasons choose nvarcharmax everywhere revolve around features plan quality performance exhaustive list probably practical among things max columns features require separate constraint enforce maximum length key index unique constraints either may prevent online ddl including index rebuilds adding new non null column generally supported newer features columnstore see product documentation specific features limitations general pattern awkward limitations restrictions around max data types limitations side 31,two answers spot reason two indexes exist told database create two indexes furthermore could make primary key clustered remove second index answer question second index needed boils limitation requirement sql server azure edition databases isnt present versions sql server sql server enterprise standard express support heap tables sql server azure edition doesnt support heap tables guess point wanted jobitems table heap table time came put database cloud forced clustered index table developer chose create duplicate index clustered instead changing primary key nonclustered clustered chose may never know however seems like plausible path situation especially legacy table lived outside azure point 0,one biggest benefit using materialized view oracle takes care keeping data sync separate aggregate table responsible keeping data synchronized generally requires reasonable amount code decent amount testing organizations manage make mistakes leave holes cause aggregate table get sync particularly true try implement incremental refreshes aggregate table another major benefit depending settings oracle use query rewrite use materialized views users issue queries base tables example bunch existing reports detail table produce daily monthly yearly aggregate results create materialized view base table aggregates data daily level optimizer utilize materialized view existing queries makes much easier optimize reporting workloads data warehouse without trying 0
rank dense rank deterministic non deterministic according official microsoft bol dense rank nondeterministic rank according ranking functions itzik ben gan,ntile interesting case seems apply sorting case tie left sql servers devices usually driven efficient choice index sorting purposes make deterministic forcing sql server make arbitrary choice add one tie breakers clause order salary employee essentially need make sorting unique employees name may choose different tie breaker column keep adding columns really ties rank dense rank ties actually crucial reason cant get different values try confuse determinism output function determinism order results queries dont order deterministic sue right robin page phil factor phil factor sue right robin page rank dense rank applied values cases sql server returned results different order 10,syntax windowfunction partition expressions partition list order expressions order list functions rank dense rank definitions guaranteed produce results long expressions clause deterministic itzik ben gun meant article lists often columns tables involved functions general deterministic implementation could taken care distinguish two cases consider deterministic upon examining partition order lists wild guess sql server developers decided easier implement always non deterministic despite contradicts way definition deterministic functions stated non deterministic msdn current implementation engine considers always non deterministic one argument two window functions row number ntile even complicated identical output expression partition order lists deterministic unique well implementing details far trivial 7,according official microsoft bol dense rank nondeterministic rank according ranking functions itzik ben gan rank dense rank functions always deterministic right right using different senses word deterministic sql server optimizers point view deterministic precise meaning meaning existed window ranking functions added product optimizer deterministic property defines whether function freely duplicated within internal tree structures optimization legal non deterministic function deterministic means exact instance function always returns output input matter many times called never true windowing functions definition single row scalar function return result within row across rows state simply using row number example row number function returns different values different 23,would expect work without table definitions cant sure pendingextressvceventsdata batch cte upd select eventid hotelid batchstatus batchid pendingextressvceventsdata batch numrows update upd set batchstatus batched batchid batchid output inserted mytablevariable simplified test dbfiddle uk issues query though whats reason behind using nolock hint suggest remove unless reason read consequences accept behaviour select top batchsize without specific order means arbitrary rows selected returned subquery combined numrows want apply next step means update may affect fewer rows specified batchsize possibly even one 0,believe covered documentation 0,reason youre seeing result sql server actually catching alter table error youll notice run see red error message rather printed line verify changing print error something like print hello case see hello printed see error instead books online list cases errors catch alternative set xact abort begin transactions rollback changes upon getting first error 0
copy postgresql data one pc another migrating server application existing system another system unfortunately existing system also database server data,perhaps easiest way full dump old server pipe result straight new server like pg dump old server ip username dbname psql localhost username dbname superuser default postgres user superuser mightve created others update case move data different server versions use pg dump latest version likely pg dump new server 12,could dump database using pg dump restore new server using psql heres couple commands link create backup pg dump mydb db sql copy db sql new server specific command depends os go new server createdb mydb utf8 dont specify utf8 encoding always psql mydb db sql answering johnp answered fine answer assumes pg hba conf edited allow remote connections postgres conf edited listen network 14,going allow external partner employee org access server like credentials belong db owner couple databases concern along service account multiple instances outside entity escalate permissions quite easily given outside entity dbo risk opened could hack elevate sysadmin dbo escalation next concern user may local admin access duration doesnt matter easily add without already login sql server sysadmin change sa password threat continues article add sysadmin another example add sysadmin based exploits would use different service account password instance make difficult outside entity move one server next 0,normalizing database schema limit redundancy tables divided smaller tables established relations one one one many many many process single fields original table appear multiple normalized tables instance database blog could look like unnormalized form assuming unique constraint author nickname author nickname author email post title post body dave dave com blah bla bla dave dave com stuff like sophie oph ie lorem ipsum normalizing would yield two tables author author nickname author email dave dave com sophie oph ie post author nickname post title post body dave blah bla bla dave stuff like sophie lorem ipsum author nickname would primary 0
whats collation columns sys databases im attempting run unpivot various columns contained sys databases across various versions sql server ranging,ok took look sp helptext sys databases broke columns coming ones latin1 general ci ks ws collation coming system table sys syspalvalues appears generic lookup table system table connect via dac order see guess set latin1 general ci ks ws handle possible lookup values see would annoying though another way see definition originally provided max comment select objectschema name objectname name objectdefinition sm definition master sys sql modules sm inner join master sys system objects sm object id object id inner join master sys schemas schema id schema id name sys name databases 4,official word microsoft columns contain pre defined strings like types system descriptions constants always fixed specific collation latin1 general ci ks ws irrespective instance database collation reason system metadata user metadata basically strings treated case insensitive like keywords always latin columns system tables contain user metadata like object names column names index names login names etc take instance database collation columns collated proper collation time installation sql server case instance collation time creation database case database collation asked emphasis mine collation columns statically set reason columns statically set queries dont need worry server database collation importantly case sensitivity work correctly 17,background collation precedence behavior seeing regards collation various fields system catalog views result field defined collation precedence looking sys databases important keep mind table past think ending sql server system catalog tables system catalog views hence source information necessarily coming current database context context specified database dealing fully qualified object master sys databases dealing specifically sys databases fields coming master database created collation based instances default collation server level collation fields expressions case statements coming hidden source mssqlsystemresource database mssqlsystemresource database collation latin1 general ci ks ws name field sourced name field master sys sysdbreg field always collation master database 7,workaround specific issue rather full answer question avoid error converting sql variant rather varchar50 declare dbname sysname set dbname db name select database unpvt databasename configuration item unpvt optionname configuration value unpvt optionvalue basetype sql variant propertyunpvt optionvalue basetype maxlength sql variant propertyunpvt optionvalue maxlength collation sql variant propertyunpvt optionvalue collation select databasename name recoverymodel convertsql variant recovery model desc compatibilitylevel convertsql variant case compatibility level sql server sql server sql server sql server sql server sql server else unknown end autoclose convertsql variant case auto close false else true end autocreatestatistics convertsql variant case auto create stats false else 5,im going go different direction answers ask defragmentation routine run however many minutes hours used sorts resources cpu memory disk probably tempdb quantify compare expenditure time server resources defragment indexes improvements youve gained realize doesnt answer question want think carefully youre rather get worked server beat non issue theres weird obsession believe used sql server users around index fragmentation usually based advice microsoft years ago lots hysterical responses forums one included people numbers reference reorg rebuild pounded head religious fervor forever keep ive seen cause sorts problems blocking deadlocks corruption interruptions production workload long running maintenance worst performing index maintenance 0,isnt possible proposed table structure declaratively would need triggers enforce unique index columns together pair check constraints scalar udfs gets quite close however create table userprofile id int primary key primaryemail varchar100 secondaryemail varchar100 create unique index ix1 userprofileprimaryemail create unique index ix2 userprofilesecondaryemail go create function dbo emailinuseasprimary email varchar100 returns bit begin return select count userprofile readcommittedlock primaryemail email end go create function dbo emailinuseassecondary email varchar100 returns bit begin return select count userprofile readcommittedlock secondaryemail email end go alter table userprofile add check dbo emailinuseasprimarysecondaryemail check dbo emailinuseassecondaryprimaryemail reason readcommittedlock avoid problems snapshot isolation one problem approach 0,added additional attributes filter conditions form cross join eliminated using min max nested queries biggest performance gain min max flank values returned inner nested query primary key values scans used retrieve additional flank attributes lat lon using seek final calculations access apply equivalent primary tables attributes retrieved filtered innermost query help performance need format strdateiso8601msec time value sorting using datetime value table equivilant sql server execution plans access cant show without final order expensive clustered index scan receiverdetails pk receiverdetails cost clustered index seek firsttable pk firsttable cost clustered index seek secondtable pk secondtable cost clustered index seek secondtable pk 0,documentation states opening paragraph driver encrypts data sensitive columns passing data database engine automatically rewrites queries semantics application preserved similarly driver transparently decrypts data stored encrypted database columns contained query results implied encrypted columns encrypted therefore database logs backups entirely encrypted setup always encrypted specify columns encrypted data columns encrypted data encrypted prior sent sql server data encrypted inside data file log file inside backups taken database data columns encrypted always encrypted visible plain text inside mdf ldf perhaps compressed inside backup files database transparent database encryption encrypt entire database mdf ldf backups taken database entirely encrypted sql server instance 0
trim whitespace spaces tabs newlines im sql server need clean whitespace start end columns content whitespace could simple spaces tabs,might want consider using tvf table valued function remove offending characters start end data create table hold test data coalesceobject iddbo trimtest begin drop table dbo trimtest end create table dbo trimtest sampledata varchar50 null insert dbo trimtest sampledata select char13 char10 char9 char13 char10 test char13 char10 go create tvf coalesceobject iddbo stripcrlftab begin drop function dbo stripcrlftab end go create function dbo stripcrlftab val nvarchar1000 returns results table trimmedval nvarchar1000 null begin declare trimmedval nvarchar1000 set trimmedval case right val char13 right val char10 right val char9 left case left val char13 left val char10 left val char9 4,anyone using sql server newer use trim built function example declare test nvarchar4000 set test nchar0x09 nchar0x09 nchar0x09 nchar0x09 content nchar0x09 nchar0x09 nchar0x09 nchar0x09 nchar0x09 select trimnchar0x09 nchar0x20 nchar0x0d nchar0x0a test please note default behavior trim remove spaces order also remove tabs newlines cr lfs need specify characters clause also used nchar0x09 tab characters test variable example code copied pasted retain correct characters otherwise tabs get converted spaces page rendered anyone using sql server older create function either sqlclr scalar udf sql inline tvf itvf sql inline tvf would follows create alter function dbo trimchars originalstring nvarchar4000 charstotrim nvarchar50 returns 7,understanding correct sql server records every operation changes data transaction log rollback change data also records transaction log well statement run write data transaction log also reserve data transaction log case statement needs rolled back true rollback transaction information written log plenty ways see action quick demo query ill use see written log select count transaction count sumdatabase transaction log bytes used used bytes sumdatabase transaction log bytes reserved reserved bytes sys dm tran database transactions database id table create table tlogdemo fluff varchar1000 begin transaction query uses minimal logging insert tlogdemo tablock select replicatea master spt values t1 cross 0,11g oracle documentation sys system users following administrative user accounts automatically created install oracle database created password supplied upon installation automatically granted dba role sys account perform administrative functions base underlying tables views database data dictionary stored sys schema base tables views critical operation oracle database maintain integrity data dictionary tables sys schema manipulated database never modified user database administrator must create tables sys schema sys user granted sysdba privilege enables user perform high level administrative tasks backup recovery system account perform administrative functions except following backup recovery database upgrade account used perform day day administrative tasks oracle strongly recommends 0
get username ip address responsible query use following query find performance improvements queries select top substringqt text qs statement start,use query pull currently executing requests corresponding session connection information select session id login name client net address host name program name st text sys dm exec requests inner join sys dm exec sessions session id session id left join sys dm exec connections session id session id outer apply sys dm exec sql textr sql handle st st text like query string search querying question sys dm exec query stats cumulative aggregated statistics arent session connection words information isnt stored could easily retrieved could put place sql trace xe session grab data youre looking basically monitoring occurrence offending queryies 9,would say best bet either extended events trace unfortunately dont know enough extended events give advice give setting trace first trace pull among things hostname server workstation query came include since sometimes username doesnt really help frequently track something machine running second filter textdata able pull queries start first however many characters query looking however may find bit slow im sure carriage returns line feeds formatting may throw trace pull queries duration higher reads higher writes higher etc using information query able keep number false positive responses fairly low using types restrictions make trace fairly low overhead edit example script 6,variation erwins answer count min max may slightly efficient big table case duplicate val select minval maxval foo 0,posting final sql process benefit community notes since approach executes loop inside explicit transaction locks obtained released iteration thus minimizing impact users accessing table time process would update rows per second thus estimated run approximately hours production million rows spec variant table however harness power mutli threading statically defining min max variable values running multiple sessions update reduce time required hours divided number sessions words run sessions update query parallelly complete hours multiple sessions possible temp table needs created global temp table populated session additionally sessions max min values need hard codedfor example etc however make possible use set transaction 0
consequences setting varchar8000 since varchar takes disk space proportional size field reason shouldnt always define varchar maximum varchar8000 sql server,assuming referring sql server think one limit size 8k row table sql lets define varchar fields could theoretically exceed limit user could get errors put much data field related starting sql 2k8 exceed limit performance implications also whole reasonableness check limiting size expect data look like want unbounded length field go text ntext 7,length constraint data like check fk null etc performance row exceeds bytes unique constraint index key column width must default set ansi padding potential lots trailing spaces stored sql server assume average length sort operations allocating memory based need find link back rust summary dont 18,history lesson mar uber switched mysql postgrssql surprisingly love affair last long jun uber switched postgresql mysql face actual question running update surprisingly micromanaged postgresql two ddl system identifiers postgresql must properly understood ctid physical location row version within table note although ctid used locate row version quickly rows ctid change updated moved vacuum full therefore ctid useless long term row identifier oid even better user defined serial number used identify logical rows oid object identifier object id row column present table created using oids default oids configuration variable set time column typeoid name column see postgresql documentation object identifier 0,might also factor dates critique null problems 3vl sql relational theory rubinsons critique dates critique nulls three valued logic ambiguity sql critiquing date critique referenced discussed length related thread options eliminating nullable columns db model 0
get unique records table structure create table dbo order details2 orderid int null productid int null unitprice money null default,join subquery select mytable inner join select orderid maxproductid productid mytable group orderid orderid orderid productid productid 6,well jnk said explained question different showed us example follow required output try query select select orderid maxproductid overpartition orderid productid uniteprice quantity discount row number overpartition orderid order productid corr temp1 data corr 6,cast isnt think select castcast100 binary int equals words casting decimal value binary taking base notation expectation actually declare integer set set cast binary set cast binary select cast integer incidentally new development highly recommend bit shifting sql server slow difficult maintain violates first normal form ill guess probably know already convert binary decimal create function ways going integer display base notation feels like would quite slow 0,least version client sigint ignore option totally ignore sigint handler appears added oct 7th around least tested keep ctrl cancelling mysql client however would nice ctrl would also cancel current command line buffer like bash shell postgresql cli ive forked mysql order see hard would implement thing ill post homebrew formula tarball link ready update ever one days created promised patch decided create video demonstrate worked however couldnt disable well turns mysql actually baked functionality core client last year yep download least mysql g901d27fs client functionality desired example command line add sigint ignore flag mysql host port user root sigint 0
tsql return wrong value power2 select power2 returns instead seems digits precision rounding 17th even making precision explicit select powercast2,online documentation power float expression arguments float expression expression type float type implicitly converted float implication whatever pass first parameter going implicitly cast float53 function executed however always case case would explain loss precision conversion float values use scientific notation decimal numeric restricted values precision digits value precision higher rounds zero hand literal type numeric declare foo sql variant select foo select sql variant property foo basetype go column name numeric dbfiddle multiply operator returns data type argument higher precedence appears sp1 precision retained select version go column name microsoft sql server sp1 kb3182545 x64 br oct br copyright microsoft 17,result exactly representable float real matter problem arises precise result converted back numeric type first power operand database compatibility level introduced sql server rounded float numeric implicit conversions maximum digits compatibility level much precision possible preserved conversion documented knowledge base article sql server improvements handling data types uncommon operations take advantage azure sql database need set compatibility level alter database current set compatibility level workload testing needed new arrangement panacea example select power10 ought throw error stored numeric maximum precision overflow error results compatibility result digits 14,starter thing current count store variable query like select count subject current isolation level concurrent pending transactions depending isolation level query see see rows inserted deleted pending uncommitted transactions way answer count rows visible current transaction note even touch even thorny subject concurrent transactions start end count mention rollbacks 0,relational algebra projection means collecting subset columns use operations projection list columns selected query optimiser step projection manifest buffer spool area description containing subset columns underlying table operator logical view based columns used subsequent operations view projection equates list columns selected query underlying view 0
transaction id wraparound read document transaction id wraparound something really dont understand document following url http www postgresql org docs,block pasted seems answer question depends logic used hide future transactions transaction id xid counter limited bits ever reaches next number instead replacing old max transaction starts fresh zero well zero postgresql hiding transactions even though transaction happened seconds ago postgresql thinks wont happen another transactions 5,database suffers transaction id wraparound would transactions past appear future dont quoted text explains postgres needs use modulo arithmatic means transactions wrap around long old transactions frozen early enough normal xids compared using modulo arithmetic means every normal xid two billion xids older two billion newer specific old row versions must reassigned xid frozenxid sometime reach two billion transactions old mark wrapping xid around would cause things break prevent postgres start emit warnings eventually shut refuse start new transactons necessary reason autovacuum fails clear old xids table system begin emit warning messages like databases oldest xids reach ten million transactions 9,suppose export data one server another best use want data use backup restore bcp bcp ssis want subset data tables use ssis bcp bcp move data depending amount size data bandwidth linked server kill performance executing source server executing target server one faster consume fewer resourcers total source target server executing source server insert destinationlinkedserver destinationdb dbo table select dbo udf getexportdata called pushing data executing query source server pushing data destination server expensive operation executing target server insert dbo table select openquery originlinkedserver select origindb dbo udf getexportdata called pulling data executing query destination server pulling data source server 0,must use dynamic sql statement drop procedure parameterized bit aside good practice always schema qualify object names didnt specify schema name question say variable contains complete dot separated name ive gone ahead extended asked include declare stored procedure schema sysname null declare stored procedure name sysname nsome stored procedure name declare sql nvarcharmax ndrop procedure isnullquotename stored procedure schema quotename stored procedure name exec sql doesnt checking see procedure actually exists attempts drop youll get error doesnt exist trivial use sys procedures check existence didnt bother didnt ask either preemptively answer comment answers quotename correctly escape object name ends surrounded 0
potential risks users connecting sql server excel sysadmin recently discovered large swath finance department using excel connect sql server instance,pretty much everything id start potential ability use xp cmdshell sp configure cant whatever account returned xp cmdshell whoami exe move onto ability drop database risks include finance users able things program finance machine gaining access sysadmin connection credentials potential risks include risk discovering one tptb set way 9,malicious user destroyed data could restore backup able calculate impact business scenario may worse system longer integrity user manipulates data non catastrophic way may discover damage backups longer available option consider impact business unable trust validity data housed server 5,allows basically anything want database could truncate alter drop tables delete insert alter specific records would highly recommend address soon possible 6,one thing financial people understand giving excel system user bypassed every internal control built database application competent auditor would eviscerate instance controls built ensure two different people must approve expense avoid potential fraud connecting excel spreadsheet way removed control data completely 6,look partitioning http technet microsoft com en us library dd578580 28sql aspx cool thing partitioning one table name opposed multiple table approach insert statements remain static works every application completely transparent queries dont worry happens end different indexes statistics tables either create partition function decides break table multiple tables behind scene function take one input parameter field case would date field function break table date week month year case youd want date hour period build sql server agent job uses sql swap last partition every day delete becomes metadata operation blazing fast swap partition drop old one 0,example multiple rows value1 equals second part order clause take place expected consider following snippet three rows exist value1 equal declare test table value1 int value2 int insert test values select test order value1 desc value2 desc output shown image info select order clause transact sql 0,possible int sql generate full create script table least build way could always write generator going information sys columns case dont need get full create script need prevent select copying identity property easiest way add calculation column instead select tmp database1 dbo table1 need write select id id column names tmp database1 dbo table1 generate statement use sys columns sql fiddle ms sql server schema setup create table dbo testtbl id int identity11 nvarcharmax column int name int two columns need name identity query select nameis identity sys columns object id object iddbo testtbl results name identity id column name 0,quick dirty method checking mysql performs successful authentication please run query select usercurrent user user reports attempted authenticate mysqld current user reports allowed authenticate mysqld sometimes user current user different thats mysql authentication follows specfic protocol according mysql certification study guide pages state following mysqls authentication algorithm two stages client access control first stage client attempts connect server either accepts rejects connection attempt succeed entry user table must match host client connects username password second stage occurs client already connected sucessfully server checks every query receives client see whether client sufficient privileges execute server matches client entries grant tables based 0
generate random bytea would like able generate random bytea fields arbitrary length 1gb populating test data best way,would like able generate random bytea fields arbitrary length function 1gb take long time scale linearly output length create function random byteap length integer returns bytea language plpgsql declare bytea begin length loop decodelpadto hexwidth bucketrandom hex end loop return end output test select random bytea2 random bytea xcf99 select random bytea10 random bytea x781b462c3158db229b3c select lengthrandom bytea100000 clock timestamp statement timestamp time taken length time taken dbfiddle 5,enhancing jack douglass answer avoid need pl pgsql looping bytea concatenation use create replace function random byteabytea length integer returns bytea body select decodestring agglpadto hexwidth bucketrandom hex generate series1 body language sql volatile set search path pg catalog simple sql function thats cheaper call pl pgsql difference performance due changed aggregation method immense larger bytea values though original function actually 3x faster sizes bytes one scales much better larger values use extension function ive implemented random bytea generator simple extension function scrapcode repository github see readme nukes performance sql version regress regress dev null regress timing regress select random 20,buffer pool used mysql caching innodb data indexes memory total memory used mysql db innodb tables small 128mb enough effectively innodb tables treated memory tables 128mb limit large buffer filled needed read 0,two links post point scripts generate commands necessary drop recreate current foreign keys havent tested glanced intent however cases scripts place commands variable print neither script shown automatically executes commands seems wise would want look scripts generated try understand maybe even compare constraints create table script generated ssms make sure seem complete first link dba se question would let delete rows table actually truncate well ignore one msdn link actually drop foreign keys youd need execute command populating strsql necessary commands drop exec sp executesql strsql actual problem need careful test thoroughly test database easily restore need must capture necessary 0
sql server uses 1gb ram normal exact uses kb occasionally shoots query takes long total system memory 3gb running winxp,sql server express edition limited 1gb ram buffer pool non express editions default limited unless configured set max either case usage typically decrease unless memory pressure forces service restarted multiple instances definitely impact performance especially memory allocations exceed physical memory contention slow things concurrent queries inactive databases impact disk space consume 8,likely nothing wrong database sql server reserves large amount memory purpose caching disk reads among things server scenarios absolutely common sql server take tens even hundreds gigabytes whats ordinary sql server taking little memory thats youre using sql server express edition 4,clear solves problem theres redundancy since physical hardware os kernel mysql binaries maybe different disks storage controller etc reason reporting db offload queries oltp db kit wheres extra power coming something else trying get setup one conceivable use would segregate users somehow perhaps would thought could done grant 0,like need wrap code create procedure syntax remove go statements begin transaction commit transaction go create procedure dbo assignusertoticket updateauthor varchar100 assigneduser varchar100 ticketid bigint begin begin transaction save transaction mysavepoint set updateauthor user1 set assigneduser user2 set ticketid begin try update dbo tblticket set ticketassignedusersamaccountname assigneduser ticketid ticketid insert dbo tblticketupdate ticketid updatedetail updatedatetime usersamaccountname activity values ticketid assigned ticket assigneduser getdate updateauthor assign commit transaction end try begin catch trancount begin rollback transaction mysavepoint rollback mysavepoint end end catch end go also note added try catch statement block allow performing rollback transaction statement case error occurs probably need 0
bulk insert empty page compressed table get full compression lot large tables around million wide rows need regularly loaded sql,according docs article compression new pages allocated heap part dml operations use page compression heap rebuilt rebuild heap removing reapplying compression creating removing clustered index would seem align youre seeing seems like youre actually getting compression table rebuild could try loading data uncompressed table see still average rows per page decreases remains youre getting compression rebuild necessary could also add clustered index table prevent table uncompressed low compressed bulk loading data 5,handyd entirely correct want highlight methods get compression inserting heap document heap configured page level compression pages receive page level compression following ways data bulk imported bulk optimizations enabled data inserted using insert tablock syntax table nonclustered index table rebuilt executing alter table rebuild statement page compression option according could leverage minimally logged bulk inserts use insert tablock get page compression without rebuilds going way get extra small compressed size directly load table without rebuild data loaded rules get page compression inserting heap add tablock bcp command get compression row compression works without prerequisites least amount compression used examples thanks 7,yes use cases timestamp without time zone common business apps type would used booking future appointments representing time day across various time zones noon 23rd tokyo paris two different moments hours apart time day tracking moments specific points timeline always use timestamp time zone without timestamp without time zone values point timeline actual moments represent rough idea potential moments possible points timeline along range hours range time zones around globe real meaning apply time zone offset utc ex christmas example say need record start holidays holy days table holiday column year type smallint column description type varchar column start type 0,workaround specific issue rather full answer question avoid error converting sql variant rather varchar50 declare dbname sysname set dbname db name select database unpvt databasename configuration item unpvt optionname configuration value unpvt optionvalue basetype sql variant propertyunpvt optionvalue basetype maxlength sql variant propertyunpvt optionvalue maxlength collation sql variant propertyunpvt optionvalue collation select databasename name recoverymodel convertsql variant recovery model desc compatibilitylevel convertsql variant case compatibility level sql server sql server sql server sql server sql server sql server else unknown end autoclose convertsql variant case auto close false else true end autocreatestatistics convertsql variant case auto create stats false else 0
getting select return constant value even zero rows match consider select statement select query id players username foobar returns column,select col1 col2 col3 query id players username foobar union select null null null exists select players username foobar alternative might faster second subselect required qid query id values select qid query id qid left join players useranme foobar write compact representation select qid query id values qid query id left join players useranme foobar think explicit cte readable although always eyes beholder 21,expecting one zero rows back would also work select maxcol1 col1 maxcol2 col2 query id players username foobar return one row values null except query id row found 6,convert implicit occurring collation column match parameters collation parameter converted columns collation explain collation coercion rules triggers conversion implicit collation column coercible default parameter parameter converted columns collation explicit different collations collation conflict error would result 0,select name ig income group el educational level country staffname left join country income group id left join incomegroup ig educational level ig id left join educationallevel el country el id 0
exact benefits using services enterprise vs standard currently using sql server sp standard edition want install services documentation said stand,check feature availability across editions sql server machine learning services page books online difference basic advanced integration scale advanced integration use available cores parallel processing data sets size computer accommodate basic integration limited cores data sets fitting memory books online goes details page deployment restrictions well like standalone server offers operationalization features included microsoft non sql branded server machine learning server installation operationalization includes web service deployment hosting capabilities database installation equivalent approach operationalizing solutions leveraging capabilities database engine convert code function run stored procedure 6,brent quoted one key differences scale calculation embarrassingly parallel row computed independently one another using sp execute external script parallel allow run calculations parallel easily note still possible bite chew run memory case add params rowsperread int rowsperread maximum amount rows would want read one go using able run calculation billion rows data less minutes obviously mileage may vary need control batching ensuring appropriate rows grouped together think achieve calling revoscaler functions within script directly toyed absolutely use standard edition get creative batching data tried found code would get quite complex also quick taking advantage enterprise edition streaming info https 4,index design put place something art science rdbms isnt smart enough take common workloads design smart indexing strategy human intervention read dba analyze workload determine best approach penalty indexes would shotgun approach add infinite number indexes data modification inserts updates deletes impact enabled indexes table going variable overhead indexes takes human design strategy smartly create indexes thatll maximize read performance least amount data modification overhead 0,microsofts training book says main reason place objects primary file group provide much isolation possible data system objects change frequently data objects minimizing write activity primary data file reduce possibility introducing corruption due hardware failures addition state primary filegroup also determines state database increase availability database minimizing changes made primary filegroup take others say necessary certain circumstances course maintain thought id provide microsofts reasoning 0
different ways replace isnull clause uses literal values isnt question catch queries accept user input use variables strictly queries isnull,answer section various ways rewrite using different sql constructs well look pros cons overall comparison first using select count dbo users age age null using gives us efficient seek plan reads exact number rows need however adds technical world calls whole mess malarkey query plan also note seek executed twice really obvious graphical operator table users scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server execution times cpu time ms elapsed time ms second using derived tables union query also rewritten like select sumrecords select countid dbo users 57,although dont local copy stack overflow database able try couple queries thought get count users system catalog view opposed directly getting count rows underlying table get count rows maybe match eriks criteria simple math used stack exchange data explorer along set statistics time set statistics io test queries point reference queries cpu io statistics query eriks query initial question select count dbo users isnullu age sql server execution times cpu time ms elapsed time ms rows returned table users scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server 7,wasnt game restore gb database one table created data age distributions match whats stack overflow obviously table wont match dont think much issue queries going hit indexes anyway im testing cpu computer sql server sp1 one thing note queries finish quickly important include actual execution plan slow things quite bit started going solutions eriks excellent answer one select sumrecords select countid dbo users age union select countid dbo users age null records got following results sys dm exec sessions trials query naturally went parallel cpu time total elapsed time logical reads query worked better erik actually performed worse machine select 17,equivalent records days ago current time day returned query comparing days using dateadd function take time part consideration function return comparing sunday monday regardless times demo declare mytable tablepk int loginserttime datetime insert mytable values dateaddhour castdateaddday cast getdate dateas datetime dateaddhour castdateaddday cast getdate date datetime declare datetime datetime getdate select mytable datediffday loginserttime datetime records select mytable loginserttime datetime record logical equivalent first query enable potential index usage either remove time part datetime set time select mytable loginserttime cast datetime date reason first query use index loginserttime column buried within function query compares column constant value enables optimizer 0,principle least privilege need know developers pass testespecially auditors sarbannes oxley come knocking next assumption developers stupid need say 3rd line support needs web monkeys typically dont database types yes expected support access needed permanently break glass access using sql login alternate windows account requires sign case data owner tech savvy business person hopefully manager approve seen developers test run queries production take ignorance saying developers take responsibility actions take server suffer accordingly got someone demoted one incident assume reasonably size shop course hats folk wear less separation duties also environment developers run queries recent data last shop prod restored 0,turned copy sql server internals dmv sys database recovery status pointed find first lsn next log backup going bol column last log backup lsn provides log sequence number recent log backup end lsn previous log backup starting lsn next log backup null log backup exists database offline database start mention well kalen also brings point get null value database simple recovery mode autotruncate mode log backup exist obtain firstlsn tail log without actually backing tail log database dont test instance try could logically conclude value returned column mentioned would first lsn next log backup case tail executing following return value 0
nonclustered indexes stored separate filegroups heard storing indexes different filegroup drive increases performance database drive doesnt go back forth index,certainly true spreading simultaneous different drives increase performance myth myth twice improve performance splitting array two partitions putting indexes one tables another waste time 5,slowest part db system disk drives eliminating bottlenecks disk level improve performance data looked index used index first looked corresponding data fetched index data disks contention happening whereas data different physical disk faster io happening thereby increasing performance main part note data index separate physical disks luns would use scenario need get better performance system provided disks perfmon counters could use physical disk avg disk sec read physical disk avg disk sec write physical disk disk reads sec physical disk disk writes sec comparison changes 10,separating indexes data onto separate filegroups performance improvement highly debatable performance improvement may happen underlying hardware support fact separating different filegroups doesnt give perf boost also easy measure perf boost ref http weblogs sqlteam com dang archive dba monkey aspx ask question first need looking improve performance backups including indexes looking improve performance reads writes indexes better manageability placement underlying objects large volumes data varying needs performance looking use ssds non clustered indexes improve performance etc looked task support need list seems like good proposal although havent acted upon yet note decision easy make need figure trying make sure hardware 5,wanted answer add another aspect well discussed field vision wide varieties roles developers example device driver development developing operating system schedulers require narrow field vision ability delve deeply small problem look purely technical viewpoint fields require broad fields vision much technical depth business application development erp framework choice databases unique well able move modes quickly seamlessly databases math engines math engines fit business environments complex ways therefore one able tackle math problem math problem also ask fits everything else look senior network engineers senior system administrators closest match senior dba area though field quite different good senior sysadmin requires even 0,replace inner joins left joins get data want left join shows results even match side cases shows null select d0 name name0 d1 name name1 d2 name name2 d3 name name3 table d0 left join table d1 d1 parent id d0 value id left join table d2 d2 parent id d1 value id left join table d3 d3 parent id d2 value id d0 dropdown id 0,chiming god goggles nothing horrible naming convention story data management team last environment stated reason using abbreviated table names db2 limitation db2 os sql server characters tables columns promptly pointed inaccurate documentation ibms site stated cobol issue yes actively developed cobol case needed talk database disproved mf jockeys finally response publish standard petitioned standards committee increase length character received character limitation resulted tables going useless names sr dly adv prd idx fdshrclas lif rtrn stats fml dozen years experience shortened table names provide tangible benefits result higher cost development maintenance must always refer data dictionaries translate garbage screen meaningful identifier 0
read html code xml get output like sample sql html code stored data base want read xml codes http rextester,exactly elegant seems job declare xml replacereplace h4 foo h4 ul ul foo select category value h4 span varchar10 selection valuedescendant self text varchar10 value replace replace replace ltrim rtrim replace replace castx queryfn datadescendant self text fn position varcharmax char10 char13 nodesdiv section foo ul li xx order category selection returns category selection value ab ad ac ag al bb bd bc bg bl im assuming want desired results table question return rest values concatenated string 7,trying establish communication nodesh4 ul use operator check node another node document order combine predicate position get first occurrence also document order select h4 valuespan text varchar10 section ul query ul nodes div section h4 h4x cross apply h4 nodeslet h4 save current h4 node return div section ul h4 ulx rextester called node order comparison operators xml fragment like n1 n1 n2 n2 n3 n3 n4 n4 n5 n5 get nodes first occurrence n3 query select query n3 result n1 n1 n2 n2 give root nodes enclosed predicate current node n3 first n3 node document order root level root 14,newer version pg check answer dustin kirkland external module postgres install postgresql contrib pg version package via apt apt get install postgresql contrib find sql install file somewhere usr share postgresql folder youll need run pgcryto sql database psql database usr share postgresql contrib pgcrypto sql cd usr share postgresql contrib psql database psql type help help database pgcrypto sql 0,shouldnt really need hacks handle leap years depends results expect typically subtract larger component first subtract month subtract day instead way around produce results dont expect ones wrong declare tabled date insert dd values2015033020150331 leap day test today last year yesterday last year month leap day test today last month yesterday last month march 1st possible leap year test yesterday test regular leap year test century divisible test century divisible leap year march 1st year possible leap year test yesterday last year test regular leap year test century divisible test century divisible leap year select today today last year dateaddyear 0
join two result sets query output came two statements two queries query select eid item item sums qty total sold,simplest solution would union error joining sales purchases sort multiplying apples oranges one purchase multiplied sales vice versa first line example rupa opposed select eid item sumtsl total sold sumtsk total stock sumtsk sumtsl balance stock select eid item item sums qty tsl tsk tbl sales join tbl matentry eid item group eid union select eid item item tsl sump qty tsk tbl purchases join tbl matentry eid item group eid group eid 5,problem likely coming tbl sales item appear table join tables together returning qty tbl purchases row tbl sales see behavior performing select tables select qty purqty qty salesqty tbl matentry left outer join tbl purchases item eid left outer join tbl sales item eid order eid see demo see sample rupa qty appears twice two entries tbl sales one way get result would calculate totalsold totalstock subqueries select eid item coalescep totalstock totalstock coalesces totalsold totalsold coalescep totalstock coalesces totalsold balancestock tbl matentry left join select item sumqty totalsold tbl sales group item eid item left join select item sumqty 14,look locks process table lock type select hostname os username login spid database tableid table name indid index name lock type lock mode status resource count lock count select convertvarchar30 rtrimp hostname hostname convertvarchar30 rtrimp nt username os username convertvarchar30 suser snamep sid login convertsmallint req spid spid convertvarchar30 db namersc dbid database rsc objid tableid convertvarchar30 object namersc objid rsc dbid table name rsc indid indid case substring lock type name none db database fil file idx index tab table pag page key key ext extent rid row id app application else substring lock type name end lock type 0,curiosity since question states sql also possible solve problem efficiently using sqlclr idea read rows one time turn order weight exceeds run rows return last name read source code using microsoft sqlserver server using system data using system data sqlclient using system data sqltypes public partial class userdefinedfunctions sqlfunctiondataaccess dataaccesskind read systemdataaccess systemdataaccesskind none isdeterministic true isprecise true return sqlfacetisfixedlength false isnullable true maxsize public static sqlstring elevator const string query select name weight dbo line order turn using var con new sqlconnection context connection true con open using var cmd new sqlcommandquery con var rdr cmd executereadercommandbehavior singleresult var 0
determinate trigger direct inserts inserts via stored procedure tablex modified two ways client direct inserts client uses stored procedure inserts,cant determine available inserted deleted tables could probably cheat gawdawful bloody hack define view extra column doesnt matter proc insert update assign one value column direct updates supply different non existent value create instead trigger view update logic based source flag create view dbo vwtablex select fake source column dbo tablex go create trigger inserthack dbo vwtablex instead insert begin set nocount exists select inserted fake source column begin perform logic proc sourced data insert dbo tablex select col1 col2 everything fake column inserted fake source column end exists select inserted fake source column fake source columns null begin perform 5,use set context info stored procedure read trigger context info trigger reset exit 4,operations log file havent committed first place operations committed uncommitted written log commit ensures log entries made durable flushed disk nothing prevents uncommitted entries flushed either log block fills another transaction commits thus forcing flush every transaction rollback must analyze transaction entries generate compensating actions every insert delete every delete insert every update update reverses data back compensating actions course logged database recovered must rollback transaction committed log must analyze log figure uncommitted transactions generate compensating actions actions belonging uncommitted transactions online recovery write compensation actions log standby recovery write compensating actions alternative stream thus allowing log applied form master 0,pivot data first apply filter order pivot mysql use aggregate function case expression select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id convert data rows columns see sql fiddle demo id views maxviews null null null null null null data column format apply filter select select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id src views maxviews see sql fiddle 0
sql server error invalid usage option first fetch statement onward sql server docs show support offset fetch im trying use,stated top documentation offset fetch offset fetch clause provides option fetch window page results result set offset fetch used order clause order mandatory use offset fetch clause select values tx order add happy offset rows fetch next rows practical simple limit thats youre going youll want stick top 10,according reference offset clause part order sql server youll also need add rows keyword offset specification select values tx order offset rows fetch first rows 9,sql server implemented offset fetch clauses part order clause pointed answers documented documentation sql standard side clauses independent query expression clause query expression body order clause result offset clause fetch first clause someone wants feature implemented full compliance standard always make request sql server team connect channel fact ms commented different request offset fetch connect item sql denali add total rows counter select statement alexey rokhin answer posted microsoft requirement offset fetch requires order restriction release ansi sql standard sql new offset fetch clauses proposed order optional restriction sql server limitation parser technology handle optional syntax without making offset reserved 17,individual queries faster joins try squeeze every info want client side one select statement use many seems convenient performance scenario test measure solutions see faster said almost always case joined result set properly tuned database faster scale better returning source rows client joining particular input sets large result set small think following query context strategies join together two tables gb result set rows thats extreme see point noticed get information multiple tables often faster get information via multiple queries individual tables maybe containing simple inner join patch data together client side try write complex joined query get data one query 0,char3 natural key short enough using adding surrogate key using joins adds overhead extra bytes per row tinyint phrasing using adds unnecessary opaqueness code complexity experience easier use even 40k writes second billions rows 0,one reported itzik ben gan told jaime lafargue declare int select case else min1 end result msg level state line divide zero error encountered trivial workarounds course point still case always guarantee left right evaluation short circuiting reported bug closed design paul white subsequently filed connect item closed fixed fixed per se updated books online accurate description scenario aggregates change evaluation order case expression recently blogged edit addendum agree edge cases time rely left right evaluation short circuiting bugs contradict documentation probably eventually fixed isnt definite see follow conversation bart duncans blog post see disagree folks say something always true 0
mysql console ctrl driving nuts something mysql console drives nuts hit ctrl cancel current command typed terminal exits every terminal,change behaviour download source mysql cli modify sigint handler behave see fit recompile install 6,ctrl behavior annoying still use cancel current query train using mysql cli use instead ctrl 19,least version client sigint ignore option totally ignore sigint handler appears added oct 7th around least tested keep ctrl cancelling mysql client however would nice ctrl would also cancel current command line buffer like bash shell postgresql cli ive forked mysql order see hard would implement thing ill post homebrew formula tarball link ready update ever one days created promised patch decided create video demonstrate worked however couldnt disable well turns mysql actually baked functionality core client last year yep download least mysql g901d27fs client functionality desired example command line add sigint ignore flag mysql host port user root sigint 21,good news mysqls functionality finally fixed changelog previously control mysql interrupted current statement one exited mysql control interrupts current statement one cancels partial input line otherwise exit 9,current tests dbcc shrinkfile log reuse wait desc simply proving right reusing transaction logs virtual log files appropriately auto growth events happen transaction log file reaction log able reused oftentimes isnt ongoing condition exactly seems right lack symptoms youre currently seeing handful things could cause behavior even simple recovery model best bet would setup data collection job routinely pull log reuse wait desc database routinely log somewhere able reverse engineer causing lack log reuse keeps saying log space reused slow growth course months doesnt seem indicated typically ongoing condition causes lack transaction log reuse save corner cases like poorly constructed 0,thats still gist ran similar problem time ago major cons time zone abbreviations presented already take dst daylight saving time account major pro simplicity resulting superior performance taking dst rules account makes time zone names slow comparison time zone abbreviations simple symbolic time offsets time zone names subject constantly changing set rules ran benchmarks related answer difference remarkable applied set typically necessary use time zone names cover possibly different dst status per row also historic differences talking cet really tricky part cet obviously time zone abbreviation also time zone name least according installation postgresql debian squeeze locale de utf others 0,triggers instance ag level youd need base something failover event better yet part sql server agent job check see replica primary something 0,issue caused sum function cast amount bigint select count records sumcastt amount bigint total dbo t1 id id reference https stackoverflow com questions prevent arithmetic overflow error using sum int column 0
cant object names start number example im creating view name 4aii sql server care starts could call table fouraii ivaii,firstly need distinguish numbers numeric literals strings string literals identifiers 4aii string literal value thing identify name thing 4aii 4aii would identifiers allowed query parser needs understand meaning token looking allowing names begin digits extension allow consist digits exclusively given select mytable would parser know integer literal name column however allow identifiers begin letters underscore characters unambiguously say youre looking identifier abc123 string literal abc123 latter enclosed quotation marks square brackets sql server backticks mysql double quotes ansi sql compliant engines signify identifiers use identifiers readily distinguished tokens start digit spaces special characters etc hence 4aii 4aii clearly tell parser 20,youre observing implementations lexer rules part process called lexical analysis fancy way saying making sense things ideally would adhere rules given sql spec identifier rules published microsoft rules regular identifiers wish use irregular identifiers quote delimit tokens tsqls double quotes eliminates possibilities ambiguous syntax strings string amirite take example take example thats one sentence importantly thats words know five words whitespace significant youll know five words youre going parse subjects objects voice make sense instruction 14,quick example 3e2 string 3e2 number variable name meant number forgot wrote 3e2 earlier script rule syntax parser understand mean may non ambiguous examples like 4aii mentioned questions subset labels ambigous avoid ambiguity rule 7,strings string amirite yes string string object item names strings statement true also relevant behavior seeing ignoring conceptual reasoning specific rules technical answer one works sql server follows minimal customization unicode standards guidelines identifiers unicode documentation found unicode standard annex unicode identifier pattern syntax identifiers enclosed either regular identifiers enclosed delimited identifiers regular identifiers names valid contexts rules naming things language software etc delimited identifiers everything else names valid work however given exemption wrap either delimiters identifiers delimited goto labels variables including table variables parameters delimited distinction seems identifiers exist purely use sql language name ever stored data file log 16,use convert correct style order convert varcharx local format proper dates datediffday convertdate t2 opening date convertdate t1 date sample supposed using german style dd mm yyyy german style select convertdate ouput must adapt settings local format replace whatever using main style style standard input ouput mm dd yy mm dd yyyy ansi yy mm dd ansi yyyy mm dd british french dd mm yy british french dd mm yyyy german dd mm yy german dd mm yyyy italian dd mm yy italian dd mm yyyy look cast convert transact sql dates bad formats mixed formats also look try convert 0,found script script repository help used many times life saver especially want transfer database roles object permissions one server another credit goes original writer bradley morris script reverse engineer sql server object user permissions written bradley morris query analyzer sure go query current connection options advanced tab set maximum characters per column high number code displayed declare databaseusername sysname set databaseusername user name goes set nocount declare errstatement varchar msgstatement varchar databaseuserid smallint serverusername sysname rolename varchar objectid int objectname varchar select databaseuserid sysusers uid serverusername master dbo syslogins loginname dbo sysusers inner join master dbo syslogins sysusers sid master 0,open transaction almost consequence simple begin transaction wait nothing wait bit longer commit worst hold bytes status values big deal programs actual work within transaction another matter point transaction sure several facts within database true simultaneously despite users writing database concurrently take cannonical example transferring money bank accounts system must ensure source account exists sufficient funds destination account exists debit credit happen neither happens must guarantee transactions happen perhaps even two accounts system ensures taking locks tables concerned locks taken much peoples work see controlled transaction isolation level lot work good chance transactions queued waiting objects hold locks reduce systems 0,mysql since possible upgrade use group concat along json array also escapes inner quotes select concat group concat json array bidid description foundationdescription engdrawingnumber detaildrawingnumber takeoffquantity job1111 civiltrackerdetails mysql json arrayagg select json arrayagg json array bidid description foundationdescription engdrawingnumber detaildrawingnumber takeoffquantity job1111 civiltrackerdetails 0
invest time change column type char36 uuid million rows database already didnt know postgresql uuid data type designed schema one,im postgres person stretch imagination based know sql server rows fit onto data page better performance going reading data disk typically expensive operation thus going 36ish1 byte wide field byte guid seems straight forward cost savings fewer reads incur faster return results course assumes guid uuid satisfies business needs table uuid satisfies would bigint thatd shave storage costs another bytes per row edit1 character data postgres additional storage cost short strings bytes byte overhead anything longer bytes second respondent came byte cost byte field also option string compression perhaps wont cost full cant tell final cost would fundamentals remain anything 6,would consider changing uuid type char36 takes bytes uuid takes youll save bytes per row equate mb day gb year plus indexes depending hardware isnt much could adds improvement opportunities like also see constraint schema ensures interaction id actually right format using right type give well like however using bigint would save even even better performance unlikely application large bigint id column wont work 13,reusing identity value general discouraged either value used entirely internally case actual value immaterial also used externally case reusing value likely going lead misidentification take obvious case invoice purchase order number might easily come identity column exposed externally would never want reuse precisely reason refer specific transactions would want get confused resolving issues big hassle companies merge acquired creating problems purpose wise 0,go far course problem may bit art isnt pure science main product analysis reporting system regard quite detail records initially designed lots joins common id child records found denormalized couple fields could cut lot joins could take away lot performance headaches knew created normalized design started using profiled actual performance hundreds millions rows across dozens tables end story profiled couldnt know sure going work us liked idea normalizing could update easily end actual performance deciding factor thats advice profile profile profile 0
add primary key column query made query list tables columns data type etc select name table name column ty name,ques first remember primary keys always clustered keys think really want however answer pretty much either way need look sys indexes see primary column tell index primary key need look sys index columns see actual columns index select object namec object id table name name column name ty name data type max length max length nullable nullability identity identity case primarycols column id null else end primary key sys columns left outer join select ic object id ic column id sys indexes ix join sys index columns ic ix object id ic object id ix index id ic index id 4,query adds primary key field column sys key constraints sys index columns catalog views identify pk columns join together use set derived table left join main query allow filtering columns part pk also want use user type id instead system type id avoid cartesian product time wont difference two fields user defined data types uddts use sysname datatype alias nvarchar128 system type id value repeated sys types catalog view following query lists fields tables adding computed field denote column part pk handles composite pks well select tbl name tablename col name columnname ty name datatype col max length maxlength col 10,sql server backup doesnt support backward compatibility steps get db older versions schema right click database tasks generate scripts next next click advanced button change option type data script schema ok next next data right click database tasks generate scripts next next click advanced button change option type data script data ok next next 0,sql server caches optimizes stored procedures ad hoc sql way example procedure create procedure dbo testsb id int select orders id id optimzed cached identically select orders id id however following ad hoc sql cached effectively hardcoded value select orders id although performance good reasons use stored procedures stored procedures provide clear separation dba application developers good extra layer defense valuable data constantly changing programs 0
creating unique constraint postgis raster type column using following command add constraints one raster image postgis postgresql alter table schema1,unique constraint creates unique index implement constraint index type postgresql supports unique indexes default tree index type cant make unique gin gist index regress create unique index indexname test using gistid error access method gist support unique indexes data types must specifically support different kinds indexing data type doesnt seem support tree indexes cant make unique constraint type details index types used postgresql types must provide operator class index type support purposes really two groups interest tree default get dont say index type supports equality less greater operations supports creation unique indexes gist gin popular flexible index types used heavily 5,craig explained well create unique index type raster without necessary operators second best bet enforce uniqueness create functional index text representation create unique index us tmin enforce scalex rast chp05 us tmin castrast text note implemented constraint code example suggested first draft unique constraints work columns expressions needs unique index details cast functional index create index integer json property postgres however may multiple possible text representations raster value identified operator responsibility rule side effects since cast value get canonical text representation additional variants due noise characters allowed input literals folded already may one columns big indexed remaining option create index 4,easy bigint nvarcharmax table student id int primary key identity name nvarchar800 table question id smallint primary key identity textofthequestion nvarchar800 correctans tinyint fk answernum num tested make composite fk answer questionid ansnum sure could would mean create question populate answer come back edit question correctans may way want go table answer questionid smalling pk fk question id answernum tinyint pk fk answernum num textoftheanswer nvarchar800 table studentchoice studentid int primary key foreign key student id questionid smallint primary key answernum tinyint questionid answernum references answer questionid answernum table answernum num tinyint pk values way change number questions future score 0,added additional attributes filter conditions form cross join eliminated using min max nested queries biggest performance gain min max flank values returned inner nested query primary key values scans used retrieve additional flank attributes lat lon using seek final calculations access apply equivalent primary tables attributes retrieved filtered innermost query help performance need format strdateiso8601msec time value sorting using datetime value table equivilant sql server execution plans access cant show without final order expensive clustered index scan receiverdetails pk receiverdetails cost clustered index seek firsttable pk firsttable cost clustered index seek secondtable pk secondtable cost clustered index seek secondtable pk 0
transact sql using create view want create view using clauses really cant find references correct syntax want smth like temptbl,create view someview ctestuff select etc select ctestuff 6,cte goes inside view take query cte cte select add create view go create view cte select go msdn describe multiple ctes see example create view cte1 cte2 cte3 select go 24,best way get information statistics command dbcc show statistics tablename indexname return information stats updated size density selective histogram shows distribution data determine stats date effective 0,might also find tee command use log output operating system file mytabs txt mysql tee mytabs txt logging file mytabs txt issue show tables command mysql show tables tables sport billy player seasons team rows set sec examine contents file within mysql client handy command mysql mytabs txt mysql show tables file contents tables sport billy player seasons team rows set sec turn output logging mysql notee outfile disabled also examine contents file via shell alternatively could mysql client open shell terminal switch two useful want permanent output refer later mysql exit bye pol localhost inst mytabs txt mysql show 0
connect sql server without specifying port trying use appears rather badly written application connects sql server database specify seem connect,named instances dont want declare ports need ensure sql server browser running default runs port udp 9,ste bov correct enable sql server browser service server running sql server however reason dont want use service connect instance using ip address port number like tcp xy ip address port alternatively specify connection tcp hostnameport hostname server name include instance name 5,use stellar like git databases stellar allows quickly restore database writing database migrations switching branches messing sql postgresql mysql partially supported 0,recently migrated 15tb across databases using mirroring simple worked perfectly couple seconds failover time edits two new virtualized sql servers databases coming servers plain outgrown impacting performance smaller databases hosted process simple wait weekend full backups complete restore recovery new servers restores complete pause backups run one additional restore latest log backup originals leaving recovery start mirroring across six resume backups chose leave asynchronous mode ready fail reduce load network etc mirroring reputation causing latency maintenance index statistics high volume activities didnt find true switched synchronous mode manual failover next maintenance window manually failed database smoke testing turned mirroring eventually 0
efficiently copy millions rows one table another postgresql two database tables one contains hundreds millions records lets call one history,dump table csv format copy table tmp table csv delimiter use copy command far efficient large amounts data copy table tmp table csv delimiter check postgres docs http www postgresql org docs current static sql copy html info 16,problem indexes history table 160m indexed rows running either copy insert select taking lot time insert rows update indexes disabled indexes imported 3m rows seconds need find faster way reindexing big table 14,use psql tool might efficient following psql daily host ip pg port db name user name copy daily stdout psql history host ip pg port db name user name copy history stdin also write shell script 11,plan keep history long periods many months suggest look partitioning options may one partition day week depend access patterns history table also run queries access data across dates lot aggregations etc look materialized views storing aggregates summaries http www postgresql org docs static ddl partitioning html http www postgresql org docs static sql creatematerializedview html 10,iis server intended externally accessible internal data network sql server instance server meant allow external connections except web interfaces applications correct adding explicit rule windows firewall server allow private ip address ranges connect via local network sql server ports acceptable assume trust internal ip addresses port dont suspect authentication attempts malicious intent allow localhost ip iis apps authenticate sql may need check site bindings etc setup internal dns pointers external side allowing port port forward nat iis server ports ports would think stop corporate firewall guess allow web app ports go server external interface firewall appliance proxies etc 0,happen set database production wondering thing cause needed compare configuration another mirroring session make sure matched correctly sys dm db mirroring connections provide want contains principal name name login validated connection permissions windows authentication value remote user name certificate authentication value certificate owner working session paused though sure would still return information believe active connections 0,collection non relational databases id say best reference stefan edlichs nosql databases org pretty comprehensive list books guides papers nosql databases staying informed learning nosql space theres mynosql blog nosql mypopescu com nb im creator writer years old nosql focused blog oodbms related topics theres roberto zicaris www odbms org 0,bookmarked phil factors blog post normalisation anima notitia copia today neatly summarises case normalising certain types data run following query sql instance see agree select sys syslanguages sql enables create relational databases however even smells bad crime hideously un relational things sql database long necessary tell difference also aware risks implications mentioned xml file contains additional information data benefit modelling metadata relational database purposes interrogation perhaps may case extracting relevant data persisting remaining xml xml document type passed json string xml required store database need ask role anima notitia copia soul database interest contents item information answer nequequam atomic value 0
handle adding nullable columns query currently whenever write query adding columns contain nulls resort wrapping field isnull coalesce coalescescore10 coalescescore20,thats pretty much since null anything else null wrap column isnull coalesce case 8,oracle provides nvl scenario isnull equivalent ms sql server could disguise view make code clearer 4,also setup proxy account using sp xp cmdshell proxy account allow non administrators use xp cmdshell allow setup less privileged windows account rather xp cmdshell always using sql server service account similar setting proxy accounts sql agent jobs 0,let schema design application requirements guide decision performance differences probably noticeable either way cases 0
insert update query previous select singularities im trying perform insert update selecting data another db far insert pdone reps veeva,insert query insert pdone reps veeva rep iddisplay nameusernamefirstlastemailavatar urlrep type select id concatucasemidfirstname11lcasemidfirstname2 ucasemidlastname11lcasemidlastname2 username firstname lastname email www static url com veeva user id 00580000003ub5vaaw print first letter capital first last name 4,ive made function put string output capital letters delimiter create definer root function lcapitalecadena varchar150 returns varchar150 charset latin1 deterministic begin declare vposicion int default declare vtmp varchar150 default declare vresultado varchar150 default declare vcadena varchar150 default ecadena null set vresultado return vresultado else set vcadena lcaseecadena repeat set vposicion locate vcadena vposicion set vposicion char lengthvcadena end set vtmp leftvcadenavposicion char lengthvtmp set vresultado concatvresultado vtmp else set vresultado concatvresultado upperleftvtmp1substringvtmp2 end set vcadena rightvcadenachar lengthvcadena vposicion char lengthvcadena end repeat return concatupperleftvresultado1midvresultado2150 end end delimiter example insert pdone reps veeva rep iddisplay nameusernamefirstlastemail select id concatlcapitalfirstname lcapitallastname display 4,found need specify exact name postgresql service find list services using systemctl also see post systemctl list units grep postgresql postgresql service loaded active running postgresql database server use service service postgresql service reload service postgresql service restart alternatively use systemctl command bin systemctl reload postgresql service bin systemctl restart postgresql service 0,best bet use explicit containing transaction acquire custom exclusive lock protect whole operation select create table using sp getapplock system objects honor isolation level requests use locks way user tables design race condition original code multiple threads conclude table exist thread gets far create table statement 0
show table name number records table mysql innodb database list tables current database together number rows table words think query,aggressive approach using brute force dynamic sql set group concat max len select concatselect group concatconcatselect quotetb tables database count1 number rows db tb separator union sql select table schema dbtable name tb information schema tables table schema database prepare sql execute deallocate prepare example test database get mysql use test database changed mysql set group concat max len query ok rows affected sec mysql select concatselect group concatconcatselect quotetb tables database count1 number rows db tb separator union sql select table schema dbtable name tb information schema tables table schema database query ok row affected sec mysql prepare sql 6,try query dynamic query select table name tablesindatabase table rows numberofrows information schema tables table schema database 6,two options really create delimited file data like following row1col1datarow1col2datarow1col3datarow1col4data row2col1datarow2col2datarow2col3datarow2col4data row3col1datarow3col2datarow3col3datarow3col4data load vertica using copy command vsql database superuser rights load file directly using vsql username password yourdatabasename copy yourtablename col1name col2name col3name col4name yourdelimitedfile delimiter omit want enter keep password bash history enter program starts dont database superuser rights still cat file vsql get data stdin quite vertica prevents loading data file without superuser rights let user pipe via cat idea follows cat yourdelimitedfile vsql username password yourdatabasename copy yourtablename col1name col2name col3name col4name stdin delimiter bunch individual insert statements file separated semicolons run file vsql follows vsql 0,find simple syntax readable less confusing use functions throw weird errors check whether variables one one null case null null null yes else end alternative conditions although less readable opinion would case exists select except select null null null null null yes else end case exists select intersect select null null null null null yes else end last two options look similar another standard sql yet implemented sql server option case distinct null null null null null yes else end 0
effective way compress store sql server backup ive testing different methods compressing storing sql server backups using sql server r2,ive testing different methods compressing storing ms sql backups using ms sql r2 enterprise edition im wondering effective compression algorithm long term storage backups outside sqls internal compression algorithms since using sql r2 enterprise edition must leverage data compression row page compression data level backup compression taking backup minimize disk footprint backups use master go exec sp configure backup compression default reconfigure override backup compression uses cpu cycles compress data leaves server vast majority scenarios compressed backups faster uncompressed backups note use open source tools need uncompress database backup file start restore process self receive sql database backup gb compressed 13,terms backup compression couple years ago make comparison backup compression options provided red gates sql backup questss litespeed sql server ideras sqlsafe benchmarking three products differences typical backup maximum compression spread three time taken somewhat wider spread backup size red gate coming top compression vs idera quest order 8,declare table sysname npersonal information schema sysname ndbo create table sz dbname nvarchar255 fullname nvarchar768 rows sysname reserved sysname data sysname index size sysname unused sysname declare sql nvarcharmax ndeclare nvarchar512 select sql nif exists select quotenamename sys tables inner join quotenamename sys schemas schema id schema id name table name schema begin set quotenamename quotename schema quotename table insert szfullname rows reserved data index size unused exec quotenamename sys sp spaceused update sz set dbname name dbname null end sys databases database id state read exec sys sp executesql sql table sysname schema sysname table schema select database dbname 0,quite common query pardon pun people running queries perform full table scans fts poster feels system make use indexes basically boils explanation given tables small optimiser say worth bother going index lookup fetching data instead ill slurp data pick need perform fts edit answer txsings comment mvcc multi version concurrency control database traverse every record count given moment thats example count much expensive mysqls innodb rather myisam excellent explantion postgresql available guy wrote post major contributor postgresql thanks dezso leading post 0
positive sum items negative return one needing find way sum positive values num return sum positive numbers individual row negative,try select salesid sumnum num num group salesid union select salesid num num want sum values one row must create maxvalue minvalue function use summaxvalue0 num summinvalue0 num described max function sql server takes two values like math max net 26,works select salesid sumnum group salesid case num else id end assumptions id starts hence use salesid else salesid id would work well considered positive number hence zero positive negative although seems make sign unnecessary helps remember case forgotten handled sum individual row sum positive numbers means sum strictly positive numbers needed must tested real data indexes table scan performances may little better cases absence index seems smaller impact query test data set count create table id int identity01 salesid intnum decimal164 insert besalesid num select castrand int rand rand go 24,another option one ive learned comes sqlcmd documentation need set codepage sqlcmd match file encoding case utf codepage youd want sqlcmd mssqlserver08 dp0 aqualogydb sql dp0 databasecreationlog log 0,long variables involved datatypes compatible sql variant case basically lob datatypes clr types user defined datatypes use select case coalesce cast startdate sql variant enddate statecode countycode producername taxid farm null yes else end necessary cast one arguments coalesce returns data type expression highest data type precedence sql variant high datatype precedence beaten user defined data types would prevent method working anyway personally dont find understandable finding conjunction individual null results though 0
sql whats efficient way loop table condition met got programming task area sql task people want get inside elevator every,try avoid loops generally normally less efficient set based solutions well less readable pretty efficient even name weight columns could include index avoid key lookups scan unique index order turn calculate running total weight column use lead ordering criteria see running total next row soon finds first row exceeds null indicating next row stop scan t1 select sumweight order turn rows unbounded preceding cume weight dbo line t2 select leadcume weight order turn next cume weight t1 select top name t2 next cume weight next cume weight null order turn execution plan practice seems read rows ahead strictly necessary looks 15,curiosity since question states sql also possible solve problem efficiently using sqlclr idea read rows one time turn order weight exceeds run rows return last name read source code using microsoft sqlserver server using system data using system data sqlclient using system data sqltypes public partial class userdefinedfunctions sqlfunctiondataaccess dataaccesskind read systemdataaccess systemdataaccesskind none isdeterministic true isprecise true return sqlfacetisfixedlength false isnullable true maxsize public static sqlstring elevator const string query select name weight dbo line order turn using var con new sqlconnection context connection true con open using var cmd new sqlcommandquery con var rdr cmd executereadercommandbehavior singleresult var 6,complementing juliens answer proactive schedule tsql job alert daily basis especially daysuntilexpiration loginproperty get logins database server going expire days select name loginname create date logincreatedate modify date loginmodifieddate policy checked expiration checked returns number days password expires loginpropertyname daysuntilexpiration daysuntilexpiration loginpropertyname passwordlastsettime passwordlastsettime loginpropertyname isexpired isexpired loginpropertyname ismustchange ismustchange sys sql logins optional filter loginpropertyname daysuntilexpiration 0,per fine documentation think might youre looking sql identifiers key words must begin letter also letters diacritical marks non latin letters underscore subsequent characters identifier key word letters underscores digits dollar signs note dollar signs allowed identifiers according letter sql standard use might render applications less portable 0
add server make sql restores faster 8tb sql database mostly data files 400gb log files currently takes around hours restore,dont backup restore use sql server snapshots takes lot disk space store sparse file size files youve snapshotted rolling back hundreds times faster available sql server enterprise sql server developer editions 7,primary bottleneck restore going disk io fix basically need either faster disks different configuration dont know enough raid sans suggest anything though might even consider ssds blindingly fast wouldnt want use something doesnt get created regular basis tempdb always good candidate since restore frequently might ok hand probably want make sure test server close possible production server performance testing couple things help first compress backups arent already course assumes sql higher reduce disk space store backup io read cpu cost involved aware also dont delete database restore way files already place overhead creating turn instant file initialization server level permission 6,made good observation activity table grow fast large done past archive older data say older days activityhistory table keeps activity table manageable size need research always look back activityhistory table 0,unfortunately easy parse record type using pl pgsql structure tables passed arguments always table type could use type directly instead record use following declare reccurs table type begin open reccurs execute execute insert events table values using reccurs work record type solution think creating query hand pl pgsql gives way dynamically get keys record type use external tools best opinion kind job hstore extension installed create database following works earlier hand create extension hstore able convert record type hstore type using hstorereccurs dynamically iterate keys values function declare reccurs record kv record cols text vals text begin open reccurs execute 0
clause pushed views query postgres im following query quite often select distinct onrecipient messages left join identities messages recipient identities,query select distinct onrecipient messages left join identities messages recipient identities name messages timestamp timea timeb order recipient timestamp desc says messages timea timeb find recipients every recipient find one message latest timea timeb query would get use view select select distinct onrecipient messages left join identities messages recipient identities name order recipient timestamp desc myview timestamp timea timeb says messages find recipients every recipient find one message latest times show messages timea timeb result first query show messages time second query show may one messages recipient later time queries logically different condition shouldnt pushed view want parameters passed view 12,create function like create replace function public get messages timestamp time timestamp time timestamp returns table recipient varchar timestamp timestamp begin return query select distinct recipient recipient timestamp messages left join identities recipient name timestamp time time order recipient timestamp desc end language plpgsql use function like table select get messages timestamp2015 6,mind general checkdb would really apply master msdb shouldnt volatile data model tempdb kin points good article corruption tempdb much less issue move modern version sql server wouldnt bother routinely tempdb deal actually get reports corruption imho youve added user objects model created every new database might increase risk may want include typically quite small doesnt really hurt include anyway defrag msdb really database worry especially backup history job history tables grow unchecked master would candidate put user tables would rather fix removing altogether try optimize shouldnt frequently changing data model tempdb permanent objects either defragging either wasteful spurious 0,main issue looks like youre regular log backups first thing would make sure understand sql servers recovery models primarily databases full bulklogged need regular log backups taken would recommend clean log file first take log backup backup log foo disk backup file location complete safely execute dbcc shrinkfile command resize log file appropriately finally schedule regular log backups database many tools either maintenance plans ola hallengrens maintenance scripts also would highly recommend reading understanding thomas stringers link bol 0
inspect global temp table outside transaction created help debugging batch sql run inside transaction inside transaction dump data global temporary,really created uncommitted global temporary table protected schema modification lock incompatible everything else including minimal schema stability lock required read uncommitted isolation hard guess need usual issues revolve around lifetime global temporary table long standing solution create global temporary table procedure marked run instance startup global temporary table created way never automatically dropped azure sql database currently public preview presumably box product future global temporary table autodrop argument alter database scoped configuration 8,file doesnt mean generate bind token first session somehow publish join transaction another session eg spid trancount rollback go begin transaction select sys objects declare bind token varchar255 exec sp getbindtoken bind token output declare bind token bin varbinary128 cast bind token varbinary128 set context info bind token bin rollback another session declare bind token varchar255 select castcontext info varchar255 sys dm exec sessions session id exec sp bindsession bind token go select 17,oracle table column comments used documentation comments easily added following commands comment table table documentation table comment column table columns documentation column 0,mysql explain uses values provide literally traverse rows associated tables provide constant key value associated table mysql explain stop error simply query associated tables values exist provide explain query everything work expected 0
create hierarchy multiple levels node random number children need create test data involves hierarchy could make easy couple cross joins,note preferred solution 4th final code block xml seems obvious choice data structure use select values1234567891011 tn select top5 abschecksumnewid n1 value select top1 abschecksumnewid n2 value select top1 abschecksumnewid n3 value n3 n2 xml pathlevel3 type n2 n1 xml pathlevel2 type n1 xml pathlevel1 rootroot trick make sql server use different values top node make sub queries correlated n1 n2 flatteing xml declare xml select values1234567891011 tn select select top5 abschecksumnewid n1 value select top1 abschecksumnewid n2 value select top1 abschecksumnewid n3 value n3 n2 xml pathlevel3 type n2 n1 xml pathlevel2 type n1 xml pathlevel1 select l1 value 9,came goal creating directory structure looking usable names directories files unable get topn working cross applys think attempted correlate queries using value parent topn wasnt random decided create type numbers table would allow inner join condition produce set elements simply randomizing number specifying table level random number trick row level1 rows level2 rows level3 hence using levelid get rows row value use directory name setup part originally specified inline part cte sake readability dont need scroll lots insert statements get lines real query broke local temporary table object idntempdb elements null begin print creating elements table create table elements elementlevel 4,interesting aim generate given number levels random number child rows per level properly linked hierarchical structure structure ready easy add extra info like file folder names wanted generate classic table storing tree id int null parentid int null lvl int null since dealing recursion recursive cte seems natural choice need table numbers numbers table start least numbers table maxlvlmax create table dbo numbers number int null constraint pk numbers primary key clustered number asc insert numbersnumber select top1000 row number overorder object id number sys objects order number parameters data generation stored table declare intervals table lvl int lvlmin int 6,use recent sql handle sys dm exec connections see last statement executed select text quotenameobject schema namet objectid dbid quotenameobject namet objectid dbid proc name connect time last request start time last request end time status sys dm exec connections join sys dm exec sessions session id session id cross apply sys dm exec sql textc recent sql handle session id blocking spid also check open transactions spid select st transaction id name transaction begin time transaction state transaction status sys dm tran session transactions st join sys dm tran active transactions st transaction id transaction id st session id 0,monitor index usage pg stat user indexes pg statio user indexes details statistics collector found manual http www postgresql org docs current static monitoring stats html careful dropping unused unique indexes though might used reading probably vital integrity data 0,finally got chance tablevaluedparameters work great im going paste whole lotta code shows im using sample current code note use ado net also note im writing code service ive got lots predefined code bits class im writing console app debug ripped console app excuse coding style like hardcoded connection strings sort build one throw away wanted show use list customobject push database easily table use stored procedure tsql code using system using system collections generic using system data using system data sqlclient using namespace eventami class db private static sqlcommand sqlcommandfactorystring sprocname sqlconnection con return new sqlcommand commandtype commandtype storedprocedure 0
support parallel scalar udf reasonable feature request fairly well documented scalar udfs force overall serial plan running functions parallel given,fairly well documented udfs force overall serial plan im certain well documented scalar sql function prevents parallelism anywhere plan scalar clr function executed parallel long access database multi statement table valued sql function forces serial zone plan may use parallelism elsewhere inline table valued sql function expanded like view direct effect see forcing parallel execution plan craig freedmans parallel execution presentation claims udfs black box must use cursor claims correct extra points explaining engine forces whole plan serial instead udf calculation stage understanding current restrictions purely result certain implementation details fundamental reason functions could executed using parallelism specifically sql scalar 17,paul rightly mentioned answer fundamental reason scalar udfs could executed using parallelism however apart implementation challenges another reason forcing serial froid paper cited paul gives information quoting paper section currently sql server use intra query parallelism queries invoke udfs methods designed mitigate limitation introduce additional challenges picking right degree parallelism invocation udf instance consider udf invokes sql queries one figure query may use parallelism therefore optimizer way knowing share threads across unless looks udf decides degree parallelism query within could potentially change one invocation another nested recursive udfs issue becomes even difficult manage approach froid described paper result parallel plans 11,issue solution write simple bash script limited single region however heres script question bin bash nowdate date backupname nowdate sql gz echo creating backup database finances backupname mysqldump user user password password database name gzip backupname echo succesfully created database backup echo uploading backup amazon s3 bucket s3cmd put backupname s3 path file backupname echo successfully uploaded backup s3 echo deleting backup file rm backupname echo done 0,interesting sensors produce kind data make sense put table amount data see youd worried performance days usual amount time produce graph could two tables main sensor data table stores data little want slack days ago today everything older goes archive table could help reduce size table reports begin generated hopefully majority gb data archive table main table archiving job scheduled run nightly maybe also consider building separate reporting database stores data structure better generating reports tables designed closely match querying maybe pre calculate aggregate values would otherwise take long time generate possible populate main database regular nightly basis course need 0
enable tls sql server database mail puzzled issue almost week hopefully someone community experienced issue already found solution problem per,since seems nobody answer question opened support case microsoft still took almost week ms support come back answer went various internal resources get definite answer summary sql server database mail uses system net mail work system net mail able send mail using tls build runtime version sql server db mail built net hence sql server db mail support tls 4,tls1 version tls considered secure march took considerable time effort discover essential additional settings required get working well known well documented microsoft web generally following could save great deal time effort new registry settings fixed problem us windows registry editor version hkey local machine software microsoft netframework v4 schusestrongcrypto dword hkey local machine software wow6432node microsoft netframework v4 schusestrongcrypto dword reference thread eventually found information buried halfway thread https stackoverflow com questions tls net framework content simple executable registry file put together make new settings settings already shown thread makes necessary registry settings windows registry editor version hkey local machine 6,alter table exampletable add new column varchar20 default value1 constraint ckexampletable check new column value1 value2 value3 0,begin transaction begin try alter table1 alter table2 additional data structural changes commit end try begin catch rollback throw want reraise exception determine reason exception end catch 0
calculating percentage row total sum apologies bad title wasnt sure would good title currently simplified view data im working agent,return agents commissions commission percentages use analytic function analytic clause partition whole table select agent commission commission sumcommission commission commissions learned ren nyffenegger ratio report function tightens syntax using package store commission sum would involve pl sql specifically excluded indicating want sql solution since already using functions assume intention exclude pl sql case package solution may help depends application works session first created calls function package get commission implicit call packages constructor could get sum store could reference stored sum get commission function would sum course soon call function different session sum would calculated also calling function every agent would 9,youre looking analytical function ratio report select agent roundratio reportcommission comm commissions 23,could try following query sumcommission calculated total commission select sumcommission total agents select agent name commission commission total commission agents total commission 5,need use function oracles sql select listaggsignbitand43 power2level within grouporder level desc bin dual connect power2 level result bin found deadly snippet fiddle number substitute column choice probably possible using recursive ctes thats bit pay grade reverse process use snippet input select reverse1000 dual select sumto numbersubstrxlevel1 power2level output input connect level lengthx result output dbfiddle recursive cte might trick substitute column kicks found another function work tweaks older versions databases dont recursive ctes excellent orafaq site select decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue bin number select value dual result bin fiddle observant among notice code remove leading 0,null mean value known row moment added known future example finishdate running project value applied row example escapevelocity black hole star opinion usually better normalize tables eliminating nulls case want allow nulls column yet want one null allowed kind relationship two tables perhaps simply change column null store instead null special value like known never appear solve uniqueness constraint problem may possibly unwanted side effects example using mean known apply skew sum average calculations column calculations take account special value ignore 0,using object identifier type regclass simplified considerably list child tables parent schema foo select inhrelid regclass child optionally cast text pg inherits inhparent schema foo regclass table name provide cast regclass optionally schema qualified search path decides visibility similarly returned table names schema qualified escaped automatically necessary safe fast convenient btw display source table per row row retrieved table select tableoid regclass source schema foo condition 0
warning sign select operator mean comparing execution plan stored procedures second one get warning sign marked red arrow mean,query planner warnings actual execution plan perhaps estimated one would expect see warnings section listing planner engine concerned common warnings operation spills data disk wanting index statistics unavailable common warning apparently missing indexes ssms gets information shows green text statements search showplan warning query plan warnings number articles detail places pick plan warnings via extended events see articles pop searching query warning extended events allows monitor problems running applications output included use set showplan xml set showplan give information ssms tools sql sentrys plan explorer analyse show warnings 5,right click warning properties see warnings 6,situation prefer limit drops using view layer create copy materialized view suffixed new also use data performance make sure indexes also created suffix dependant objects discovered via drop cascade create view upon new materialized view provide layer abstraction need change one place alter existing dependencies instead refer new view refreshing data needed beforehand drop original materialized view indices dependants alter materialized view indices drop suffix restore original names eg create table test myfield int insert test values create materialized view mv test select myfield test create view test select myfield mv test select test create materialized view mv test new 0,try alter sequence foo fee restart alter sequence foo fee restart http msdn microsoft com en us library ff878572 aspx 0
sql find records prefixes string table column named prefix storing aaaa application long string eg aaaabbbbccccdddd want select rows particular,first way using like wrong string like pattern string first pattern second furthermore prefixes starting wildcard needed something like work select table aaaabbbbccccdddd ilike prefix small proof table prefix values abcde abcd abce bcde select table abcdefghijkl ilike prefix prefix abcde abcd 7,another option would create prefixes selected word test equality ilike strings table select prefix select aaaabbbbccccdddd text word generate series0 lengthw word lateral select prefix table prefix leftw wordg prefix ilike leftw wordg might efficient big tables many length word equality checks using index seeks performing full index table scan need case insensitive check ilike used commented code improvement case abcdefghijkl ilike prefix answer prefix column used left ilike right side query performing column ilike aaabbbccdd checks arbitrary number aaabbbccdd ilike column checks ways number different ilike conditions one case number length searching word number rows table 6,personally find alias expression easier read comprehend reason troubleshooting select statement lengthy expressions probably want find expression via column name way around quick find expression application sees alias2 select alias1 long expression aggregates multiple column references long expression aggregates multiple column references alias2 preference may different true advantage using one except subjective taste reasons important thing pick one way consistently unless flip coin able defend choice come someone likes way write code dba fussy prepared rewritten ive blogged one thing feel even stronger use single quotes around alias names column alias alias column one form deprecated difficult read many newbies 0,using aarons link even finer adaption process would recommend function view query report size per row per table including indexes create function dbo getcolumnsize typename sysname max length int precision int returns int begin return select case typename tinyint smallint int bigint numeric precision decimal precision real float case precision else end money smallmoney time timestamp date smalldatetime datetime datetime2 datetimeoffset char max length varchar max length nchar max length nvarchar max length binary max length varbinary max length bit end end select schemaname objectname sumceilingbytes ceilingcountdistinct columnname rowsize floorpower230 power230 sumceilingbytes select name schemaname name objectname name columnname name 0
dba would go transitioning oracle sql server im oracle dba also sybase experience major architectural conceptual differences two rdbms platforms,ive swapped working oracle sql server past years wrote blurb going way number idiomatic architectural differences various bits terminology get used differently vendor developer dba communities surrounding product physical architecture sql server organises various things bit differently oracle one two key concepts direct analogues oracle database separate item sql server user permissions schemas name spaces storage youre familiar sybase work much databases sybase due common origins product filegroups roughly equivalent table spaces although local database schema distinct concept database user sql server although users default schema mvcc works somewhat differently sql server relatively recent feature maintaining different copies row locks 49,main product works sql server oracle differences work around might good keep mind date time handling different different precisions different set functions work empty strings nulls oracle sql server handling character encoding unicode different sql server normal varchar unicode nvarchar columns mixed database oracle decide database level kind encoding use 8,named pipes protocol useful application designed around netbios lan based protocols named pipes provides easy access remote procedure calls rpc within single security domain thus advantageous applications usually tcp protocol good practice dont care network 0,column order matter column orders match example insert items ver select items item id dont match could example insert items veritem id item group name select items item id relying column order bug waiting happen change number columns also makes sql harder read good shortcut explicitly list columns table inserting query using source data eg insert items ver item id name item group select item id name item group items item id dbfiddle 0
order rows randomly keep groups together table looks like groupid int null somevalue int null id like keep rows groupid,need random order groupid values first get distinct values random number derived table select tablename join select groupid newid rnd another random function tablename group groupid groupid groupid order rnd somevalue already groups table holds groupid values replace tablename group groupid groups 8,could also use thanks ypercube fix select groupid somevalue tablename order first valuecrypt gen random4 partition groupid order groupid groupid case groups get random number somevalue online demo 4,using ssms chain restore backups one operation would multiple restores youll want use sql order efficient restore recent full backup restore database mydb disk path full backup norecovery stats restoring full change recovery restore recent diff backup restore database mydb disk path diff backup recovery stats info restore http msdn microsoft com en us library ms186858 aspx 0,generally yes services r2 continue running normally unless manually make changes impact sql plan run performance intensive operations nothing else apart good article msdn site side support different versions sql server http msdn microsoft com en us library ms143694 aspx 0
find iteration day week month date dimension table need add new column define iteration day week within month second mon,actually surprisingly easy using day doesnt matter name day always hold true select case dayyourdate dayyourdate dayyourdate dayyourdate else end occurance yourtable case short circuits 5,alternatively could calculate result using formula select dayyourdate iteration dbo yourtable cases operands division operation integers result also integer rounded one dayyourdate give yield still get result 4,noted system web unsupported library order reference system web need make call create assembly seems like tried reference location system web dll copy paste different location sql server try locate dependent assemblies location words reference location system web dll dependent libraries living directory work fine working example able add system web assembly well assembly create assembly system web windows microsoft net framework64 v4 system web dll permission set unsafe go create assembly systemwebtest sqlserver systemwebtest dll permission set safe go see client messages assemblies sql server loads take note sql server displays following warning registering fully tested sql server hosted 0,pretty broad ill give general answer ctes unindexable use existing indexes referenced objects constraints essentially disposable views persist next query run recursive dedicated stats rely stats underlying objects temp tables real materialized tables exist tempdb indexed constraints persist life current connection referenced queries subprocedures dedicated stats generated engine far use different use cases large result set need refer put temp table needs recursive disposable simplify something logically cte preferred also cte never used performance almost never speed things using cte disposable view neat things speeding query isnt really one 0
risks enable read committed snapshot sql server read extra data stored per row might see performance degradation risks eg affect,summary locking problems problem code isnt database engine isnt magic bullet may add problems load also increase load tempdb cpu also see performance impact potential cost read committed snapshot linchi shea safety important snapshot isolations safe many cases default read snapshot isolation wikipedia write skew anomalies next section making snapshot isolation serializable get around general therefore snapshot isolation puts problem maintaining non trivial constraints onto user may appreciate either potential pitfalls possible solutions upside transfer better performance also see potential dangers read committed snapshot isolation level jimmcleod disputed comments alex kuznetsov deadlocked read committed snapshot explained nick berardi serializable vs 48,believe give us something closer oracle one transaction updating transactions still read old data correct yes correct well worth reading links gbns answer believe applies oracles default mvcc sql server snapshot isolation mode would add understand potential pitfalls imo benefits far outweigh added difficulties speaking oracle perspective course locking problems legitimately go away point mvcc also class locking problems go away due code issues assuming understand 19,couple additional points add answers set allow snapshot isolation enables snapshot isolation database take advantage recode set transaction isolation level snapshot transactions want apply calling code need changed handle update conflict errors set read committed snapshot statements read committed use row versioning note statement level row versioning reads updates real row retrieved update locks applied see summary behaviour section understanding row versioning based isolation levels either route without exhaustive testing youre likely introduce completely new set problems system 26,know old thread would say large degree snapshot isolation magic bullet eliminate blocking readers writers however prevent writers blocking writers way around experience additional load tempdb negligible benefits row versioning reducing blocked readers huge reference row versioning snapshot isolation method oracle used decades achieve isolation without blocking readers oracle dbs ive worked nearly years experience far less blocking issues sql server sql developers hesitant use snapshot isolation though theyve tested code databases use default setting 35,using snapshot isolation projects use sql server db sql errors caused wrong application code default page locking row locking behaviour performance impact minimal far years passed hundreds millions operations processed different systems problems regarding snapshot isolation situations several different threads updating business critical information single row parallel extremely exceptional chances snapshot isolation cause inconsistency problem much near zero oltp system design updates single row based current row data many threads course snapshots acceptable cases 9,rolandomysqldba course another valiant answer point question reading problem copy myisam table files frm myd myi gets write transaction yes cant get consistent backup even single myisam table unless type locking prevent writes rolando gave pretty thorough answer options available locking one option people use backing myisam data lvm snapshots see http www lenzg net mylvmbackup great tool assist final recommendation stop using myisam use innodb instead fast non locking physical backups percona xtrabackup comment reading large file isnt instantaneous atomic backup progressing table concurrent updates could change rows backup already read rows backup hasnt reached yet take textbook example 0,background collation precedence behavior seeing regards collation various fields system catalog views result field defined collation precedence looking sys databases important keep mind table past think ending sql server system catalog tables system catalog views hence source information necessarily coming current database context context specified database dealing fully qualified object master sys databases dealing specifically sys databases fields coming master database created collation based instances default collation server level collation fields expressions case statements coming hidden source mssqlsystemresource database mssqlsystemresource database collation latin1 general ci ks ws name field sourced name field master sys sysdbreg field always collation master database 0,postgresql takes time roll back commit two operations effectively identical terms disk dont actually think question optimized commit much question queries one optimizing basic question address disk layout affects commit vs rollback major dbs roll back slowly committing tend move data particularly clustered tables main data structures put rollback segment updating data means commit drop rollback segment roll back copy data back postgresql tables heap tables indexes separate means rolling back committing data arranged makes commit rollback fast however makes things little slower primary key lookup example traverse index file hit heap table assuming covering indexes applicable huge deal add 0,asked stack overflow drop null constraints postgresql table one go appears give good range solutions accepted answer denis de bernardy group alter statement alter table tbl alter col1 drop null alter col2 drop null also retrieve list relevant columns catalog feel like writing block generate needed sql instance something like select attname pg catalog pg attribute attrelid tbl regclass attnum attisdropped attnotnull note include primary key related fields youll want filter dont forget use quote ident event ever need deal potentially weird characters column names 0,one index art id skl id doc id tip likely enough conditions start art id skl id give seek residual lookup either doc id tip done afterwards index could also skl id art id doc id tip based number unique values column uniqueness first multiple indexes mean cached query plan may suboptimal different conditions course one art id skl id clustered index already may need index seek clustered index residual doc id clustered index need extra index 0
way test whether delete fail due constraints id like able predict whether delete run constraint violation without actually performing delete,one option begin transaction run delete always rollback begin tran delete table1 col1 test whether select table1 col1 rollback tran confirm still select table1 col1 7,goal process deletes succeed use try catch begin transaction begin try delete delete delete commit transaction end try begin catch rollback transaction end catch goal allow successful deletes succeed even one fail use individual try catch begin try delete end try begin catch print end catch begin try delete end try begin catch print end catch 23,actual query wasnt executed explicit transaction though explain existence target table yes exactly simple select outside explicit transaction two transactions autocommit mode first creates table second fills prove way dedicated database test server simple recovery model first make checkpoint ensure log contains rows case related checkpoint run select one row check log looking begin tran associated select checkpoint select sys fn dblognull null select col dbo t3 select sys fn dblognull null operation lop begin xact transaction name select youll get rows showing transactions assumptions sketched correct likely scenario happened yes correct insert part select rolled back release data space 0,clarification rpo recovery point objective point time restoring case data loss rto recovery time objective duration take restore time defined rpo might able answer question supply ample information give ideas ponder recovery point objective rpo rpo defined business really feasible business survive rpo one day hours recovery time objective rto bring databases back online point time defined rpo please dont forget rpo duration database crashed 2pm afternoon able restore data least 2pm afternoon day 6pm night took backup transaction log backup yes seeing know rpo rto relevant backup concept answer question databases transaction log differential full backup database free transaction 0
way generate table create script tsql way generate create script existing table purely sql without using smo since sql access,possible int sql generate full create script table least build way could always write generator going information sys columns case dont need get full create script need prevent select copying identity property easiest way add calculation column instead select tmp database1 dbo table1 need write select id id column names tmp database1 dbo table1 generate statement use sys columns sql fiddle ms sql server schema setup create table dbo testtbl id int identity11 nvarcharmax column int name int two columns need name identity query select nameis identity sys columns object id object iddbo testtbl results name identity id column name 5,back asked easy way generate create table script via sql rather using ui smo summarily rejected however sql server makes easy lets pretend table schema across multiple databases dbo whatcha create table dbo whatcha id int identity11 varcharmax decimal102 sysname following script uses new sys dm exec describe first results set dynamic management function retrieve proper data types columns ignoring identity property builds tmp table need inserts databases list selects tmp within single dynamic sql batch without using loop doesnt make better simpler look allows ignore database ref entirely set nocount declare sql nvarcharmax cols nvarcharmax select cols name system 28,far know listener work listener ora file starting listener ora optional default listener starting listener without listener ora doc id purpose publicise fact oracle8i onward listener ora longer required order start default oracle net listener 0,protecting agains accidental media loss lost laptop database drive showing flea market copy database backup etc etc scheme processing database engine application requires access data without user providing password offer access agains media loss encryption key hierarchy rooted system dpapi encrypted key words password service account scenario never protects agains hacker gets access sql server meant protect alternative ask user password time uses application use password open key top encryption key hierarchy usually certificate database scheme seldom deployed cases multi tenant scenarios tenants trust hosting operations administrators 0
optimal drive configuration sql server 2008r2 fairly busy database server running sql server r2 following setup sata raid drives os,depends workload drives limit options workload heavily dependent tempdb things sorts hash tables snapshot isolation might better using sas drives together raid however know metrics prove tempdb heavily utilized keep separate 7,variants question come semi regularly sql server build help choose raid level combination sql server instance configure disks sql server bi configuration also occasional bun fights data log separation best practice placing transaction log separate volume solid state sql server separating data log tempdb files san without detailed analysis server advice applies given previously raid os raid disk data logs tempdb rarely point split spindles available single array larger iops capacity typically soak lumps bumps workload better smaller arrays one variant worth testing putting tempdb os drive representative workload replay repeatedly ensure fair comparison configuration go arrangement production make sure 14,optimiser doesnt go first index create nonclustered index commonqueryindex dbo heartbeats dateentered asc deviceid asc pad index statistics norecompute sort tempdb ignore dup key drop existing online allow row locks allow page locks primary matter selectivity dateentered column told us table million rows row size bytes id bytes device id bytes date byte bit columns thats bytes bytes overhead tags null bitmap var col offsetcol count totals bytes per row would rougly translate 140k pages store million rows optimiser two things could scan table clustered index scan could use index every row index would need bookmark lookup clustered index certain 0,regardless length define varchar column storage space used empty column char varchar types addresses space used varchar column consider total storage space used row indexes primary keys columns ypercube mentions comment additional considerations row storage whole least one nullable column present innodb physical row structure variable length part record header contains bit vector indicating null columns anywhere columns null bit vector uses two bytes variable length part header also contains lengths variable length columns length takes one two bytes depending maximum length column columns index null fixed length record header variable length part yes storage space used changes based type 0
monitor transaction log usage sql server would like monitor transaction log usage regarding following aspects task job query making fill,easiest way buy shelf monitoring tool give kind information idera sql dm quest spotlight sentryone sql sentry kinds things really low impact next easiest way build something youre going route id start logging sp whoisactive table especially get transaction info switch try roll approach need aware queries arent thing cause transaction log grow example youre using replication database mirroring always availability groups sql server needs retain history one replicas offline learn whats causing check log reuse wait desc select name log reuse wait desc sys databases 8,windows performance monitor display graph showing percentage transaction log currently use time sql server database counter called percent log used sql server databases category useful monitoring quickly log filling also detecting auto grow events gives intuitive visual indication transaction log activity 4,would also strongly recommend looking olas index maintenance scripts reason recreate wheel flexible script many dbas community use recommend 0,sql server create collisions system generated constraint names depends type constraint version sql server create table t1 int primary key check int default references t1 int unique check select name object id castobject id binary4 object id hex castcase object id object id else object id end binary4 object id offset hex sys objects parent object id object idt1 order name drop table t1 example results name object id object id hex object id offset hex ck t1 1d498357 0x1d498357 0x1c555f1e ck t1 1a6d16ac 0x1a6d16ac 0x1978f273 df t1 1b613ae5 0x1b613ae5 0x1a6d16ac fk t1 1c555f1e 0x1c555f1e 0x1b613ae5 pk t1 3bd019ae15a8618f 0x169c85c8 0
set names attributes creating json row json possible rename default f1 f2 f3 names using row json function columns row,something like select bla name1 otherbla name2 select row jsonr course achieved select row jsonr select bla name1 otherbla name2 found former readable part construct rows structure fly 9,common table expression allows specify aliases explicitly cte columns datacol1col2colacolb values 12fredbob select row jsondata data different dezsos example doesnt use col alias col select list aliases column names cte table alias ive used values expression subquery use select whatever like point whatever column aliases provided assumed subquery overridden cte definition specifying column name list thing subquery instead using alias select row jsondata values 12fredbob datacol1col2colacolb doesnt work row expression directly cast row concrete type alias regress select row12fredbob xabcd error syntax error near line select row12fredbob xabcd 17,select id select row json select first name last name first last age customers want without performance impact verbose id first last age fisrt name john last name smit 23,use json build object select json build objectid data customer id first name data first name last name data last name json data 9,wait stats numbers server anything youll likely kind waits appear also definition must one wait highest percent doesnt mean anything without kind normalization server days im reading output task manager correctly means wait seconds cxpacket per second overall addition since youre sql server cxpacket waits include benign parallel waits actionable waits see making parallelism waits actionable details would jump conclusion maxdop set incorrectly based presented would first measure throughput actually problem cant tell depends workload oltp system might measure transactions per second etl might measure rows loaded per second problem system performance needs improved would check cpu times experience problem 0,im unsatisfied answer couldnt manage get flow distinct operator along results guaranteed correct however alternative get good performance along correct results unfortunately requires nonclustered index created table approached problem trying think combination columns could order get correct results applying distinct minimum value updateid per objectid along objectid one combination however directly asking minimum updateid seems result reading rows table instead indirectly ask minimum value updateid another join table idea scan updates table order throw rows updateid isnt minimum value rows objectid keep first rows based description data distribution shouldnt need throw many rows data prep put million rows table rows 0,put clear passwords text files recommended since mysql use mysql config editor save passwords encrypted also provide different passwords different connections https dev mysql com doc refman en mysql config editor html 0,query syntactically correct sql even table name column reason scope resolution query parsed first checked whether table name column since doesnt table checked would throw error neither tables name column finally query executed select table name select name table results query would give every row table subquery select name table select name table table single column name value many rows table table rows query runs select table name name name name select table name name select table name null table empty query return rows thnx ughai pointing possibility fact dont get error probably best reason column references prefixed table name 0
polling way updating apps data database application needs data freshly updated database possible case way getting data besides timer based,service broker sql server sorry im sure rdbms 5,oracle use built dbms alert package facilitate dbms alert supports asynchronous notification database events alerts appropriate use package database triggers application notify whenever values interest database changed suppose graphics tool displaying graph data database table graphics tool reading graphing data wait database alert waitone covering data read tool automatically wakes data changed user required trigger placed database table performs signal signal whenever trigger fired 10,another oracle solution weve developed applications using dotnet framework microsoft take advantage database change notification feature oracle conjunction odp net oracle data provider dotnet using database actually notifies dotnet application new data arrived allowing us avoid constant polling link reference oracle tutorial hope helps dont know rdbmss 5,certain database vendors also provide integrated message buses app simply subscribe oracle advanced queueing ibm db2 mqseries called websphere mq sybase rtms alternative would route data database first place via message bus like tibco rv simply branch stream going db one going application use caching layer like coherence app db 7,listen notify postgresql http www postgresql org docs current static sql notify html database notify static channel name static message function trigger perform pg notifydynamic channel name dynamic message database client listen channel name note lack quotes listen client receive postgresql process id channel name message value standard jdbc driver postgresql doesnt like notifications however use https github com impossibl pgjdbc ng driver purpose 7,recovery conf file add line tells postgres failover master slave add trigger file file trigger create file given path nodes change file dont include anything trigger find additional information streaming replication hand may possible make automatically created tricks using monitoring tools making fail manual better 0,dual table exactly one row following sql statement show select dual dual2 table rows insert one see behavior expression oracle evaluate without actually using data table evaluate every row like would normal column expression row result returned two rows get twice 0,think index table contents ordered list pointers positions file aka offsets say millions records stored table rather search table matching criteria much faster reference ordered list matches stack pointers specific matching rows perfect example index tables primary key field typically id field want row id much faster ask index pointer data scan data source position heres obvious use indexing create table activity log id int unsigned null auto increment activity type id smallint unsigned null datetime created datetime keyactivity type id primary keyid create table activity log date key activity log id int unsigned null date created key int unsigned 0,comments agree extended ascii really bad term actually means code page maps characters code points range beyond standard code point range defined ascii sql server supports many code pages via collations non ascii characters stored varchar long underlying collation supports character character stored varchar char columns sql server collation code page greater query bellow list select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name subset also support character column collation need one following support select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name 0,may referring federated database system 0
lock oracle based windows username logon trigger allow certain users log oracle database even correct password enter database create replace,suggest multi phase approach implemented stages minimize impact changing secure approach assume development environment test support manager interested support effort use existing oracle audit logging start logging users logon logoff period time consistent usage days fiscal quarter year end identify unused accounts lock identify service accounts used people log identify remaining accounts try link usernames people job roles create oracle profiles service accounts read accounts privileged user accounts set password expiration complexity reuse failed attempts lockout profiles example may decide service accounts never change password characters one failed attempt lockout whereas persons password characters three failed attempts lockout one one 4,reason trigger work users like sys system administer database trigger privilege administer database trigger privilege allows create database level triggers server error login logout triggers also allows log regardless errors thrown login trigger failsafe answer prevent login users least login trigger 5,actually exists maria db alter table test add column exists column varchar255 bonus works modify well alter table test modify exists column varchar255 0,http www sqlservercentral com scripts tempdb task space usage sum alloc delloc pages select session id request id suminternal objects alloc page count alloc pages suminternal objects dealloc page count dealloc pages sys dm db task space usage nolock session id spid group session id request id select tsu session id tsu alloc pages internal object mb space tsu dealloc pages internal object dealloc mb space est text extract statement sql text isnull nullif substring est text erq statement start offset case erq statement end offset erq statement start offset else erq statement end offset erq statement start offset end 0
count set difference union fairly simple question cant seem find answer im working unions differences would like perform count results,go select count select tbl1 id tbl1 except select tbl2 id tbl2 union select tbl3 id tbl3 5,another way result select count select id tbl1 except select id tbl2 except select id tbl3 4,maybe use lpad rpad create temporary table tt category int unsigned insert ttcategory values select lpad category tt granted wouldnt work youd replace single null value idea least 0,according docs article compression new pages allocated heap part dml operations use page compression heap rebuilt rebuild heap removing reapplying compression creating removing clustered index would seem align youre seeing seems like youre actually getting compression table rebuild could try loading data uncompressed table see still average rows per page decreases remains youre getting compression rebuild necessary could also add clustered index table prevent table uncompressed low compressed bulk loading data 0
possible restore sql server bak shrink log time bak file client transferred developer offices problem investigation backup currently 25gb restored,believe reason backup gb restored database gb transaction log guess database files gb allocated space theres gb actual data database theres difference allocated database file space utilized data space case former gb latter gb 9,way shrink backup part restore process restored database must look exactly like source database exception change drive letters folders around 16,ever live environment hack use space limited restoring log file compressed folder attempt compressing existing folder restoring result error cheat symbolic link create compressed folder logcompressed create symbolic link compressed folder mklink log logcompressed restore database ldf file pointing log shrink log file appropriate size detach database move log file uncompressed folder attach dirty cheating ever live works quick test newly created database 32mb log file shows occupying 330kb disk compressed decompress folder disk size back 32mb 34,alter table blog change read read varchar255 null mentioned query correct need use column keyword quotes around table column name using mysql database alter table blog change read read varchar255 null 0,ive got post something similar basically im using recursive cte go loop replacing one bad character time im using stuff strip character although use replace space patindex find location character want remove could modify slightly looking however creates good list doesnt actually update existing list declare pattern varchar50 za z0 fixbadchars select stringtofix stringtofix fixedstring mycounter id badstringlist union select stringtofix stufffixedstring patindex pattern fixedstring collate latin1 general bin2 fixedstring mycounter id fixbadchars fixedstring collate latin1 general bin2 like pattern select stringtofix fixedstring mycounter id fixbadchars mycounter select maxmycounter fixbadchars fixed fixed id fixbadchars id option maxrecursion able modify bottom 0,spatial coordinates store spatial geography point create table pt1 geography pt2 geography insert pt1pt2 values geography point77 geography point72 select pt1 stastext pt1 pt1 stastext pt2 pt1 pt2 point point third parameter spatial reference identifier srids srid corresponds spatial reference system based specific ellipsoid used mapping sql server currently supports one value note theyre direction ie pt1 pt2 represents plane route something would use line instead see also sql server point constructor version st pointxy sql server line constructor version st makelinept1pt2 0
pl sql function receive number return binary format im trying write function receive number return binary format ive developed far,oracle pl sql issue matter implementing proper algorithm example https www orafaq com wiki binary create replace function dec2bin number return varchar2 binval varchar264 n2 number begin n2 loop binval modn2 binval n2 trunc n2 end loop return binval end dec2bin sql select dec2bin22 dual dec2bin22 overhead listagg hierarchical query sql functions bigger overhead simple pl sql function sql select dual connect level select countdistinct dec2binrownum bincd gg bincd elapsed select dual connect level g2 select rownum select countdistinct bin bincd select select listaggsignbitandr power2level within grouporder level desc bin dual connect power2 level bin g2 bincd elapsed without udf 11,need use function oracles sql select listaggsignbitand43 power2level within grouporder level desc bin dual connect power2 level result bin found deadly snippet fiddle number substitute column choice probably possible using recursive ctes thats bit pay grade reverse process use snippet input select reverse1000 dual select sumto numbersubstrxlevel1 power2level output input connect level lengthx result output dbfiddle recursive cte might trick substitute column kicks found another function work tweaks older versions databases dont recursive ctes excellent orafaq site select decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue bin number select value dual result bin fiddle observant among notice code remove leading 4,refactor query altogether try performing clauses earlier joins later select count1 detailstable dt join select useridid mastertable created date1 date2 mt mt id dt masterid join select id usertable role null ut ut id mt userid even run explain plan refactored query looks worse original try anyway temp tables created internally perform cartesian joins tables smaller work got idea youtube video tried principles video complex question stackoverflow got point bounty gbn mentioned making sure right indexes place case please index created column mastertable give try update edt run queries select count1 allroles usertable select count1 nullroles usertable role null nullroles 0,dbcc shrinkdb cousin shrinkfile extremely slow lot single threaded execution going code much faster way shrink database file allocate new filegroup database make filegroup large use sp spaceused determine large rebuild indexes new filegroup drop old filegroup index rebuilds massively parallel technique often results much faster shrinking database course require bit extra space new filegroup process going however need enough space new filegroup hold largest filegroup instance reclaiming space go along technique also added benefit defragmenting indexes process 0
mysql password hashes internally saved star asterisk reading upon mysql internals going mysql user table mysql shell get mysql select,ok found documentation change introduced mysql earlier password lengths characters newer password lengths characters could simultaneously supported password column made bytes chars long newer passwords would begin mandatory identify documentation password hashes format always begin character whereas passwords pre format never 14,addition password starting asterisk algorithm password set plaintextpassword whatever password want select concat uppersha1unhexsha1 plaintextpassword example mysql set plaintextpassword whatever password want query ok rows affected sec mysql select uppersha1unhexsha1 plaintextpassword pwd creation pwd creation d09af2704d843a5e4e84362830c7ec1cea40df8a row set sec mysql select password plaintextpassword pwd function pwd function d09af2704d843a5e4e84362830c7ec1cea40df8a row set sec mysql learned algorithm long ago hashing algorithm mysql password 6,youre probably running autovacuum might want enable importantly consider upgrading branch 0,want something useful future would probably steer clear trying search registry hives sql server changed bit years troublesome keep method sqldatasourceenumerator flaky times although use concrete evidence instances network believe depends sql browser service well time find disabled utilize wmi class win32 service use offers information service get service cmdlet write everything functions generally use actually daily check verification service troubleshooting function get servicestatus string server foreach server iftest connection count quiet get wmiobject win32 service computer displayname match sql server select systemname displayname name state status startmode startname bit usually use case someone else comes across wants use test 0
nls nls date format stand example alter session set nls date format dd mon yyyy hh24 mi ss changes date,national language support americans weird idea using mm dd yyyy date format different languages different words months 12,oracle concepts guide says following note particular interest globalization support environment globalization support environment includes client application database control language dependent operations setting parameters environment variables client server may exist separate locations note previous releases oracle referred globalization support capabilities national language support nls features nls actually subset globalization support provides ability choose national language store data specific character set oracle database provides globalization support features native languages territories local formats date time numbers currency calendar systems gregorian japanese imperial thai buddha multiple character sets including unicode character semantics 9,default mysql consider case strings quite true whenever create database mysql database schema character set collation character set default collation see information default collation character set latin1 latin1 swedish ci happens case insensitive choose case sensitive collation example latin1 general cs mysql grammar create schema exists myschema default character set latin1 collate latin1 general cs effect things like grouping equality example create table casetable id int primary key thing varchar50 select casetable id thing abc abc abc abc case sensitive database get select thing count casetable group thing thing count abc abc abc abc select casetable thing abc id thing 0,could use partition note type allows calculation reset groups example create table oldnotes oldnoteref varchar null oldnotetype varchar data goes go insert oldnotes oldnoteref oldnotetype values cabc123 insert oldnotes oldnoteref oldnotetype values cabc456 insert oldnotes oldnoteref oldnotetype values cabc789 insert oldnotes oldnoteref oldnotetype values dxyz001 insert oldnotes oldnoteref oldnotetype values dxyz034 insert oldnotes oldnoteref oldnotetype values xzyz100 go select oldnoteref oldnotetype row number partition oldnotetype order oldnoteref newnoteid oldnotes go would produce following results give sequence within note type oldnoteref oldnotetype newnoteid cabc123 cabc456 cabc789 dxyz001 dxyz034 xzyz100 btw table produced http www sensefulsolutions com format text table html 0
empty columns take space table table holds basic info title date fields theres one field called comments varchar4000 time leave,think separate table would better improve page density reduce fragmentation especially dont always populate field data page holds around bytes rows say bytes rows bytes long rows page rest page wasted space db takes likely never hold data add data long field record mostly full page likely overrun page result pointer page rest record empty pages pointers lead poor performance normalize field 10,predictable performance avoid high variation rows per page would lean storing data related table especially populated small percentage time especially retrieved queries rows value null contribute space overhead minimal important one page might fit two rows next page fit rows really impact statistics might better splitting stored separately doesnt impact operations core table 9,takes minimal space used one bit null bitmap two bytes length zero null overhead minimal optimisation premature know issue keep one table break kiss introducing outer joins add overhead querying data see https stackoverflow com questions come limits bytes per row per varchar nvarchar valu 12,regarding window aggregates sum avg sumsalesytd partition territoryid order datepartyymodifieddate msdn page clause transact sql rather hidden remark general remarks order specified entire partition used window frame applies functions require order clause rows range specified order specified range unbounded preceding current row used default window frame applies functions accept optional rows range specification example ranking functions accept rows range therefore window frame applied even though order present rows range means code results sumsalesytd partition territoryid order datepartyy modifieddate range unbounded preceding current row sum calculated row getting rows territoryid year part modifieddate less equal year part row often called cumulative 0,create another table log inserts updates deletes using dml triggers lets say table want track create another table say tbl row stats create dml triggers table inserts updates deletes whenever three actions happen table insert row tbl row stats respective row id table action took place later select using group clause see number times action took place per row tracking selects add application code stored procedure whenever select done application corresponding insert tbl row stats done 0,short answer qualified yes number rows grows precise schema datatypes operations choose grows importance much normalize data depends operations plan perform stored data datapoints table particular seems problematic planning comparing nth point given spectra mth storing separately could mistake datapoints stand alone make sense context associated spectra dont need primary key foreign key spectra nth column index column suffice define inter intra spectrum operations must perform figure cheapest way accomplish equality thats needed may denormalized possibly pre calculated statistical metadata assist operations absolutely need sql access individual datapoints ensure reduce size row bare minimum number fields smallest datatype possible largest 0
expanding dataset based weight field table contains records un expanded form record associated integer weight essentially informs us many times,use cte sql fiddle http sqlfiddle com 0b172 create table tsampn int weight int attrib1 int attrib2 int attrib3 int insert values insert values insert values solution using cte cte sampn weight attrib1 attrib2 attrib3 repeatedtime select sampn weight attrib1 attrib2 attrib3 weight take care weight zero union select sampn weight attrib1 attrib2 attrib3 repeatedtime cte weight repeatedtime select sampn attrib1 attrib2 attrib3 cte order sampn option maxrecursion change recursions 4,efficient way perform task uses auxiliary table numbers simply table integers perhaps million numbers table comes handy sorts regular tasks create table dbo demo samplenumber integer identity null sampleweight integer null attribute1 integer null attribute2 integer null attribute3 integer null constraint pk dbo demo samplenumber primary key samplenumber constraint ck dbo demo sampleweight check sampleweight insert dbo demo sampleweight attribute1 attribute2 attribute3 values select samplenumber attribute1 attribute2 attribute3 dbo demo join dbo numbers sampleweight drop table dbo demo execution plan output samplenumber attribute1 attribute2 attribute3 sqlfiddle 8,agree paul numbers table probably best solution going come terms manageability performance said could approach problem different angle using xml solution fairly easy understand performance small number rows fine large number rows test update added exact code using working test environment use tempdb go object idtempdb dbo demo null begin drop table dbo demo end go create table dbo demo samplenumber integer identity null sampleweight integer null attribute1 integer null attribute2 integer null attribute3 integer null constraint pk dbo demo samplenumber primary key samplenumber constraint ck dbo demo sampleweight check sampleweight insert dbo demo sampleweight attribute1 attribute2 attribute3 values go 4,would deemed answer situation since already marked take bit testing created event session using sql server instance ssms version minus application name event exists select name sys dm xe sessions name pysoup tracing begin drop event session pysoup tracing server end create event session pysoup tracing server add event sqlserver rpc completed actionsqlserver sql text add event sqlserver sp statement completed actionsqlserver sql text add event sqlserver sql batch completed actionsqlserver sql text add event sqlserver sql statement completed actionsqlserver sql text add target package0 event fileset filename ntracing xel add target package0 ring bufferset max events limit go alter 0,might want consider using tvf table valued function remove offending characters start end data create table hold test data coalesceobject iddbo trimtest begin drop table dbo trimtest end create table dbo trimtest sampledata varchar50 null insert dbo trimtest sampledata select char13 char10 char9 char13 char10 test char13 char10 go create tvf coalesceobject iddbo stripcrlftab begin drop function dbo stripcrlftab end go create function dbo stripcrlftab val nvarchar1000 returns results table trimmedval nvarchar1000 null begin declare trimmedval nvarchar1000 set trimmedval case right val char13 right val char10 right val char9 left case left val char13 left val char10 left val char9 0,unfortunately atoms visible universe one atom given could possible describe data purely position universe database records bytes database size bytes means possibe states database could adopt using single atom place exactly one total unique places would possible describe data stored database unfortunately location coordinates would contain data well protip avoid using bool datatype store every bit seperately database solutions convert automatically bit int making everything even worse 0
check constraint array column verify length im playing postgres table validation rules trying set check constraint array column idea allow,array lengthwords return null specified array dimension exists use cardinality instead create table words table id serial primary key words varchar20 check cardinalitywords insert words table words values results error new row relation words table violates check constraint words table words check detail failing row contains 5,crucial point may aware quoting manual noted check constraint satisfied check expression evaluates true null value bold emphasis mine using cardinality fixes case like horse explains explicitly disallowing empty arrays would cheaper check words solutions still allow words null might want add null constraint either way aware null always passes check constraints general solution disallow null expressions check expression true 6,rolandmysqldba given right hint answer question problem seems lie query results given back fields read somehow database dropped indexes primary key inserted new index alter table newbb innopost add index threadid visible dateline index threadidvisibledatelineuseridattachipaddress link explains happens covering index queried fields query postidattach extracted key saves checking real data using hard disk queries run seconds thanks lot help edit actual underlying problem solved circumvented technique innodb needs serious fixing area 0,smaller value work cases takes varchar24000 work cases little choice use larger value creating separate table values coordinating inserts selects etc would add complexity would obliterate memory performance benefits extending field 0
mysql complex sql statement table contains posts another table contains meta options post first table meta options table key value,assuming post id meta key unique constraint join twice meta table select replace columns need castm1 meta value signed views castm2 meta value signed maxviews posts join meta m1 m1 meta key views m1 post id id join meta m2 m2 meta key maxviews m2 post id id castm1 meta value signed castm2 meta value signed sql fiddle thnx bluefeet 7,pivot data first apply filter order pivot mysql use aggregate function case expression select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id convert data rows columns see sql fiddle demo id views maxviews null null null null null null data column format apply filter select select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id src views maxviews see sql fiddle 6,case still interested potential workaround employ updated code introduces deferresolution temporary table query procedure temporary table exist runtime procedure able compile even though proper columns dont yet exist mytable even get execution plan reference deferresolution table statement procedure due way query optimizer prove exists always evaluates true said terrible hack presented mostly intellectual interest could edge case breaks aaron mentions would likely better making schema changes proper order say table already exists create table mytable nvarcharmax go code creates something like begin tran go sproc gets generated first create procedure mysproc begin create table deferresolution dummy int null select ab 0,dual table works almost way table works table select records means example describe table sql plus sql set lines sql desc dual name null typ dummy varchar21 table one column named dummy varchar21 table design one record least nobody fiddled sql select count dual count order get behaviour dual2 dual insert one record dual better yet create create table select ctas sql create table dual2 select dual query works sql select dual2 earlier said dual almost works like table work like table behaves differently value table selected queries let oracle explain sql set lines sql explain plan select dual2 explain 0
join datetime date using cast range question take excellent one posed cast date sargable good idea case concerned clause joining,agree martin cardinality estimates potentially suffer using approach vs date range approach ill also add using convertdate still getting sargability might imply folks reading code learning good idea general use functions column particularly column indexed since exception works cases actually forces scan seek might possible dont think good practice use exception doesnt real benefit except author code even short lived save seconds writing concise expression something face opposition time answering questions see others posting answers include bad habits declaring varchar without length often comment excuse hear back works fine case thats point folks learn case apply learned cases might work 4,depends one advantage predicate cast date join hash merge range version force nested loops plan useful indexes seek datetimecolumn tasks would make substantial difference setting 5k million rows test data mentioned question create table events eventid int identity primary key datecolumn date null details char1000 default insert events datecolumn select top dateaddday row number order spid getdate spt values v1 spt values v2 create table tasks taskid int identity primary key datetimecolumn datetime2 null details char1000 default select number spt values number type insert tasks datetimecolumn select dateaddminute number castdatecolumn datetime2 events turning set statistics io set statistics time trying 6,deadlock detection resolution issues around long rdbmss even though oracle owns innodb expect oracle fix innodb applications blame deadlocks much rdbms regardless oracle mysql rdbms deadlock error rear ugly head oracle acquired innodb october partnership innobase oy mysql expired mysql lax renewing imho oracle tried stem tide mysql sure made commitments improve innodb mysql choice due pr possible antitrust issues light expect innodb improvements would overshadow become comparable oracle getting away business politics look innodb variable innodb lock wait time option provide increasing decreasing length time permit lock succeed nothing deadlocks occur internally would normally look clustered index victim innodb 0,supplement aarons completely valid answer might decide depth validation gstin number validating correctness check digit appears last digit number code creates number functions used check constraint validate new rows modifications gstin number first two functions provide arbitrary mapping ascii characters contained gstin number base maps maps maps maps object idndbo map char nfn null drop function dbo map char go create function dbo map char char1 returns int begin declare val int set upper ascii ascii set val ascii ascii ascii set val ascii return val end go object idndbo unmap char nfn null drop function dbo unmap char go 0
safe disable innodb doublewrite buffering mysql innodb allows us disable doublewrite buffering setting innodb doublewrite databases doesnt seem allow setting,situation think reloading large mysqldump check pictorial representation innodb percona cto vadim tkachenko picture see innodb buffer pool writes dirty pages log buffer insert buffer ibdata1 double write buffer ibdata1 ibd file innodb table shutting double write buffer let mysqldump write data index pages tables faster since write 16k pages ibdata1 production servers never double write buffer disabled loading data faster maintenance course enable immediately reloading db server words add innodb doublewrite cnf run set global innodb fast shutdown restart mysql load mysqldump remove innodb doublewrite cnf run set global innodb fast shutdown restart mysql 12,issue well dealt post yves trudeau seems suggest safe conclusion conclusion like zfs ext4 transactional replacing innodb double write buffer file system transaction journal yield increase performance write intensive workload performance gains also expected ssd mixed spinning ssd configurations hes basically saying suitable file system yes safe perconas people really know stuff 8,updates yves trudeau blog https www percona com blog update innodb double write buffer ext4 transactions short probably safe comments seems point survive pull plug test fs ext4 journal zfs wont survive simple kill oom suspect fs wont reject partial written data app layer 4,typically drop temp tables explicitly habit like cleaning anything create myth dropping temp table explicitly prevent temp table caching reuse true see blog post paulwhite lot details one even 0,aaronbertrands answer great heres variation bit info select physical memory use kb memory usedby sqlserver mb locked page allocations kb locked pages used sqlserver mb total virtual address space kb total vas mb process physical memory low process virtual memory low sys dm os process memory also good blog post taskmanager isnt task performance dmv magic cant go past glenn berrys sql performance blog 0,clustered index table setting either validate comparing cases select object namep object id sys partitions inner join sys indexes object id object id index id index id data compression sql server also provides several different ways make column unique create table dbo foo1bar int unique create table dbo foo2bar int constraint x2 uniquebar create table dbo foo3bar int alter table dbo foo3 add constraint x3 uniquebar create table dbo foo4bar int create unique index x4 dbo foo4bar end underlying implementation different names different ways get work doesnt mean dont still end office 0
running pg dump hot standby server disclaimer admittedly havent tried yet im sure would know wasnt working correctly wanted ask,afaik running pg dump hot standby one major things standbys useful perfectly safe though isnt perfectly reliable dumps fail standby aborts transaction falling far behind master thing really need watch make sure standby current keeping standby lost connection master fell far behind dont want merrily backing three week date standby need allow standby fall quite far behind master backup since itll otherwise cancel pg dump transaction order continue replaying wal see documentation hot standby particularly handling query conflicts section max standby archive delay max standby streaming delay parameters note master must willing keep enough wal archives allow slave catch 21,backup standby perfectly fine avoid cancelled statement conflict backup standby system need pause replication standby using select pg xlog replay pause run backup finished run select pg xlog replay resume resume replication keep mind running commands cause recovery lag slave might quite large depending database size also take account space wal segments take replayed slave pause may find useful administractive functions documentation instance check server actually recovery prior pausing select pg recovery 12,dba sacred duty take whatever data entrusted leave null field usually means data entered data applicable meanings context specific may actually mean something business layer empty string may actually user entered wants course generalizing possibly reason get either nulls empty strings due sloppy coding business layer let us hope though either way chat whoever provides middle tier see using null vs empty string go maybe thinking always pass empty string always nulls aware randomness case advise think standard maybe null really information 0,related database little complicated normal deadlock deadlock youre hitting within parallel query two queries tell xml produced consumer exchange wait type waitpipegetrow theres similar victimless entries deadlock graph stuff aaron noted head first responder kit use sp blitzlock itll check system health session default point extended event session captures deadlock xml 0
computed columns computed values computed columns determined value retrieved value changed time im guessing novice question since im finding anything,depends define computed column persisted computed column calculated stored data inside table define column persisted calculated query run please see aarons answer great explanation proof pinal dave also describes detail shows proof storage series sql server computed column persisted storage 19,easy prove create table computed column uses scalar user defined function check plans function stats update select see execution gets recorded lets say function create function dbo mask varchar32 returns varchar32 schemabinding begin return select xx substring len xxxx end go table create table dbo floobs floobid int identity11 name varchar32 maskedname convertvarchar32 dbo maskname constraint pk floobs primary keyfloobid constraint ck name check lenname go lets check sys dm exec function stats new sql server azure sql database insert select select name execution count sys dm exec function stats inner join sys objects object id object id database id 34,wrote case closed ill reopen much gone wrong database design test setting create table patient patient id int primary key site held int null create table messageq messageq id varchar primary key varchar patient id int null references patient message body varchar null create index patient site idx patientsite held create index messageq patient id idx patientpatient id insert patient values insert messageq values m1 aaa1 m2 aaa2 m3 aaa3 m4 aaa4 m5 aaa5 m6 aaa6 m7 aaa7 m8 aaa8 m9 aaa9 m10 bbb10 major points simplify names better readability dont use non descriptive column names like entity id replaced 0,would say depends install sql server default ones seen case insensitive run select serverpropertysqlsortordername get back nocase iso would mean searches case insensitive using upper would give results would add bit work cpu 0
creating stored procedure adding date manually filename new sql administration tasked creating nightly jobs send email certain details contained within,declare variable first use format like declare query attachment filename nvarchar255 npca test convertnchar8 getdate csv exec msdb dbo sp send dbmail profile name support attach query result file query attachment filename query attachment filename query result header query result separator 5,sql server stored procedures argument either string literal variable certainly string expression like attempt resolve issue could declare variable assign expression pass variable sp send dbmail stored procedure declare myfilename nvarchar255 set myfilename pca test convertvarchar10 getdate csv exec msdb dbo sp send dbmail query attachment filename myfilename note also issues method need use select convert function see simply omitted select dd mm yyyy column alias however use select expression need enclose round brackets like select convertvarchar10 getdate dd mm yyyy aliasing result convert perfectly unnecessary case though omit dd mm yyyy part convert style return string slashes however since 4,alternatively since happen twice dac trick didnt work shut sql server service bring back minimal configuration mode modified version single user mode via command line go path sqlservr exe executable listed service instead running sqlservr exe use sqlservr exe sqlcmd allow connections named application everything else fail youll able connect change setting issue shutdown command within sqlcmd bring normally 0,several tables document compare features across editions sql server editions supported features sql server 0
export security related information sql server database guys probably know sql server provide box solution export security related statements declared,im sorry havent response since yesterday heres least starting point try pulling pieces need always read discussion threads regrettably unable find script endorsed big names recognize test thoroughly schema object server column level permissions often missing havent actually used theyre set starting points particular order list permission databases automated permissions auditing powershell sql part permission scripting databases script db level permissions v2 stored procedure script user permissions security queries database level security queries server level 8,idera sql permissions extractor seems product youre looking script server object permissions free also commercial edition called sql secure features feature comparison two editions found 6,database homes installed properly central inventory list central inventory windows located program files oracle inventory linux unix platforms location central inventory found etc orainst loc inventory contentsxml inventory xml list installed homes xml format inventory version info saved saved minimum ver minimum ver version info home list home name oragi12home1 loc opt oracle grid12102 type idx home name oradb12home1 loc opt oracle base product db12102ee type idx home name oradb11g home1 loc opt oracle base product db11204ee type idx home list inventory parse list find detailed information opatch example opt oracle grid12102 opatch opatch lsinventory oh opt oracle grid12102 details 0,profile database usage load identify bottlenecks due missing indexes due many indexes choose proper index require good knowledge specific database indexing techniques 0
force drop db others may connected need remove database postgresql db cluster even active connections need sort force flag drop,postgresql drop database clients connected least dropdb utility simple wrapper around drop database server query quite robust workaround follows connect server superuser using psql client use database want drop psql localhost postgres postgres using plain database client force drop database using three simple steps make sure one connect database use one following methods second seems safer prevent connections superusers method update system catalog update pg database set datallowconn false datname mydb method use alter database superusers still connect alter database mydb connection limit force disconnection clients connected database using pg terminate backend select pg terminate backendpid pg stat activity datname 158,way shell utilities dropdb pg ctl pg ctlcluster debian derivates filiprems method superior several reasons disconnects users database question need restart whole cluster prevents immediate reconnects possibly spoiling dropdb command quote man pg ctlcluster force option fast mode used rolls back active transactions disconnects clients immediately thus shuts cleanly work shutdown attempted immediate mode leave cluster inconsistent state thus lead recovery run next start still help postmaster process killed exits success server running failure conditions mode used machine shut pg ctlcluster main restart force pg ctl restart datadir fast pg ctl restart datadir immediate immediately followed dropdb mydb possibly script 6,using filiprems answer case simplifying connecting current user localhosts postgres instance psql making sure database exists select pg database datname database name disallow new connections update pg database set datallowconn false datname database name alter database database name connection limit terminate existing connections select pg terminate backendpid pg stat activity datname database name drop database drop database database name 7,dbas probably going throw eggs tomatoes im going throw anyway could use integration services job run first create file loop set run file location map result variable create execute sql task within container thats done able create sql job schedule appropriately im saying best solution work 0,postgresql doesnt directly support pivot keyword usually used platforms something like crosstab functions tablefunc module using crosstab function excellent stackoverflow answer 0,innodb physical row structure bulletpoint redundant row format sql null value reserves one two bytes record directory besides sql null value reserves zero bytes data part record stored variable length column fixed length column reserves fixed length column data part record reserving fixed space null values enables update column null non null value done place without causing fragmentation index page innodb physical row structure bulletpoint compact row format variable length part record header contains bit vector indicating null columns number columns index null bit vector occupies ceilingn bytes example anywhere columns null bit vector uses two bytes columns null occupy 0
understanding cleanuptime ola hallengrens sql server scripts relation full backups log backups trouble understanding exactly expect cleanuptime option ola hallengren,cleanuptime always specified specific backup job example create full backup job differential backup job transaction log backup job cleanuptime always relates extension job lets take look full backup example full backup create full backup job normally add one following parameters databases databases get backed really relevant example directory directory store backups backuptype full differential tlog cleanuptime much hours worth backups keep fileextensionfull extension backup backup job place create full backup according schedule defined job lets assume following job runs fileextensionfull set bak directory set sqlbackup cleanuptime set hours look maintenancesolution sql file find description parameter set cleanuptime null time hours 15,upvoted hot2uses answer covers question detail want share easy way test stuff might help helped fully understand script works install backup script dependencies test instance tested local computer alter script search replace hh minute references find hh script dealing cleanup time allows quickly run backups various types full diff log see effects execution retention minutes hours run full diff log backup test database note files created individual folders used note cleanuptime minute due alter script hours minutes exec dbo databasebackup databases test directory olabackuptest backuptype full verify cleanuptime cleanupmode backup exec dbo databasebackup databases test directory olabackuptest backuptype diff verify 8,opened case microsoft regarding issue microsoft confirmed bug also affects sql server planning release fix sql server service pack released time writing answer microsoft releases service pack sql server users bypass issue disabling lock partitioning via trace flag note issue applies machines processors information lock partitioning thanks microsoft support prompt helpful update bug fixed sql server cumulative update sql server sp 0,really depends much data changing lets say table columns also indexes diff column values columns changing even data columns changing columns indexed may better deleting inserting columns changing lets say part non clustered indexes may better updating records case clustered index updated indexes updated research find comment sort redundant sql server internally separate mechanism performing update place update ie changing columns value new original row place update delete followed insert place updates rule performed possible rows stay exactly location page extent bytes affected chnaged tlog one record provided update triggers updates happen place heap updated enough space page updates also 0
sql server allow make visible ddl inside transaction transaction prior commit postgresql create table test data transaction migrate new column,youre looking begin transaction alter table foo add varchar exec sp executesql nupdate foo set cast varchar alter table foo drop column commit 12,generally speaking sql server compiles whole batch current scope execution referenced entities exist statement level recompilations may also happen later main exception deferred name resolution applies tables columns deferred name resolution used reference nonexistent table objects objects must exist time stored procedure created example reference existing table stored procedure list nonexistent columns table common workarounds involve dynamic code joes answer separating dml ddl separate batches specific case could also write begin transaction alter table dbo foo alter column varchar11 null online execute sys sp rename objname ndbo foo newname nb objtype column commit transaction still able access renamed column batch 17,detailed behaviours isnumeric documented probably fully known anyone without source code access said may interpretation depends unicode categorization numeric equally weird cases mention may bugs preserved backwards compatibility yes know sounds crazy happen using sql server need use isnumeric use try convert synonymous try cast instead check string convertible given type provide adequate functionality preferable try parse latter involves expensive processing via clr integration 0,based experience would use load data infile import csv file load data infile statement reads rows text file table high speed example found internet load data example tested example box worked fine example table create table example id int11 null auto increment column2 varchar14 null column3 varchar14 null column4 varchar14 null column5 date null primary key id engine innodb example csv file tmp example csv column1column2column3column4column5 1afoosdsdsd4 2bbarsdsa4 3cfoowewqe3 4dbarasdsad2 5efoobarwewqe5 import statement run mysql console load data local infile tmp example csv table example fields terminated lines terminated ignore lines id column3column4 column5 set column5 str date column5 result 0
transform number symbols mysql select want know possible transform numeric return int column sequence symbols direct mysql select table like,maybe use lpad rpad create temporary table tt category int unsigned insert ttcategory values select lpad category tt granted wouldnt work youd replace single null value idea least 4,select id repeat ifnullcategory symbolcategory assumes want null equivalent 5,output clause access data target rows constants variables data elsewhere source select like operating trigger https docs microsoft com en us sql sql queries output clause transact sql states reference columns table modified must qualified inserted deleted prefix get original id would need include destination table output clause echo back like existing table declare mytable table id int identity11 name nvarcharmax sourceid int data want insert declare myinsertdata table id int name nvarcharmax insert myinsertdata idname values bla 2test 3last declare mycrossref table newid int oldid int insert mytable name sourceid output inserted id inserted sourceid mycrossref select name id 0,id use either pg dump create table foo log select foo added actually one idea add insert update trigger table shapshoted write changes another table partitioned daily basis please tell god idea sounds good works rules create rule insert yourtable log ins also insert yourlogs select new daily begin create table yourlogs date inherits yourlogs create replace rule yourlogs ins insert yourlogs instead insert yourlogs date select new commit main difference creating table fly daily basis overhead youll introducing every insert update opposed grabbing updated rows one go 0
authoritative source identical performance sql server consider answer reassures asker operator commenter pipes says true functionally however sql optimizer uses,think following proves doesnt comparisions sql standard defines equals operator http www contrib andrew cmu edu shadow sql sql1992 txt technically extension standard even though cant think rdbms doesnt implement sqlserver treated operators one would fact syntax error 8,work microsoft sql support asked jack li senior escalation engineer subject matter expert sql server performance sql treat differently said 59,parsing sql server calls sqllangdecodecompop determine type comparison operator present occurs well anything optimizer gets involved comparison operators transact sql tracing code using debugger public symbols sqllangdecodecompop returns value register eax follows op code return indistinguishable later operations including compilation optimization though secondary point also possible using undocumented trace flag look logical tree passed optimizer confirm map scaop comp cmpne equal scalar operator comparison example select productid production product productid option querytraceon querytraceon select productid production product productid option querytraceon querytraceon produce logop project qcol productid logop select logop get tbl production productalias tbl scaop comp cmpne scaop identifier qcol 145,idea perform another option try old school way without conflict items name values foo bar zzz added insert tag name select name items except select name tag returning id select id added union select id tag name select name items insert unique names found tag table return ids combine ids names exist tag final output also throw name output suggested ypercube know id matches name 0,able upgrade sql server sql server may able take advantage much improved performance especially frameless window aggregates provided new batch mode window aggregate operator almost large data processing scenarios work better columnstore storage rowstore even without changing columnstore base tables still gain benefits new operator batch mode execution creating empty nonclustered columnstore filtered index one base tables redundantly outer joining columnstore organized table using second option query becomes get batch mode processing window aggregate operator create table dummy integer null index dummycc clustered columnstore identify uniquedates greater groupdate within groupid select calc maxud uniqueid calc maxud groupid calc maxud groupdate 0,actually three problems query first maxrow return string row second subquery needs alias try like select maxrow select row number overorder id desc row users userquery third problem count much better way expertly described answer gbn note also row reserved keywords list avoided well 0
sql server restrict memory cpu specific database sql server standard ed windows core 128gb machine vm actually max prod db,cant theres thing per database restrictions query cross multiple databases think joins fully qualified objects sql servers resource governor theory exists support works things like cpu count query workspace memory disk io doesnt work buffer pool still wont meet needs 9,sure licensing would allow would get degree fine tuned control install second sql instance host set max memory instance individually even ability adjust cpu settings processor affinity although really understand setting messing problem even scenario management asked guarantee wont impact production database primary instance id answer cant guarantee youd want run entirely separate vm 7,using like single line like search textvalue like search contains list allowable characters adding spaces around textvalue removes need multiple textvalue like without padding need specifically handle string appearing start end string textvalue like search middle textvalue like search start textvalue like search end using fulltext index edit2 full text purists create fulltext index msdn states auto option default although changes propagated automatically changes might reflected immediately full text index may give correct results seconds later also upto 10k rows perform adequately scales use like table around 25k rows users know perform badly search way advanced page gain trade managing 0,feel really guilty typing sadly thats first time ive heard use case makes perfect sense best submit request http connect microsoft com grandchildren able 0
use without group sql queries order use sql queries must group aggregate column names special cases possible use without group,used filter groups clause used filter rows 4,dont coexist proved fact following query oracle works select dual similarly postgresql following query works select doesnt require group applied aggregation phase must used want filter aggregate results reverse isnt true following wont work select count mytable group need replace case follows select count mytable group nb following query form also work select select count mytable group see using simply shorthand version last query summary applied group phase whereas applied group phase 25,limited number values want convert columns easily implemented using aggregate function case expression select user id sumcase message type private else end private sumcase message type public else end public yourtable group user id see sql fiddle demo postgresql ability use crosstab used installing tablefunc module perform similar data transformation rows columns creating dynamic version query straight forward process great solution dynamic crosstab stackoverflow 0,try following code create table names type varchar50 colnum smallint colname varchar50 coldatatype varchar20 insert names values customer customerid int customer customername varchar50 customer customerjoindate date customer customerbirthdate date account accountid int account accountname varchar50 account accountopendate date customeraccount customerid int customeraccount accountid int customeraccount relationshipsequence tinyint create table data type varchar50 col1 varchar50 col2 varchar50 col3 varchar50 col4 varchar50 col5 varchar50 col6 varchar50 col7 varchar50 insert data values customer mr john smith null null null customer mrs hayley jones null null null customer acme manufacturing ltd null null null null customer mr michael crocker null null null account smith 0
sql large table design general question sql server tables design currently table 600gb grows 3gb day table appropriate indecies becoming,partitioning isolation may sufficient may get better results combining partitioned views multiple tables much depends pattern querying growth current limitation partitioning column statistics maintained table rather partition level pattern querying would benefit accurate statistics combining table partitioning partitioned views could yield significant performance benefits nature data varying month month year year partitioned views also help imagine retailer changed product lines continually little consistency product productid ranges use year year single order orderdetail table therefore single statistics histogram stats offer little query optimiser table per year order order orderline orderline partitioned month combined partitioned views order orderline provide granular potentially useful 7,table appropriate indecies becoming major hangup running queries partitioning alone doesnt help query performance unless sql server able eliminate partitions running query clause needs line way partition get one field use partitioning field field isnt included clause youre still likely scan entire table despite partitions size partitioning make certain maintenance operations easier theres still things cant partition partition basis index maintenance stats updates causing problems youre better splitting design archive table live updated table need periodically move data live table archive table rebuild indexes fill factor update stats full scan set filegroup read partitioning help archive table loads partitioning live 11,im inclined agree catcall database recovery top list implications backup recovery options typically poorly understood outside dba team likely result disaster ensure defined agreed technical non technical management rpo recovery point objective rto recovery time objective databases systems document backup recovery procedures extent could followed non technical staff ensure documentation held printed well electronic form offsite disaster recovery runbook stored local network wont much use buildings fire test every aspect recovery procedures often backups irrelevant restores matter next database agnostic perspective understanding database server built provide atomicity consistency isolation durability transactions data commonly misunderstood frequently cause performance problems primary source 0,please refer postgres manual copy pgadmin sql string pass via script db connection would use copy prefix enter something like copy tablename need make sure relevant privileges run command case need able log database write access tablename postgres also needs able reach file path home uploads accessible database server postgres user able read file check permissions file directory 0
alternatives network backup environment servers always availability group standalone normally backup network share recently observed databases growing bigger time taken,alternative mentioned seems best choice step process take native sql server backups compression using olas backup solution locally use robocopy transfers network share decoupled run windows scheduled task way backups local fast need disk space obviously redundancy backup disk fails dont want lose backups alternatively recommended max vernon robocopy step backup job ensure robocopy occurs backup successfully completed soon possible backup complete backup risk data long stays local also regularly test restores since restore backup purpose serve also refer answer sql backup tuning large databases 10,ways tune backups messing different knobs like maxtransfersize buffercount striping file youve noted youre already problem touching knobs may still result hitting limits network storage real impact backup time first job benchmark storage youre backup using crystal disk mark diskspd thatll give idea fast expect writes best next thing need test reads drives youre backing run backup nul time long takes read portion backup without write disk numbers mind start messing knobs see ones get closest regardless backup target local networked 15,couple potential solutions going full weekly full backup nightly differential easy solution number performance related parameters tweak olas scripts might able tweak get performance want blocksize specify physical blocksize bytes blocksize option databasebackup uses blocksize option sql server backup command buffercount specify number buffers used backup operation buffercount option databasebackup uses buffercount option sql server backup command maxtransfersize specify largest unit transfer bytes used sql server backup media maxtransfersize option databasebackup uses maxtransfersize option sql server backup command 9,many possible options databases get larger full backups take longer likely incorporate differential backups havent already creating differential backups fast compared creating full backup differential backup records data changed since full backup upon differential backup based facilitates taking frequent data backups decrease risk data loss understanding olas scripts even set decide full differential backup based amount change database using modificationlevel parameter use emc dd boost youre welcome opinion found due client side de duplication methods uses full backups even multi tb databases fast point dont worry sql server differential backups effect using emc dd differential backups sql server using multiple 5,sql server use clustered index plus sort algorithm instead using non clustered index even execution time faster latter case sql server uses cost based optimizer based statistics runtime info cost estimation process query actually evaluate lookup plan estimates take effort note estimated subtree cost hovering select execution plan thats necessarily bad assumption either test machine lookup plan takes 6x cpu sort scan look rob farleys answer sql server might cost lookup plan higher 0,even local temporary table table would work long stayed session context global temporary table table global seen sessions probably happening create table session going scope context session goes away goes temp table whether local global process created temporary table scope temp table gone words problem isnt scope database connected temporary table access regardless database using temp table isnt created user databases db1 db2 always created tempdb try using derived table consider staging table etc 0,list recent restores database server lastrestores select databasename name create date compatibility level collation name rownum row number partition name order restore date desc master sys databases left outer join msdb dbo restorehistory destination database name name select lastrestores rownum 0,great question dont know answer imagine going goto internals team dont know theyll big site meantime help deduce answers starters see trigger cache detect metadata underlying objects changed trigger uses table table changed since trigger loaded cache trigger operates using outdated metadata makes think something going recompiling sql even monitoring metadata means engine concern token read block think thing engine prevent problems interaction server threads client issues statement server uses snapshot routines triggers available execution statement server calculates list procedures functions triggers may used execution statement loads proceeds execute statement means statement executes see changes routines performed threads im entirely 0
effect execution plans table variable primary key read great deal differences temporary tables table variables sql server experimenting switching mostly,since declaring primary key table variable implicitly creates index key columns fact way index table variable prior sql server presence definitely effect resulting query plans optimizer make use primary key index appropriate see action running short script execution plan enabled table scan change clustered index seek primary key index declare t1 table id int null data varchar50 null insert t1 values aaaaa bbbbb ccccc ddddd eeeee select t1 id primary key index declare t2 table id int null primary key clustered data varchar50 null insert t2 values aaaaa bbbbb ccccc ddddd eeeee select t2 id whether declaring primary key instead 6,query optimizer going make choices differently theres primary key constraint yes might estimating one row understanding estimates incorrect different knowing table contains unique values certain plan space explorations require key example good general rule thumb provide much information data query task optimizer key say explicitly declaring key add much cost beyond little keyboard work cases personally rarely use table variables lack statistics including distribution density cardinality information separate considerations provides less information optimizer equivalent temporary table experience much table variable plans adapt well changing circumstances time use table variable special reasons sure always adequate query optimization point view enough information 7,possible solution create server logon trigger would check app name created following server trigger local instance tried connect via odbc connection rejected would modify needs create trigger trggetappname server logon app name like microsoft sql server management studio app name like net sqlclient data provider begin rollback end word caution logon triggers logon trigger effectively prevent successful connections database engine users including members sysadmin fixed server role logon trigger preventing connections members sysadmin fixed server role connect using dedicated administrator connection starting database engine minimal configuration mode 0,sql injection attacks untrusted input directly appended queries allowing user effectively execute arbitrary code illustrated canonical xkcd comic thus get situation userinput getfromhtml robert drop table students query select students studentname userinput stored procedures general good defenses sql injection attacks incoming parameters never parsed stored procedure dbs programs dont forget precompiled queries count stored procedures look like following create stored procdure foo select students studentname program desires access calls foouserinput happily retrieves result stored procedure magical defense sql injection people quite able write bad stored procedures however pre compiled queries stored database program much difficult open security holes understand sql 0
upper lower case optimisation ms sql server anyone know ms sql server text queries optimised upper lower case strings read,since default installations sql server give case insensitive searches going limb say upper lower case optimization searches always adding upper lower convert text query impact performance depending volume data queried use upper lower dont 5,would say depends install sql server default ones seen case insensitive run select serverpropertysqlsortordername get back nocase iso would mean searches case insensitive using upper would give results would add bit work cpu 6,many applications dont want individual user accounts connecting database users passwords rights permissions handled application layer single dedicated service account used connect database back end dba dont want manage database level active users medium sized public facing web application maybe million users suddenly popular app real sense difference philosophy application developers database developers dbas many application developers going want pass responsibility major aspects app functionality business rules database layer instead view database tool simply store retrieve data cases might short sighted many rdbmses awesome features could make app dev lives much easier row level security columnar indexes filestream storage etc 0,capturing dml operations individual tables among auditing features available standard edition want trace dml operations another possible hook extended events wont able use auditing package could use lock acquired events capture read write operations blogged https spaghettidba com tracking table usage identifying unused objects options triggers easy set even automatically generating trigger code downside increased write overhead app side logging control application inject code needed log accesses tables 0
solution assigning unique values rows finite collaboration distance table created populated following code create table dbo examplegroupkey int null recordkey,iterative sql solution performance comparison assumes extra column added table store super group key indexing changed setup drop table exists dbo example create table dbo example supergroupkey integer null default groupkey integer null recordkey varchar12 null constraint iexample primary key clustered groupkey asc recordkey asc constraint ix dbo example recordkey groupkey unique nonclustered recordkey groupkey index ix dbo example supergroupkey groupkey supergroupkey asc groupkey asc insert dbo example groupkey recordkey values archimedes newton euler euler gauss gauss poincar ramanujan neumann grothendieck grothendieck tao able reverse key order present primary key extra unique index required outline solutions approach set super group 7,recursive cte method likely horribly inefficient big tables rcte anchor select groupkey recordkey cast castgroupkey varchar10 varchar100 groupkeys cast castrecordkey varchar10 varchar100 recordkeys lvl example union recursive select groupkey recordkey case groupkeys like caste groupkey varchar10 castr groupkeys caste groupkey varchar10 varchar100 else groupkeys end case recordkeys like caste recordkey varchar10 castr recordkeys caste recordkey varchar10 varchar100 else recordkeys end lvl rcte join example recordkey recordkey groupkeys like caste groupkey varchar10 groupkey groupkey recordkeys like caste recordkey varchar10 select row number order groupkeys supergroupkey groupkeys recordkeys rcte exists select rcte lvl lvl groupkeys like castc groupkey varchar10 lvl lvl groupkey 6,problem following links items puts realm graphs graph processing specifically whole dataset forms graph looking components graph illustrated plot sample data question question says follow groupkey recordkey find rows share value treat vertexes graph question goes explain groupkeys supergroupkey seen cluster left joined thin lines picture also shows two components supergroupkey formed original data sql server graph processing ability built sql time quite meagre however helpful problem sql server also ability call python rich robust suite packages available one igraph written fast handling large graphs millions vertices edgeslink using igraph able process one million rows minutes seconds local testing1 compares 10,theres little bit overhead transaction log file written disk even changes occurred db databases sql server compression turned backups transaction logs written periods inactivity generally kb per trn file overhead transaction logs going contain changes made since last trn file written amount total data wont vary significantly frequently write files less data loss risk experts recommend run log backups every minute yes really generally try run every minutes business hours bulk activity going every minutes peak times systems real activity work hours system hour operation 0,many perhaps database products allow define derived table calculated column explicitly assigned alias words let select select expression instead would something like select select expression somealias reason assign name expression automatically contrast oracle oracle fine derived table calculated column without explicit alias column default name problem name basically name expression oracle applies certain transformations original expression using name particular unquoted identifiers including names functions uppercased spaces removed thus expression count default name column becomes count expression default name note presence special symbols example prescribes enclose name quotation marks able use reference words derived table like select count must write select 0,similar setup recently encountered messages logs using dell compellent san things check receiving messages helped us find solution review windows performance counters disks warning messages pointing specifically disk avg read time disk avg write time disk read bytes sec disk write bytes sec disk transfers sec avg disk queue length averages many database files one drive averages skew result mask bottle neck specific database files check query paul randal returns average latency file dmv sys dm io virtual file stats case average latency reported acceptable underneath covers many files ms average latency check timings pattern happen frequently certain time night 0
error state sql server used read error state help distinguish different states locations source code type error occur really clear,help find anything error occurred quick example try divide get error message bunch details select result msg level state line divide zero error encountered see one called state value error state returns value use try catch begin try select end try begin catch select error state end catch result thats useful scenarios suggest reading error handling general dive deep specific functions sound useful http msdn microsoft com en us library ms175976 aspx http www sommarskog se error handling html 5,purpose sql server error states sql server development team able identify code exact place system errors raised given many errors raised multiple places end user ie developer applications using sql server similarly use state passed raiserror product support identify place procedure raises error example create procedure usp proc somecondition raiserrornerror foo bar someothercondition raiserrornerror foo bar go see two state allow distinguish later error case hit say look error message im telling one word internationalization 8,structure like probably solved multi column primary key constraint table lookup foreign key constraint referencing primary key table optimal index looking values one direction provided automatically primary key lookup table create table ticket ticket id integer primary key possibly serial instead integer stuff text create table lookup outside data id integer ticket id integer references ticketticket id update cascade optional delete cascade optional constraint lookup pkey primary key outside data id ticket id never use non descriptive column name id smart orms generally unhelpful anti pattern use descriptive distinct name like ticket id need know association exists good old foreign 0,comments im guessing client side command timeout aborted sql query rollback transaction connection stays open sql server due connection pooling need use set xact abort add client rollback code see sql server transaction timeout gory details 0
install sql server management studio locally install sql server management studio desktop access database sql server instance find installer google,download install sql server management studio using links provided scott hanselmans website links various ssms versions x86 x64 well links various sql server express versions way also use latest ssms manage database 12,want shiny new monthly releases management studio include handy check updates mechanism negates need full setup program download since retrieves components need well release cycle completely independent sql server go read use new web installer much longer applies want use previous versions currently sp2 sp1 theyve made easier find download see page much still apply well legal copy sql server able install management studio media wherever need manage sql server said express version management studio since service pack fully functional product absolutely differences except download free instead finding licensed media would recommend downloading version use older version support sooner philosophy 75,problem see dynamic allocation resources leads unpredictable performance reporting query cpus available yesterday ran minutes today took considerably longer also see latency guest waits cores become available jonathan kehayias gives practical warnings subscribing cpu memory frankly trust experience advice typical vm admin offense suspect lot direct experience combination accidental dba day virtualization considerations 0,firstly generate script unix prompt echo select concatdrop table table name information schema tables table name like prefix mysql user root password blah batch drop sql replace prefix batch option suppresses fancy formatting mysql default produce runnable sql script review script looks ok run drop sql mysql prompt 0
storing vs calculating aggregate values guidelines rules thumb determine store aggregate values calculate fly example suppose widgets users rate see,often need calculate display values relative often underlying numbers changed updated website 10k daily hits thats displaying value thats going change hour id calculate underlying values change could database trigger whatever tool go look stats stats changing second three people access look couple times day id likely calculate fly unless takes couple minutes calculate stale data first place isnt big deal boss tells generate thing cron every hour doesnt wait wants look 11,depends pre calculating aggregate values places larger load writes deriving makes reads difficult frequently accessing derived value pre calculation valid de normalization step however instance recommend using materialized view view written disk linked trigger parent tables materialized view designed store frequently asked tedious derive data useful high numbers writes low numbers reads high write high read scenario consider task background mimics effects materialized view less real time present good enough average preserving write read performance circumstances treat derived column like normal column make sure data presented widgets view present elsewhere table entire tuple derived whatever processes emplace question also strongly 59,use stalewidgets table queue invalid recalculated widgets use thread asynchronous task recalculate values period moment recalculations depends system requirements read end month user start day 4,percona toolkit way go least super short time conversion took table 500gb master slave setup tested bit 24h production took better hardware almost month funny sidenote days would run ids therefore started already plan plan working offline backups removing slaves delay mainly due waiting replication happening towards slaves allowed max 50sec time lag also make sure limit number concurrent threads 2m inserts day many million reads also aware coversion started cant stop least didnt find way restart 0,would trust rsync copy ldf mdf files user system databases would trust anything hacked together vss otherwise production environment sql server fussy things get written ldf mdf files software rsync isnt designed mind might get right especially doesnt understand ldf files mdf files need treated interrelated system files software doesnt get things right nothing might noticed failover try go live databases flagged suspect due sql server sees data corruption even worse getting right might dependant much load system might find problem lightly loaded test environment seen enough examples people thought replicating files left corrupt files recovery site inaccessible backup files 0,possibly oracle bitmap indexes looking oracle index types think permit union intersection admit im familiar 0
delete low priority row visibility large myisam table mysql 5windows xp x64 run delete low priority queries delete low priority,docs specify low priority server delays execution delete clients reading table affects storage engines use table level locking means delete low priority statement begin processing read locks finished another read lock comes delete low priority statement begins wait delete low priority statement lock though acquire table lock run complete killed normal table lock situations write requests given higher priority read requests example write lock going read request goes read queue another write request comes second write request execute read requests using low priority situation reversed read lock going write comes wait another read comes write gets lock second read execute 5,rows remain visible test shows delete prevent rows visible subsequent queries test illustrates table lock taken immediately normal delete query waits delete finishes returns zero count testbed long running query create database stack use stack create table table id int auto increment primary key varchar val varchar10 insert table varchar val select hello select union select union select union select union select union select union select union select union select union select s1 select union select union select union select union select union select union select union select union select union select s2 select union select union select union select 4,oracle provides nvl scenario isnull equivalent ms sql server could disguise view make code clearer 0,talked similar issue worked case setting sql server dependent disk drivers youll want test make sure works setup trick 0
mongodb ram requirements enough entire index memory ram mongodb even try allocate much ram possible store even data fast reads,mongodb use available free memory caching swap disk needed yield memory applications server best performance youll want enough ram keep indices frequently used data working set memory helpful reading mongodb faq mongodb require lot ram mongodb wiki checking server memory usage 6,real reason cant ask limit memory mongodb doesnt manage memory uses directly lets os mongodb memory maps data os page memory needed result direct management amount used possible mongodb implements completely different way os allows possible linux since days way truly segregate resources present use virtualization solution isolate mongodb vm yes overheads involved though hypervisors gotten lot better moment price paid level resource control terms oom killer even processes host long data set indexes overall exceed available memory mongodb hit oom killer issues data gets paged memory memory pressure nothing else wants resident memory keep adding touching new data indexes 18,think backing data directory want moving whole directory new server might look move everything within cluster drop move single databases need 0,sql server must version collations since collations determine sort order data persisted database collation must guaranteed remain stable releases otherwise collation change eg fixing bug collation new release windows would result two rows int database sort differently release may sound trivial effect means index corruption items sorted according new fixed collation rules database present according old buggy collation rules therefore sql server must snapshot windows collation release stick release version number collation name since names explicit versioning scheme collation choose subject many factors consider interact older server exchange data may newest collation use 0
resources understanding sql server locking concurrency demonstrated recent question mine locking concurrency hard suggest good resources intermediate advanced sql professionals,best book subject complete resource kalen delaneys sql server internals really cant better another good book subject chris boltons sql server internals troubleshooting dont think complete kalens book cover things pretty well especially around locking concurrency 7,transaction processing concepts techniques readings database systems principles transaction processing product specific books great job explaining use products kalen delaneys series awesome dont really stand chance black book red book database systems specially black book first link pretty much mandatory reading want understand concepts red book collection research papers many available online keep page links third book linked basically date rewrite black book also cheaper available kindle also several good blogs would first foremost recommend sql server css blog 7,locking concurrency topic learnt testing observation start reading isolation levels database engine concurrency effects understanding relationship two experiment construct sql test transaction something simple self contained context domain understand shopping cart checkout bank transfer ensure method verifying data expected state testing run scenario observing type duration sequence locks applied database engine alter isolation level apply hints observe change locks taken think transaction survive concurrency load test tend check lock sequences trace flag profiler trace believe extended events could also used ive tried yet 5,one resource kendra littles clever poster isolation levels also links presentation resources discussion 4,friendship intended symmetrical possible friends vice versa would store one way relationship check constraint ensuring relationship represented one way also would ditch surrogate id composite pk instead possibly composite unique index also reversed columns create table friends userid1 int null references usersuserid userid2 int null references usersuserid constraint checkoneway check userid1 userid2 constraint pk friends userid1 userid2 primary key userid1 userid2 constraint uq friends userid2 userid1 unique userid2 userid1 dont say queries makes difficult always create view create view foo select userid1userid2 friends union select userid2userid1 friends 0,sql compilations sec good metric coupled batch requests sec compilations per sec doesnt really tell much seeing batch req per sec little exaggerated effect yes need get bottom cause likely overuse ad hoc querying single use plans batch req per sec measuring compilations per sec bad general rule thumb compilations sec less total batch requests sec really want drill whats cached run following query utilizes appropriate dmvs select db namest dbid database name cp bucketid cp usecounts cp size bytes cp objtype st text sys dm exec cached plans cp cross apply sys dm exec sql textcp plan handle st 0,according martin fowler approaches problem table inheritance single table inheritance one table represents types unused attributes nulled concrete table inheritance one table per concrete type table columns attributes type relation tables class table inheritance one table per type table attributes new inherited attributes tables related reflecting actual type inheritance hierarchy start starting point search pros cons approach gist approaches major disadvantages none overwhelming advantage better known object relational impedance mismatch problem yet find solution personally find type problems bad relational design lead orders magnitude serious kind problems arising bad type design bad database design leads slow queries update anomalies data 0,computed column isnt persisted indexed virtual column expression taking space 0
generate series standard sql sql given two numbers want generate series form repeat times instance want sequence following numbers know,postgres easy using generate series function parameters values select case g2 gn else gn end xi parameters generate series1 gn generate series1 g2 generate series1 gm order gm g2 gn standard sql assuming reasonable limit size parameters less million use numbers table create table numbers int null primary key fill preferred method dbms insert numbers values mildly complex sql need type million numbers use instead generate series parameters values select case g2 gn else gn end xi parameters join numbers gn gn join numbers g2 g2 join numbers gm gm order gm g2 gn 4,need plain sql theoretically work dbmss tested postgresql sqlite recursive sinz select values1113 union select case end mm select union select select order explanation generate series assuming recursive sn select union select select quite simple could found almost docs recursive ctes however wee need two instances values generate series nn recursive sn select values11 union select select doubling initial value two rows second bunch need reverse order well introduce order bit introduce order observe also thing two rows starting condition three columns still single column conditional still increasing value recursive sinz select values111111 union select select likewise mix bit watch 5,postgres make work single generate series basic math see mathematical functions wrapped simple sql function create replace function generate seriesn int int returns setof int func select case n2 n2 else n2 end select n2m n2m n2 generate series0 n2m sub order n2m func language sql immutable call select generate series3 generates desired result integer overflow int4 subquery generate desired total number rows simple ascending number name n2m simplify following modulo operation take modulo operator get series ascending numbers times name n2 outer query add lower half n2 upper half n2 mirror lower half n2 added order guarantee requested order 10,strange dilemma mysql grant tables windows recommend following step get zip file distribution installer step unzip contents mysqlzipstuff step look folder mysqlzipstuff data mysql step copy files mysqlzipstuff data mysql mysql folder desired data directory step net start mysql mysql folder already instead step edit ini adding line mysqld section mysqld skip grant tables skip networking step net stop mysql step net start mysql point type mysql however run grant commands grant tables disabled step enter one superuser manually like insert mysql user set user roothost localhost select mysql user user root host localhost show columns user table manually change 0,protecting agains accidental media loss lost laptop database drive showing flea market copy database backup etc etc scheme processing database engine application requires access data without user providing password offer access agains media loss encryption key hierarchy rooted system dpapi encrypted key words password service account scenario never protects agains hacker gets access sql server meant protect alternative ask user password time uses application use password open key top encryption key hierarchy usually certificate database scheme seldom deployed cases multi tenant scenarios tenants trust hosting operations administrators 0,inspired pauls answer research found true stack space limit number concatenations stack space function available memory thus varies following two points also true way cram additional concatenations single statement using method go beyond initial stack space limitation actual logical limit appear vary found first adapted pauls test code concatenate strings declare sql nvarcharmax set sql declare varcharmax varcharmax set set sql replicateconvertnvarcharmax set set sql nselect datalength chars execute sql test highest could get running great laptop gb ram returns total chars using sql server express edition localdb returns total chars using sql server developer edition sp4 kb4018073 getting error 0
displaying estimated execution plan generates cxpacket pagelatch sh latch ex access methods dataset parent waits im running microsoft sql server,appears request actual execution plan triggered stats updates since mention happens mornings imagine theres overnight process lot modifications tables involved thus sql server uses stats create plan hit modification threshold executes automatic stats updates part operation xml estimated plan shared see close together update dates stats morning lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 large busy tables seems likely based sampling percentages surprising stats updates taking lot horsepower 13,see long estimated plan times ssms one following order likelihood query optimizer decided needed create update statistics size estimated plan large say mb simply takes ssms long time display query compilation actually took long time due cpu usage looking good enough plan server extreme duress example might wait gateway become available various bugs lead extremely long running compile times situation answer almost certainly sql server updating creating statistics clues size query plan small query plan relatively simple low cost compile cpu significantly lower compile time new contributor josh darnell also pointed good clue last updated time statistics xml sql server 9,thought read lot topic broad topic configuration control change management strategy cmmi domain topic even companies accreditation cmmi sometimes version control databases question answered keeping mind following constraints keeper every ddl executed keeper people ability execute ddl statements need log changes done need compare vast differences database design done via external tool published database external tool ddl scripts source control even key point source control publish database need know instantaneous changes time time hourly daily defined server structure development test production good testing strategy answer true use external source control example embercadero database change management tool http www embarcadero com 0,feature want could exist principle would efficient current database structures see max vernons answer sql server would either maintain set diff maps compare current db contents full backup specify base applications deduplicate large files make two full backups changed data actually stored like diff custom base exdupe example nice thing works set backup files fact starting 3rd full backup file pay incremental differential space usage space usage difference previous backup file first deduplicating storage similar behavior feature describe exist feature consumes budget causing features present one apparently make far enough priority list im sure would good seems like fairly esoteric 0
conditionally stop psql script based variable value lets consider following example start psql script db run truncate important table tried,option psql stops executing commands error error stop could raise error somehow would want problem test variable produce error somehow since one cant use control structures psql none idea use sql testing well producing error conditionally something pl pgsql quite good wrote function would generate error call function simple case structure simple example lets assume clarity function name database create replace function error generator returns boolean body begin raise meaningful error message return false aesthetical purposes end body language plpgsql set error stop begin test variable value notice var set fails well syntax error select case var error generator else 13,postgresql postgresql brings conditionals psql longer issue db run dev database truncate important table endif guess could also use db run dev database begin raise meaningful error message end language plpgsql endif 5,gut says performance gain get unlikely worth extra hassle potential bugs resulting needing enforce separation perform multiple lookups application logic lot small values querying none rest would see performance gain rows would fit page overall less pages need processed ram read disk given query soon need properties one go mix benefit going blown water either needing query multiple tables separately via union way sure course rig reasonably realistic large dataset run performance tests arrangements considering much doubt see change worth extra complexity data split logical ways ways implied business logic suggest look data partitioning particularly split partitions different drives whenever 0,may easiest solution however would love clarify things making snapshot running mysql instance could affect one file respsonsible secondary index manipulation ibdata1 system tablespace ibdata1 home classes innodb information data pages innodb file per table disabled index pages innodb file per table disabled data dictionary included list tables tablespace ids double write buffer provides checksum info prevent data corruption insert buffer changes secondary indexes redo logs undo logs pictorial representation pivotal classes would worry double write buffer insert buffer live snapshot either properly written introduce data corruption flush tables read lock halt writes ibdata1 one would think wrote subject used 0
whats difference cte cte column names shown using common table expressions msdn define cte expression name column name cte query,nearly answer one questions already msdn page line directly quote explains basic syntax structure cte expression name column name cte query definition list column names optional distinct names resulting columns supplied query definition emphasis added would mean would need specify column names situations would work test table noname cast function select cast1 char1 dbo casttochar1 select test table would test table select noname cast1 char1 cast dbo casttochar1 function select test table would distinct names columns test table select cast1 char1 dbo casttochar1 select test table 23,ultimately column needs valid name assign two ways column list cte foo select col tab select foo cte using original column names aliases cte select col tab select col cte alias column list column list aliases cte foo bar select col1 valid outer select col2 colx valid outer select tab select foo bar cte similar definition view derived table specify list column names column list lots complex calculations easier spot name theyre scattered source code easier got recursive cte assign two different names column original name aliases assign alias calculation want must rename column 7,anecdotally prefer name columns inside cte instead inside cte xxx as1 clause since youll never inadvertently mismatch names vs column contents take instance following example mycte select mt mt myschema mytable mt select mycte mycte mycte display shows contents column heading contents column heading realization never specify column names xxx clause instead like mycte select alias1 mt alias2 mt myschema mytable mt select mycte alias1 mycte alias2 mycte removes doubt column definitions totally unrelated side note always specify schema name referencing object names end statements semi colon 10,based post appears clear message rdbms lookup time optimizations replaced hardware makes io time negligible absolutely true ssd database servers combined high actual ram makes io waiting significantly shorter however rdbms indexing caching still value even systems huge io boon io bottlenecks poorly performing queries caused bad indexing typically found high workload applications poorly written applications key value rdbms systems general data consistency data availability data aggregation utilizing excel spreadsheet csv file method keeping data base yields guarantees ssd doesnt protect primary server become unavailable reason network os corruption power loss ssd doesnt protect bad data modification ssd doesnt make 0,since using standard edition cant use tde options using encryption keys instance database level sql server two kinds keys symmetric asymmetric symmetric keys use password encrypt decrypt data asymmetric keys use one password encrypt data called public key another decrypt data called private key sql server two primary applications keys service master key smk generated sql server instance database master key dmk used database also encryption column level creating master key encryption along create certificate create symmetric key example done described encrypt column data reference sql server database encryption keys database engine drive level using bitlocker drive encryption data protection 0,odds statistics date one instances causing crappy execution plans generated 0
optimistic locking faster pessimistic locking forms locking cause process wait correct copy record currently use another process pessimistic locking lock,duplicate question https stackoverflow com questions optimistic vs pessimistic locking copy pasting answer link optimistic locking strategy read record take note version number check version hasnt changed write record back write record back filter update version make sure atomic hasnt updated check version write record disk update version one hit record dirty different version abort transaction user start strategy applicable high volume systems three tier architectures necessarily maintain connection database session situation client actually maintain database locks connections taken pool may using connection one access next pessimistic locking lock record exclusive use finished much better integrity optimistic locking requires careful 8,misunderstand optimistic locking optimistic locking cause transactions wait optimistic locking possibly causes transaction fail without lock ever taken transaction fails optimistic locking user required start word optimistic derives exactly expectation condition causes transactions fail reason occur exceptionally optimistic locking approach says taking actual locks hope wont needed anyway turns wrong accept inevitable failure 11,second option flexible im sure youre worried tons tables usually would done single table contact types id pk name contact details id contact type id fk contact types id value organization contacts id pk contact detail id fk contact details organization id fk organizations populated like contact types id name web url facebook url phone contact details id type id value www stuff com facebook com profileid stuff organization contacts id contact detail id organization id schema tables tons many contact types want joins arent complicated contact details table large record per piece contact info unless tons contact info organization 0,first query get foreign key constraints database well owning table referenced table select name fkname object nameparent object id referencingtable object namereferenced object id referencedtable sys foreign keys want drop fk constraint youd something like alter table childtable drop fk childtable roottable rootid go warning drop foreign key constraint destroy referential integrity two tables carefully 0
execute stored procedure whose name variable given variable contains stored procedure name declare stored procedure name varchar512 set stored procedure,use variable directly exec stored procedure name please see bol reference execute 13,procedure name identifier name sql server follows rules database identifiers object names prefer use special data type sysname synonym nvarchar128 order execute youd need use dynamic sql using execute system procedure sp executesql ps see thomasstringers example execution ps2 later youll need add parameters best way execute sp executesql 7,sql server use clustered index plus sort algorithm instead using non clustered index even execution time faster latter case sql server uses cost based optimizer based statistics runtime info cost estimation process query actually evaluate lookup plan estimates take effort note estimated subtree cost hovering select execution plan thats necessarily bad assumption either test machine lookup plan takes 6x cpu sort scan look rob farleys answer sql server might cost lookup plan higher 0,quantity quality dealing tables size helps think fact table table think segment level collection discrete tables old enough remember rolling partitioning partition views helps tim gormans scaling infinity paper invaluable resource 0
natural keys provide higher lower performance sql server surrogate integer keys im fan surrogate keys risk findings confirmation biased many,general sql server uses trees indexes expense index seek directly related length key storage format hence surrogate key usually outperforms natural key index seeks sql server clusters table primary key default clustered index key used identify rows gets added included column every index wider key larger every secondary index even worse secondary indexes explicitly defined unique clustered index key automatically becomes part key usually applies indexes usually indexes declared unique requirement enforce uniqueness question natural versus surrogate clustered index surrogate almost always win hand adding surrogate column table making table bigger cause clustered index scans get expensive secondary indexes workload 18,believe best lies middle natural keys overview make data model obvious came subject area somebodys head simple keys one column char4 char20 saving extra bytes need watch consistency update cascade becomes critical keys might changed lot cases natural keys complex consists two columns key might migrate another entity foreing key add data overhead indices data columns might become large performance loose key large string probably always loose integer key simple search condition becames byte array comparison database engine cases slower integer comparison key multilanguage string need watch collations also benefits watchouts artificial identity keys overview need bother creation handling cases 10,key logical feature database whereas performance always determined physical implementation storage physical operations run implementation therefore mistake attribute performance characteristics keys particular example however two possible implementations tables queries compared example answer question posed title comparison made joins using two different datatypes integer character using one type index tree obvious point hash index type index used would quite possibly measurable performance difference two implementations fundamental problems example however two queries compared performance two queries logically equivalent return different results realistic test would compare two queries returning results using different implementations essential point surrogate key extra attribute table table also meaningful 4,look ms docs limitations restrictions top used insert update merge delete referenced rows arranged order order clause directly specified statements need use top insert delete modify rows meaningful chronological order must use top together order clause specified subselect statement see examples section follows topic update top500 instances set thing new thing something somethingelse update instances set thing new thing select top id instances order something t1 instances id t1 id update derived table directly without join update t1 set thing new thing select top instances order something t1 example approach found dbfiddle 0,database set offline alter database adventureworks2012 set offline go would indeed see logged message sql server error log setting database option offline database adventureworks2012 select sys messages language id text like setting database option database message id youre monitoring database option changes 0,pages read memory required free memory available oldest unmodified page replaced incoming page means execute query requires data fit memory many pages live short life memory resulting lot see effect looking page life expectancy counter windows performance monitor look https sqlperformance com sql performance knee jerk page life expectancy great details counter comments asked specifically happens results query larger available buffer space take simplest example select big table assume table 32gb max server memory mb configured 24gb 32gb table data read pages page buffer one time latched formatted network packets sent across wire happens page page could queries running time 0
possible take database offline backup using sql job scenario generate backup database sql server restore new server sql server taking,yoy couple options situation turn applications change data database set database single user mode back something like alter database dbname set single user rollback immediate backup database dbname disk locationandfilename restore redirect app new server 5,possible restore read db new server yes example create database readonlydb go alter database readonlydb set read backup database readonlydb disk share readonly bak destination server already read writeonline database name entirely sure mean could restore database different name would like remember remove read property afterwards restore database readonlydb2 disk share readonly bak move readonlydb datalocation readonlydb2 mdf move readonlydb log loglocation readonlydb log2 ldf stats go alter database readonlydb2 set read write could also replace existing database replace keyword possible restore offline db new server possible sql server removes handles database files able access offline alter database readonlydb set 11,think itzik ben gan ken henderson books may usable websites one favorite resource sqlservercentral com question day concerned sql may help improve knowledge 0,recommend use dbcc shrinkfile check available space available file using query answer link determine used free space within sql database files one data file one log file itll probably file file pass file number shrinkfile command necessary recommend small batches maybe gb time data file gb data gb following dbcc shrinkfile1 go dbcc shrinkfile1 go dbcc shrinkfile1 go would leave certain amount space free dont go way gb leave maybe gb free work operations within file shrink file block usually gets suspended operations working data file shrinking large chunk batch may take long time complete part unused space log file 0
unable create filtered index computed column previous question mine good idea disable lock escalation adding new calculated columns table creating,unfortunately sql server ability create filtered index filter computed column regardless whether persisted connect item open since please go ahead vote maybe microsoft fix one day aaron bertrand article covers number issues filtered indexes 20,create index whereclause possible creates filtered index specifying rows include index filtered index must nonclustered index table creates filtered statistics data rows filtered index filter predicate uses simple comparison logic reference computed column udt column spatial data type column hierarchyid data type column comparisons using null literals allowed comparison operators use null null operators instead source msdn 4,although create filtered index persisted column fairly simple workaround may able use test ive created simple table identity column persisted computed column based identity column use tempdb create table dbo persistedviewtest persistedviewtest id int null constraint pk persistedviewtest primary key clustered identity11 somedata varchar2000 null testcomputedcolumn persistedviewtest id persisted go created schema bound view based table filter computed column create view dbo persistedviewtest view schemabinding select persistedviewtest id somedata testcomputedcolumn dbo persistedviewtest testcomputedcolumn convertint next created clustered index schema bound view effect persisting values stored view including value computed column create unique clustered index ix persistedviewtest dbo persistedviewtest viewpersistedviewtest id 21,dont rely behaviour ssms historically buggy poorly documented also sometimes changes version version easiest way sure click script button top dialog compare output documentation example click script button adding several files ssms version comes sql server like see exact script used backup database mydatabase disk ng location file1 bak disk ng location file2 bak noformat noinit name ndatabasename full database backup skip norewind nounload stats go refer documentation backups see script creates striped backup mirrored backup would need mirror clause like backup database mydatabase disk ng location file1 bak mirror disk ng location file2 bak go dont know version ssms 0,anything dbo avoided default sql server also descriptive like default names since preknown makes hackers life much easier although theyre point theyre trying figure schema name youre probably already borked work use schemas divy database logical sections assign permissions schemas instance may inventory system database main tables might inv schema import anything database staging schema would used part import process system stored procedures users dont need access put sp schema 0,imagine may lot backup jobs full backups differential backups transaction log backups imagine need move backups local disk san use backup devices need create devices paths new disk backup scripts written use full paths need rewrite every job every job step change paths scenario need use backup devices example use custom scenario dynamically changes backup path file name example adding date time folder file name 0
resetting sql server sequence im process testing populating specific table leverages sequence object process im testing populating table tens thousands,try alter sequence foo fee restart alter sequence foo fee restart http msdn microsoft com en us library ff878572 aspx 15,using script slight changes create schema foo go create sequence foo fee start increment cycle cache go create table foo sample table data order number bigint primary key null sample column one nvarcharmax null sample column two nvarcharmax null sample column three nvarcharmax null go set nocount go insert foo sample table data order number sample column one sample column two sample column three values next value foo feeblahblah blahblah blah blah go select minorder number maxorder number foo sample table data stwd go drop sequence foo fee go drop table foo sample table data go drop schema foo reproduce 5,great question dont know answer imagine going goto internals team dont know theyll big site meantime help deduce answers starters see trigger cache detect metadata underlying objects changed trigger uses table table changed since trigger loaded cache trigger operates using outdated metadata makes think something going recompiling sql even monitoring metadata means engine concern token read block think thing engine prevent problems interaction server threads client issues statement server uses snapshot routines triggers available execution statement server calculates list procedures functions triggers may used execution statement loads proceeds execute statement means statement executes see changes routines performed threads im entirely 0,interesting question small test appears function modifications deletes transactional meaning isolation level transaction modifies deletes function transaction oblivious still uses old version function changes function become visible transaction commits transactions start commit isolation level irrelevant function tested reading data tables create function create replace function returns integer select immutable language sql create function tran begin begin select row tran begin begin drop function drop function commit commit tran select row commit commit commit select error function exist line select hint function matches given name argument types might need add explicit type casts slightly different scenario transactions try modify function one 0
system disk run space running heavy sql queries sql server im quite new sql server would grateful someone help restored,ssms query results cache drive default go tool options see attached change another volume storage fine 12,okay figured eric right path dialog said default path saving query results query results cached disk wrong local profile temp folder users username appdata local temp case checked doesnt appear obvious way turn caching takeaways avoid running ssms directly sql box dont select huge table ssms unless result set fit profile folder make sure sql server max memory setting configured correctly may may contributed problem respect page file growth 11,suffered issue reading answers found following tools options answer mine set drive yet watched query ran space drive dived 9gb 04mb killed query thought probably caching results large every row returned containing big chunk xml temp directory jon said however wasnt sure youd change change temp files written open environment variables edit user variables temp tmp set temp write temp confirm change watched query create large file temp directory 7,db ddladmin vs db owner tell tested read part list looks accurate except db ddladmin allow create schema confirm security permissions listed indeed denied denied ddladmin alter user backup database backup log checkpoint alter application role alter role drop database noting db datareader allow select access tables db datarwriter allow insert update delete access tables db executor allow execute access executable objects additonally db ddladmin role permissions may mean note since many different versions sql server may best small set users test initially see screams iron kinks etc objects role owned dbo may deal ownership chaning issues theres ever problem 0,another way cte select username col2 cnt count partition username null countcol2 partition username yourtable select username col2 cte cnt null null 0,would want sql limits amount servers could get results instead would create powershell script like servers server1 server2 query select database version databaseversion dbo awbuildversion results comment loop servers foreach server servers databases get dbadatabase sqlinstance server object name like adventureworks write host going server server loop databases foreach database databases reset variables result data null comment null database tables name contains awbuildversion write host querying database database data invoke dbasqlquery sqlinstance server database database name query query else comment could find table awbuildversion result pscustomobject server server database database name databaseversion data databaseversion comment comment join results result results 0
full back network failed due unknown network error recently seeing unknown netowrk error saves full back ups file share servers,network reliability issues speak network san team use different file share doesnt exhibit problem back file temporarily local sql server system assuming adequate space copy backup network share typically using tool resume interrupted transfers delete original copy confirmed usually includes validating backup restored copy fails big deal start copy havent lost anything agree max doesnt seem something altering session timeout fix thats kind like changing tire headlight might better prepared next pothole youre still driving one light 4,aarons answer provides great advice work around issue thought id add answer provide technical details according microsofts err exe utility error defined winerror unexpected network error occurred problem session timing would almost certainly seeing status network session expired error 0xc000035c microsoft docs states resolution clients session expired therefore client must authenticate continue accessing remote resources registry value mention question hklm system currentcontrolset services lanmanworkstation parameters relates lanmanworkstation windows service also known simply workstation via services control panel applet service initiates network connections workstation end therefore need modify registry value youd modify client case sql server machine microsoft docs registry value 4,several problems main issue cmd shell executes line comes via return wait end command indicator attempt figure via parsing like sql upon batch submitted since way know return ends command typically use line continuation seem work within input paramter least one unclosed quote need reduce everything single line simple enough replace carriage return character empty string line feed newline character space alone work since powershell statements need separated either newline semicolon currently newlines used replace spaces semicolons required hence step append semicolon actual powershell command probably good form general sql step would two replace calls ps variable transform single line 0,thats pretty much since null anything else null wrap column isnull coalesce case 0
combined index implemented sql server understand single indexed column works sql server implemented using balanced trees plenty interesting videos youtube,store rows tree perform seek needed ordering rows sorted like sort category also sort tuple category offerstate latter case rows first sorted category ties broken sorting offerstate resulting index use tree structure value entry tree category offerstate tuple speedup query like query sql server perform seek following way seek first row matches category done using tree seek familiar sql server needing use category portion category offerstate tuple begin reading rows continue row offerstate found way sql server able seek directly beginning range rows needed read rows stop end range rows note see seek works looking seek predicate property index seek 5,traditional indexes implemented trees whether single multi column key speedup query like select id ranking items category offerstate order composite index keys significant query optimization specify columns used equality predicates first order selectivity followed columns used inequality predicates facilitate touching rows match equal condition access rows within specified range query column order index category offerstate rather vice versa 4,challenges question indexes sql server following efficiently logical read check row exists check row doesnt exist find next row starting point find previous row starting point however used find nth row index requires roll index stored table scan first rows index code heavily relies fact efficiently find nth element array cant think algorithm isnt usable sql without data model change second challenge relates restrictions binary data types far tell perform addition subtraction division usual ways convert binary64 bigint wont throw conversion errors behavior defined conversions data type binary data types guaranteed versions sql server addition lack conversion errors somewhat problem 0,thanks matthewhs answer managed compose query would work searching large number users use mysql select user host user user like user note user whatever component username need search may need sides dont know start username also merely note variable dont include actual query unless username includes 0
justify using nolock hint every query ever justify using query hint seeing nolock every single query hits busy server point,discussed https stackoverflow com questions using nolock hint ef4 https stackoverflow com questions happen result using nolock every select sql sever define busy high volumes 50k new rows per second large aggregates etc dont see need get dodgy data 11,pick battles battles like cant easily system every dml hinted rowlock hint irrespective modifying one row several thousand rows showed several examples really hurts performance system already working resistance change note convinced enough use going forward though nolock place recommend good references showcasing troubles using microsoft sql server development customer advisory team blog previously committed rows might missed nolock hint used itzik ben gan sql magazine clustered index scans part iii itkiz ben gan sqlpass org beware nolock hint 17,explain colleagues importance understanding isolation levels show examples nicest easiest explanation found little kendras poster isolation levels ask think need nolock hint dont use set transaction isolation level statements ask exactly situation want fix maybe deadlocks blocking etc dont want hold locks might consider snapshot isolation level asking clear picture 9,general depends one case cte nicer derived table need reference several times query silly example select xy select xy select maxx vs cte xy select xy select xy cte select maxx cte different database engines treat multiple references cte different ways sql server cte generally fully evaluated reference 0,find default directory depending install instance red hat var log mysql query time time seconds start recording done startup runtime log slow queries var log mysql mysql slow log long query time 0,according documentation also cant start pg reserved looks fairly freeform 0
regenerating cube relational schema visual studio analysis services cube project company received outside contractor im trying get developers work local,thats works dsv data source view generated defining tables queries want use cube flow create one ds data sources defining connect source databases create dsv data source view adding tables named queries defining get data ds create dimensions cubes defining data dsv needs look multidimensional model process dimensions cubes load data source system technically suppose could recreate source systems schema analyzing xml dsv file contains data types table names queries could theoretically map back recreate source databases would require lot manual work writing something parse xml dsv contains named queries would still need analyze sql queries reconstruct underlying table depending 8,time need use schema generation top development never generated schema previously generated sql schema script objects ssms provide script along ssas project developers new developer would deploy sql scripts edit data source ssas project point database straightforward approach believe 4,answer first question tools default walk id index fetch data write disk yes tools similarly impact working set would generally recommend running secondary preferably hidden secondary possible ill echo stennie comments recommend backup methods dealing large amounts data second question assume looking mongodump equivalent fields option mongoexport dump specific fields query option used filter results used projection select fields returned feature request tracked tools yet scheduled stennie also mentioned option write custom exporter fits needs would still recommend running secondary protect working set 0,isnt performance gain recoverabily gain made file corruption happens system tables database lost keep user data separate file group groups restore files keeping rest database online restore assuming enterprise edition state cant say would benefit multiple file groups system objects primary filegroup however kick junk saying autoshrink enabled 0
mysql set utc time default timestamp set timestamp column whose default value current utc time mysql uses utc timestamp function,specify utc timestamp default specify automatic properties use default current timestamp update current timestamp clauses also insert utc timestamp values like though table create table test ts timestamp null default current timestamp update current timestamp insert query would like insert utc timestamp insert test ts values utc timestamp 4,go along ypercubes comment current timestamp stored utc retrieved current timezone affect servers timezone setting default time zone option retrieval allows retrieval always utc default option system system time zone set may may utc mysql select global time zone session time zone global time zone session time zone system system row set sec mysql select current timestamp current timestamp row set sec set dynamically mysql set session time zone query ok rows affected sec mysql select global time zone session time zone global time zone session time zone system row set sec permanently cnf mysqld variables default time zone restart 49,solution trigger delimiter create trigger update utc insert table row begin set new field utc timestamp end delimiter every new inserted row timestamp utc 4,cant reference alias clause order sql server parses statement many discussions stackoverflow couple examples give background https dba stackexchange com questions select clause listed first queries parsed way disallows use column aliases clauses alternative would select dv name maxhb dateentered de devices dv inner join heartbeats hb hb deviceid dv id group dv name maxhb dateentered 0,great explanation richard wanted add links official documentation topics written sql server much concepts remain today understanding avoiding blocking understanding locking sql server edit additions five ways fight blocking video fresh video kendra little published today dba detective troubleshooting locking blocking rodney landrum identify blocking problems sql profiler brad mcgehee well known sql server authors mvps 0,lose one seconds worth transactions default value helps keep innodb acid compliant according mysql documentation innodb flush log trx commit value innodb flush log trx commit log buffer written log file per second flush disk operation performed log file nothing done transaction commit value default log buffer written log file transaction commit flush disk operation performed log file value log buffer written file commit flush disk operation performed however flushing log file takes place per second also value note per second flushing guaranteed happen every second due process scheduling issues default value required full acid compliance achieve better performance setting 0
tools identifying needed indexes want create best indexes table database query tool sql server help process,tool talking called database engine tuning advisor dta short take number inputs give recommendations things like missing indexes worth noting though suggestions careful consideration taken instead blindly following dtas recommendations link dta tutorial also worth noting built dmvs gathering type information sys dm db missing index columns sys dm db missing index details sys dm db missing index group stats sys dm db missing index groups 7,yes tools like database engine tuning advisor ships sql server pretty nasty things allows consider small workload even single query suggest indexes may help small workload regard whatsoever rest workload indexes help speed queries slow others especially insert update delete never mind rest business cycle often recommend redundant indexes differ trailing column included column really really really likes included columns think may fetish proper approach index tuning consider entire workload complete business cycle sometimes better turn 3rd party tools reinvent wheel dbsophic makes free product called qure workload analyzer think much better job dta licensed tool qure workload optimizer absolutely 10,prefer one file per backup db full yyyymmddhhnn bak db diff yyyymmddhhnn bak db log yyyymmddhhnn bak dont want query backup device see stored copy entire backup device get one database ftp otherwise shift need around place tb size database partitions filegroups want partial backups restores problems multiplied dr situation want everything simple clear large small shops ive simpler deal self describing files device concept goes back sybase sql server earlier disk devices separate mdfs ldfs introduce sql server 0,georges answer solve problem leaves wide open sql injection attacks converting int varchar likely going cause issues sysname equivalent nvarchar jam lot extra code make code totally safe youd want declare stringsvar nvarchar1000 declare emp id int declare strvar sysname nemployee test set stringsvar nselect quotename strvar emp id iemp id print stringsvar exec sys sp executesql stringsvar iemp id int iemp id emp id using sp executesql issue parameterized dynamic sql quotename make table name non executable code much safer choice little reference id suggest heading curse blessings dynamic sql 0
advice date column sql server two datetime columns sql server need query without time portion datetime currently query looks something,steps use dateadd datediff technique per get current date without time part likely unable index computed column wont deterministic varchar method 5,able use computed columns index deterministic going static value may depend get value computed column itll work 4,encountered problem case able fix setting random page cost low value query set random page cost course set random page cost low may want keep way could locally single transaction begin set local random page cost select id content tsvectordanish text tsquerydanishfelter commit case cleared issue allowed hack query force bitmap scan solve problem could build function well create replace function matched idsin text text returns setof content id type set local random page cost select id content tsvectordanish text tsquerydanishin text language sql call using select matched idsfelter 0,instead using huge list join values expression list large enough use temp table index join itd nice postgresql could internally automatically point planner doesnt know similar topics https stackoverflow com https stackoverflow com 0
convert right side join many many array using join many many relationship result split multiple rows id like convert right,need add group clause use array agg select id title array aggi title items inner join items tags item id id inner join tags id tag id group id title 5,first typo query 3rd column would title added aliases clarify select id title item title title tag title items join items tags item id id join tags id tag id aside id title typically distinctive useful identifiers implement many many relationship postgresql querying items typically substantially faster aggregate first join select id title item title tag array items join left join select item id id array aggt title tag array items tags join tags id tag id group item id using id use left join outer query items without tags excluded inner join joining aggregation also gets hands one table 12,first size smallint two bytes twice size bool select pg column size1 bool bool pg column size1 smallint si bool si lets create small table rows check create table foo select case smallint else smallint end si case true else false end select random generate series11e3 create index si foo si create index foo see table 40kb test dt foo list relations schema name type size public foo table kb indexes size 40kb self joins returning rows case using seq scan index scan set enable seqscan complete amount time explain analyze select foo join foo f2 using explain analyze select 0,ms tech talk netherlands discussed stuff starts slowly gets meat hadoop around minute mark gist depends sensibly arranged least somewhat easy partition set data least somewhat homogeneous fairly easy scale high data volumes rdbms depending upon youre hadoop mr seem geared situations forced large distributed scans data especially data arent necessarily homogeneous structured find rdbms world limitations big data solutions bound biggest limitation theyre bound make rigid schema ahead time big data solutions shove massive amounts data box add logic queries later deal lack homogeneity data developers perspective tradeoff ease implementation flexibility front end project versus complexity querying less immediate 0
one better performance approach store rows varchar column optiona tablea tablea id pkint tableb tableb id pkint tablec tablec id,option bad choice answer question performace perfomance wise option better choice option simple query option need run complex sql query option offers better indexing options turn offers better performance option neat clean design 4,option worst choice keeping values csv format single column violating first nf http en wikipedia org wiki first normal form critical drawback violation first nf complex join conditions apply join using like operators worst wildcard options using string functions secondly rdbms able create correct data distribution statistics ultimately query optimizer cant select efficient execution plan 7,database check constraints effectively better code integrity constraints help database find effective execution plan application sees read consistent view therefore hardly guarantee uniqueness database also see non commited data 0,three things immediately spring mind master data services part sql server arsenal roll build central database server store data servers link server information matches single point truth description may entirely practical use software automate rollout deployment process red gates deployment manager multi script tool edit quite forgot dont know blame early morning lack caffeine replication another part sql server arsenal mostly used data used objects stored procedures functions 0
row number without partition still generates segment iterator im writing upcoming blog post mine ranking aggregate window functions specifically segment,found year old blog post mentioning behavior looks like row number always includes segment operator whether partition used guess would say makes creating query plan easier engine segment needed cases cases needed essentially zero cost non operation lot simpler always include plan windowing function used 12,according showplan xsd execution plan groupby appears without minoccurs maxoccurs attributes therefore default making element compulsory necessarily content child element columnreference type columnreferencetype minoccurs maxoccurs unbounded making optional hence allowed empty element manually attempt remove groupby force plan get expected error msg level state line xml validation invalid content expected elements http schemas microsoft com sqlserver showplan groupby http schemas microsoft com sqlserver showplan definedvalues http schemas microsoft com sqlserver showplan internalinfo found element http schemas microsoft com sqlserver showplan segmentcolumn instead location showplanxml batchsequence batch statements stmtsimple queryplan relop sequenceproject relop segment segmentcolumn interestingly found manually remove segment operator 11,table name prefix good reasons consider tablea id int identity stringdata varcharmax tableb id int identity stringdata varcharmax want delete tablea records exist tables easy enough inner join delete tablea inner join tableb id id wiped tablea inadvertently compared bs id every record matched every record got deleted fields named tableaid tablebid would impossible invalid field name tableaid tableb personally issue using name id table really better practice preface table name entity name tablea people peopleid would work fine avoid accidentally comparing wrong field blowing something also makes obvious fields come long queries lots joins 0,theyre integers try select select convertnumeric1 convertnumeric2 0
possible backup restore part database sql server sql server database regularly transfer client site takes long time dont direct connection,honest easiest backup restore copy locally remove unwanted data copy delete truncate table drop ship copy wouldnt bother filegroups added complexity noted 15,way speak needing primary file group sounds like want database structure small amount data would suggest want update database structure objects tables etc simply script database objects done quickly easily powershell data actually need regenerated simply export powershell script could sure export file compressed small size transfered connection site course like one gbn suggestion could scripted would determine option takes least amount time 7,subject external regulatory controls standards sox pci etc fail audit failing monthly pci checks developers db owner network sql server development sql servers network standard build service account sa one confers rights service account local admin developers full control servers upsides 0,experience decision making process made prior afaik hasnt world changers sql server installation within ms sql server product potential problems rolling software millions lines code something bad could happen youre stuck rollback option however alternatives place could consider making snapshot system restore elsewhere perform upgrade see happens test give lot comfort doesnt absolutely guarantee problems arise prod box however option available back sql days would assume worst case scenario place upgrade fails miserably recover within rto rco business understand risks plans place mitigate business ok dont would advice 0
function use timestamp timestamp stored bigint format turn human readable format points date may 11th however function timestamp turns id,timestamp expects parameter given seconds value miliseconds divide select timestamp1462975819 gives 8,seems like storing milliseconds since adams answer right another way would use interval addition epoch timestamp select timestamptz epoch interval millisecond timestamp tested test select timestamptz epoch interval millisecond timestamp timestamp row adams test select timestamp1462975819250 timestamp row epoch timestamp test select timestamptz epoch start times start times row hour diff local settings utc related jaca docs class timestamp gettime public long gettime returns number milliseconds since january gmt represented timestamp object overrides gettime class date returns number milliseconds since january gmt represented date see also settimelong 6,microsoft docs page sql server statistics states operations rebuilding defragmenting reorganizing index change distribution data therefore need update statistics performing alter index rebuild dbcc dbreindex dbcc indexdefrag alter index reorganize operations query optimizer updates statistics rebuild index table view alter index rebuild dbcc dbreindex however statistics update byproduct creating index query optimizer update statistics dbcc indexdefrag alter index reorganize operations 0,cant make queries optimiser friendly understanding works complex beast necessary know inside id suggest something time critical fix query rather changing sql server operates example youd want know query starts scaling less efficiently data volume data distribution changes giving time optimiser adds value 0
oracle import problem caused different character sets im trying import oracle export oracle xe get following messages import xe fehlerhaft,dont choice character set xe change suit database trying import would practical migrate source database export import work character set conversion might mean text columns non ascii characters wont look import rows rejected long new character set case converting utf8 mean possible single byte character grow conversion theory may need increase column size export adjust target schema import data separate step see possible data truncation problems 4,actual ddl using create table could use nls length semantics parameter set char rather default byte varchar25 allocated enough space store characters database character set potentially bytes rather bytes could allow character unfortunately changing nls length semantics probably wont terribly helpful youre relying import process create table dump file inherently add char byte keyword would actually issue statement create table bdata artikel key varchar23 byte null name varchar260 byte null abkuerzung varchar25 byte null 8,use solution already mentioned another stackoverflow post ref https stackoverflow com mysql alter table add column exist set dbname database set tablename tablename set columnname colname set preparedstatement select select count information schema columns table name tablename table schema dbname column name columnname select concat alter table tablename add columnname int11 prepare alterifnotexists preparedstatement execute alterifnotexists deallocate prepare alterifnotexists 0,first thing thought setup db servers identical hw os configs installed mysql percona mariadb get fourth server installed monyog eval version lasts days register db servers monyog use performance metrics charts monyog set charts monyog use sysbench db servers discretion basic outline believe use monyog right box check testimonial monyogs website look name page eventaully convinced company purchase utlimate version outright testimonial watched happened caveat get monyog db servers sysbench ready get best day usage monyog update edt keep innodb buffer pool small default innodb buffer pool size 8m keep binary logs disabled include log bin cnf metrics measured innodb 0
limit results first ranking rows sql server using rank partition col2 order col3 desc return data set rank hundreds records,select select subject name rank partition subject order score desc rn table rn 16,could put original query using rank subquery wrap query filters results 15,hard look table many columns frequently use select statement look fields mentally categorize group fields categorize group create view 0,problem definitely network related based info dealt network professionals one things might help faster nic cards sql server adding allocated specific nic card subnet servers web server sql server web server sub net sql server routers bridges etc many possible changes sql server output data sent sql server proprietary ms tds protocol default size tds buffer kb see msdb network packet size option compressing data sql server external application depends upon nature data using default size see stats tds packets received server 4mb 1k 4kb yes size tds buffer changed see google tds protocol batch size good discussion topic sqls 0
recover postgresql database wal errors startup im trying set openstreetmap server ubuntu machine using ubuntu packages listed switch2osm org initially,clear wal files see pg resetxlog data directory ubuntu var lib postgresql main note pg resetxlog located usr lib postgresql bin usr bin necessarily path also run postgres user clear recreate entire cluster dont care data run pg dropcluster main recreate cluster pg createcluster main 4,update looks like bug debian ubuntu packaging postgresql init scripts extremely unsafely kill postmaster remove postmaster pid see post pgsql general see https bugs launchpad net debian source postgresql common bug http bugs debian org cgi bin bugreport cgibug personally ive gone edited init scripts get rid rather hairy dangerous code original answer please go back logs restart see find errors wal corruption absolutely happen important look upload copy whole log pastebin something thatd really handy time wal corruption accepted possibility postgresql running fsync set postgresql conf system crashes unexpectedly loses power thats cause itd really good look happened please 5,extract transform load preparation foreign data inserted database data warehouse looking basics etl noted data warehouse designer bill inmon notes upon time distant past etl extract transform load software wanted build data warehouse write code order get data one source appropriate target lots code lots repetitive code wrote code maintain every time legacy system changed manual maintenance code every time target definition changed manual maintenance code every time end user wanted something new maintenance code plethora etl products proliferated inmon describes brief history etl products popular software tools designed extract data changing systems transform according specific rules load data warehouses 0,many mysql temporary tables potential issues table open cache tunables could likely hit limit ram bloat could likely use ram leads swapping really bad would worry many unless really hundreds temp tables many threads show processlist ignore sleep running even busy system rarely implicit temp tables lets see selects causing may possible redesign queries avoid temp tables dont like using ram disk takes ram away caching possibilities runs risk hitting hard limit disk size 0
combine column multiple rows single row ive got customer comments split multiple rows due database design report need combine comments,relatively trivial correlated subquery cant use coalesce method highlighted blog post mention unless extract user defined function unless want return one row time typically declare table id int row num int customer code varchar32 comments varchar32 insert select 11dilberthard union select 12dilbertworker union select 21wallylazy select id customer code comments stuffselect comments x2 id id order row num xml path group id customer code order id case data comments could contain unsafe xml characters change xml path elaborate approach xml path type valuen text nvarcharmax sure use right destination data type varchar nvarchar right length prefix string literals using nvarchar 18,youre allowed use clr environment tailor made case user defined aggregate particular probably way go source data non trivially large need type thing lot application strongly suspect query plan aarons solution scale well input size grows tried adding index temp table didnt help solution like many things tradeoff politics policy even using clr integration clients environment clr function likely faster scale better given real set data clr function reusable queries wont duplicate debug complex subquery every time need type thing straight sql simpler writing managing piece external code perhaps dont know program vb etc edit well went try see actually 6,seems like storing milliseconds since adams answer right another way would use interval addition epoch timestamp select timestamptz epoch interval millisecond timestamp tested test select timestamptz epoch interval millisecond timestamp timestamp row adams test select timestamp1462975819250 timestamp row epoch timestamp test select timestamptz epoch start times start times row hour diff local settings utc related jaca docs class timestamp gettime public long gettime returns number milliseconds since january gmt represented timestamp object overrides gettime class date returns number milliseconds since january gmt represented date see also settimelong 0,heres another xquery solution select cs allvalues query contains positive columnset cs cs allvalues exist contains positive since changed sample data wont work could instead select cs columnset cs cross apply cs allvalues nodes xc existtext positive im sure compete mikaels answer scale 0
find current scn given version oracle find current scn maximum possible scn,current scn oracle 9i select dbms flashback get system change number current scn dual oracle 10g select current scn database scn limits scn hard limit imposed format soft limit imposed artificially oracle described ive quoted relevant portions emphasis added hard limit architects oracles flagship database application must well aware scn needed massive integer bit number would take eons oracle database eclipse number transactions cause problems might think soft limit soft limit derives simple calculation anchored point time years ago take number seconds since multiply figure current scn value well processing continues normal put simple terms calculation assumes database running constantly 16,query came check databases sanity regarding scn bug issue show amount scn keyspace used far database default scn max 10g 11g instance bit integer select name current scn pct scn keyspace used roundsysdate created days since db creation round1 current scn sysdate created est days scn exhausted round1 current scn sysdate created est years scn exhausted database databases use db links exhausted mark continue current rate years without issue mean safe someone tickling scn bug least didnt find database way higher others close limit 6,cost based optimizers work via variety proprietary algorithms read open source databases typically work assigning reference operation value example sql server operation cost estimate takes 320th second reference computer developers desk redmond costing relative guess expensive query many rdbmses use cost establishing priority case deadlocks kill cheaper queries take less time run guess based information query optimizer disposal time query run peter correct best hope running benchmark queries ideal scenarios using base best guesses deal lot different points contention rdbms difficult specifically determine given query perform real world 0,amazes nobody mentioned benchmarking evancarroll came along excellent contribution would spend time yes know precious commodity setting systems running think get end user input say common queries thoughts nosql solutions work well particular use cases frequently inflexible ad hoc queries amusing take nosql brian aker former chief architect mysql see agree mr brownstone data eminently suited relational solution opinion confirmed evan carroll commit expenditure would disk technology would spending money disposal nas san maybe ssd disks hold rarely written aggregate data first would look available run tests show results decision makers already proxy form ecs work quick test two whipped 0
possible keep max number records postgresql basically part postgresql table used keep server access logs sometimes production get pretty large,define trigger maintain desired row number create replace function trf keep row number steady returns trigger body begin delete many rows select countid log table rownum limit assume id auto incremented value log table delete log table id select minid log table end end body language plpgsql create trigger tr keep row number steady insert log table row execute procedure trf keep row number steady probably best performing option reach limit never exceeded space fluctuation check row number periodically delete excess rows beginning edit really large logs say million per month partitioning easiest solution simply drop unnecessary tables say maxtimestamp 12,created generic table independent function create replace function keep row number steady returns trigger body declare tab text keyfld text nritems integer rnd double precision begin tab tg argv keyfld tg argv nritems tg argv rnd tg argv random rnd executeformatdelete select order desc limit offset tab keyfld keyfld tab keyfld nritems end return null end body language plpgsql create trigger log table keep row number steady trigger insert log table statement execute procedure keep row number steadylog table id function takes parameters tab table name keyfld numeric progressive key field nritems number items retain rnd random number bigger frequent 4,know old new record variables row triggers transition tables statement equivalents theyre tables old new tuples triggers see changed 0,understanding incorrect believe confusing case tipping point issue issue worth reading understanding well sql server seek operation uses tree start stop reading rows specific values theoretically seek read every row exact order scan would oracle maybe rdmss operation reading multiple rows defined start stop values called range scan heres quick demo seek reads every single row drop table exists dbo seektest go create table dbo seektest id int primary key clustered filler char8000 get row per page insert dbo seektest id values go set statistics io one scans select dbo seektest one seeks select dbo seektest id remember seek read 0
denali sequences supposed perform better identity columns answer better identity columns generated unique id values mrdenny says sql denali comes,ill answer well internals identity sequence work identity sql server pre caches values memory readily available see martin smiths answer details values used background process generates values imagine pool run pretty quickly leaving application mercy background process generating values sequence sql server allows define large cache sql server doesnt actually keep values cache keeps current value top end value greatly reduce amount io needed create values dont set cache high reduce number numbers used sql server crash values specified current cache range werent used would lost row insertion specify default value column like default next value audit eventcounter 37,since itzik ben gan article written hardcoded cache size identity seems changed comments connect item size pre allocation based size data type column identity property defined sql server integer column server pre allocates identities ranges values bigint data type server pre allocates ranges values sql querying book contains following table emphasises values documented guaranteed unchanged datatype cachesize tinyint smallint int bigint numeric article tests various sequence cache sizes insert batch sizes comes following results appears show large inserts identity performs sequence doesnt test cache size however also results one test looking specifically cache size various batch sizes inserts got following 21,always avoid use subqueries either select block block makes code dirtier sometimes less efficient think elegant way find times greater time row join idtimes table constraining join id times greater time current row use left join avoid excluding rows times greater one current row select i1 id i1 time time i2 time greater time idtimes i1 left join idtimes i2 i1 id i2 id i2 time i1 time problem mentioned multiple rows next time greater time id time greater time null null find rows greater time greater next time best way filter useless rows find times time greater greater time 0,use set identity insert tablename remember turn identity one table identity insert one time 0
shrinking database insert update process archiving wherein take backup current database restore xxx archive database database contains previous data insert,instead backup restore consider moving data across new presized database creating new indexes data keep file size small indexes unfragmented also make sure compression turned 4,database used reporting thinking shrinking database one time sensible shrink ok disk space cheap increases index fragmentation side effect shrink operation advisable shrink using dbcc shrinkfile maintenance window minimal activity shrink database chunks using script options consider recovering space another option would script database bcp bcp data take backup restore route make sure enable backup compression source server instant file initialization source destination server help cut restore time process change recovery model simple backup database move database pseudo simple state planning shrink database sure related shrinking database shrink reorg take backup since going use database reporting keep database read mode 4,key lookup required get referenced query assume used calculate comp columns referenced query plan used seek t2 also query use index t2 t1 optimizer decided scanning clustered index cheaper scanning filtered nonclustered index performing lookup retrieve values columns explanation real question optimizer felt need retrieve index seek would expect read comp column using nonclustered index scan perform seek index alias t2 locate top record query optimizer expands computed column references optimization begins give chance assess costs various query plans queries expanding definition computed column allows optimizer find efficient plans optimizer encounters correlated subquery attempts unroll form finds easier reason find 0,unfortunately design taken bol page revert database database snapshot limitations restrictions reverting unsupported following conditions database must currently one database snapshot plan revert read compressed filegroups exist database files offline online snapshot created alternative could drop first snapshot db basis understand seems limiting look way snapshots sparse files based original data files reverting specific snapshot would invalidate snapshots anyway base data files would changed revert operation limitation annoying doesnt look unreasonable 0
tracking stored procedure usage besides using sql server profiler way track stored procedures used least last executed,look plan cache get pretty good idea stored procedure usage take query instance select db namest dbid database name object namest objectid name size bytes size kb usecounts st text sys dm exec cached plans cross apply sys dm exec sql textp plan handle st objtype proc st dbid db idsomedatabase order usecounts desc give usecounts stored procedures cached pertaining somedb note plan cache contains execution plans retention plans many factors involved whereas give good idea used often definitely running total stored procedures often executed bol reference plan cache 17,look well contains info last execution time every stored procedure select db namedatabase id object nameobject iddatabase id cached time last execution time execution count sys dm exec procedure stats 10,likely depends trigger stops working triggered doesnt run completion put something trigger message exception block disabled entirely youd find characteristic thats true running eg triggers populate sort materialized view thats difficult compute fly cron job checks see recent record table old reports dont something obvious like could always table thats tracking triggers last run update current time course neither really help case justin cave mentioned commands misinterpreted variable name resolution could put else stepsysid branch put reporting something never actually happen 0,block pasted seems answer question depends logic used hide future transactions transaction id xid counter limited bits ever reaches next number instead replacing old max transaction starts fresh zero well zero postgresql hiding transactions even though transaction happened seconds ago postgresql thinks wont happen another transactions 0
get next value sequence could somebody tell wrong obvious query db2 select next value schema name sequence name result im,db2 select next value schema name sequence name sysibm sysdummy1 trick 4,need select something another option db2 values next value schema name sequence name noted also use sysibm sysdummy1 create virtual table db2 select next value schema name sequence name values 6,collation nvarchar nchar ntext column bearing encoding used store data column nvarchar data always utf little endian le collation nvarchar data impacts sorting comparison collation impact encoding varchar data since collation determines code page used store data column variable literal arent dealing sepupic mentioned seeing view data binary form difference endianness oracle using big endian sql server using little endian however seeing view binary form string oracle data actually stored using al32utf8 utf encodes character bytes e2 a6 also possible hashes rows include unless hashing oracle done without conversion hence using utf encoding hashing sql server accidentally converting varchar first 0,addition comments database given table updated one applications code paths placing appropriate constraints database means applications wont duplicating constraint code benefits simplifying maintenance reducing number places change data model change ensures constraints consistently applied regardless application updating data 0
see list databases oracle equivalent mysql show databases statement possible find databases cluster databases present network system could analyze files,equivalent mysql show databases statement thing query listeners machine lsnrctl status see services registered doesnt map one one database could multiple listeners machine otherwise tools commonly used connect one database instance instance belongs single database youre talking oracle rac clusters instance knows peers instances service database find instances currently started database using gv instance view also use crsctl utility list services including databases registered cluster status youre talking another vendors clustering software im pretty sure types resource management utilities query youre talking bunch machines theres reliable way enumerating databases network find active started databases look pmon processes unix theres one 22,simply speaking direct analogy mysql databases cluster oracle closest match schema still different apparently going change 12c introduction pluggable databases dividing database operations cleanly users content major transformation traditional database architectures oracle 11g predecessors could run one database time kyte said organization wanted run multiple databases single server would need run multiple instances oracle 11g one database name implies pluggable databases allow multiple tenant databases run one copy oracle 12c 11,oracle databases schemas list select username users order username something like select tablespace name user tablespaces 12,think answer future browsers nix could cat etc oratab 6,looking ini two suggestions suggestion would bump following settings ini sort buffer size 4m join buffer size 4m make joins sort stay memory course join order needs 4m page disk myisam table login root localhost restart mysql net stop mysql net start mysql login root localhost restart mysql use settings run mysql client set fourmegs set global sort buffer size fourmegs set global join buffer size fourmegs suggestion since data drive may disk drive please run query mysql show variables like tmpdir variable name value tmpdir windows temp row set sec since run mysql desktop defaults temp tables written drive 0,found reference recovering disk space section table need reclaim excess disk space occupies need use vacuum full alternatively cluster one table rewriting variants alter table commands rewrite entire new copy table build new indexes 0,using parameters achieve basically thing klins answer without creating user defined types move variables declare block argument list parameters create replace function get user info id varchar banned boolean reputation integer vip boolean completed games integer returns clause necessary output structure controlled parameters returns xxx body begin select true banned pref ban id id select countnullifnice false countnullifnice true reputation pref rep id id select vip vip pref users id id select completed completed games pref match id id return statement necessary output values already stored parameters return xxx end body language plpgsql return record exactly one select values normal 0,phpmyadmin perhaps could select phpmyadmin operations tab phpmyadmin click table want reset change auto increment value click operations tab table options box find auto increment field enter new auto increment starting value click go button table options box since one frequently asked questions phpmyadmin learn blog http trebleclick blogspot com mysql set auto increment phpmyadmin html supplemental info empty table another way reset auto increment attribute run truncate table mydb tablename dont run data want hose data guest phpmyadmin click sql tab enter command run nonempty table may want adjust auto increment attribute highest existing id use case higher entries 0
varchar datatype allow unicode values table varchar column allowing trademark copyright unicode characters shown create table varcharunicodecheck col1 varchar100 insert,trademark registered symbols unicode characters wrong strings contain ascii characters simple test shows characters ascii extended ascii ascii codes declare varcharunicodecheck table col1 varchar100 insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany select rightcol1 1as last char asciirightcol1 last char ascii varcharunicodecheck clearly see characters byte encoded yes pure ascii characters extended ascii show real unicode character trademark code binary representation declare table uni ch nchar1 ascii ch char1 insert values select unicodeuni ch unicode asciiascii ch ascii castuni ch varbinary10 uni ch 15,comments agree extended ascii really bad term actually means code page maps characters code points range beyond standard code point range defined ascii sql server supports many code pages via collations non ascii characters stored varchar long underlying collation supports character character stored varchar char columns sql server collation code page greater query bellow list select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name subset also support character column collation need one following support select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name 7,definition varchar says allows non unicode string data trademark registered symbols unicode characters definition contradicts property varchar datatype answers incorrect think would help point confusion base terminology emphasized two words quote question example confusion sql server documentation speaks unicode non unicode data talking characters speaking byte sequences represent certain characters primary difference unicode types nchar nvarchar xml deprecated evil ntext non unicode types char varchar deprecated evil text types byte sequences store non unicode types store one several bit encodings unicode types store single bit unicode encoding utf little endian answers mentioned characters stored bit non unicode encoding depends code 4,yes one bit column table storage uses byte bit columns stored byte next free respect also bit per column storage need null bitmap rounded next byte data pages contains bit columns irrespective whether allow null exception nullable columns added later metadata change via alter table row yet updated 0,right click green text showing suggested index select missing index details open new tab ssms index create script 0,file doesnt mean generate bind token first session somehow publish join transaction another session eg spid trancount rollback go begin transaction select sys objects declare bind token varchar255 exec sp getbindtoken bind token output declare bind token bin varbinary128 cast bind token varbinary128 set context info bind token bin rollback another session declare bind token varchar255 select castcontext info varchar255 sys dm exec sessions session id exec sp bindsession bind token go select 0
use coalesce check variables null use coalesce check variables different type precedence null without encountering errors declare startdate datetime null,coalesce must return valid datatype think problem related way works determine datatype return quick workaround startdate null statecode null begin select yes end else select 4,long variables involved datatypes compatible sql variant case basically lob datatypes clr types user defined datatypes use select case coalesce cast startdate sql variant enddate statecode countycode producername taxid farm null yes else end necessary cast one arguments coalesce returns data type expression highest data type precedence sql variant high datatype precedence beaten user defined data types would prevent method working anyway personally dont find understandable finding conjunction individual null results though 11,find simple syntax readable less confusing use functions throw weird errors check whether variables one one null case null null null yes else end alternative conditions although less readable opinion would case exists select except select null null null null null yes else end case exists select intersect select null null null null null yes else end last two options look similar another standard sql yet implemented sql server option case distinct null null null null null yes else end 10,add solution similar problem tl dr recreate table foreign key specification different name previously held table drop resulting table also drop original orphan foreign key recreate table original foreign key detail ran nasty situation alter table statement failed due foreign key dropped earlier led inconsistencies innodb data dictionary probably due http bugs mysql com bug phpid related question https stackoverflow com questions error foreign key constraint droped table mysql alter table visits change column variation visitor id variation visitor id int11 null error rename db sql 482c 8448f db visits errno mysql show engine innodb status error foreign key constraint 0,heres came switch using test database mytest instead tempdb collect transaction log start collecting tlogs alter database mytest set recovery full wait backup database mytest disk nul clear logs time clear tlogs checkpoint backup log mytest disk nul run alter table statements one time examine effects transaction log examine tlog statement declare transactionid nvarchar28 select top transactionid transaction id fn dblognull null transaction name alter table select count1 allocunitname fn dblognull null transaction id transactionid group allocunitname assuming statement alter table statement run since cleared log 0,another idea would simply set nightly job copy backup restores dev server server one might great idea nice thing restore go server multiple servers completely decoupled activity primary database server backup database db disk someshare file bak copy init compression server restore database db dev disk someshare file bak replace recovery may need also add move commands disk layout servers different youre putting copy server restore database db dev disk someshare file bak replace recovery move data file name somepath somefile mdf move log file name somepath somefile ldf youre restoring server shouldnt issues users restore different server users exist 0
whats better large changes table delete insert every time update existing making project need change around 36k records one table,investigated merge command sql basic example merge yourbigtable ybt using select distinct recordid yourothertable yot yot recordid ybt recordid matched target insert recordid values yot deviceid basically upsert command update exists insert fast cool command 8,really depends much data changing lets say table columns also indexes diff column values columns changing even data columns changing columns indexed may better deleting inserting columns changing lets say part non clustered indexes may better updating records case clustered index updated indexes updated research find comment sort redundant sql server internally separate mechanism performing update place update ie changing columns value new original row place update delete followed insert place updates rule performed possible rows stay exactly location page extent bytes affected chnaged tlog one record provided update triggers updates happen place heap updated enough space page updates also 9,checked delete insert vs update table 30million 3crore records table one clustered unique composite key nonclustered keys delete insert took min update took min one column updated row request people guess equations change dealing large table many columns much data 4,clarify loops work rightfully said need create stored procedure cant run anonymous blocks like oracle instance sometimes dont work well generic sql editors pl sql dont always work well either use mysql workbench also command line test delimiter test create procedure testwhile begin declare int set select set end end query ok rows affected sec test delimiter test call testwhile side note variables work well theyre session variables case prefer local stored procedure variables cant seen modified outside scope procedure 0,select invnr detailline tbl invoice invnr select distinct top invnr tbl invoice order invnr 0,sounds like orphaned user orphaned user user sid doesnt match login sid friendly name see likely caused login deleted server database restored different server need delete user create existing permissions would lost ok role say individual permissions objects stored procedures tables etc feature allow map user login even name use db1 go alter user user name login login name would like like use wtest go alter user ls readonly login ls readonly see details ms documentation 0
stored procedure null parameter within clause want use parameter within clause value provided strongly typed dataset trying moment get right,select table1 table1 url like parameter1 table1 id parameter2 parameter3 null table1 id2 parameter3 take look example change clause nested clause specifying initial expression well parameter3 null demand nested expression true parameter3 null 12,ive always fan dynamic sql approach type problem find provides optimal balance complexity versus quality query plan following code define base query whatever would need add filters provided parameter null create procedure dbo getdata parameter1 varchar256 parameter2 varchar256 parameter3 int null begin set nocount declare basequery nvarcharmax nselect dbo table1 paramlist nvarcharmax p1 varchar256 p2 varchar256 p3 int whereclause nvarcharmax parameter1 null begin set whereclause whereclause url p1 end parameter2 null begin set whereclause whereclause id p2 end parameter3 null begin set whereclause whereclause id2 p3 end set basequery basequery whereclause execute sp executesql basequery paramlist p1 parameter1 p2 parameter2 12,pull using mysqldump catch ship data may cost associated shipping data example lets says want rename mydb ourdb step create new database mysql create database ourdb step get schema without triggers mysqldump hrdshost uuser ppassword skip triggers mydb tmp schema sql step get triggers mysqldump hrdshost uuser ppassword skip routines triggers mydb tmp triggers sql step generate script insert select across tables foreign key constraints etl data script tmp datatransfer sql echo etl data script echo set old character set client character set client etl data script echo set old character set results character set results etl data script echo 0,strictly speaking difference input query optimizer two forms input tree iso select name total suminv quantity production product production productinventory inv inv productid productid group name option recompile querytraceon querytraceon input tree iso select name total suminv quantity production product join production productinventory inv inv productid productid group name option recompile querytraceon querytraceon see clause predicate tightly bound join using modern syntax older syntax logical cross join followed relational select row filter query optimizer almost always collapses relational select join optimization meaning two forms likely produce equivalent query plans actual guarantee 0
merge formatting data two tables table called slot follows default data 1st table day time venue free rm rm rm,table creation sample data script create table slot day tinyint null time time0 null venue char4 null free smallint null check free primary key day time venue insert slot day time venue free values rm rm rm rm rm rm rm rm create table booking day tinyint null time time0 null venue char4 null user varchar10 null insert booking day time venue user values rm jill rm jill rm jack rm mary rm mary rm jill rm ken rm ken query select day time venue free used select count big booking day day time time venue venue slot order day 6,create table slot day tinyint time char4 venue varchar10 free int insert slot day time venue free values rm rm rm rm rm rm rm rm create table booking day tinyint time char4 venue varchar10 user varchar30 insert booking day time venue user values rm jill rm jill rm jack rm mary rm mary rm jill rm ken rm ken select day time venue free isnullg used used slot left join select day time venue count used booking group day time venue day day time time venue venue drop table slot drop table booking 4,wanted first thank mark posting script saved bunch time writing scratch modified little bit since ran issue received error stating database principal owns schema database dropped error modified script generate commands schema error also role error get one well hope helps someone declare sql nvarchar2000 declare name nvarchar128 declare database id int set nocount exists select name tempdb sys tables name like orphan users begin create table orphan users database name nvarchar128 null user name nvarchar128 null drop command text nvarchar200 null drop schema text nvarchar200 null drop role text nvarchar200 null end create table databases database id int null 0,able everything learn 10g 11g well fact would recommend go current version anyway choice 0
best way automatically create snapshot table certain time running quite loaded postgresql server need take snapshots certain tables certain time,id use either pg dump create table foo log select foo added actually one idea add insert update trigger table shapshoted write changes another table partitioned daily basis please tell god idea sounds good works rules create rule insert yourtable log ins also insert yourlogs select new daily begin create table yourlogs date inherits yourlogs create replace rule yourlogs ins insert yourlogs instead insert yourlogs date select new commit main difference creating table fly daily basis overhead youll introducing every insert update opposed grabbing updated rows one go 4,running mixed workload oltp table analysis snapshot could consider replication hot standby reduces impact heavily loaded server need though 4,make view username null put unique index view youll never need use view itll exist also use technique enforce uniqueness across several tables would normally impossible 0,someone considers primarily sysadmin secondly accidental dba think part comes amount knowledge required stand job perhaps importantly understand job old mcdba certification sums quite well think required four exams passed sysadmin exam network infrastructure exam database development exam sql administration exam thats quite broad range topics realistically youre likely come via one first would argue much sql administration stands shoulders three people come via one routes initially instance sysadmin handling sql backups first foray sql many years ago developer designing database code theyre writing starting wont know everything youll grounding least part instance systems sql runs permissions work programming methods 0
improve innodb delete performance audit table tracks actions table database create table track table id int16 unsigned null userid smallint16,try using temp table approach try something like step create table track table new like track table step insert track table new select track table action delete date insert date subcurdate interval day step alter table track table rename track table old step alter table track table new rename track table step drop table track table old include tuple field step please see produces desired effect want may want ditch tuple field altogether unless use tuple field reasons 6,could delete data batches sql server syntax delete top rows table loop transaction batch one statement course keep transactions short maintain locks short periods mysql syntax delete usertable limit restrictions cant use limit deletes joins instance case might able way additional danger using limit delete comes replication rows deleted sometimes deleted order slave deleted master 11,fast tell sql server generate execution plan quickly returning number rows defined note estimates per since telling sql server retrieve rows fast possible running query fast total rows select businessentityid totalpurchaseytd datefirstpurchase birthdate maritalstatus yearlyincome gender totalchildren numberchildrenathome education occupation homeownerflag numbercarsowned adventureworks2012 sales vpersondemographics order businessentityid option fast est vs actual rows option fast est vs actual rows without option fast use case would application caching load large amount data background wants show user slice data quickly possible another interesting use case ssis land rob farley describes using fast catalyst speeding data retrieval adding hint felt like magic wand 0,thank post probably one kind test simple create temp table make sure shows run queries post one two truly succeeded corrected join sql optimized longer runs made pretty useful let know missed something far got automated looped script provides way assessing query spid offender period time using standard deviation stdev query runs every minutes times hours modify parameters see fit pages filter folks may want clear case lots small tables otherwise catch nuance enjoy declare minutes apart int set minutes apart declare many times int set many times drop table tempdb tempdbusage select tempdb tempdbusage select session id stdevpages stdev 0
get max serial according sum corresponding amount table data follows amount need query find maxs according sumamount sumamount need result,select mins select sumamount order running total mytable running total 4,another way works versions prior use recursive cte tested rextester com ct select top amount running total amount tablex order union select amount amount previous total select amount previous total ct running total rn row number order ct join tablex ct ct running total rn select ct running total option maxrecursion limit recursion ctes anchor union alls first leg selects first row iteration ctes recursive part second leg selects next row computes running total sum row number trick subquery overcome limitation top allowed recursive parts ctes 6,short sentence collation defines sorting comparing collation determines rules sql server uses compare sort character data rules language locale aware may also sensitive case accent kana width collation suffixes identify dictionary rule insensitivity cs case sensitive ci case insensitive accent sensitive ai accent insensitive ks kana sensitive binary collations identified suffixes bin binary bin2 binary code point sensitive regards different collations certainly demand workarounds avoid resolve collation conflict errors kill performance due known non sargable expressions dealing different collations nightmare thats recommendation pick one stick references collation hell questions sql server collations shy ask collation mean 0,working linux storing images filesystem database significant better performance see excerpt brad edigers book advanced rails 0
relating executioninstanceguid ssisdb release sql server integration services ssis delivered ssisdb catalog tracks operations packages among things default package execution,much comment trying something msdn page system table catalog executions get execution id bigint unique identifier id instance execution article ssis view connection manager information past executions understand ssis provides new system variable serverexecutionid use inside packages custom logging notifications good variable include direct pointer catalog views use find connection string information catalog executions contains one row per execution filter execution id sample query declare execution id bigint execution id serverexecutionid goes select package name start time end time status emc package path castemc property value varchar1000 connection string catalog executions join catalog event messages em execution id em operation 5,created ssis project using deployment model consisting single package package added ole db connection manager pointed tempdb dropped script task onto canvas also turned explicit logging using ole db connection manager captured oninformation event scr fire info configured script task grab two parameters system executioninstanceguid system serverexecutionid admit point noticed second variable marians answer inside task raise information events get values recorded logged explicit table dbo sysssislog free logging catalog operation messages public void main bool fireagain true string description string empty string variable string empty string value string empty variable system serverexecutionid value dts variables variable value tostring description 9,dont fancy worried scared restarting sql server make sure dont long running transactions best restart sql server using console shutdown command low minimum activity period also called maintenance window minimize impact business dr setup dont want best failover restart passive secondary node clean shutdown sql server occurs scenarios stop sql server using services console shutting server running shutdown command ssms situations sql server cleanly shutsdown databases terminates service involves commiting rolling back transactions writing dirty pages disk writing entry transaction log improper shutdown sql server shutdown nowait pulling power cable server access killing sqlserver exe task manager dirve failure sql 0,agree using auto features best instead running database wide cases per table tuning necessary dont quite agree design choice postgres tie together vacuum analyze seen several instances databases lot insert update little delete never get analyze done start perform badly solution go tables get used heavily subject large queries set auto analyze settings tables something getting analyzed every day get per table settings gui auto vacuum tab see analyze settings set independently vacuum settings end reloptions table seen query select relname reloptions pg class reloptions null sample value agressive analyze might autovacuum enabled trueautovacuum analyze threshold 10autovacuum analyze scale factor 0
oracle using unique index long key table 250k rows test database hundred millions production observe issue table nvarchar250 string identifier,found solution beautiful actually learned lot oracle one word histograms started reading lot oracles cbo works stumbled upon histograms didnt fully understand took look user histograms table voil several rows sick table practically nothing cloned table sick table one row different identifier starting parts key cut characters sign said first part keys highly repetitive become different sign seems histograms powerful simple fact unique index always cardinality given value querying rows oracle looked histogram thought could tens thousands values identifier starting part threw cbo course deleted histograms column old table problem went away reading https blogs oracle com optimizer entry drop 8,answers question histograms different histograms created default based column skew whether column used relevant predicate copying ddl data enough workload information also important according performance tuning guide drop table workload information used auto histogram gathering feature saved statistics history used restore stats procedures lost without data features function properly example table skewed data histogram drop table test1 create table test1a date insert test1 select date level dual connect level insert test1 select date dual connect level begin dbms stats gather table statsuser test1 end select histogram user tab columns table name test1 histogram none running thing query statistics gathered generate 7,emailed jonathan lewis got helpful reply oddity calculation consequence limits character based histograms see particularly http jonathanlewis wordpress com frequency histogram http jonathanlewis wordpress com frequency histograms looking example query list single row initial guess would optimizer used generic strategy calculating multi row selectivity rather special case piece code list primary key guess wouldnt hard recognise case developers probably considered worth effort highly recommended reading blog posts links describe detail limitation histograms running eg conclusion fairly long similar strings column good candidate frequency histogram descriptive status column problem value rare looks identical popular value first characters may find solution change 6,due mvcc model postgres according rules sql update writes new row version every row excluded clause less substantial impact performance directly indirectly empty updates cost per row update fire triggers present like update wal logged produce dead rows bloating table causing work vacuum later like update indexes entries toasted columns none involved columns changed stay true updated row related postgresql initial database size redundant data update statements almost always good idea exclude empty updates actual chance may happen provide table definition question always good idea assume first name null wouldnt surprising first name hence query use null safe comparison update 0,computed column isnt persisted indexed virtual column expression taking space 0,first character isnt unicode per se mean technically characters xml within sql server encoded utf little endian sense unicode seeing escaped notation character case hex binary value problem simply xml names start number following tests show attribute name element name starting number gets error starting underscore letter fine select convertxml test row test msg level state line xml parsing line character illegal qualified name character select convertxml test test msg level state line xml parsing line character illegal qualified name character select convertxml test row test test row test select convertxml test row x2016 test test row x2016 test need 0
sharing single primary key sequence across database acceptable practice use single sequence primary key across tables instead primary key unique,cant imagine might reason behind single sequence across tables create bottleneck generating new values matter small overhead generating sequential key values generator single resource access must synchronized requests gets higher chances requesters wait turn tap obvious single sequence generator shared tables accessed frequently clients thus producing contention one multiple generators contention may become pronounced business rules impose constraints upon generated values absence gaps strict ordering clustered database even efficient sequence generator workload causes untolerable contention 4,acceptable sure common beneficial doubtful old job inherited system central sequence generator sql server system long sequence introduced sql server wasnt really performance bottleneck shouldnt unless youre generating hundreds thousands values per second made code lot complex good reason intent design sure something system assigned id value one thing system could id seemed quite obtuse never understood customer customerid preclude order orderid see usefulness central sequence generator multiple systems generating ids certain type entity say customer order multiple systems central sequence dole new values multiple systems without bottleneck single point failure without fear two systems generating id 12,idea merit complex database people could accidentally join table using wrong column get invalid rows int ids chose sequential guids primary keys order avoid index fragmentation pitfalls guids sadly quite large sql server generate sequential guids via default invoking newsequentialid function table issued keys maintain blocking bottleneck given us unique ids across whole databases across entire enterprise actually truly unique price course space problematic trying bring data across data warehouse cube speed size predicated using smaller integer keys convinced avoided many bugs app result using 6,addition answer already provider feel worth pointing developers often lazy working modern orms entity framework whilst dbas try hardest avoid select developers often write semantically equivalent eg linq var somevariable db mytable whereentity entity firstname user tolist essence would result following select mytable firstname user also additional overhead hasnt already covered resources required process column row relevant object furthermore every object kept memory object must cleaned selected columns needed could easily save excess 100mb ram massive amount cumulative effect garbage collection etc cost client side yes least always big also need educating hidden costs also addendum sample pulling data need 0,close first last access sql server starting sql server first value last value better azure sql database heres one way get answer create table fl identitycolumn int identity value int insert flvalue select insert flvalue select insert flvalue select insert flvalue select insert flvalue select select top last valuevalue order identitycolumn first valuevalue order identitycolumn fl order identitycolumn desc go drop table fl 0,enable trace flag also issue update statistics dbo observationdates rowcount get plans shown statistics table missing upload figure came table cardinality information plan explorer attachment used sql server sp1 cu4 build two logical processors maximum memory set mb trace flags aside get execution plan features dynamic partition elimination select od year avgvalue avgobservationvalue dbo observation join dbo observationdates od observationdatekey od datekey od year od year group od year option querytraceon plan fragment might look worse filters start filters example predicate per iteration loop start predicate tested returns true clustered index seek executed hence dynamic partition elimination perhaps quite efficient 0
penalty using binary16 instead uniqueidentifier ive recently inherited sql server database uses binary16 instead uniqueidentifier store guids everything including primary,always concerned system may migrated system doesnt support uniqueidentifier compromises dont know designer may known uniqueidentifier type things didnt know technically though shouldnt major concern 5,concerned well couple things little concerning first true uniqueidentifier guid byte binary value also true data stored binary form int could stored binary4 datetime stored binary8 etc hence probably reason separate datatype guids outside mere convenience sysname alias nvarchar128 three behavioral differences find comparing uniqueidentifier values sql server better worse actually done way comparing binary16 values according msdn page comparing guid uniqueidentifier values comparing uniqueidentifier values sql server last six bytes value significant values frequently sorted slight difference two types according msdn page uniqueidentifier ordering implemented comparing bit patterns two values given differences guid values handled sql server net noted 21,entity attribute value eav considered anti pattern many including alternatives use database table inheritance use xml data sqlxml functions use nosql database like hbase 0,index search predicate might used range locks lock first row next range next third row etc last row range essentially number row locks blocks range inserts also values locking range happen sql server need index work without indexes locking indexes predicates youll know get table locks 0
two counts single table single query similar questions answers forums think problem simpler two quesries eg select count agent select,use case statement scenario select count total agents sumcase active else end active agents agent sample execution sample data create table agent id int active char10 insert agent active values select count total agents sumcase active else end active agents agent 6,couple others ways addition arulkumars answer sample data create temp table foox values truefalsefalsetruefalse using filter prefer method syntactically simpler sum method work way select count count total count filter count foo count total count row group grouping sets want results pivoted explain analyze select count foo group grouping sets true null count true total rows really want fun false sounds cool 5,one valid reason move another mysql flavour performance latest mariadb versions drop replacemants mysql respectively included major improvements query optimization read site mariadb examples improvements features index condition pushdown cost based choice range vs index merge subquery materialization subquery cache new joins methods like hash join several features affect performance statements subqueries derived tables views details found optimizer feature comparison matrix page additional features like microsecond support time datatypes improvements replication binary logging previous versions also features found mainstream mysql like persistent virtual columns course migration server mysql mariadb examined tested whether features useful databases applications whether difference performance big 0,create master master setup follows create second master masterb masterb acts slave logtable create logtable new innodb run insert logtable new select logtable psuedocode masterb sends replication mastera logtable new mastera finishes syncing swap tables 0
storing phone numbers proper design first im dba im software engineer building applications database backed entire career one things remember,anti null brigade would insist normalize create suggested phonenumber table practical folk would consider original design perfectly acceptable im latter camp normalize hurts de normalize works 5,reasoned argument mark storey smith damir sudarevic would add two points think important keep mind never say never well hardly ever policy never imagined inevitable point dont believe users tell something never happen purposes means need think likelihood seriousness change design around phone numbers design making informed compromises makes sense taking consideration risk involved requirements change normalization vital unless isnt unless really good reason first instinct always normalize schema normalization great rule thumb keep creating big data consistency code maintenance problems reasons denormalize one big one reporting performance reading static data another reason might consider denormalizing data question isnt meaningful 5,define problem mean take vastly longer management nightmare yes convert field decimal184 maybe dont care sub penny precision 0,create view tables persisted computed column defined leftlastname table compare computed persisted column values test bed showing create table dbo persons personid int null constraint pk persons primary key clustered identity11 firstname nvarchar500 null lastname nvarchar500 null create table dbo persons2 personid int null constraint pk persons2 primary key clustered identity11 firstname nvarchar500 null lastname nvarchar500 null go create view dbo personsview schemabinding select p1 personid p1 firstname p1 lastname lastnameinitial leftp1 lastname dbo persons p1 go create view dbo personsview2 schemabinding select p2 personid p2 firstname p2 lastname lastnameinitial leftp2 lastname dbo persons p2 go create unique clustered index 0
reasonable mark columns one primary key table representing movies fields id pk title genre runtime released tags origin downloads database,id column advantage comes uniqueness want need enforce uniqueness whatever combination attributes never going enforced adding meaningless id advantage shows ever get point youd need new table needs foreign key one case included id use one fk new table dont think free lunch downside approach youll likely find writing joins mere purpose fetching information could perfectly well part new table made 4,table definition looks reasonable columns null unique constraint work expected except typos minor differences spelling may rather common afraid consider horses comment alternative functional unique index option would functional unique index similar dave commented would use uuid data type optimize index size performance cast array text immutable due generic implementation indexing array full text search hence need little helper function declare immutable create replace function movie uuid title text runtime int2 released int2 genres text tags text origin text returns uuid language sql immutable faking immutable select md5 title runtime text released text genres text tags text origin text uuid 4,imagine group friends conversation turns movies someone asks think three musketeers respond one additional information would need absolutely certain thinking movie directors name production studio year released one stars names combination two answer question however would think genre would good candidate one reason genre much subjective criteria three musketeers action drama adventure comedy action adventure romantic comedy often see movie listed different genres even allow multiple genres user may select entirely different one listed actual movie looking even runtimes differ especially theater vcr dvd ray versions need hard objective attributes change one media release another unfortunately exclude name movie movies 6,open transaction almost consequence simple begin transaction wait nothing wait bit longer commit worst hold bytes status values big deal programs actual work within transaction another matter point transaction sure several facts within database true simultaneously despite users writing database concurrently take cannonical example transferring money bank accounts system must ensure source account exists sufficient funds destination account exists debit credit happen neither happens must guarantee transactions happen perhaps even two accounts system ensures taking locks tables concerned locks taken much peoples work see controlled transaction isolation level lot work good chance transactions queued waiting objects hold locks reduce systems 0,query run fairly infrequently example report building table fly probably better1 query run frequently temp table required performance potentially problem table cheap build temp table long database fast enough may get away however need keep eye performance table doesnt totally date subject relatively frequent reporting activity periodic rebuild probably best way go table expensive build needs date may need manage denormalised structure either maintained indexed view triggers rather complicated places additional burden write operations extreme cases large data volumes may need hybrid approach historical data queried denormalised structure optimised performance current data queried live application extreme cases get low latency 0,introduction transactions savepoints scenario creating new user basic process could following begin insert users username password values xxx set userid last insert id insert profile pictures user id profile picture path values userid somewhere filesystem jpg commit whatever reason process end middle would enough issue rollback query somewhere instead commit changes would undone one basic tasks accomplished transactions mentioned savepoints rolando might come handy example one inserts row profile pictures first tries copy image desired location fail one wants undo second insert first copying picture fails reason cancel whole user registration one define savepoint inserting users inserting profile pictures begin 0
use unique key via combinations table fields take look following sqlfiddle http sqlfiddle com dacb5 create table contacts id int,try create table contacts id int auto increment primary key name varchar20 network id int network contact id int unique key network id network contact id 9,change tables definition adding unique key constraint combination two columns create table contacts id int auto increment primary key name varchar20 network id int network contact id int constraint network id contact id unique unique key network id network contact id also check answer bill karwin differences insert ignore replace insert duplicate key update 6,assuming talking data encrypted sql server keys way find columns key name function return name key used encryption particular value return null isnt anything encrypted known key 3rd party simple encrypted knowlegde test every column see contains least one row varbinary value returns key name functionality key name create test database create database test encr go change context use test encr go possible encrypt different rows different keys ill create keys demo create symmetric key create symmetric key symmetrickey1 algorithm aes encryption password password01 go create second key create symmetric key symmetrickey2 algorithm aes encryption password password02 go create table 0,restating question storing users passwords simplest answer still store passwords alternative method use databases built system manage storage method good databases may still better whatever might otherwise avoiding storage really avoiding coding storage also might better compromise database get copies files access tables directly theres little point using brilliant password management system someone access system level table containing passwords hashed passwords also access files directly bother cracking passwords already data like encrypt data easily either user needs able access user level encryption wont work well really seem asking use application level authentication scheme database already one security ignoring possibility could 0
using database engine tuning advisor generate scripts recommendations run server side profile trace hour produce trc file activities one databases,check view tuning output want save transact sql scripts create drop database objects recommendation one script file click save recommendations actions menu always review test recommendations blindly applying prod environment would highly suggest look sane tools like sp blitzindex folks brentozar com 8,please note computer generating list missing indexes swallowed whole still need decide indexes create recommended indexes near duplicates existing indexes want handle issues still requires making decision since generated recommendations need serious review using bart duncan produced script list missing indexes using dynamic management views dmv indexes exactly database engine tuning advisor dta valuable suggest compare dta dmv recommendations missing indexes sanity check bart duncans query generate code used create recommended indexes slightly reformatted readability select migs avg total user cost migs avg user impact migs user seeks migs user scans improvement measure create index missing index convert varchar mig 4,built mpp system mysql discarded system two reasons oracle lack hash joins nested loop index joins scale level required mpp system oracle inhibited promised delivery hash joins code line took ownership mpp big data systems must joins geometric complexity linear log linear complexity joins must strong preference true big data systems deployed actian vectorwise instead new deepcloud mpp system maintaining drizzle mysql compatibility user level users wanting fast big data analytics download deepcloud http www deepcloud co 0,options rather limited requirements constantly check sync cant make change things materialized view logs dbms alert streams standby database table tables constantly rows updated jack douglas said materialized view would easiest setup likely event records dont change moment moment probably want setup package packages select merge delete necessary date frequency run given requirements may best specifically package following delete rows exist merge updating matched inserting matched want avoid hitting table multiple times could insert entirety table global temporary table delete merge concerning minus minus tell rows query union ing minus query get rows different would probably take longer process even 0
possible alias column based result select table like tb1 cod a001 a002 a003 cars baby nasa second table tb2 cod,declare sql nvarcharmax select sql stuff select quotenamec column name quotenamef descricao information schema columns inner join tabbase column name col tabbase table name tabela entrada order ordinal position xml path12 select sql select sql tabela entrada print sql execute sql better way didnt know could use stuff found query microsoft page 4,sorry say table structure difficult work considering want various ways probably get result one way would use unpivot pivot ugly could start unpivoting data tb1 columns rows select tb1 t1 unpivot val col a001 a002 a003 going return data format cod val col cars a001 baby a002 nasa a003 could take result join tb2 select cod val t2 description select tb1 t1 unpivot val col a001 a002 a003 inner join tb2 t2 col t2 col tb1 gives result cod val description cars something baby lasagna new column names description val rows want columns apply pivot function select select cod val 25,add existing answers sql server internals book pp implies detaching database deleting log file reattaching mdf file ought quite safe says detaching database ensures incomplete transactions database dirty pages database memory conditions met detach operation fails one benefit using sp detach db procedure sql server records fact database shut cleanly quick way shrink log file become much larger would like however seem case blog post demo method leaving database broken 0,first size smallint two bytes twice size bool select pg column size1 bool bool pg column size1 smallint si bool si lets create small table rows check create table foo select case smallint else smallint end si case true else false end select random generate series11e3 create index si foo si create index foo see table 40kb test dt foo list relations schema name type size public foo table kb indexes size 40kb self joins returning rows case using seq scan index scan set enable seqscan complete amount time explain analyze select foo join foo f2 using explain analyze select 0
calling sql loader sql plus wondering way execute sql loader script sql plus using oracle 10g,able run via host command sql host path ora bin sqlldr parfile 6,alternative technique may use external table need take csv whatever file load separate step simply declare external table correct format almost identical sql loader parfile wrapped create table statement issue select directly file right place create bad log files like sql loader records cant cast datatypes table could expect better performance days would consider sql loader legacy apps 6,reference problem seems related connect bug view properties database using db owner account jimmy rudley posted microsoft pm corrected next version sql walter jokiel program manager sql server problem sql server management studio anyone finds stuck problem try update sql server management studio way dont need grant permission view server state user using sql server management studio problem seems gone 0,backup access unc impersonation context double hop situation requires kerberos constrained delegation configured already answered https stackoverflow com questions restore database shared folder 0
raid suitable mysql installation raid suitable mysql installation let explain application application socket programming connect gps device receive gps string,random access raid almost certainly better raid course expensive whether raid fast enough installation thats going pretty tough anyone else answer 4,read heavy low write environment raid5 would leave ones budget tolerance blood pressure write heavy low read write heavy read heavy environment raid5 simply question especially true innodb think innodbs table interaction innodb use innodb file per table omg activity would centered around one file ibdata1 contained ibdata1 table data pages table index pages table metadata managing tablespace ids mvcc data acid compliance transaction isolation even reads innodb tend shroud rows mvcc protection allow repeatable reads permit transactions hit rows read thus reads well writes produce disk ibdata1 using innodb file per table may relieve disk separating table data index 11,two options refer exception directly number begin execute immediate create sequence test start increment exception others sqlcode null suppresses ora exception else raise end end option use exception init pragma directive bind known oracle error number user defined exception declare name use exception declare user defined exception pragma exception init name use bind error code begin execute immediate create sequence test start increment exception name use null suppress ora exception end btw syntax catch errors providing error codes yes ive demonstrated first example reading variations oracle reference documentation handling pl sql exceptions 0,error state code caused typo name database connection string 0
find duration dataset dataset following structure target polltime value null null null null null null null would like build resultset,pointed better solved gaps islands select dense rank partition target order polltime dense rank partition target value order polltime grp schema table select target minpolltime start maxpolltime finish datediffmi minpolltime maxpolltime duration group target grp maxvalue null order start original source code getting status change table 4,sqlserver op commented uses sqlserver query used first tried convert new idea came converted select target polltime value id row number partition target order polltime isnull case value null else end table1 select t1 target startdate t1 polltime enddate coalescet3 polltime t1 polltime t1 value block t1 id sumt2 isnull t1 inner join t2 t1 target t2 target t2 id t1 id left join t3 t1 target t3 target t3 id t1 id group t1 target t1 polltime t1 value t1 id t3 id t3 polltime select target startdate min startdate enddate maxenddate durationmin datediffmi minstartdate maxenddate value null group 4,following alternative sql server r2 solution efficient relatively nulls right supporting indexes available gaps islands method may faster table large minimizing sorting costs nulls form high proportion data sample data required indexes create table dbo table1 target char1 null polltime datetime null value varchar5 null primary key target polltime optional helpful filtered index find null rows required order create unique index fi1 dbo table1 target polltime include value value null sample data insert dbo table1 target polltime value values null null null null null null null solution step find start end dates select t1 target calc startorend calc polltime groupid 4,postgresql different documentation create view documentation create materialized view mention replace keyword seems shortcut aside dropping dependent objects rebuilding one recommend two small things use drop materialized view blabla cascade get list dependent objects drop recreation dependent object one transaction 0,past ive wanted restrict access database use restricted user mode however doesnt work users members db owner role realize recommended role users sometimes end standardized disabling login maintenance windows didnt worry users elevated access example alter login user1 disable dont forget enable done example alter login user1 enable 0,im reaching think least one dangerous scenario restore database filetable files network default specifically sql server could restore virus wont anything course virus doesnt suddenly become sentient users try access file could infected hey said reaching im envisioning scenario outside hacker wants get malware door sends email bob accounting saying heres file sqlserver filetableshare myvirus exe point gone past firewalls without detection internal antivirus anti malware tools 0
delete related records multi key merge sql server suppose something like source table variable values leftid int null rightid int,separate delete operation mind delete dbo mapping exists select values leftid leftid exists select values leftid leftid rightid rightid outline left anti semi join exists pattern often outperform left join null pattern sure overall goal clarity performance judge work better requirements matched source option youll look plans qualitatively plans runtime metrics quantitatively know sure expect merge command protect race conditions would happen multiple independent statements better make sure true changing merge dbo mapping holdlock target dan guzmans blog post personally would without merge unresolved bugs among reasons paul white seems recommend separate dml statements well heres added schema prefix always 6,filter rows need consider target table cte use cte target merge select leftid rightid customvalue mappings exists select values leftid leftid merge using values leftid leftid rightid rightid matched target insert leftid rightid customvalue values leftid rightid customvalue matched update set customvalue customvalue matched source delete 9,try ps ef grep ysql identify process id strace cp pid leave seconds minute tell process spending time could waiting disk seen read write dominate 0,site ive consulted oracle swore toad interface queries dbadmins mostly developers 0
spatial index used table geometry column one record one point stored spatial index created queries searching nearest location use index,correct spatial indexes dont get leveraged situation sadly spatial indexes provide set grids allow system identify geometries geographies overlap grids best bet set threshold acceptable closeness try using something like stbuffer stintersects works well increase threshold nothing found eg using second outer apply nothing found edit check post http blogs lobsterpot com au sql spatial getting nearest calculations working properly summary get index used need make sure order value cant null add null clause work 6,see http technet microsoft com en us library bb895373 28v sql aspx number methods use spatial index limited used join clause youre trying use stdistance method order clause index usage supported edit might get away creating distances table joining thus theoretically using spatial index ordering joined tables columns something ala inner join distances locationpoint stdistance currentlocation distances maxdistance notlocationpoint stdistance currentlocation distances mindistance using columns dont necessarily scale linear 4,typically start looking sys dm exec requests specifically wait time wait type wait resource insert requests give clear indication blocking insert results indicate whether lock contention file growth events log flush waits allocation contention manifests pfs page latch contention etc etc etc measure update question accordingly strongly urge stop read waits queues troubleshooting methodology proceed 0,please check maximum script size set unlimited menu option tools options text editor transact sql general intellisense intellisense started working 0
optimizing queries range timestamps two columns use postgresql ubuntu need select records inside range time table time limits two timestamp,postgres later create index idx time limits ts inverse time limits id phi start date time end date time desc cases sort order index hardly relevant postgres scan backwards practically fast range queries multiple columns make huge difference closely related postgresql index used query range consider query select time limits id phi start date time end date time sort order first column id phi index irrelevant since checked equality come first got right related answer multicolumn index performance postgres jump id phi next time consider following two columns matching index queried range conditions inverted sort order index qualifying rows come 169,erwins answer already comprehensive however range types timestamps available postgresql temporal extension jeff davis https github com jeff davis postgresql temporal note limited features uses timestamptz style overlap afaik also theres lots great reasons upgrade postgresql 5,probably timestamp selective possible even unique point adding fields key increase size key without contributing selectivity included columns instead allows added leaf pages saving overall size index select top1000 count cnt timestamp group timestamp order cnt desc return 0,recommend cautious use tuning technique since found missing index suggestions popped query plans consistently less reliable queries db schemas become progessively complex due variety reasons experience percent improvement way simplest queries obvious indexes estimate derive actual costs incurred actual rowcounts query runs ive seen query costs go implementing suggested index doesnt even get used plan remains query plan optimal either due construction query joins clause optimized etc rowcount estimates due missing date statistics indexing brutally bad query plan often best band aid solution incremental improvement performance might seeing whole picture especially true using graphical plan viewing xml see one missing 0
insert record table trigger function would like insert record data type variable new variable table trigger would sql look like,must expand record ive also added column list insert used much safer execute using form execute insert table col1 col2 col3 values using new col1 new col2 new col3 please explain use case detail doesnt solve issue 5,actually expand record manually long number sequence types columns match two tables use much simpler form execute insert table select using new since table name seems stable even need dynamic sql execute plain insert insert table select new parentheses required composite type make syntax unambiguous ask may change app future target table stay sync source table target source better list columns explicitly want select certain columns want change possible updates source target table better supply column list 14,think may overcomplicated answer required case doubt roland rick james correct creation temporary table injecting rows pass filter like solution easier important error unaware apologize ran query mysql interactive prompt noticed error message mysql delete slugs slug like error hy000 total number locks exceeds lock table size googleing error found solution increase innodb buffer pool size via etc cnf file rebooting mysql daemon server set default 8m increased 1g server 32gb table currently innodb mysql delete slugs slug like query ok rows affected min sec able run command delete million records minutes curious innodb buffer pool size set take note 0,hard look table many columns frequently use select statement look fields mentally categorize group fields categorize group create view 0
msdtc required sql server fail cluster making node sql server failover cluster need install msdtc component yes installed single shared,msdtc required sql server fail cluster however plan use linked servers need create clustered msdtc resource good news setup cluster already built sql server installed 4,microsoft distributed transaction coordinator msdtc provides mechanism ensuring atomicity consistency across multiple data sources required plan performing explicitly transacted operations across multiple data sources installing sql server applies equally clustered non clustered sql server installations dont plan using distributed transactions dont need msdtc installed following shows example distributed transaction might look like set xact abort begin try begin transaction select linkedservername master sys sysobjects commit transaction end try begin catch trancount begin rollback transaction end end catch plan running sql like clustered instance youll need install msdtc onto separate lun configured part cluster resource group sql server instance install onto 4,created backup sql server database trying restore sql server instance receive error best bet would script database long schema objects compatibile cant downgrade database 0,cases usually helps look whole query try remove unneeded tables rethink outer joins left right join possible eliminate view definition replacing inner joins try increase planner constants server put effort planning phase increasing join collapse limit collapse limit geqo threshold know plan order best lower join collapse limit force proper ordering explicit join order read postgresql documentation controlling planner explicit joins query planning configuration update one option consider rewrite query isolate selector part another subquery using selection select distinct business orders id business orders id order business orders id limit without aggregates like count minmaxavg use basis whole query calculate 0
query 100x slower sql server row count spool row estimate culprit query runs milliseconds sql server takes seconds sql server,query need row count spool operator specific optimization trying provide cust nbr column existingcustomers nullable actually contains nulls correct response return zero rows null always yield empty result set query thought select potentialnewcustomers exists select existingcustomers e1 cust nbr e1 cust nbr exists select existingcustomers e2 e2 cust nbr null rowcount spool avoid evaluate exists select existingcustomers e2 e2 cust nbr null seems case small difference assumptions make quite catastrophic difference performance updating single row update existingcustomers set cust nbr null cust nbr query completed less second row counts actual estimated versions plan nearly spot set statistics time set statistics 8,query need row count spool operator dont think necessary correctness specific optimization trying provide see martins thorough answer question key point single row within null boolean logic works correct response return zero rows row count spool operator optimizing necessary logic sql server estimate join row count spool operator removes rows microsoft provides excellent white paper sql cardinality estimator document found following information new ce assumes queried values exist dataset even value falls range histogram new ce example uses average frequency calculated multiplying table cardinality density often change good one greatly alleviates ascending key problem typically yields conservative query plan higher 7,martin smiths answer self answer addressed main points correctly want emphasise area future readers question understanding specific query plan depth less phrase query differently stated purpose query prune existing customers set potential new customers requirement easy express sql several ways one chosen much matter style anything else query specification still written return correct results cases includes accounting nulls expressing logical requirement fully return potential customers already customers list potential customer exclude null potential existing customers whatever null customer means write query matching requirements using whichever syntax prefer example distinctpotentialnonnullcustomers select distinct pnc cust nbr potentialnewcustomers pnc pnc cust nbr null 4,exists efficient cases left join internally matches rows filters null exists doesnt row multiplication also happens join based code may need extra aggregate distinct fix output generally wrong nulls cause match also use except gives plan exists select projectid recordtypecid ntid1 getdate getdate ntid check chk except select projectid recordtypecid ntid1 getdate getdate ntid subscriptions projectid projectid ntid ntid fyi per sql standard page case 3a select bit exists ignored 0,buffer pool size pages bytes see buffer pool size gb run select formatbufferpoolpages pagesize power102432 bufferpooldatagb select variable value bufferpoolpages information schema global status variable name innodb buffer pool pages total select variable value pagesize information schema global status variable name innodb page size database pages number pages data inside buffer pool see amount data buffer pool size gb run select formatbufferpoolpages pagesize power102432 bufferpooldatagb select variable value bufferpoolpages information schema global status variable name innodb buffer pool pages data select variable value pagesize information schema global status variable name innodb page size see percentage buffer pool use run select 0,found answer mssqltips link article found also posting tsql possible link rot future sql script drop sql server indexes declare schemaname varchar256declare tablename varchar256 declare indexname varchar256 declare tsqldropindex varcharmax declare cursorindexes cursor select schema namet schema id name name sys indexes inner join sys tables object id object id type ms shipped name sysdiagrams primary key unique constraint open cursorindexes fetch next cursorindexes schemaname tablename indexname fetch status begin set tsqldropindex drop index quotename schemaname quotename tablename quotename indexname print tsqldropindex fetch next cursorindexes schemaname tablename indexname end close cursorindexes dedeallocate cursorindexes sql script create sql server indexes declare 0
combining separate ranges largest possible contiguous ranges im trying combine multiple date ranges load max cases may may overlap largest,ive come declare date daterange empty day range daterange extreme value date begin select distinct generate series lowerrange coalesceupperrange interval day extreme value interval day date rangetest order loop day range daterangei begin isemptya day range else day range end exception data exception raise info day range end end loop uppera extreme value interval day daterangelowera null end raise info end still needs bit honing idea following explode ranges individual dates replace infinite upper bound extreme value based ordering start building ranges union fails return already built range reinitialize finally return rest predefined extreme value reached replace null get infinite 6,assumptions clarifications need differentiate infinity open upper bound upperrange null either way simpler way null vs infinity postgresql range types since date discrete type ranges default bounds per documentation built range types int4range int8range daterange use canonical form includes lower bound excludes upper bound types like tsrange would enforce possible preventing adjacent overlapping entries exclude postgresql solution pure sql ctes clarity select range coalescelowerrange infinity startdate maxcoalesceupperrange infinity order range enddate test select lagenddate order range startdate null step select countstep order range grp select daterangeminstartdate maxenddate range group grp order subqueries faster less easy read select daterangeminstartdate maxenddate range 22,means spaces table names particular appealing course sql servers 0,comments one solution might make master slave replication setup link mysql replication would make backend master front end slave front end needs write contact forms tracking etc would update code front end read slave write master downside depending load backend writes might delayed seconds anecdote ive got server handles commands second averaged reported munin slave server isnt behind seconds 0
pk constraint require separate index already suitable unique clustered index part table definition create table dbo jobitems itemid uniqueidentifier null,doesnt need separate index done way youve actually told script could functionality create table dbo jobitems itemid uniqueidentifier null lots columns constraint jobitemsindex primary key clustered itemid asc 6,two answers spot reason two indexes exist told database create two indexes furthermore could make primary key clustered remove second index answer question second index needed boils limitation requirement sql server azure edition databases isnt present versions sql server sql server enterprise standard express support heap tables sql server azure edition doesnt support heap tables guess point wanted jobitems table heap table time came put database cloud forced clustered index table developer chose create duplicate index clustered instead changing primary key nonclustered clustered chose may never know however seems like plausible path situation especially legacy table lived outside azure point 4,short sentence collation defines sorting comparing collation determines rules sql server uses compare sort character data rules language locale aware may also sensitive case accent kana width collation suffixes identify dictionary rule insensitivity cs case sensitive ci case insensitive accent sensitive ai accent insensitive ks kana sensitive binary collations identified suffixes bin binary bin2 binary code point sensitive regards different collations certainly demand workarounds avoid resolve collation conflict errors kill performance due known non sargable expressions dealing different collations nightmare thats recommendation pick one stick references collation hell questions sql server collations shy ask collation mean 0,sql statement short circuit rely evaluating expressions order 0
select multiple values like operator sql query given want select multiple value using like operator query correct select top employee,use condition select top employee id employee ident utc dt rx dt employee inner join employee mdata history employee ident employee mdata history employee ident employee id like emp1 employee id like emp3 order rx dt desc look transact sql ms docs ive set example create table employeesemployee id varchar10 employee name varchar100 insert employees values emp10 bryan nelson emp12 rosalyn sanders emp13 rose tudler emp20 julio gomez emp30 ian mcgregor emp40 anne hatt go select employee id employee name employees employee id like emp1 employee id like emp3 go employee id employee name emp10 bryan nelson emp12 rosalyn sanders 5,alternatively try following method select values emp1 emp3 pattern row count cross apply query select top row count employee id employee ident utc dt rx dt employee inner join employee mdata history employee ident employee mdata history employee ident employee id like pattern order rx dt desc values row constructor represents pattern list table additionally supplying pattern number rows retrieve pattern cross apply operator applies query every row pattern list every pattern limiting number rows pattern corresponding value pattern list side note please let take opportunity suggest always qualify columns table alias query reading two tables makes query easier read 13,size one consideration int hold four bytes char need bytes hold value built functions manipulate various data types dateadd datediff two examples possible date stored text constantly casting back forth make efficient processing legible code automatic validation another foregone benefit text approach may think column contains dates nothing stop someone entering value sorting unlikely give intended result columns really numbers example column contained integers query sorted column one would expect result however actual result likely may similar concerns dates depending chosen character representation type casting cure costs mentioned previously occasionally find strings contain digits examples national identity numbers bank account 0,databases failed independently theres harm failing test database production database stay 0
merge output better practice conditional insert select often encounter exists insert situation dan guzmans blog excellent investigation make process threadsafe,using sequence use next value function already default constraint id primary key field generate new id value ahead time generating value first means dont need worry scope identity means dont need either output clause additional select get new value value insert dont even need mess set identity insert takes care part overall situation part handling concurrency issue two processes exact time finding existing row exact string proceeding insert concern avoiding unique constraint violation would occur one way handle types concurrency issues force particular operation single threaded way using application locks work across sessions effective bit heavy handed situation like frequency 8,updated answer response srutzky another albeit minor issue serializable transaction output clause approach output clause present usage sends data back result set result set requires overhead probably sides sql server manage internal cursor app layer manage datareader object simple output parameter given dealing single scalar value assumption high frequency executions extra overhead result set probably adds agree reasons use output parameters prudent mistake use output parameter initial answer lazy revised procedure using output parameter additional optimizations along next value srutzky explains answer create procedure dbo namelookup getset byname vname nvarchar50 vvalueid int output begin set nocount set xact abort set 7,put two lines cnf mysqld general log general log file users ugrad linehanp mydb logfile txt log queries server source php phpmyadmin careful though enabling general log place heavy load server used sparingly short periods debugging documentation available fro disable enable general query log change log file name runtime use global general log general log file system variables set general log disable log enable set general log file specify name log file general log general log synonyms 0,try sql alter authorization schema yourschemaname dbo go drop user theuseryouwanttodelete go cant drop principal schema owner alter authorzation changes owned schema used yourschemaname obviously substitute owned schema database dbo likewise change ownership whatever principal need environment allow drop previously schema owning user example purposes used theuseryouwanttodelete thatll non owner want drop 0
best practices backing mysql db ive recently discovered production web servers run mysql backed regularly im used backing sql server,would recommend setting dedicated replica use backup let perform backup tasks without impacting primary add complexity architecture youll want monitor replication lag ensure everything working actual process couple options without third party tools snapshots taken using mysqldump command assuming youre using innodb mysqldump databases single transaction databases sql depending data size may preferable shut mysql backup data files directly replica restarted replay events primary received duration youre using mysql enterprise mysqlbackup utility incremental backups taken enabling binary log replica obviously records events mutate data youll need combine snapshots 7,best practices take mysql server backup mysql replication setup replication mysql setup master slave server read writes db could go slave server advantage replication take backup slave server without interrupting master server application continue work master without downtime using mysql dump data set small realize small relative term qualify lets say 10gb mysqldump probably work great easy online flexible things mysqldump backup everything certain databases tables backup ddl optimize dump faster restore make resultant sql file compatible rdbmses many things however important options related consistency backup favorite options single transaction option gives consistent backup tables using innodb storage engine non 29,logically identical exists closer antisemijoin youre asking generally preferred also highlights better cant access columns used filter opposed available null values many years ago sql server ish left join quicker hasnt case long time days exists marginally faster biggest impact access join method complete join filtering constructing joined set memory using exists checks row doesnt allocate space columns plus stops looking finds row performance varies bit access general rule thumb exists tends little faster id less inclined say best practice factors involved 0,clear asking believe group one misunderstood concepts sql ill add answer anyhow may may help understanding concept group assume table like create table year int null parameterno int null mark int null primary key year parameterno insert year parameterno mark values would select year summark group parameterno mean grouping parameterno means two groups apply aggregate function sum year come play could mean group group hardly useful result another possibility randomly pick one row group like group group results unpredictable may get different result data query sql92 requires columns select clause part group clause want select year summark would add least 0
figuring physical size group tables sql server group tables want know physical size disk tables plus indexes easier way gui,use sp spaceused stored procedure example execute sp spaceused person person execute sp spaceused person address gives hope youre looking 8,looking get size information tables database use query select type desc indexsize obj name name index name indexsize reserved mb indexsize used mb indexsize row count indexsize object id indexsize index id select quotenameobject schema nameddps object id quotenameobject nameddps object id obj name sumddps reserved page count reserved mb sumddps used page count used mb sumrow count row count ddps object id ddps index id sys dm db partition stats ddps group ddps object idddps index id indexsize join sys objects indexsize object id object id join sys indexes indexsize object id object id indexsize index id index id 9,alternative horse names solution simplest option use row array comparision array select city cities temps see sqlfiddle necessary unnest array work arrays well rowsets unlike using array operators operation benefit use index tree gin temps normalized design splits temps separate table foreign key reference cities may actually faster lots cities lots samples earlier suggested may want use gin array index array operators mistaken support desired operation made error testing made appear frequent updates id normalize another table btree index table theres small amount data wouldnt bother indexes id use temps 0,building temporary indexes etl jobs necessarily bad practice index builds fairly quick might efficient relatively small incremental updates large tables sounds like case caveat expect tables grow substantially time work tables etl may well ok tables fact tables accumulate large data volumes next years index rebuilds may get slower time staging data dropping indexes loading make bulk loads staging much quicker may well need add indexes staging tables order support queries supplying etl process 0
examples sql transaction procedures sales tracking financial database making database accounting sales type system similar car sales database would like,depending complexity database design would probably need use transacations maintain data consistency tables change data within system examples would basic every database schema would different database data architect comes handy project 5,may want take car dealership data model model give idea transactions needed business requirements 4,classic financial example bank transfer funds begin transaction update account set amount amount account number update account set amount amount account number errors commit errors rollback unless post detailed description process application database schema generic examples like one give 4,yes use transactions inserting records multiple tables want make sure go accordingly really reason thing note financial databases including main project ledgersmb use call snapshot log aggregation model almost always dealing append data part really want make sure everything goes correctly 4,tried using adam machanics sp whoisactive theres option get outer command see really within proc could application holding open transaction instead committing try looking dbcc opentran well 0,depends disable constraint alter table table modify primary key disable drops index enable constraint index rebuilt preserves index alter table table modify primary key disable keep index disable constraint way index remains usable need rebuild enabling constraint 0,query optimizer treats inline table valued function exactly like view create function dbo inlineudf arg1 int returns table return query multi statement table valued function run like stored procedure typically executed multiple times rather folded main query create function dbo multistatementudf col1 int returns result table id int primary key null begin declare var1 int set var1 insert result select return end 0,think question stated collation meant given accepted answer talks encoding rather collation let answer stated question rather intended one think interesting wikipedia says collation assembly written information standard order computing collation taken meaning specification order words collation implies definition three way comparison function think short answer definitely maybe least im aware following shenanigans usr bin python name jonas xf6lker xf6 umlaut enc name encodeutf assert lenname xf6 one character assert lenenc two bytes utf import locale locale setlocalelocale lc collate da dk utf8 works machine long form locale strxfrmenc assert lenlong form locale strxfrm function returns string behaves cmp locale 0
sql server implementation longest common substring problem sql server implementation longest common substring problem solution checks rows column sql server,done rather easily sqlclr user defined aggregate uda aggregate operates set rows would able operate rows subset based condition optional group wanting operate separate sets rows whether depends planning result one project research wont repeated probably best make small console app read rows process accordingly however need use returned value within database centered process sqlclr fine assuming follow two recommendations mentioned following tricks used reduce memory usage implementation pseudocode section need find creative way dealing situation multiple results considered longest common substring common substrings tie first place using example wikipedia page shows matches strings abab baba returns bab aba perhaps 7,solomon probably right clr solution shows heres sql version play create test data sql creates 470k rows test duplicates object idtempdb strings null drop table strings select name string lena name stringlength strings sys columns sys columns name like refer set nocount nulls mean longest common string existsselect strings string null begin return end need know number rows sample length shortest string longest common substring longer shortest string set declare totalrows int declare minlen tinyint declare result varchar50 select minlen minstringlength totalrows countdistinct string strings raiserrornmaximum possible length total distinct rows d00 minlen totalrows nowait check backwards longest possible string 4,one mentioned thus suggestion take look massively sharded mysql solutions example see highly regarded tumblr presentation concept instead one extra large database use many small ones holding parts original data thus scale horizontally instead trying improve vertical performance googles bigtable gfs also using cheap horizontally scalable nodes store query petabytes data however troubles need run queries different shards anyone interested made hello world sharding application ago discussed blog post used ravendb details irrelevant idea 0,need pass refcursor procedure use output parameter quick procedure test create replace procedure passenger details passenger details sys refcursor begin open passenger details select test test full name age alien nationality foo category name airline name wobble class type dual end passenger details test sql plus sql variable mycursor refcursor sql exec passenger details mycursor pl sql procedure successfully completed sql print mycursor full name age natio cat airl class test test alien foo name wobble sql 0
mysql replication read slaves manually mysql manual replication scale solutions spreading load among multiple slaves improve performance environment writes updates,good solution define two datasources applications one datasource pointing master writing one datasouce pointing slave node reading applications must aware data delayed respect master applications could tolerate slight delay data applications must sure read data fresh example data updated infrequently always read slave like bills previous month data inserted must read master virtual ip assigned master another vip assigned slave applications connect master slave using vip master node changes vip assigned new master application dont reconfigured using vips usefull slave promoted master master demoted slave one slave load balancer aka haproxy could put slaves vip slave assigned load balancer connection 4,adding additional answer giovannis one completely right defines scaling ha master slaves split read queries among slaves architectural point view think op question read write split answer depends software stack used many mysql connectors orms frameworks etc allows define several connections read write read example php connector mysql plugin read write splitting send read statements marked different connection connector support always use fabric aware connector framework handle ha read write split among things course always program manually opening two connections use one writing another reading managing connections low level reopening fail retrying statements etc tricky main problem read write split 4,confusing various concepts major error primary key clustered key misunderstanding majority guidance incorrect brutally honest probably well placed writing guidelines primary key clustered key unique identifiers make poor clustered keys sql server well fine tuned use integer pks case narrow clustered key covered link matter sql server optimised integer clustered keys 0,value indexed non persisted computed column persisted data pages table persisted pages index remains non persisted table regardless whether persisted multiple indexes illustrate brents description taking example gave lets insert row insert dbo invoicecustomerid invoicestatus values1nsent lets see index pages dbcc traceon3604 dbcc indndbname ndbo invoice obviously change dbname index id might case output surely differ finally lets inspect page pagetype dbcc page7 likely need change match database id multiple data files may need change second argument match pagefid first result output thats index page 0
whether create separate tables different product types im process designing database im second thoughts initial design decisions product types follows,would suggest start correct relational model option typical usage model leads toward denormalising areas dont afraid discussing colleague last week schema designs often considered something set stone ever change strange considering refactoring accepted practice every layer application refactoring database schema still viewed impractical interface database well designed theres nothing stopping adapting schema learn systems usage patterns 7,design decision would probably go option modified option first option one thing like clarity product table affords one big table field determine type relation isnt clear another indexing strategy would always require type field listed since types index cardinality extremely low select product table type basically full table scan anyway option create parent table holds columns types share create product type table individual columns one extra link parent table create link table product option model option etc links respective keys reciprocal links model option option model go ahead create tables well add clarity joins anyone looking downside complexity making sure 8,92k 4m still records id suggest first option throw want deleted database let database sort give database chances itll efficiently parcel time id recommend chunking application time critical youre worried slowing case solution hours taking records table id recommend dropping indexes delete recreating indexes followed optimize table records table index rebuild overhead isnt worth faster keep indexes date deleting records said youve done delete running optimize table might bad idea dont peak hours though 0,appears greatest per group problem get maximum seats per engine results select engines maxseats max seats planes group engines using derived table join back source get rows matching maximums select engines manufacturer model es max seats planes inner join select engines maxseats max seats planes group engines es engines es engines seats es max seats 0
sql server ntext columns string manipulation table ntext column called comments second string lets call anothercomment varchar needs placing inside,converting nvarcharmax work unless something wrong charindex try code snippet output want create table create table dbo philstable comment ntext null anothercomment nvarchar null primary textimage primary go insert long string insert dbo philstable comment anothercomment values nthis test updatehere end test replicate castnx nvarcharmax goes verify data select datalengthcomment dbo philstable perform replace select castreplacecastcomment nvarcharmaxupdatehereupdatehere anothercomment ntext dbo philstable drop table dbo philstable thanks go andriy helping replicate statement 8,converting nvarcharmax back ntext make life simpler code point view mean converting rewriting whole perhaps large value cpu logging overhead implies alternative use updatetext deprecated like ntext reduce logging overhead significantly downside means using text pointers operates one row time following example code uses cursor work around limitation uses patindex instead charindex since former one functions work directly ntext sample data create table dbo philstable comment ntext null anothercomment nvarchar50 null insert dbo philstable comment anothercomment values convertntext nthis test updatehere end test replicate convertnvarcharmax nx convertnvarchar50 inserted convertntext nthis test updatehere end test replicate convertnvarcharmax nx convertnvarchar50 inserted convertntext 10,sql tools alternative tried didnt suit needs might sql tools built sql tools 0,one way works older versions well select result select top value counter order id desc select top value counter order id asc 0
encryption data log backup files using always encrypted sql server implement always encrypted feature sql server mdf files ldf files,whole entire data log file doesnt get encrypted specific fields encrypt encrypted yes encrypted data log backup files sql server never sees unencrypted values docs always encrypted microsoft explains always encrypted allows clients encrypt sensitive data inside client applications never reveal encryption keys database engine sql database sql server isnt great wording means data encrypted sql server engine doesnt even key decrypt thus name far sql server engine concerned data always encrypted 4,documentation states opening paragraph driver encrypts data sensitive columns passing data database engine automatically rewrites queries semantics application preserved similarly driver transparently decrypts data stored encrypted database columns contained query results implied encrypted columns encrypted therefore database logs backups entirely encrypted setup always encrypted specify columns encrypted data columns encrypted data encrypted prior sent sql server data encrypted inside data file log file inside backups taken database data columns encrypted always encrypted visible plain text inside mdf ldf perhaps compressed inside backup files database transparent database encryption encrypt entire database mdf ldf backups taken database entirely encrypted sql server instance 4,possible restore sql server database sql server without restoring r2 first answer aaron bertrand details alternate options get data sql database https dba stackexchange com 0,norecovery specifies roll back occur allows roll forward continue next statement sequence case restore sequence restore backups roll forward recovery default indicates roll back performed roll forward completed current backup recovering database requires entire set data restored roll forward set consistent database roll forward set rolled forward far enough consistent database recovery specified database engine issues error source restore sequence understanding norecovery recovery 0
lots indexes mysql vs mongodb migrating ive never used mongodb read lot think going good project also lots experience mysql,given description strongly suggest using mongodb would necessarily bad choice although believe case reasons pure technical ones points caught eye data modeling trying use mongodb relational data model adaptations almost always leads tears misery except trivial use cases better end story worse end loosing money potentially big time reason sql identify entities attributes relations bang head wall hours get upper left beyond joins right get questions derived use cases answered avoiding data redundancy like devil holy water data modeling mongodb works different identify use cases questions derived model data way questions answered efficient way since bit abstract let give example 7,deem multiple important aspects need consider deciding tool going employ develop project primary objective manage pertinent data quite valuable organizational asset reliable manner achieve said objective way technical means supported sound theory regard worth mentioning success determined database depend database management system dbms choice also number factors logical model physical implementation settings qualified administration since considering sql platform tentative dbms fact suggests intention implement relational database focus respect throughout present answer although dr codd turing award recipient published seminal paper relational model large shared data banks back really consider exceptional work remains unparalleled state art solidly based first order logic 5,index used optimize group order uses different columns sorting use index index would help database would able read rows table sort order collate nocase index help use different collation query add normal index use group seriesname collate nocase allowed using offset clause pagination efficient database still group sort rows begin stepping better use scrolling cursor note guarantee dbid dlstate values come specific row sqlite allows non aggregated columns aggregated query bug compatibility mysql 0,one time another ive worked databases mention unfortunately found doesnt take long syntax deviate flavours anything simplest select insert update delete gets categories suggest quickly get vendor specific ive always pretty much port sql one platform another although going back years amazed different sql server teradata sql even update joins 0
value use select top return rows im allowing end user define many rows returned query select top value entered rows,select top percent used bypass errors using top query 9,well looks like top bigint arent using percent means could pass max value bigint select top table1 seriously doubt ever see table large im sure kind effect would query plan though 17,could also consider set rowcount select foo bar order baz instead select top foo bar order baz value would need set disable deprecated data modification statements deprecated select different plan compiled case rowcount vs non zero value order baz give meaning top rather provide presentation order results dont index supporting splitting two queries would avoid unnecessary sort case 12,according comment deleted post load rows java module conduct search searching better done database thats database good return rows actually need really need rows many little things make faster 1m rows never fast though postgres later make index covering appending fcv id create index factura venta orden factura venta fcv fecha comprobante fcv numero comprobante fcv id way provided table isnt updated much postgres retrieve results index scan additional column comes last since contribute sort order explanation composite index also good queries first field postgres later could make create index factura venta orden factura venta fcv fecha comprobante fcv numero 0,would need create table even columns let alone granted denormalized data prevent joins retrieve many columns data however columns stop think would happen hood data retrieval column table undergoes select would generate large temp tables processing fetching columns unnecessary creating many scenarios communication packets max allowed packet would pushed brink every query earlier days developer worked company back db2 main rdbms company single table columns dozens indexes performance issues retrieving data contacted ibm consultants look architecture system including one monolithic table company told normalize table next years db2 fail queries stage2 processing queries requiring sorting non indexed columns told multi 0,sounds prime front end display solution query would pull back data query would pull back column names code build ever structure use display set headers second query pure sql method may possible dynamic sql code maintnence would nightmare also probably looking sp executesql execute nquery string may fix issue command completed successfully 0
sql server updating fields huge table small chunks get progress status large 100million row table need update couple fields log,answering second part print output loop long running maintenance procedures sys admin sometimes run run ssms also noticed print statement shown ssms whole procedure finishes im using raiserror low severity declare vartemp nvarchar32 set vartemp convertnvarchar32 getdate raiserror nyour message current time vartemp nowait im using sql server standard ssms complete working example run ssms declare varcount int declare vartemp nvarchar32 varcount begin set vartemp convertnvarchar32 getdate raiserror nyour message current time vartemp nowait print vartemp waitfor delay set varcount varcount end comment raiserror leave print messages messages tab ssms appear whole batch finished seconds comment print use raiserror messages 4,aware question answered related question explicit transactions needed loop sake completeness address issue part suggestion linked answer since suggesting schedule via sql agent job million rows dont think form sending status messages client ssms ideal though ever need projects agree vladimir using raiserror nowait way go particular case would create status table updated per loop number rows updated thus far doesnt hurt throw current time heart beat process given want able cancel restart process weary wrapping update main table update status table explicit transaction however feel status table ever sync due cancelling easy refresh current value simply updating manually count 12,note tested server lying around preeeettty sure though work though caution noted comments erny note high cpu load due operations may expected pretty much time using temporary tablespace time form exclusive locks table vacuuming happen client queries simply wait lock acquired access table question dont need close existing connections one thing aware though moving table vacuum full need wait exclusive lock first first obviously need additional storage st phane mentions comments needs least twice big table question vacuum full full copy lucky dynamically add disk machine worst case attach usb disk risky slow though next mount new device make available 0,typical user select insert delete update create temporary tables execute first four pretty obvious though may also set read users select create temporary also handy typically harmless temporary tables help optimizing queries breaking smaller faster parts limited executing connection automatically dropped closed execute depends type system stored routines would like users access make sure also know security definer invoker definition stored routines case make sure apply specific schemas avoid using grant select insert update delete user host also grants privileges mysql system tables effectively allowing user create new accounts upgrade set privileges instead grant select insert update delete schema user 0
error code many columns mysql column limit table table columns im trying insert columns get error code many columns table,im trouble imagining anything data model could legitimately contain columns properly normalised table guess youre probably sort fill blanks denormalised schema youre actually storing different sorts data one table instead breaking data separate tables making relations youve got various fields record type data stored given row fields null even though want get columns yikes solution problem rethink data model youre storing great pile key value data thats associated given record model way something like create table master id int primary key auto increment fields really relate master records basis create table sensor readings id int primary key auto increment master 25,mysql column count limits emphasis added hard limit columns per table effective maximum may less given table exact limit depends several interacting factors every table regardless storage engine maximum row size bytes storage engines may place additional constraints limit reducing effective maximum row size maximum row size constrains number possibly size columns total length columns exceed size individual storage engines might impose additional restrictions limit table column count examples innodb permits columns 15,would need create table even columns let alone granted denormalized data prevent joins retrieve many columns data however columns stop think would happen hood data retrieval column table undergoes select would generate large temp tables processing fetching columns unnecessary creating many scenarios communication packets max allowed packet would pushed brink every query earlier days developer worked company back db2 main rdbms company single table columns dozens indexes performance issues retrieving data contacted ibm consultants look architecture system including one monolithic table company told normalize table next years db2 fail queries stage2 processing queries requiring sorting non indexed columns told multi 36,measurement system sensors ignore comments shouting normalization asking could sensible database design ideal world perfectly well normalized unusual pointed elsewhere rdbmss usually simply designed many columns although hitting mysql hard limit one factors mentioned link probably preventing going higher others suggest could work around limitation child table id sensor id sensor value simply could create second table contain columns fit first use pk 18,first flaming real solution mostly agree flames already thrown disagree key value normalization queries end horrible performance even worse one simple way avoid immediate problem limitation number columns vertically partition data say tables columns would primary key except one might auto increment perhaps better would decide dozen fields important put main table group sensors logical way put several parallel tables proper grouping might join tables time indexing values need search probably search datetime need index lots columns punt need index put main table heres real solution applies dont need vast array sensors indexed dont make columns yes heard instead collect 7,adding previous answers reason plan regression might due known cardinality estimation bug plan includes anti semi join see kb assuming plan produced acceptable performance may find bringing server version includes fix enabling tf4199 activate return good plan said many opportunities improve query might good time concentrate instead 0,nosql stands sql usually means database relational database popular last decades reason nosql popular last years mainly relational database grows one server longer easy use words dont scale well distributed system big sites mentioned google yahoo facebook amazon dont know much digg lots data store data distributed systems several reasons could data doesnt fit one server requirements high availability cap theorem properties distributed system described cap theorem three properties two onsistency vailability tolerance network artitioning amazon dynamo uses eventual consistency come close get three properties paper dynamo amazon highly available key value store worth reading learning nosql databases distributed systems 0,took another approach case create login map database go database create schema called public view example owner schema must owner tables views gon na refer grant new user access new schema create many views want new schema new user access good thing new user access tables wont even able see tables hope helps 0,documented sort number times function specified query actually executed vary execution plans built optimizer example function invoked subquery clause number times subquery function executed vary different access paths chosen optimizer user defined functions query form query plan execute newid multiple times change result confusing actually critical newid useful key generation random sorting whats confusing non deterministic functions actually behave like instance rand getdate execute per query 0,handful queries use general concepts table information catalog operation messages interested events error type depending robust query want build following two derived tables might also interest http technet microsoft com en us library ff877994 aspx query translates message type ssisdb catalog operation messages useful text select message type message desc values 1unknown 120error 110warning 70information 10pre validate 20post validate 30pre execute 40post execute 60progress 50statuschange 100querycancel 130taskfailed 90diagnostic 200custom 140diagnosticex whenever execute package task executes child package logs event event message consists parameter values passed child packages value message column diagnosticex xml text 400nondiagnostic 80variablevaluechanged message type message desc error 0
causes solutions mutating table errors understand mutating table errors caused design flaw problematic query old query recently put production throws,likely cause mutating table error misuse triggers typical example insert row table trigger table row executes query table example compute summary column oracle throws ora table mutating trigger function may see expected normal behaviour oracle wants protect since oracle guarantees statement atomic either fail succeed completely ii statement sees consistent view data likely write kind trigger would expect query see row inserted would contradiction points since update finished yet could rows inserted oracle could return result consistent point time beginning statement examples seen try implement logic people see multi row statement serie successive steps expect statement see changes made previous 17,mutating table occurs statement causes trigger fire trigger references table caused trigger best way avoid problems use triggers suspect dba take time could done one following changed trigger trigger changed row level trigger statement level trigger convert compound trigger modified structure triggers use combination row statement level triggers made trigger autonomous commit 9,since requires copying data master child active production environment personally went creating new master prevents disruptions original table actively use issues easily delete new master without issue continue using original table steps create new master table create table new master id serial counter integer dt created date default current date null create children inherit master create table child constraint pk primary key id constraint ck check dt created date inherits new master create index idx child dt created create table child constraint pk primary key id constraint ck check dt created date dt created date inherits new master create index 0,a1 plan handle hash group statements batch a2 dm exec query plan returns query plan xml format need click order see graph a3 try select query plan sys dm exec text query plan 0x06000100a27e7c1fa821b106000 0
adding row number column order im working code golf puzzle need add int number column result maintaining current order lets,cant rely insert generate identity values order original string may observe lucky coincidence certainly guaranteed following work dont duplicates declare str varchar255 onetwothreefourfive select value row number order charindex value str string splitonetwothreefourfive order code golf maybe declare char99 onetwothreefourfive select valuen rank overorder charindex value string splitonetwothreefourfive order duplicates becomes lot complex ideas maybe removing duplicates strings sql server solve old problems sql server new string agg string split functions 6,canonical way following row number overorder select null youre golfing might try something like select value row number overorder select string splitonetwothreefourfive works simple case posted question say documented guarantee row number value precise order expect code golf seems good enough 10,another option use sequence drop sequence exists dbo create sequence dbo start select value next value dbo string splitonetwothreefourfive output one two three four five annoying string split implemented without built numbering option vote change https feedback azure com forums sql server suggestions string split feature complete 5,postgres easy using generate series function parameters values select case g2 gn else gn end xi parameters generate series1 gn generate series1 g2 generate series1 gm order gm g2 gn standard sql assuming reasonable limit size parameters less million use numbers table create table numbers int null primary key fill preferred method dbms insert numbers values mildly complex sql need type million numbers use instead generate series parameters values select case g2 gn else gn end xi parameters join numbers gn gn join numbers g2 g2 join numbers gm gm order gm g2 gn 0,im inclined agree catcall database recovery top list implications backup recovery options typically poorly understood outside dba team likely result disaster ensure defined agreed technical non technical management rpo recovery point objective rto recovery time objective databases systems document backup recovery procedures extent could followed non technical staff ensure documentation held printed well electronic form offsite disaster recovery runbook stored local network wont much use buildings fire test every aspect recovery procedures often backups irrelevant restores matter next database agnostic perspective understanding database server built provide atomicity consistency isolation durability transactions data commonly misunderstood frequently cause performance problems primary source 0,would consider using outfile part select instead mysqldump solve problem produce whatever select statement want append outfile path outfile csv end appropriate configuration csv style output simply use something like load data infile syntax load data new schema location example using sql select table1 id table1 level table2 name table2 level table1 join table2 table1 id table2 table1 id join table3 table3 id table2 table3 id table3 name fee fi fo fum outfile tmp fee fi fo fum csv fields terminated optionally enclosed lines terminated keep mind youll need enough available storage space target disk partition 0
select distinct multiple columns supposing table four columns abcd data type possible select distinct values within data columns return single,clear id use union ypercube suggests also possible arrays select distinct unnest array aggdistinct array aggdistinct array aggdistinct array aggdistinct order unnest dbfiddle 10,could use lateral like query select distinct atable cross join lateral values lateral keyword allows right side join reference objects left side case right side values constructor builds single column subset column values want put single column main query simply references new column also applying distinct 12,update tested queries sqlfiddle 100k rows separate cases one distinct values another lots around 25k values simple query would use union distinct think would efficient separate index four columns would efficient separate index four columns postgres implemented loose index scan optimization hasnt query efficient requires scans table index used query ms 368ms select abcd tablename union means union distinct select tablename union select tablename union select tablename another would first union use distinct also require table scans use indexes bad efficiency values values becomes fastest extensive test query ms ms select distinct abcd select tablename union select tablename union select 24,shortest select distinct observations unnestarray abcd less verbose version andriys idea slightly longer elegant faster many distinct duplicate values select distinct observations lateral values abcd tn fastest index involved column distinct many duplicate values recursive ta select observations order limit parentheses required union select ta lateral select observations order limit tb select observations order limit union select tb lateral select observations order limit tc select observations order limit union select tc lateral select observations order limit td select observations order limit union select td lateral select observations order limit select table ta union table tb union table tc union table 7,grave digging correct something previous answer stating use nice ionice help limit impact cache impact however theres cost backup take longer complete backup run pg stop backup system understand accumulating wal delete accumulating checkpoint debt big checkpoint end backup run accumulating table index bloat cant clean dead rows really cant afford backup take forever especially high churn tables thats true system keep number wal stated configuration cf online documentation basically higher value checkpoint completion ratio checkpoint segments wal keep segments lets imagine case backup taking long time theres hundreds gigs copy small wal retention checkpoint segments example dont setup wal 0,oracle free tool sql developer presumably reverse engineer oracle database database isnt large could possibly use tool like dbvisualizer either free low cost depending version diagram one schema time results arent directly editible really exploration tool save diagram gml edit tool yed free download note need edit gml file slightly bringing yed replacing instances customconfiguration simplerectangle type rectangle note sql developer dbvisualizer yed cross platform tools use system java installed update tried reverse engineering tables using sql developer appears work reasonably well isnt going win beauty contests 0,look lock pages memory way give preference sql service account use available ram rather paging disk read lock pages memory check link snippet follows windows policy lock pages memory option disabled default privilege must enabled configure address windowing extensions awe policy determines accounts use process keep data physical memory preventing system paging data virtual memory disk bit operating systems setting privilege using awe significantly impair system performance locking pages memory required bit operating systems please test feature using systems 0,basically reason dont typically name parameters parameter1 parameter2 accurate descriptive see tableid probably safely assume used hold pk table regardless context whoever used ident entirely misses point given choice ident id use id ident even confusing id context id assumed primary key table extremely useful unless id guid ident doesnt even tell least would eventually figure ident short identity one way another time spent figuring would wasted 0
possible mysqldump subset database required reproduce query background would like provide subset database required reproduce select query goal make computational,mysqldump util tables option lets specify tables dump lets specify list tables dont know easier automated way 6,mysqldump option execute clause given table although possible mysqldump join query export specific rows table every row fetched table involved join later given query would need mysqldump three times first mysqldump table3 rows name feefifofum mysqldump name feefifofum mydb table3 table3 sql next mysqldump table2 rows matching table3 id values first mysqldump mysqldump lock tables table3 id select id table3 name feefifofum mydb table2 table2 sql mysqldump table1 rows matching table1 id values second mysqldump mysqldump lock tables id select table1 id table2 table3 id select id table3 name feefifofum mydb table1 table1 sql note since second third mysqldumps require 52,would consider using outfile part select instead mysqldump solve problem produce whatever select statement want append outfile path outfile csv end appropriate configuration csv style output simply use something like load data infile syntax load data new schema location example using sql select table1 id table1 level table2 name table2 level table1 join table2 table1 id table2 table1 id join table3 table3 id table2 table3 id table3 name fee fi fo fum outfile tmp fee fi fo fum csv fields terminated optionally enclosed lines terminated keep mind youll need enough available storage space target disk partition 7,terms backup compression couple years ago make comparison backup compression options provided red gates sql backup questss litespeed sql server ideras sqlsafe benchmarking three products differences typical backup maximum compression spread three time taken somewhat wider spread backup size red gate coming top compression vs idera quest order 0,option psql stops executing commands error error stop could raise error somehow would want problem test variable produce error somehow since one cant use control structures psql none idea use sql testing well producing error conditionally something pl pgsql quite good wrote function would generate error call function simple case structure simple example lets assume clarity function name database create replace function error generator returns boolean body begin raise meaningful error message return false aesthetical purposes end body language plpgsql set error stop begin test variable value notice var set fails well syntax error select case var error generator else 0,select count table returns one row count relatively light way get datum select physical legal allowed however problem select cause lot data movement operate every column table select includes columns might able get answer index indexes reduces also impact server cache yes recommended general practice wasteful resources real benefit select typing column names ssms use drag drop get column names query delete need analogy someone uses select need every column would also use select without limiting clause need every row 0
retrieving row count without using count function wrote query shoots syntax error would select maxrow select row number overorder id,actually three problems query first maxrow return string row second subquery needs alias try like select maxrow select row number overorder id desc row users userquery third problem count much better way expertly described answer gbn note also row reserved keywords list avoided well 13,theres another variant without scanning table using system tables select si rowcnt sys objects join sysindexes si si id object id type desc user table si indid heap clustered index name users compare plans durations time youll see faster variant 6,want get exact count rows efficient manner count ansi standard look scalar expressions states count give row count table intended optimised start count specified result cardinality row number function isnt practical option isnt counting function row number run badly add rows show bad sum1 may optimised count internally id never use rowcount require rows returned first select huge unnecessary overhead live approximate sql server use sys dm db partition stats marians answer date since sql server added dmvs select total rows sumst row count sys dm db partition stats st object nameobject id mytable index id see info https stackoverflow 21,assumptions clarifications need differentiate infinity open upper bound upperrange null either way simpler way null vs infinity postgresql range types since date discrete type ranges default bounds per documentation built range types int4range int8range daterange use canonical form includes lower bound excludes upper bound types like tsrange would enforce possible preventing adjacent overlapping entries exclude postgresql solution pure sql ctes clarity select range coalescelowerrange infinity startdate maxcoalesceupperrange infinity order range enddate test select lagenddate order range startdate null step select countstep order range grp select daterangeminstartdate maxenddate range group grp order subqueries faster less easy read select daterangeminstartdate maxenddate range 0,actually wrote article type calculation basically use following code find 3rd friday month date range use tempdb set nocount object iddbo null drop table dbo create table date datetime year smallint quarter tinyint month tinyint day smallint 1st 366th day year week tinyint 1st 54th week year monthly week tinyint 1st 2nd 3rd 4th 5th week month week day tinyint mon tue wed thu fri sat sun go use tempdb populate table day week defined mon tue wed thu5 fri sat sun c0 select values11 dc c1 select c0 cross join c0 c2 select c1 cross join c1 c3 select 0,quoting brent ozars article sql server virtualization question regarding virtualization san recommendation setting block size sql server related settings including ntfs allocation unit size raid stripe size partition offset check san vendor documentation see right cases oltp databases decent shape 64k ntfs allocation unit size raid stripe size 1mb partition offset yes default youll safe 64k block size see also storage documentation maybe specify preferable unit database server great information blog san storage best practices sql server 0
switch schemabinding view without recreating switch schemabinding view without recreating,yes good use schemabinding always sometimes remove change dependent object alter view alter view myview remove schemabinding select go 11,wont alter view allow get done create view would create view schemabinding select stmt go lose clause alter view viewname select stmt go see alter view msdn 8,looking around hours created stored proc hope helps someone create procedure viewremoveschemabinding viewname varcharmax begin declare positionshemabinding int declare command nvarcharmax select command object definitionobject id viewname set positionshemabinding charindexwith schemabinding command positionshemabinding begin schema binding present lets remove set command stuff command charindexwith schemabinding command lenwith schemabinding set command replace command create view alter view execute sp executesql command end end put schemabinding create procedure viewaddschemabinding viewname varcharmax begin declare positionshemabinding int declare command nvarcharmax declare objectname varcharmax select command object definitionobject id viewname objectname object nameobject id viewname set positionshemabinding patindex schemabinding command positionshemabinding begin schema binding present 4,state codes meaning account locked user id valid undocumented user id valid undocumented login used disabled incorrect password invalid password related sql login bound windows domain password policy enforcement see kb925744 login valid server access failed login valid permissioned use target database password expired initial database could found login valid database unavailable login permissioned detailed information available aaron bertrands blog 0,foreign key cant made conditional question business rule appears employee work one one physical store given super type store two sub types suggested physical online physical store may staffed one employees employee must assigned one one physical store physical stores two sub types brick mortar kiosk three direct sub types kiosk online brick mortar hides property possessed every store whether found physical location design relies human understand semantics inherent sub type names understand online stores dont employees readily apparent declared schema code form trigger must written express understanding way dbms enforce developing testing maintaining trigger impact performance much difficult solution 0,use join create populate new table one go select dbo newtable dbo tablewithidentity left join dbo tablewithidentity condition right side matches thus prevent duplication left side rows outer join left side rows eliminated either finally join identity property eliminated selecting left side columns therefore produce exact copy dbo tablewithidentity data wise identity property stripped said max vernon raised valid point comment worth keeping mind look execution plan query notice source table mentioned execution plan instance eliminated optimiser optimiser correctly establish right side join needed plan reasonable expect future version sql server may able figure identity property need removed either since 0
sql server agent jobs availability groups im looking best practice dealing scheduled sql server agent jobs sql server availability groups,within sql server agent job conditional logic test current instance serving particular role looking availability group select ars role desc sys dm hadr availability replica states ars inner join sys availability groups ag ars group id ag group id ag name youravailabilitygroupname ars local primary begin server primary replica something end else begin server primary replica optional something end pull current role local replica primary role whatever job needs primary replica else block optional handle possible logic local replica isnt primary course change youravailabilitygroupname query actual availability group name dont confuse availability groups failover cluster instances whether instance primary secondary 40,im aware two concepts accomplish prerequisite based thomas stringers answer created two functions master db two servers create function dbo svf agreplicastate availability group name sysname returns bit begin exists select ag name sys dm hadr availability replica states ars inner join sys availability groups ag ars group id ag group id ars local ars role desc primary ag name availability group name return return end go create function dbo svf dbreplicastate database name sysname returns bit begin exists select adc database name sys dm hadr availability replica states ars inner join sys availability databases cluster adc ars group id 9,rather per job basis checking every job state server deciding continue ive created job running servers check see state server primary enable job step targeting database ag server secondary disable job targeting database ag approach provides number things works servers databases ag mix dbs ags anyone create new job worry whether db ag although remember add job server allows job failure email remains useful jobs failure emails right viewing history job actually get see whether job actually ran something primary rather seeing long list success actually didnt run anything secondary script checks database field proc executed every mins server added 14,yes nested loop join run bottom inner input row outer input using examples data change query full outer join without join condition machine result plan merge join select full outer join 0,big roughly kind drives behind laptops 5400rpm drives database tiny incurring lot autogrow events suspect kind configuration problem ran following code seconds win7 vm 7200rpm drives create table dbo test1 id int identity11 text varchar2000 timestamp datetime null defaultgetdate go insert dbo test1 text select sure long text supposed go questions machine vm application connecting via tcp ip shared memory named pipes activity going box triggers table resource governor implemented etc checked sys dm exec requests profiler see long individual inserts actually taking else sending server blocking waiting might occurring 0,purpose subquery understand select rows latest related entry billing pricequotestatus qualifying name incorrect 2nd query immediately clear modification didnt produce equivalent output 1st query picks latest row billing pricequotestatus checks whether name qualifies name adjustmentpaymentbillable 2nd query backwards check row qualifying name last one also doesnt make sense compute aggregate exists semi join dont want equivalent consequently get rows 2nd query incorrect time range predicate mess inefficient possibly incorrect least ticking bomb pq date applied time zone pst 02t00 timestamp 03t22 timestamptz column date applied type timestamptz construct time zone pst converts type timestamp shifts time offset hard coded time 0
set get custom database variables using pgadmin iii right click database navigate variables tab put variable name value property database,add variable end postgresql conf like customized options custom variable classes general list custom variable class names general application version v1 restarted add general application version manually pgadmin show drop least otherwise use like postgresql variable update version newer postgresql onwards dont set custom variable classes anymore one set whatever variable want limitation seems still two parts set something bla error unrecognized configuration parameter something set thing something bla set guess avoid collision builtin parameters 8,add dezsos answer variables changed select set configclass name value valid transaction boolean read select current settingclass name see link info http www postgresql org docs static functions admin html 8,ideally would create database load bit sample data measure size extrapolate far accurate method estimating size database years want compute database size would generally start figuring many rows fit single block simplicity well assume rows never deleted updates never change size row well also assume compression used otherwise things get bit complicated calculate size data row fixed size data types date char thats size type variable size data types number varchar2 thats average size data column couple bytes additional overhead pretty safely ignore theyre going swamped errors estimating size actual data subsequent estimate number rows per block expect row bytes 0,depends add column require adding data rows quite quick example adding int char requires physical row movements adding nullable varchar default shouldnt unless null bitmap needs expand need try restored copy production get estimate creating new table copying renaming may take longer add indexes keys billion row table changed billion row tables took second add nullable column say take backup first 0
mysql benchmarking tools ive heard long time ago tool helps tweek mysql settings better performance cant seam find aware use,benchmarking tuning tool imho theres tool specific latter unless super generic usage need identify usage pattern tune database hosts accommodate youre write heavy different configuration read heavy scenario bottom line tuning follows applications usage benchmarking use sysbench heres example blog added heres beef config tools changes versions vs vs lot tweaks dont jive really need savvy dba evaluate whats going host load storage traffic application specific requirements theres lot go optimal configuration tool may get part way may leave something include something may cause failure buffers flushing plugins threading config tool may provide false confidence implementing correct config 6,think monyog handle request monyog mysql monitor advisor mysql dba box helps mysql dbas manage mysql servers tune mysql servers fix problems mysql database applications monyog finds problem sql monitors advisors well suggests parameter use mysql system variables editing cnf ini file helps fine tune mysql server many features dba would like day day activites like trend report dashboard server config management snmp smtp alerts etc details refer http www webyog com en monyog feature list php 4,correct different grains must mixed fact table reserve balance end month sum payments end month grain one facts semi additive type fact additive define tables grain describing see grain monthly claim snapshot makes fact table periodic snapshot fact table article kimball example additive semi additive facts fact table example periodic snapshot semi additive facts data warehouse toolkit page best practice transactional fact table reflect every change reserve payments adjustments lowest atomic level deal claims often atomic level claim sub claim insurance company may term generally sub claim represent different party claim payments reserves party example may payments insured payments insured 0,start sql instance minimal configuration mode net start mssqlserver connect instance cmd window change max memory setting sqlcmd server instance see prompt looks like default maximum memory setting may available viewing changing modify behavior use query sp configure show advanced options go reconfigure go sp configure max server memory go reconfigure go restart instance configuration manager 0
performance considerations using broad pk vs separate synthetic key uq several tables records uniquely identified several broad business fields past,significant disadvantage using natural key clustered index non clustered indexes foreign keys referencing table parent row downside would increased page splits data inserts would distributed throughout data instead end fks nc indexes using narrow numeric increasing clustered index advantages repeat bytes data per nc fk entry business natural key read articles google note avoided use primary key clustered index surrogate key keep pk business rules non clustered make sure clustered unique becauuse sql add uniquifier make finally may make sense surrogate key blindly every table many many tables need one compound key parent tables suffice 11,although risk stating obvious index surrogate key id number useful need locate things id number users going deal id number theyre going deal human readable text pass around text id number lot user interface display text operate id number dbms use kind index support foreign keys define way sometimes improve performance using id numbers foreign keys absolute improvement oltp system foreign keys using natural keys outperformed foreign keys using id numbers test suite think representative queries important information often carried keys using natural keys avoided lot joins median speedup factor joins using id numbers took times longer return rows tests 4,please check maximum script size set unlimited menu option tools options text editor transact sql general intellisense intellisense started working 0,question sensible strategy though question raised strategy optimal wouldnt default cases sensible strategy reason general oltp databases rows returned end user going whole lot generalization question ask isif seeking key columns many rows returned seek operation repeat queries seeking column consider following table returning whole lot columns selectiveidfield select columnacolumnc columnz dbo bigtable selectiveidfield one row returned seek selectiveidfield additional key lookup bad thing guessing clustered indexes otherwise rid lookup one extra key lookup one extra execution join operator even even would huge impact also depends much query executed important execution time case negligible create index selectiveidfield call day worth 0
sql server changes execution plan part previous question sql server changes execution plan using sql server developer edition sql server,likely explanation sessions different settings sql server various session settings affect execution plan selected results values settings depend connect sql server since different tools set options different ways connect like sql server management studio allow override defaults well example image reproduced erland sommarskogs definitive article topic slow application fast ssms understanding performance mysteries whole thing well worth reading definitely read section titled default settings make sure settings value connections get execution plans maximum compatibility features like indexed views ensure settings follows many settings maintained backward compatibility strongly recommended set shown table use tool sets right way automatically books online references 11,discount difference mvc app checked query executing using sql profiler similar problem recently turns query executed mvc app using entity framework executing sql statement sp executesql causing sql server use different execution plan compared running pure sql management studio changed use stored procedure rather linq 4,would instead recommend official method reproduce convenience change number size innodb log files mysql earlier use following instructions procedure use depends value innodb fast shutdown determines whether bring system tablespace fully date shutdown operation innodb fast shutdown set stop mysql server make sure shuts without errors ensure information outstanding transactions redo log copy old redo log files safe place case something went wrong shutdown need recover tablespace delete old log files log file directory edit cnf change log file configuration start mysql server mysqld sees innodb log files exist startup creates new ones innodb fast shutdown set set innodb fast 0,let start saying hate alter evil imho say current table schema create table table love id int unsigned null auto increment value varchar40 date created date primary keyid engine myisam charset utf8 heres path recommend create new table object replace old one create table table love new id int unsigned null auto increment value varchar40 date created date primary keyid engine innodb charset utf8 insert rows old table name new table insert table love new idmy valuedate created select idmy valuedate created table love smoke test migration select count table love new select count table love select ida valuea date 0
compare two tables amount respect priortization fundtype two tables one paymentdetail another ledger want prioritize paymodetype manner cash considered first,exactly output want youll get ideas test sql fiddle order paymentdetail ledgersortorder select sortorder fundtype union select union select ledgerordered select row number partition recptno order lso sortorder ledgerid rn ledger join ledgersortorder lso lso fundtype fundtype ledgercte select l1 recptno l1 amount l1 fundtype suml2 amount sumamount ledgerordered l1 join ledgerordered l2 l2 recptno l1 recptno l2 rn l1 rn group l1 recptno l1 rn l1 amount l1 fundtype order ledger paymentdetailsortorder select sortorder cash paymodetype union select credit card union select dd union select cheque paymentdetailordered select row number partition recptno order pso sortorder paymentid rn paymentdetail join 4,think reasonable way meet requirement sql non cursor solution following sqlclr procedure returns result set solution performs single scan table test payment rows ledger rows completed seconds warm buffer pool seconds cold start including time display rows results ssms procedure uses external access permission set alter database name set trustworthy easiest way testing database must also valid owner deployment might prefer sign grant permission explicitly unable upload sql fiddle limit characters ddl statements create assembly dbase 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 permission set external access go create procedure dbo matchledgerrecords execute caller external name dbase storedprocedures matchledgerrecords sample data reproduced sql fiddle create table 7,many possible options databases get larger full backups take longer likely incorporate differential backups havent already creating differential backups fast compared creating full backup differential backup records data changed since full backup upon differential backup based facilitates taking frequent data backups decrease risk data loss understanding olas scripts even set decide full differential backup based amount change database using modificationlevel parameter use emc dd boost youre welcome opinion found due client side de duplication methods uses full backups even multi tb databases fast point dont worry sql server differential backups effect using emc dd differential backups sql server using multiple 0,table name unique table name unique identifier table name could exist another schema may work fine use table names queries without schema qualifying long search path set properly right tables picked wont help querying catalog tables tables name exist get back columns tables may even notice add schema name make unique select array select column name text information schema columns table name gis field configuration stage table schema public whatever schema information schema slow information schema good cross plattform portability hardly ever works anyway views information schema monstrous slow dont intend port another rdbms future dont use exotic features might 0
sql server dynamic data masking safe providing database backup sql server sp1 data ready share partner developer except columns example,would probably need contained users partner create new users connect existing logins sure youre sharing backup contained database whatever want data sysadmins im sure access servers see data way share database backup still trust masked columns safe physically delete data delete data cycle transaction log multiple times taking backups using vlfs inserting dummy data way cant mine log better yet create schema copy database insert data needed note still mine information statistics wouldnt script making copy send wanted secure another option would restore backup staging area encrypt data sensitive columns using form encryption column ae etc drop requisite keys sending 11,im sorry partner sysadmin login sql server sysadmin everything everything inside sql server instance including databases suggest develop routine clear sensitive data perform backup restore different database name clean produce new backup 13,following query uses idea amazing answer ypercube select select null somecol dummy cross apply select id somecol mytest dbo customers works like dbo customers column named somecol somecol somecol mytest resolve dbo customers somecol table column reference still valid resolved dummy somecol dummy columns referenced context specify multiple spare columns way trick use table alias columns frowned upon practice situations case omitting table alias helps resolve issue table used join table somecol probably need use query derived table using join order keep trick working something like select select select null somecol dummy cross apply select id somecol mytest dbo customers 0,tldr appears disabling enabling replication probably fixed issue exec sp replicationdboption dbname ndatabasename optname npublish value nfalse exec sp replicationdboption dbname ndatabasename optname npublish value ntrue guess equivalent switching back longer version workmate go trying fix tried things didnt get far one change make giving disable replication tried codys suggestion sp dropsubscription command complained subscriptions exists tried sp droppublication command complained replication enabled database enabled ran command time complained publication exist refreshed local publications node ssms sure enough gone ran replication set script generated new snapshot every thing working properly joy im certain disabling enabling replication actually fixed problem definitely 0
nested view good database design read somewhere long time ago book states allow nested view sql server sure reason cant,sometimes nested views used prevent repeating aggregates lets say view counts messages groups userid might view counts number users messages kind thing effective base view indexed view dont necessarily want create yet another indexed view represent data slightly different grouping youre paying index maintenance twice performance probably adequate original view nested views youre select changing ordering top seems would better encapsulated stored procedure parameters inline table valued functions bunch nested views imho 26,regardless platform following remarks apply nested views harder understand debug table column view column refer lem dig levels view definitions make harder query optimizer come efficient query plan see anecdotal evidence compare shows optimizer often smart enough correctly unpack nested views select optimal plan without compilation cost measure performance cost comparing view query equivalent one written base tables hand nested views let centralize reuse aggregations business rules abstract away underlying structure say database developers ive found rarely necessary example using nested views centralize reuse certain business definitions eligible student valid use nested views maintaining tuning database weigh cost keeping removing 47,later versions sql seem better optimizing use views views best consolidating business rules eg work telecom product database product assigned rateplan rateplan get swapped rates rateplan get activated deacitvated rates increased modified make easy make nested views 1st view joins rateplans rates using whatever tables needed returning necessary data next levels views would need 2nd views isolate active rateplans active rates customer rates employee rates employee discount business vs residential customer rates rateplans get complicated point foundation view ensures overall business logic rateplans rates joined together properly one location next layer views give us focus specific rateplans types active inactive 7,real issue nested views real issue proliferation nested views developers layer additional tweaks existing views found queries nested view layers actually joined one views definition tendency take easy way rather analyze solve problem root issue 4,paul white explained excellent lucid manner reason behind sql server behaviour running servers memory also huge thanks swasheck first spotting issue opened case microsoft suggested problem resolved using trace flag t2335 startup parameter kb2413549 using large amounts memory result inefficient plan sql server describes details trace flag cause sql server generate plan conservative terms memory consumption executing query limit much memory sql server use memory configured sql server still used data cache query execution consumers please ensure thoroughly test option rolling production environment 0,found answer yesterday help friend mine log via ssms user windows login attempting use delete old login add windows login able transfer ownership job properly sql able get user data windows right world 0,dont put nulls warehouse marts warehouse well normalized least bcnf therefore exclude nulls nulls might preserved staging tables exist data sources shouldnt needed warehouse marts designed support presentation tools user queries nulls get way things never displayed make user queries complex error prone especially foreign key columns frequently subject joins 0,database id indicated error message database affected tempdb one method fixing type corruption tempdb simply restart sql server instance database ids follow recommendations may need restore backup however try member sysadmin role execute dbcc page look metadata indexid value heap clustered index need restore backup rebuild non clustered index run dbcc command let us know find check https www mssqltips com sqlservertip using dbcc page examine sql server table index data details dbcc page command 0
way ignore pages memory query way ignore cached query plans data pages memory single query batch something like table hint,using option recompile query hint friend regards forcing new plan time execute query preventing data read cache impossible however clear cache assuming youre using production machine tests clearing cache force data read disk prior running query time production since cause quick noticeable drop performance queries running server use dbcc dropcleanbuffers test queries cold buffer cache without shutting restarting server may want run checkpoint command first ensure dirty pages written disk first clearly youre going want look actual execution plans clicking query include actual execution plan menu ssms press ctrl prior running query may also want turn statistics io time start 6,use cached plans could add optionrecompile query dont think similar option pages memory could use dbcc dropcleanbuffers clear pages buffer query run production ideally would want data memory tune query likely scenario unless query reading rarely used data 6,use wmsys wm concat instead select num1 replacewm concatdistinct num2 listaggtest note function undocumented unsupported see https forums oracle com forums message jspamessageid 0,brent points real data even smallest part sensitive dev test systems even temporarily depersonalise real data use test environments precautions take real data dev test systems even temporarily restore copy depersonalise signed production environments full security measures implies move munged data environments may want extra load main production setup case separate db server environment task make sure process fails safe fails hard even means fails often make fall new tables columns present give new list expect mitigate issue brent discusses temporary schema changes also make sure failed step stops whole process error altering data block move away production best practise 0
auto commit sql server oracle worked oracle many years used execute commit command manually bulk insert sql server auto commit,oracle asktom method oracle operates client tells us commit autocommit mode various tools apis eg tell sqlplus autocommit means sqlplus issue commit statement still true probably always detailed description commit see oracle docs sql server inverted turn autocommit setting implicit transactions prefer docs marks direct link 7,sql server would set implicit transactions implicit transaction mode dml select insert delete update etc ddl create alter drop etc start transaction must explicitly commit rollback alternatively explicit transaction started begin transaction 5,answered sql set implicit transactions connection set implicit transactions setting use links mark jack provided would caution looking across board even often connections flip side making change transaction actions dont properly manage cleanup open transactions hanging around blocking dml system concerned accidentally deleting updating wrong rows manual cleanup better options take backup first backup table first selecting rows new table select tmpincaseimessup tablename mw tablename get habit typing begin transaction writing update insert delete system sometimes ill start session type begin tran commit tran commented doesnt happen typing dont get busy forget ill work begin tran write select statement session 4,well strictly true oracle autocommit mode look underlying oci see oci commit success flag pass ocistmtexecute would well using common pattern insert commit insert logging time series data halves number network round trips need significant performance enhancement use flag especially wan links pass oci default flag yes autocommit default behavior 4,restore recent full backup restore database database name disk path full backup recovery restore recent differential backup restore database database name disk path differential backup norecovery restore transactional log backups taken recent differential backup restore database database name disk path first transactional log backup norecovery restore database database name disk path last transactional log backup recovery 0,managing databases really different managing embrace automation scalability plan place dont plan use high cost per database features like mirroring across clients previous job used architecture would never ever thought merging two clients single database even though challenges hard big benefits independent recovery models simple full etc ability restore point time remove entirely client without disrupting others ability seamlessly move resource heavy client storage completely different server little way transparency update config file table tells app find client address several objections approach problems posts handling growing number tenants multi tenant database architecture multi tenant database using sql server automation backups 0,locked need following stop mongodb instance remove auth keyfile options mongodb config disable authentication start instance without authentication edit users needed restart instance authentication enabled 0,article help set advance event happened past didnt kind auditing mechanism set still hope though lets say create login flooberella password nx check policy information default trace eventclass audit addlogin event however change password using either methods alter login flooberella password ny exec sp password ny nz nflooberella events captured default trace obvious security reasons possible anyone access default trace figure someone elses password want make easy even find password changed polling frequency events example reveal certain properties security strategy else relies information still log also relies using undocumented dbcc command system database may wish back master restore elsewhere get 0
ssis purpose flagging package entry point package visual studio designer right click ssis package designate entry point package search found,theres one behaviour think packages entry point package set executed catalog create execution see http msdn microsoft com en us library ff878034 aspx 8,side note default scope parameters configuration page integration services catalogs called entry point packages project probably want disable flag child packages parameters doesnt show clutter configuration dialog 6,using sql server express addition answers trying connect using port ensure use comma syntax colon syntax myservername1433 instancename wrestling couple hours following suggestions still connecting used instead connected right away sql server management studio remotely first step success 0,daniels answer focuses cost reading individual rows context putting fixed size null columns first table helps little putting relevant columns first ones query helps little minimizing padding due data alignment playing alignment tetris columns help little important effect mentioned yet especially big tables additional columns obviously make row cover disk space fewer rows fit one data page kb default individual rows spread pages database engine generally fetch whole pages individual rows matters little whether individual rows somewhat smaller bigger long number pages read query fetches relatively small portion big table rows spread less randomly whole table supported index result roughly 0
automatically restart sql server best way automatically restart sql server regular basis read could create batch file net start net,wont go detail great idea since youre already aware anybody else stumbling across answer try possible occasions need schedule hours restart patching configuration changes etc normally run batch file scheduled via windows task scheduler net stop sqlserveragent net stop mssqlserver net start mssqlserver net start sqlserveragent note service names may vary arent restarting default instance also user account scheduled task runs need permission restart service dont see reason wouldnt work recurring schedule opposed one time runs 15,sake thorough also powershell like db2 suggested schedule powershell script run scheduled routine task scheduler stop service name mssqlserver start service name mssqlserver one fell swoop restart service name mssqlserver essentially bounce sql server service reiterate well routinely restarting sql server advised 9,also migrate fly another server sending data pipes ssh useful target host different mysql version example wrote blog post explaining http blog techutils space fly database migration two html explanation dont care run something like target host nc gunzip pv mysql tdb user ptdb pass targetdatabase source host mysqldump sdb user psdb pass sourcedatabase pv gzip ssh sshuser targethost nc 0,according comment deleted post load rows java module conduct search searching better done database thats database good return rows actually need really need rows many little things make faster 1m rows never fast though postgres later make index covering appending fcv id create index factura venta orden factura venta fcv fecha comprobante fcv numero comprobante fcv id way provided table isnt updated much postgres retrieve results index scan additional column comes last since contribute sort order explanation composite index also good queries first field postgres later could make create index factura venta orden factura venta fcv fecha comprobante fcv numero 0
effective way discover running instances sql server using powershell ive tasked discovering instances sql server running within domain several cases,way know discovering instances across environment without knowing possible owning servers particular names would make call system data sql sqldatasourceenumerator getdatasources method comes lot footnotes though snippet pulled directly msdn resource due nature mechanism used sqldatasourceenumerator locate data sources network method always return complete list available servers list might every call plan use function let users select server list make sure always also supply option type name list case server enumeration return available servers addition method may take significant amount time execute careful calling performance critical call simple powershell system data sql sqldatasourceenumerator instance getdatasources method returns datatable object handle 5,want something useful future would probably steer clear trying search registry hives sql server changed bit years troublesome keep method sqldatasourceenumerator flaky times although use concrete evidence instances network believe depends sql browser service well time find disabled utilize wmi class win32 service use offers information service get service cmdlet write everything functions generally use actually daily check verification service troubleshooting function get servicestatus string server foreach server iftest connection count quiet get wmiobject win32 service computer displayname match sql server select systemname displayname name state status startmode startname bit usually use case someone else comes across wants use test 12,yes chance installation succeed work around issues wouldnt supported configuration might work also try docker image 0,look ms docs limitations restrictions top used insert update merge delete referenced rows arranged order order clause directly specified statements need use top insert delete modify rows meaningful chronological order must use top together order clause specified subselect statement see examples section follows topic update top500 instances set thing new thing something somethingelse update instances set thing new thing select top id instances order something t1 instances id t1 id update derived table directly without join update t1 set thing new thing select top instances order something t1 example approach found dbfiddle 0
sql join syntax ms sql taught mssql classes join two tables select firsttable join secondtable id id professional life came,queries second type fall call sql antipattern check nice book written bill karwin second query almost resembles cartesian join whose clause evaluated fly first cleaner order execution better managed could compare ways getting explain plan seeing execution time creating third type refactors query get explain plan running time see indexes evaulated three query styles better going join syntax generate result sets left joins right joins may far different maybe desirable inner joins 5,posting good explanation differences ansi sql complaince queries produce result find always good idea explicitly state joins much easier understand especially queries contain non join related evaluations clause explicitly stating joins also saves inadvertently producing cartesian product rolandomysqlsba alluded 2nd query whatever reason forgot include clause query would run without join conditions return result set every row firsttable matched every row secondtable addition returning want make mistake like large tables hundreds thousands even millions rows cause performance issues database attempts fulfill query 5,developer centric dba centric ive heard good things rhino etl https github com ayende rhino etl 0,recovered mysql ibd frm files using mysql utilites mariadb generating create sqls get create sqls frm file must use https dev mysql com doc mysql utilities en mysqlfrm html shell mysqlfrm server root pass localhost t1 frm port way may create sqls create tables create tables database alter table xxx discard tablespace discard tables want replace ibd files copy ibd files mysql mariadb mariadbs data path first try use mysql restrore database crashes immediately stops tablespace id broken error error hy000 got error storage engine used mariadb succesfully recovered data alter table xxx import tablespace run statement mariadb warns file 0
bug pl pgsql function creation dont know question better suits script id like launch code function copied question mydb create,try dispensing explicit cursor begin set role dba create role stack grant stack dba create schema authorization stack set role stack create table fooid serial insert foo default values create replace function truncate tablesusername varchar returns void declare record begin select tablename pg tables tableowner username loop execute truncate table quote identr tablename cascade end loop end language plpgsql select truncate tablesstack select foo id rows rollback 4,particular example simpler truncate multiple tables aggregate tablenames execute single statement create replace function truncate tables username text returns void func begin execute select truncate table string aggquote identt tablename cascade pg tables tableowner username schemaname public end func language plpgsql call select truncate tablespostgres string agg requires postgresql later substitute array stringarray aggquote identt tablename v8 would write aggregate function rather simple yet simpler looping solution performance degrades deleting truncating many tables postgresql fix upcoming version quote release notes improve performance checkpointers fsync request queue many tables dropped truncated tom lane related thread pgsql hackers craigs related answer helped 4,cant stress enough triggers place business logic would use triggers auditing purposes last updated datestamps putting business logic inside triggers may seem like good idea first likely end shooting foot future triggers tend get forgotten begin interfere bulk operations may perform later date time time seen happen heard poor hapless dba scream gah forgot triggers move business logic application least stored procedures data manipulation triggers place use constraints enforce data integrity triggers 0,add rlf provided answer following also result transaction rollback alter database set single user rollback immediate rollback open transactions disconnection client server bit infrastructure underlying disk subsystem goes away open connections database terminated uncommitted transactions rolled back database subsequently opened server shutdown quits reason uncommitted transactions rolled back starts opens associated database set xact abort set sql statement raises run time error cause open transaction roll back batch scoped transactions applicable multiple active result sets mars committed roll back batch completes completeness rollback transaction rather obviously roll back transaction transaction log becomes full transaction open database subsequently closed reason roll 0
availability group listener im looking alwayson availability groups look appears availability listener group single point failure exactly listener actually run,ag listener virtual network name vnn virtual ip vip availability group listener virtual network name vnn clients connect order access database primary secondary replica alwayson availability group point applications listener registered dns directs traffic ag unless dcs go time redundancy number dcs dns servers availability group listener consists domain name system dns listener name listener port designation one ip addresses tcp protocol supported availability group listener dns name listener must also unique domain netbios create new availability group listener becomes resource cluster associated virtual network name vnn virtual ip vip availability group dependency client uses dns resolve vnn multiple ip 10,listener failover resource within wsfc ag built upon follow primary replica fails redundancy portion mentioned dns side based ad infrastructure dns uptime example secondary data center want configure wsfc multi subnet configuration allowing listener hold ip subnets data centers ag fails secondary dc listener become active subnet applicatoins see downtime ip come online secondary dc replica switch well 4,older sql refers fields components datetime items fields datetime items specifies fields make date time value datetime value made subset fields fields year month term field doesnt seem meaning rest document newer sql standard columns fields attributes terms column field attribute refer structural components tables row types structured types respectively analogous fashion structure table consists one columns structure row type consist one fields structured type one attributes every structural element whether column field attribute primarily name paired declared type later field described field descriptor field descriptor includes name field data type descriptor declared type ordinal position within row type simply 0,interesting sensors produce kind data make sense put table amount data see youd worried performance days usual amount time produce graph could two tables main sensor data table stores data little want slack days ago today everything older goes archive table could help reduce size table reports begin generated hopefully majority gb data archive table main table archiving job scheduled run nightly maybe also consider building separate reporting database stores data structure better generating reports tables designed closely match querying maybe pre calculate aggregate values would otherwise take long time generate possible populate main database regular nightly basis course need 0
identify blocking send alert need create alert notify query blocked seconds example someone transaction open table forgets run commit rollback,sure easier way built one way would first configure blocked process threshold seconds sp configure show advanced options go reconfigure go sp configure blocked process threshold go reconfigure go set event notification blocked process report see answer example code event notifications activation procedure could use sp send dbmail send email 4,liked martins suggestion using event notifications followed example link put together server youll need either put email address call sp send dbmail modify procedure read email addresses config table sort also adjust blocked process threshold liking result concise report message info blocked blocking processes error handling really basic ends conversation receives error message end dialog message make sure database mail configured system allow use sp send dbmail use msdb go exec sp configure show advanced options reconfigure go exec sp configure blocked process threshold reconfigure go create queue blockedprocessqueue create service blockedprocessservice queue blockedprocessqueue http schemas microsoft com sql notifications 7,please look architecture innodb picture percona cto vadim tkachenko rows deleting written undo logs file ibdata1 growing right duration delete according mysqlperformanceblog coms reasons run away main innodb tablespace lots transactional changes long transactions lagging purge thread case reason would occupy one rollback segment along undo space since deleting rows rows must sit ibdata1 delete finished space logically discarded diskspace shrink back need kill delete right kill delete query rollback deleted rows instead create table tablename new like tablename insert tablename new select tablename columnname like rename table tablename tablename old tablename new tablename drop table tablename old could done 0,unfortunately optimize views like one time trick would use query divide two non overlaping queries use union merge results optimizer would choose one resultset another condition isbig specified otherwise returns everything obviously harder mantain select stateabbreviation isbig tblstates stateabbreviation ak tx union select stateabbreviation isbig tblstates stateabbreviation ak tx 0
tell insert certain table slow know insert sql table slow number reasons existence insert triggers table lots enforced constraints checked,things look reduce batch size something smaller like didnt say large row size try turning io stats see much io fk lookups taking waiting caused insert happening master dbo sysprocesses lets start see go 10,brad examine wait stats query sql2000 could use dbcc sqlperf waitstats syntax get details 7,try using set statistics io set statistics profile statistics io useful telling tables amount table scans logical reads physical reads use three focus part query plan needs tuning statistics profile primarily return query plan tabular format look io cpu columns costing amount query table scan temp table vs sort insert clustered key etc 5,say looking analyzing performance query maybe helps analyze query execution plan check index scans table scans usage convert implicit functions sql data types parallelism run query set statistics io set statistics time see execution time read write io insert check waittime sysprocesses session spid run profiler select standard template select following performance statistics repeated plan compiled many times good rpc completed sql batchcompleted sql batchstarting add column rowcounts see exactly number rows batch filter results see query last collect page life expectancy counter windows perfmon min sql low memory also collect disk counters disk queue length disk time data files 6,single table use sp spaceused mytable tables database use sp msforeachtable follwoing create table temp table name sysname row count int reserved size varchar50 data size varchar50 index size varchar50 unused size varchar50 set nocount insert temp exec sp msforeachtable sp spaceused select table name row count count col count data size temp inner join information schema columns table name collate database default table name collate database default group table name row count data size order castreplacea data size kb integer desc drop table temp 0,aside using third party tool run problem lot deal sqlserver2005 doesnt support meta data however suggest cos used bit loved redgates sqlprompt product incredibly handy however cost money theres tradeoff someone workaround would love hear 0,based context previous question sql query combinations without repitition think looking way find combinations users include name id result set following script demonstrates one way achieve sample data declare users table userid integer username nvarchar50 insert users userid username values ntom nann ndina nmark load source data table set make easier find combinations working table find combinations declare combination table item id tinyint identity11 primary key nonclustered item nvarchar500 null item value integer null bit value convert integer power2 item id persisted unique clustered add user details working table insert combination item item value select username userid users use fact 0,nice ear author kalen suspects enforcement sort minimum row length anything padded course cases possible find phantom byte tinyint bit well varchar1 char1 wont increase beyond move smallint char2 increase move say char3 essentially point efficiencies gain choosing data types wisely point edge cases rules dont hold due factors storage layer edit hope concrete information wanted let know author internals book currently thinks shes certain 0
make sqlcmd return errorlevel sql script fails im running sqlcmd batch file wondering make return errorlevel something goes wrong backup,msdn sqlcmd utility page http msdn microsoft com en us library ms162773 aspx raiserror used within sqlcmd script state raised sqlcmd quit return message id back client example raiserror50001 could wrap backup database command try catch block raise appropriate error message sqlcmd would return batch file instance consider following errorleveltest sql file error exit begin try creates error backup restore operations allowed database tempdb backup database tempdb end try begin catch declare msg nvarchar255 set msg error occurred error message raiserror end catch following bat file first line wrapped readability needs single line echo program files microsoft sql server tools 10,use option sqlcmd specifies sqlcmd exits returns dos errorlevel value error occurs value returned dos errorlevel variable sql server error message severity level greater otherwise value returned http msdn microsoft com en us library ms162773 aspx 55,tried backup restore make backup sql server use restore db r2 instance end restore upgraded automatically different sbs cant make backup able transfer also logins could get help following kb articles transfer logins passwords instances sql server transfer logins passwords instances sql server sql server ps posted initially comment never saw sbs close enough 0,4th way use except aka minus rdbms supports give execution plan exists declare nodatewanted date select id nodatewanted tarih kimlik except select id nodatewanted siparis tarih nodatewanted youll add date filter constant check rows row pull data course 0
merge two select queries different clauses one table services need merge two select queries different clauses example select regn region,treat two current queries resultsets tables join select firstset region firstset openservices firstset dfc secondset closedyesterday select regn region countcallid openservices sumcase descrption like dfc else end dfc oscl status group regn order openservices desc firstset inner join select regn region countcallid closedyesterday oscl datediffday closedate getdate group regn order closedyesterday desc secondset firstset region secondset region order firstset region prettiest bit sql ive ever written hopefully youll see works understand maintain suspect better performing query would single select oscl grouped regn three counters separate sumcase statements akin currently dfc single table scan depending indexes schema 15,building michaels suggestion select regn region sumcase status else end openservices sumcase status description like dfc else end dfc sumcase datediffday closedate getdate else end closedyesterday oscl group regn order openservices desc 6,ibm db2 would use following select tabschematabname syscat columns colname column name note db2 column names upper case unless defined inside double quotes something upper case supply exact casing column name well 0,want know best convenient way migrate whole data includes security permission users memberships databases best method migrate data objects related particular database using backup restore method perhaps since want migrate whole database new instance backup user system databases restore new sql server instance backup database current server copy new server restore backup system database restore new instance backup restore system databases first backup restore system databases backing restoring system databases would provide logins jobs packages stored msdb new instance new server would also include complete securables permissions sql server mainstream support highly advisable upgrade latest sql server version stay make 0
using identity column increment need run sql server logging db main tables seperate datacentres writing time idea restoring db new,wont cause problems sql server lets create table decrement id integer identity0 test int insert decrement test select number numbers select top id test decrement order id asc go id test good idea long term might different problem others may end confused ie think order query upside normal happens someone else restores database reseeds identity normal way youve got overlapping ids possible modify schema site column use site id composite key 7,problems may rise setting following link martin smiths comment negative values identity column may cause issues applications database designers make identity columns start min value rather another issue related values negative decreasing identity also clustered key table tree structures efficient traversed left right lower higher values inserts done right higher side key ever increasing property matters ever clustered key table see blog post kimberly tripp best properties clustered keys especially ever increasing decreasing key inserting data always wrong left side index causing fragmentation index effects may critical case think mind identity also chosen clustered key martin suggests clustered index also 7,going backwards feels wrong two data centers could also implement identity ranges unless cycle identity values alarming rate reason cant data center create table dbo table id int identity11 primary key data center create table dbo table id int identity10000000001 primary key would allow generate billion well values data center danger collision data center could add check constraint data center prevent overlapping values depending prioritize errors vs duplicates could also implement recurring job periodically checks close lower bound data center youre concerned youll really generate billion values either data center apps lifetime never mind billion enough two alternatives give room 9,right click database tasks look export data selected database preconfigured data source next screen define destination change flat file destination go later use template ssis need often 0,dont put nulls warehouse marts warehouse well normalized least bcnf therefore exclude nulls nulls might preserved staging tables exist data sources shouldnt needed warehouse marts designed support presentation tools user queries nulls get way things never displayed make user queries complex error prone especially foreign key columns frequently subject joins 0,definitely create non clustered index update metadata tested sql really tested production system create table int identity11 null int null constraint primary key clustered asc go create nonclustered index nc asc go insert values fun part dbcc ind give us database pages table non clustered index stored find pagepid indexid pagetype following dbcc traceon3604 sure allowed dbcc page pagepid tableresults notice null bitmap header lets alter table alter column int null really impatient try run dbcc page command fail lets check allocation dbcc ind page moved magic changing nullability column affect storage non clustered indexes cover column metadata needs updated 0
specify trigger execution order postgresql im using classic time based partitioning using triggers found need separate trigger runs original table,postgresql executes triggers alphabetical order name make sure use names get order want docs sql specifies multiple triggers fired time creation order postgresql uses name order judged convenient btw sql creates trigger twice 7,couldnt say better manual instead trigger returns null operation abandoned row subsequent triggers fired row neither alphabetically later triggers event triggers fire trigger cancels 10,think process event want analyze lets say building lougle analytics want analyze visits opposed single page requests site visiting website process fact table represents process event want analyze case list site visits many fact tables want one per process event things might useful analyzing site visits information web browser brand screen resolution information user country state city isp based ip address information visit started year quarter month week day duration visit referring page landing page exit page title url path number pages visited visit technically could put one table would excel thatd get real big real fast well one level 0,isnt access issue problem attaching database higher version sql server instance mdf originally attached read sql server trying upgrade database part attach db read mode database read sounds like case based error messages need files log data attach per technet documentation look attach clause explicitly describes read database log rebuilt primary file updated therefore attach read database whose log unavailable must provide log files files attach clause need accompanying ldf database would contact whoever provided database ask provide mdf ldf files attach following syntax create database foo filename path mdf log filename path ldf attach read status removed database without 0
mysql return json standard sql query read json objects json object type want select return json necessarily want store json,took bit figuring used postgresql things much easier consult fine manual functions create json values theres json array function much use really least case answer question select return json two ways rather painful either use hack see db fiddle use one new mysql supplied json functions ironically appears even hack hack mysql fiddle answers use mysql group concat function post helped might want set group concat max len system variable bit default paltry first query imagine messy ddl dml bottom answer select concat better result best result select group concat json separator better result select concat name field name field 10,converting row json doesnt sound like want aggregate json state want equivalent row json suggest checking much simpler json object select json object name field name field address field address field contact age contact age contact aggregating json side note need aggregate resultset json upcoming mysql json arrayagg return result set single json array json objectagg return result set single json object 14,sql server index scan since thinks cheaper seeking required row likely sql server correct given choices setup aware sql server may actually range scan index opposed scanning entire index provide ddl tables along indexes may may able help make much less resource intensive side note never ever use date literals like instead declare orderstartdate datetime2 feb declare orderenddate datetime2 feb use declare orderstartdate datetime2 27t00 declare orderenddate datetime2 28t00 aarons post may help clarify 0,key preserved means key value goes table giving counter examples may help understand concept better example1 view contains aggregation suppose following view structure groupid averagesalary example values comes one rows try update averagesalary view database way find rows update example2 view shows values one table view shows values person person contact detailsidpersonidcontacttypecontactvalue table example rows 11emailddd example com 11phone898 join table show business friendly information view personidnamelastname phone1email1 would like update phone1 email1 personid maps two different rows may rows example view database way find rows update note restrict view sql makes clear find rows update may work two example 0
configure aws aurora separate write read operations want migrate database instance aws rds mysql aurora doubt replication aurora management write,afaik youre right aws rds aurora mysql fork support automatic transparent read write splitting http docs aws amazon com amazonrds latest userguide chap aurora html order way thats completely transparent application would need intermediate proxy application would always connect proxy proxy would packet inspection examine incoming query determine read write gets forwarded master read get forwarded replicas aware notable implications means proxy needs understand mysql protocol needs inspect packet query determine rw ro needs forward query appropriate backend mysql instance likely needs keep track connection maintaining map front end connections app proxy backend connections mysqld instances front end connection would 10,wanted point aws updated cluster read endpoint load balancing case anyone runs google https aws amazon com blogs aws new reader endpoint amazon aurora load balancing higher availability 9,short many indexes rule bit misleading think long given average database around reads higher reads need optimised insert read unique index example update read even write intensive database still reads poor quality indexing examples wide clustered indexes sql server especially non monotonic clustered indexed overlapping indexes eg cold cole cold cole colf many single column indexes also overlapping useful indexes useless queries includes covering eg single column indexes note quite typical indexes several times bigger actual data even oltp systems generally id start clustered index usually pk unique indexes constraints cant covering foreign key columns id look common queries see 0,design decision would probably go option modified option first option one thing like clarity product table affords one big table field determine type relation isnt clear another indexing strategy would always require type field listed since types index cardinality extremely low select product table type basically full table scan anyway option create parent table holds columns types share create product type table individual columns one extra link parent table create link table product option model option etc links respective keys reciprocal links model option option model go ahead create tables well add clarity joins anyone looking downside complexity making sure 0
get members given group relationship relationship create table auth user id integer null primary key username character varying150 null unique,something like select auth user id select aug user id auth group ag join auth user groups aug aug group id ag id ag name group1 6,exists query returns unique users regardless whether duplicate entries auth user groups select auth user exists select auth user groups user id id group id select id auth group name group1 typically fast notes postgres seem using sequence index columns matters defined uniqueuser id group id implemented corresponding unique index particular query index group id user id would preferable either switch columns unique constraint create additional optionally unique index columns reversed need common case related composite index also good queries first field also aware columns included unique constraint still null unlike primary key constraint makes member columns null automatically youd 8,oracle index type called bitmap index describes database index database stores bitmap index key instead list rowids table bitmap index hint used use bitmap access plan regular tree indexes bitmap indexes joined union ed intersected excellent explanation use index luke com includes following implementations combining multiple tree indexes db2 db2 supports multiple index access luw 9r7 using dynamic bitmap zos v10 mysql mysql index merge optimization starting release oracle oracle database uses bitmap conversions combine multiple indexes fly introduced 9i postgresql postgresql uses bitmaps combine multiple indexes since version sql server sql server use multiple indexes index intersect starting v7 0,think question stated collation meant given accepted answer talks encoding rather collation let answer stated question rather intended one think interesting wikipedia says collation assembly written information standard order computing collation taken meaning specification order words collation implies definition three way comparison function think short answer definitely maybe least im aware following shenanigans usr bin python name jonas xf6lker xf6 umlaut enc name encodeutf assert lenname xf6 one character assert lenenc two bytes utf import locale locale setlocalelocale lc collate da dk utf8 works machine long form locale strxfrmenc assert lenlong form locale strxfrm function returns string behaves cmp locale 0
way get estimate cost executing query mysql postgresql explain explain analyze show estimate cost executing query explain mysql doesnt provide,isnt much mysql except following explain extended followed show warnings show profiles older releases mysql mysql performance schema read carefully see think 5,short run select query show status like last query cost answer rerun query select sql cache step 9,queries xml happen sql server xml capabilities use xml type store xml avoid casting keep mind xml type may stored little bit slower due xml validation underlying type xml ordinary varbinarymax 0,best practice putting sa strong password forget 0
fill factor setting performance ive read typically indexing recommendation leave fill factor logic behind seems new data may inserted table,default fill factor zero identical per product documentation fill factor value percentage server wide default means leaf level pages filled capacity trying find opitmal value fill factor well researched problem described scenario much done unless table doesnt use default values 4,considered updates change row new data fit space row would moved new page rule filling pages percent reduce space table uses consequentially give slight performance improvement 4,use sql profiler production time done correctly filtering get back small amount data server risk minimal tracing everything would useless 0,data likely long outlive application code rule critical data useful time like foreign key constraints help keep integrity data must database otherwise risk losing constraint new application hits database multiple applications hit databases including might realize important data rule data imports reporting applications may able use data layer set main data entry application frankly chances bug constraint much higher application code experience personal opinion based years dealing data experience hundreds different databases used many different purposes anyone doesnt put contraints database belong eventually poor data sometimes bad data point unusable especially true financial regulatory data needs meet certain criteria auditing 0
scripts disable constraints indexes data warehouse load load data data warehouse presently script use drop foreign key constraints indexes prior,disabling foreign keys loading data bit worrying case enabling scrip error handing try catch blocks data loaded checking referential integrity database simply enabling foreign keys guarantee referential integrity database atleast run dbcc checkconstraints make sure runs clean refer foreign keys states another point see double work point updating stats rebuilding indexes remember rebuilding index update stats columns associated index big window load dont need worry users accessing data load dont want impact unrelated data tables database highly suggest read various techniques mentioned data loading performance guide answer 4,things may considered index starts disabled youll end rebuilding script finishes index starts page compression youll rebuild without compression performance since process accessing table could rebuild online option possibly small performance boost due less locking depending size tempdb indexes could use sort tempdb option performance boost youre updating statistics tables even table hasnt modified rows changed could try restrictive updates look tables modified since last stats updated modified looks like youre running queries single session considered splitting work among multiple sessions 5,always majorly impressed mysql seems order type comparison mysql implicitly converts string literal passing clause unsigned integer mechanics implementation mysql convert function datatype switch simply fail case giving value links show examples convert function behaving well badly imo 0,execution result case success completion failure precedence link create relationship two tasks bol precedence link two tasks establishes relationship tasks second task dependent task executes execution result first task precedent task matches specified criteria typically execution result specified success failure completion wanted say plan execute statement designed around success failure could use completion establish relationship proceed next task example purge records type database abc execute database backup dont care many records found deleted run statement done back database example reorganize indexs database abc successfully checking integrity integrity check fails reorganize indexs maint plans precedence constraints 0
nvl stand nvl stand im talking oracle informix perhaps others function used filter non null values query results similar coalesce,quite simply null value function substitutes nulls given resultset column value given second parameter 19,null value logic according http www abbreviations com term references found support phils null value supposition havent found definitive origin 8,prevention steps could make sure one one sysadmin access restored database put db single user mode restore completed check code inside stored procedures functions triggers inside database perform dbcc checkdb make sure integrity issues check users used access database remove start allowing access restricted specific objects checked like shawn said code execute unless stored procedure seems vbalid exec another malicious code reason checking code inside one putting multi user mode 0,might want select insert products discounts product id discount amount discount description values convertvarchar30 product id top product products left outer join products discounts pd product id pd product id top ten pd product id null generates statements top products dont product discount need excel copy paste results new query window ssms run said youll likely want automate procedure could accomplished using either cursor like declare cmd nvarcharmax declare cur cursor local forward static select insert products discounts product id discount amount discount description values convertvarchar30 product id top product products left outer join products discounts pd product id pd 0
add indexes temp tables testing stored procedures ssms sometimes says missing index sometemptable 000000000000005b somefield etc etc add like sp,add index youll use temp table index twice query run maintain usual index tasks like uniqueness data loaded temp table already sorted create temp table clustered index sort data taking account sql servers feature temp tables reuse decide create index temp table try create table statement youll add index explicitly table creation prevent sql server reuse table next time 12,hard answer without lot information generally would say depends generally speaking add indexes onto temp tables benefit index greater original execution cost plus cost creating index would benchmark using set statistics io example would also want review execution plans used index added sometimes recommended indexes really good recommended indexes 5,done generally considered fairly dangerous basic level set trustworthy flag database use execute sp take advantage server level principals security access dangerous dont want go detail however ive blogged specific instructions said make sure absolutely need way opening big security hole decide put sp database grant users connect execute access sp 0,dont want use recursive cte could use query example select id rn start mile rn convertdecimal15 end mile dbo miles cross apply select topconvertint end mile start mile convertdecimal15 3start mile convertdecimal15 3row number overorder select null rn master spt values spt1 cross apply master spt values spt2 work 6m miles one id spt values table could replaced numbers table sort db fiddle 0
add column foreign key constraint table already exists following tables create table users id int primary key already exists data,want exactly possible however relatively easy column exist order make fk following documentation create table xt int primary key create table ys int alter table add column int alter table add constraint fkey foreign key references update cascade delete cascade couple points note always give foreign keys reasonably legible names told key xyz 0004b violated useful chocolate tea pot oracle great dont name fks desired feature perhaps nice isnt really critical since ddl something want application regular basis also risks adding already fairly substantial documentation 15,im sure everyone telling two steps fact dont tried add foreign key assumes design column throws error column add column explicitly make foreign key creation references alter table message add column sender int references users references tableunique column work fine see syntax alter table alter table exists name action action add column exists column name data type collate collation column constraint examples even docs alter table distributors add constraint distfk foreign key address references addresses address alter table distributors add constraint distfk foreign key address references addresses address valid isnt needed rely autonaming primary key resolution table name specified youre 5,another way pretty looking seeing ypercubes suggestion still select dbname msgval dbo list cross apply select top msgval dbo msg dbname dbname alldbs order case dbname alldbs else end asc 0,many bit columns defined table found msdn says less bit columns stored one byte http msdn microsoft com en us library ms177603 aspx 0
many vlfs bad database size gb vlfs hand one size tb vlfs indicate smaller database high count vlfs actually problem,whether use full recovery matters massively course requires much larger log file means backup isnt protection data loss important losing data severely bad business us stand lose days data occasionally relevant reading https www brentozar com blitz high virtual log file vlf count https www sqlskills com blogs kimberly transaction log vlfs many https www sqlskills com blogs paul important change vlf creation algorithm sql server first asserts virtual log files reason plan take action convenient links instructions well outline approach second comments vlfs large third describes formula many vlfs created within given increment size sql server behaviour changed let 5,vlf count going problem recovery times combination count vlfs size means recovery process require longer duration recovery time objective single number vlfs good bad generally keep vlf 512mb keep number vlfs well order know actual recovery time given log file youll need identical hardware setup test environment test recovery full log database shutdown active transactions running database hundreds thousands vlfs scared recovery time ask know view current info vlfs looking dbcc loginfo context desired database output command looks like recoveryunitid fileid filesize startoffset fseqno status parity createlsn status column indicates status given vlf indicates vlf active log records used records 14,one disadvantage contained database user forced change password like logins could must change users manage password unless grant alter user permission tell change using sql statement nowhere easy manage via user interfaces least dont know additional note found unexpected undocumented metadata usage pivot unpivot clause thought system catalog sys tables sys columns etc documented msdn contained database catalog collation latin1 general ci ws ks sc collation contained databases instances sql server changed didnt mention pivot unpivot clause also use system catalogs execution mechanism produce collation conflicted error everywhere near use pivot unpivot clause migration repro step1 create table belongs contained 0,select relname pg class cpg inherits oid inhrelid guy asked adam found solution added future reference others 0
replace space space one column table like id propinsi kota aceh denpasar aceh banda aceh sumatera asahan table many rows,try show without space select trimkota yourtable change data update yourtable set kota trimkota trim function different replace replace substitutes occurrences string trim removes spaces start end string want remove start use ltrim instead end use rtrim 16,run new query mysql select replacekota table name show result looks like trimming spaces column update update table name set kota replacekota save 4,dont believe allow run ddl access tables could make role make member db ddladmin db datareader db datawriter roles 0,use sql server extended events capture sql statements execute sql server management studio includes xevent profiler item object explorer every connected sql server version higher right click tsql session launch session aware capturing sql statements across entire server negatively affect performance youd likely want short period time stopping session 0
dbms allow foreign key references view base tables inspired django modeling question database modeling multiple many many relations django db,think youll find lot cases complex business rules enforced via model alone one cases least sql server think trigger preferably instead trigger better serves purpose 8,oracle one way enforce sort constraint declarative fashion would create materialized view set refresh fast commit whose query identifies invalid rows bookaspectrating rows match bookaspect view create trivial constraint materialized view would violated rows materialized view benefit minimizing amount data duplicate materialized view cause problems however since constraint enforced point youre committing transaction many applications arent written expect commit operation might fail constraint violation somewhat hard associate particular row particular table 8,business rule enforced model using constraints following table solve problem use instead view create table bookaspectcommontaglink bookid int null aspectid int null tagid int null tagid deliberately left pk primary key bookid aspectid foreign key bookid tagid references booktag bookid tagid foreign key aspectid tagid references aspecttag aspectid tagid 9,sira prise allows although fk isnt called fk anymore database constraint view fact even defined view include view defining expression inside declaration database constraint constraint would look something like semiminusbookaspect joinbooktag tagaspect youre done sql dbmss however youd analysis work constraint determine violated implement needed triggers 4,found bizarre informative article reasons one use enum even without article know easy method adding new values techniques high risk numbers never used use strings dtest already mentioned answer 0,ran test environment adventureworks found case approach works best using approach made engine use clustered index case statement let use non clustered index query could look like select mytable case myparam mycolumn myparam else end however say lots parameters unlikely going able use index case might leave alone testing simple query 0,another option one ive learned comes sqlcmd documentation need set codepage sqlcmd match file encoding case utf codepage youd want sqlcmd mssqlserver08 dp0 aqualogydb sql dp0 databasecreationlog log 0,introduction transactions savepoints scenario creating new user basic process could following begin insert users username password values xxx set userid last insert id insert profile pictures user id profile picture path values userid somewhere filesystem jpg commit whatever reason process end middle would enough issue rollback query somewhere instead commit changes would undone one basic tasks accomplished transactions mentioned savepoints rolando might come handy example one inserts row profile pictures first tries copy image desired location fail one wants undo second insert first copying picture fails reason cancel whole user registration one define savepoint inserting users inserting profile pictures begin 0
guid vs int better primary key ive reading around reasons use guid int int smaller faster easy remember keeps chronological,youre synchronizing data external source persistent guid much better quick example using guids tool sent customer crawl network certain classes auto discovery store records found customer records integrated central database back end used integer would itd lot harder keep track 18,asked stack overflow jeffs post explains lot pros cons using guid guid pros unique across every table every database every server allows easy merging records different databases allows easy distribution databases across multiple servers generate ids anywhere instead roundtrip database replication scenarios require guid columns anyway guid cons whopping times larger traditional byte index value serious performance storage implications youre careful cumbersome debug userid bae7df4 ddf 3rg 5ty3e3rf456as10 generated guids partially sequential best performance eg newsequentialid sql server enable use clustered indexes certain performance planning replicate merge records use int set auto increment identity seed sql server 89,used hybrid approach success tables contain auto increment primary key integer id column guid column guid used needed globally uniquely identify row id used queries sorting human identification row 15,client takes long time receive data turn send acknowledgement sql server received data sql server wait due wait sql server release locks held query unless acknowledgement received client accurate dependent isolation level default read committed locks held duration statements execution read committed provide statement level read consistency guarentee read uncommitted data shared lock acquired held read row released unless lob types lob types potentially large buffered shared lock must acquired held statement completes essentially giving repeatable read behavior read committed im making single call mssql database high latency network table locks occur due latency latency isnt causing table lock however 0,select seq nextval enabled user id dual 0,turns two queries least part problem modified two queries db execute update people set iphone device id null iphone device id people id deviceid user people id db execute update people set company id name dad password pass temp password null reset password hash null email redacted gmail com phone null mobile null iphone device id iphone device id blah iphone device time last checkin location lat lat location long lng gps strength picture blob id authority active date created last login panic mode battery level battery state unplugged people id db execute update people set iphone device id null 0
find value optimize unknown using optimize unknown stored procedure able see value database decided optimal,benjamin nevarez good article optimize unknown blog essentially sql server using statistics table along math determine value use post density defined number distinct values salesorderdetail table distinct values productid density calculated shown statistics object one assumption statistics mathematical model used sql server uniformity assumption since case sql server use histogram uniformity assumption tells given value data distribution obtain estimated number records sql server multiply density current total number records shown plan also divide total number records number distinct values also gives 5,optimize unknown doesnt use value instead uses density vector run dbcc showstatistics value listed density column second result set example im using stackoverflow demo database density vector reputation column 962674e take value times number rows table 962674e get thats number rows sql server expect come back given reputation filter 17,free march microsoft sql server developer edition free sql server team blog upgrade current sql express edition developer download appropriate edition x86 x64 mount iso run installer package select maintenance select edition upgrade follow prompts select express instance wish upgrade enable sql agent sp configure show advanced options reconfigure go sp configure agent xps reconfigure go sp configure 0,general always run log backups serial many instances couple dozen databases several active transaction log backups take seconds total half minute particularly busy running backups parallel really would beneficial following conditions true databases log files unique independent spindles solid state disks combination log backups log files would need fulfill requirement backup targets database separate spindles arent using shared san hba iscsi bandwidth sql server instance media iops reading database writing backup use disks reading database writing backup true possible degree parallelism decrease amount total calendar time true likely youll cause one sets disks thrash parallel backups actually take calendar time 0
many one subselection single query two tables foreign key t1 t2 one many relationship tuple table t1 associated tuples t2,easiest shortest way go select cars join imperfections i1 uid i1 uid i1 imperfection join imperfections i2 uid i2 uid i2 imperfection inner joins test existence holds uid imperfection unique would plausible defined could subtypes imperfections youd need distinct group using uid would even shorter uid i1 uid imperfections would go clause next idea would kevin describes answer wont include 5,could two separate exists clauses select cars t1 exists select imperfections t2 t1 uid t2 uid imperfection exists select imperfections t2 t1 uid t2 uid imperfection means two left semi joins imperfections table index uid imperfection trivial two advantages using exists join case first left semi join stops first match quicker join needs go records second somehow dont good data constraints two records imperfections uid imperfection could end duplicate rows back main query 7,per fine documentation think might youre looking sql identifiers key words must begin letter also letters diacritical marks non latin letters underscore subsequent characters identifier key word letters underscores digits dollar signs note dollar signs allowed identifiers according letter sql standard use might render applications less portable 0,must go innodb file per table need cleaning innodbs current infrastructure seen many db hosting clients setup mysql leave innodb default state causes system tablespace better known ibdata1 grow wildly even switched innodb file per table ibd file would extracted ibdata1 ibdata never shrink example table called mydb mytable inside ibdata1 taking 2gb extract following step add etc cnf mysqld innodb file per table step service mysql restart step alter table mydb mytable engine innodb make file var lib mysql mydb mytable ibd unfortunately 2gb space occupied table change reclaimed wrote past posts cleanup innodbs infrastructure innodb store databases one 0
limit maximum number rows table configuration table sql server database table ever one row help future developers understand id like,two constraints would create table dbo configuration configurationid tinyint null default rest columns constraint configuration pk primary key configurationid constraint configuration onlyonerow check configurationid need primary key unique constraint two rows id value check constraint rows id value arbitrarily chosen combination two almost opposite constraints restrict number rows either zero one fictional dbms current sql implementation allows construction allows primary key consisting columns would solution create table dbo configuration configurationid needed rest columns constraint configuration pk primary key columns 52,could define id computed column evaluating constant value declare column unique create table dbo configuration id cast1 tinyint bit columns constraint uq configuration id unique id 24,anyone using sql server newer use trim built function example declare test nvarchar4000 set test nchar0x09 nchar0x09 nchar0x09 nchar0x09 content nchar0x09 nchar0x09 nchar0x09 nchar0x09 nchar0x09 select trimnchar0x09 nchar0x20 nchar0x0d nchar0x0a test please note default behavior trim remove spaces order also remove tabs newlines cr lfs need specify characters clause also used nchar0x09 tab characters test variable example code copied pasted retain correct characters otherwise tabs get converted spaces page rendered anyone using sql server older create function either sqlclr scalar udf sql inline tvf itvf sql inline tvf would follows create alter function dbo trimchars originalstring nvarchar4000 charstotrim nvarchar50 returns 0,use bcp utility bcp select col1col2col3 mydatabase dbo mytable queryout mytable csv servername argument specifies character output opposed sqls native binary format defaults tab separated values changes field terminator commas specifies windows authentication trusted connection otherwise use myusername mypassword doesnt export column headers default need use union headers use sqlcmd sqlcmd servername select col1col2col3 mydatabase dbo mytable mydata csv use powershell link article end using might want append notype export csv extractfile get rid unnecessary column output file 0
key made explicit new subject databases may sound ignorant curious key made explicit within table primarily tell user given column,obviously suggesting constraints database enforced applications access database many reasons bad bad bad idea building roll constraint engine within application code merely emulating oracle sql server mysql postgresql whoever spent years writing constraint code tested years literally millions end users due respect team going get right even matter years mysql code alone cost million dollars mysql cheapest servers dont even implement check constraints obviously getting referential integrity completely right difficult used frequent oracle forums cant tell number times poor manager programmer project thrust upon genius job bright idea suggest jonathan lewis wrote page book fundamentals oracle optimiser gives design disasters 32,create key database dbms engine enforces uniqueness constraint key attributes serves least three related purposes data integrity duplicate data entered key attributes dependencies keys therefore guaranteed identification users able rely keys means identifying updating data accurately optimisation information metadata attributes unique available dbms query optimiser information allows optimiser simplify query execution certain ways queries execute faster 10,ill add one aspect existing excellent answers documentation often important see kinds keys use identify entity combination unique columns candidate key primary key tends especially useful concept practice whether enforce key probably documentation valuable right 8,another reason use constraints instead inside application code happens developer dba use insert update delete statement modify data direct db case nice application based referential integrity useless know devs like possibility modify data direct without bother ri know least time always ps course could create triggers usually terrible slow compared constraints 5,think goes like declare enabled user id pls integer disabled user id pls integer begin enabled user id seq nextval disabled user id seq nextval insert users id usr name values enabled user id andrew insert car car id car name usr id values carseq nextval ford enabled user id insert users id usr name values disabled user id andrew insert car car id car name usr id values carseq nextval ford disabled user id end 0,psql could connect server file directory server running locally accepting connections unix domain socket var run postgresql pgsql error generally means server running based dpkg output thread comments due postgresql main package somehow uninstalled since uninstall hasnt called purge option dpkg data configuration files still apt get install postgresql fix problem 0,see nothing question autovacuum would take care largely depends pattern writing activities mention million new rows per week insert copy typically dont create table index bloat autovacuum take care column statistics visibility map minor jobs update delete dominant cause table index bloat especially targeting random rows dont see question autovacuum come long way great job postgres later would look autovacuum settings vacuuming tends interfere work load also look cost based vacuum delay manual vacuuming rare exception lots random updates may want set fillfactor something lower allow hot updates right away reduce need vacuum hot updates redundant data update statements note 0,adding previous answer helps automate action need time time management studio simply right click header save results choose csv file want select statement run change output direction results file use tools options query results results file another way automated easily makes use ssis using management studios export data feature right click database tasks export data wizard lots options choose source database destination options destination make sure flat file browse choose csv type choose whatever formatting need end youll obtain ssis package saved locally repeated need 0
merge statement deadlocking following procedure sql server r2 create procedure usp savecompanyuserdata companyid bigint userid bigint datatable tt couserdata readonly,ok looking everything couple times think basic assumption correct whats probably going match part merge checks index matches read locking rows pages goes row without match try insert new index row first request row page write lock another user also gotten step row page first user blocked update second user also needs insert page theyre deadlock afaik theres one simple way sure get deadlock procedure would add tablockx hint merge would probably really bad impact performance possible adding tablock hint instead would enough solve problem without big effect performance finally could also try adding paglock xlock paglock xlock might work 12,would problem table variable ever held one value multiple rows new possibility deadlock suppose two concurrent processes run table variables containing company process reads destination finds row inserts value holds exclusive row lock value process reads destination finds row inserts value holds exclusive row lock value process needs process row process needs process row neither process make progress requires lock incompatible exclusive lock held process avoid deadlocks multiple rows rows need processed tables accessed order every time table variable execution plan shown question heap rows intrinsic order quite likely read insertion order though guaranteed lack consistent row processing order leads 31,think sql kiwi provided good analysis need solve problem database follow suggestion course need retest still works every time upgrade apply service pack add change index indexed view three alternatives serialize inserts collide invoke sp getapplock beginning transaction acquire exclusive lock executing merge course still need stress test one thread handle inserts app server handles concurrency automatically retry deadlocks may slowest approach concurrency high either way determine impact solution performance typically deadlocks system although lot potential made mistake one deployment half dozen deadlocks occur hours following scenario fixed soon deadlocks year mostly using approach system works really well us 8,since asked use scenario queries select use depending requirement data table users records table phoneno records relation meaning entry phoneno reference one entry users one entry phoneno reference given entry users requirement show users phone numbers ignore users without phone number query select uid name phonno user inner join phones uid uid result shows users phone number requirement show users phone number user phone display available query select uid name ifnullp phonnon user left outer join phones uid uid result shows records note ifnull mysql syntax transform null value used function make db engine show phonno null look appropriate function 0,completeness option using plain old mirroring advantages include two copies database without complexity using availability groups without needing shared storage failover clustering disadvantage although slight mirroring deprecated failover times mirroring order seconds although application code needs able retry transactions occurring time failover 0,want prevent making temp tables much possible prevent copying temp table sql work well mysql replication binlogs updated mysql temporary tables since temp tables fact life db world may make unorthodox changes accommodate existence normally mysqld habit placing tmp tables tmp wherever tmpdir configured thats usually poor unsuspecting disk interesting alternative would setup ram disk reconfigure tmpdir use step create mount point ram disk mkdir var tmpfs step add ram disk etc fstab 16gb echo none var tmpfs tmpfs defaultssize 16g etc fstab step add line etc cnf mysqld tmpdir var tmpfs step enable ram disk one following reboot db 0
way find changed password login trying find changed password login sql server r2 already checked default trace log event default,article help set advance event happened past didnt kind auditing mechanism set still hope though lets say create login flooberella password nx check policy information default trace eventclass audit addlogin event however change password using either methods alter login flooberella password ny exec sp password ny nz nflooberella events captured default trace obvious security reasons possible anyone access default trace figure someone elses password want make easy even find password changed polling frequency events example reveal certain properties security strategy else relies information still log also relies using undocumented dbcc command system database may wish back master restore elsewhere get 11,longer comment posting answer select top10 transaction id begin time transaction name transaction sid suser sname transaction sid fn dblognull null operation lop begin xact transaction id begin time transaction name transaction sid 00002b12 event session startup null 00002b13 dbmgr startupdb null 00002b14 addguestusertotempdb null 00002b15 dbmgr startupdb null 00002b16 dbmgr startupdb null 00002b17 dbmgr startupdb null 00002b18 dbmgr startupdb null 00002b19 dbmgr startupdb null 00002b1a autocreateqpstats 0x010500000000000515000000a065cf7e784b9b5fe77c877084b65600 00002b1b test ack 0x010500000000000515000000a065cf7e784b9b5fe77c877084b65600 rows affected 4,backup types dependent sql server recovery model every recovery model lets back whole partial sql server database individual files filegroups database table level backup created option workaround taking backup sql server table possible sql server various alternative ways backup table sql sql server bcp bulk copy program generate table script data make copy table using select save table data directly flat file export data using ssis destination explaining first one rest might knowing method backup sql table using bcp bulk copy program backup sql table named person contact resides sql server adventureworks need execute following script sql table backup developed 0,sql server real problem avoiding logging deleted rows proposal set second database simple recovery mode copy whole table select sem copy original truncate original insert original select sem copy target server sql server version move data backup restore cases take bulkcopy option edit although time learn partitioned tables seems option enterprise edition 0
rely reading sql server identity values order tl dr question boils inserting row window opportunity generation new identity value locking,best expect identities consecutive many scenarios leave gaps better consider identity like abstract number attach business meaning basically gaps happen roll back insert operations explicitly delete rows duplicates occur set table property identity insert gaps occur records deleted error occurred attempting insert new record rolled back update insert explicit value identity insert option incremental value transaction rolls back identity property column never guaranteed uniqueness consecutive values within transaction values must consecutive transaction use exclusive lock table use serializable isolation level consecutive values server restart reuse values use identity values create separate table holding current value manage access table number assignment 5,inserting row window opportunity generation new identity value locking corresponding row key clustered index external observer could see newer identity value inserted concurrent transaction yes allocation identity values independent containing user transaction one reason identity values consumed even transaction rolled back increment operation protected latch prevent corruption extent protections specific circumstances implementation identity allocation call cmedseqgen generatenewvalue made user transaction insert even made active locks taken running two inserts concurrently debugger attached allow freeze one thread identity value incremented allocated able reproduce scenario session acquires identity value session acquires identity value session performs insert commits row fully visible session performs 26,paul white answered absolutely correct possibility temporarily skipped identity rows small piece code reproduce case create database testtable create database identitytest go use identitytest go create table dbo identitytest id int identity c1 char10 create clustered index ci dbo identitytest id dbo identitytestid perform concurrent inserts selects table console program using system using system collections generic using system data sqlclient using system threading namespace identitytest class program static void mainstring args var insertthreads new list thread var selectthreads new list thread start threads infinite inserts var insertthreads addnew threadinfiniteinsert insertthreads start start threads infinite selects var selectthreads addnew threadinfiniteselectandcheck selectthreads 7,go far course problem may bit art isnt pure science main product analysis reporting system regard quite detail records initially designed lots joins common id child records found denormalized couple fields could cut lot joins could take away lot performance headaches knew created normalized design started using profiled actual performance hundreds millions rows across dozens tables end story profiled couldnt know sure going work us liked idea normalizing could update easily end actual performance deciding factor thats advice profile profile profile 0,sounds like need sparse columns filtered indexes go option fully supported documented features exactly scenario sql server database engine uses sparse keyword column definition optimize storage values column therefore column value null row table value requires storage cant imagine xml solution performing well scenario huge overhead redundant metadata slow query 0,cant something like first without order since query could give different order dont specify lets try way select select table order putyourorderfieldhere limit name like john limit 0
accidentally change database name sql server didnt happen yet thinking messing around training environment mistake clicked database name touched letter,could look sql server logs last time sql server started look instance starting database dbname could compare list results sys databases new databases one changed sql server log list another perhaps better way would query default trace filter fn trace gettable database id assuming recent db usage databasename column show old name recent row new name event type object altered select castvalue nvarchar1000 fn trace getinfodefault traceid property select ftg starttime ftg endtime te name ftg sessionloginname ftg objectname ftg databasename ftg servername ftg loginname ftg hostname ftg ntusername ftg databaseid ftg textdata ftg targetusername space10 space fn trace gettablez 5,right click database go files see original file names help find correct database name easily file names wont changed rename also try take look fn dblog undocumented see filter latest actions select fn dblognullnull 12,doubt would database production whose name know documented somewhere incase happens production could look list existing backups using restore headeronly disk backuplocation use dbo backupset msdb select distinct database name msdb dbo backupset 7,find simple syntax readable less confusing use functions throw weird errors check whether variables one one null case null null null yes else end alternative conditions although less readable opinion would case exists select except select null null null null null yes else end case exists select intersect select null null null null null yes else end last two options look similar another standard sql yet implemented sql server option case distinct null null null null null yes else end 0,put right direction declare results table database name sysname db user name sysname login name sysname insert results exec sp msforeachdb use select db name database name dp name db user name sp name login name sys database principals dp inner join sys server principals sp sp sid dp sid select results sp msforeachdb ugly undocumented unreliable show information stored individual database 0,single transaction option mysqldump flush tables read lock causes mysqldump setup repeatable read transaction tables dumped question stated mysqldumps select db external notification table holding hundreds insert command table happening likely thing lock gen clust index better known clustered index paradigm causes data index pages table coexist index pages based either primary key auto generated rowid index event primary key able spot running show engine innodb status look page gen clust index exclusive lock inserts table clustered index requires exclusive lock handling primary keys btree well serialization auto increment discussed phenomenon aug innodb deadlocks exclusive insert update delete dec mysql 0
deny access sql server certain login ssms allow net sqlclient data provider situation developers update permissions work applications see connection,use server logon trigger make custom logon validations reject whenever see fit see trigger listed server objects inside triggers using ssms example create trigger strrejectssmsconnectionforsqllogin1 server logon begin original login nsqllogin1 program name like nmicrosoft sql server management studio begin raiserrordirect connection ssms refused rollback end end rollback inside trigger reject connection theres implicit transaction wrapping call trigger logon event careful implementing logon triggers coded properly rejecting logins able login including make sure test test dev environments first keep mind code executed session created system views rely session id spid wont contain currently checked login triggers ends without rollback high 11,think reliable solution problem since application name modifiable parameter cam changed user change within ssms connect database object dialog choose options open additional connection parameters choose name application name like sys dm exec sessions dmv program name show passed connection string application name parameter 13,cant cut specific client already detailed answers solution remove access privileges production systems developers accounts change must scripted dba run script deployment performed sysadmin devs produce package give someone proper privileges devs never see configs used production systems debugging arranged case case basis copy production data staging environment preferred solution temporary account limited privileges needed 4,ideal sense process policy management issue even someone knows password company policy anyone dba connect production well might release engineering team sys admins etc penalties breaking rules sufficient assuming rules enforced trying prevent particular application connecting impossible sepupic demonstrated fairly easy change program name even developer cant figure plenty programs connect sql server people access sqlcmd exe even deprecated osql exe developer connect within visual studio even create app connect via net sqlclient data provider oh even azure data studio many still might could still possible approach direction instead preventing application connecting allowing application connect sure get program name even 4,individual queries faster joins try squeeze every info want client side one select statement use many seems convenient performance scenario test measure solutions see faster said almost always case joined result set properly tuned database faster scale better returning source rows client joining particular input sets large result set small think following query context strategies join together two tables gb result set rows thats extreme see point noticed get information multiple tables often faster get information via multiple queries individual tables maybe containing simple inner join patch data together client side try write complex joined query get data one query 0,part initial query follows dbo calendar left join dbo colleaguelist date daya datestart coalesce dateend datestart section plan shown revised query datestart isnull dateend datestart join difference seems isnull simplifies result get accurate cardinality statistics going next join inline table valued function calling literal values something like datestart isnull dateend datestart isnullnull2013 equi join predicate date plan also shows equality predicate date result cardinality estimate rows likely pretty accurate case coalesce version datestart dateend value simplifies ok equality expression gives plan datestart dateend null goes far date case else null end also applies implied predicate colleaguelist estimated number rows time 0,sql statement short circuit rely evaluating expressions order 0,materialized views automatically updated base tables updated 0
foreign keys link using surrogate natural key best practice whether foreign key tables link natural key surrogate key discussion ive,neither sql relational model disturbed foreign keys reference natural key fact referencing natural keys often dramatically improves performance youd surprised often information need completely contained natural key referencing key trades join wider table consequently reduces number rows store one page definition information need always completely contained natural key every lookup table term lookup table informal relational model tables tables table us postal codes might rows look like ak alaska al alabama az arizona etc people would call lookup table big systems unusual find tables one candidate key also unusual tables serve one part enterprise reference one candidate key tables serve 14,main reason support surrogate keys natural keys often subject change means related tables must updated put quite load server years using variety databases many topics true natural key often fairly rare things supposedly unique ssn things unique particular time become non unique later things like emails addresses phone numbers may unique used different people later date course things simply dont good unique identifier like names people corporations avoiding joins using natural key yes speed select statements dont need joins cause places still need joins slower int joins generally faster also probably slow inserts deletes cause performance problems updates key changes 5,good news mysqls functionality finally fixed changelog previously control mysql interrupted current statement one exited mysql control interrupts current statement one cancels partial input line otherwise exit 0,checking variables context execute following query select name context pg settings name variable name case wal keep segments context sighup means requires server reload use pg ctl reload shell prompt select pg reload conf psql database client latest version types context internal context means modified compilation time postmaster means service restart needed others session backend specific 0
answer suddenly need indexes query needs changed junior dba years experience job fine tune queries advise developers particular code written,well could getting different plan plan could evicted cache due service restart manual clearing plan cache service restart failover inadvertent change certain sp configure changes flush cache change underlying objects indexes statistics dependencies triggered recompile could getting different plan users previous invocations query text might identical includes case sensitivity whitespace never mind different columns join criteria filters etc query could run different users different set options different default schemas object plan fully qualified name including schema query plan could could getting different performance plan cached using different parameters plan optimal current set parameters commonly called parameter sniffing amount data based 7,answer changed question dev common question dev applies every team business changed answered facts figures facts refer example increase amount users accessing database change server configuration parameter database maintenance update stats reorg rebuild indexes performed due plans incorrectly generated amount data increased changes made network side os patched new service pack cu sql server deployed without full regression testing application business cycle underlying san become suddenly slow figures derived data show example baselining server critical situation alleviate blame game since support facts solid figures start collecting data using dmvs sp whoisactive table data gets persisted sql server reboot workout based 10,usual aaron bertrand kin provided excellent answers however answers contain common thread analyze either answer see reason xyz isnt working like worked yesterday isnt something person reason things changed database decided things differently due xyz reasons database living breathing entity databases make decisions change mind due combinations assumptions statistics heuristic tools dramatically different application layer programming machine learning notable exception im going use military references cant think something better right general metaphor would appreciated pun intended applications programmer acts drill instructor tell computer exactly order sometimes long programming database like acting commanding officer tell want high level offer guidance needed 7,default trace running hopefully recycled logged select spid loginname ntusername ntdomainname hostname applicationname starttime servername databasename case eventclass create drop else end eventclass case objecttype database else end objecttype fn trace gettable convertvarchar150 select top value fn trace getinfo null property default objecttype database eventsubclass committed starttime dateaddhh getdate get last hrs order starttime go sort similar question answered underlying concept trace 0,youre already hanging situation dont forget search open connections across server sp who2 another database master scan results database one trying take offline note spid process id column value rows one time run kill spid certainly take care make sure processes arent important leave running terminated without even finishing latest command without warning 0,stored procedure sp change users login reports sql users windows users actual reporting query stored procedure uses get stored procedure text sp helptext sp change users login select username name usersid sid sysusers issqluser sid null sid 0x0 lensid suser snamesid null order name note first predicate statement issqluser sys sysusers view issqluser column indicates account sql user windows users dont meet criteria therefore arent reported 0
lob data slow table scans questions rather big table one columns xml data average size xml entry kilobytes columns regular,presence xml field causes table data located lob data pages fact table pages lob data merely xml column table effect presence xml data certain conditions causes portion rows data stored row lob data pages one maybe several might argue duh xml column implies indeed xml data guaranteed xml data need stored row unless row pretty much already filled outside xml data small documents bytes might fit row never go lob data page correct thinking lob data pages cause slow scans size also sql server cant scan clustered index effectively theres lot lob data pages table scanning refers looking rows course 11,correct thinking lob data pages cause slow scans size also sql server cant scan clustered index effectively yes reading lob data stored row leads random io instead sequential io disk performance metric use understand fast slow random read iops lob data stored tree structure data page clustered index points lob data page lob root structure turn points actual lob data traversing root nodes clustered index sql server get row data sequential reads get lob data sql server go somewhere else disk guess changed ssd disk would suffer much since random iops ssd way higher spinning disk considered reasonable table structure 10,data types changed time variable length columns removed indexes defragmented often never rebuilt lot rows deleted lot variable length columns updated significantly good discussion 0,create custom aggregate function oracle database provides number pre defined aggregate functions max min sum performing operations set records pre defined aggregate functions used scalar data however create custom implementations functions define entirely new aggregate functions use complex data example multimedia data stored using object types opaque types lobs user defined aggregate functions used sql dml statements like oracle database built aggregates functions registered server database simply invokes aggregation routines supplied instead native ones user defined aggregates used scalar data well example may worthwhile implement special aggregate functions working complex statistical data associated financial scientific applications user defined aggregates feature 0
mysql version change data capture process phasing old system migrating onto new one last time phased old system ran systems,linkedin open sourced cdc tool databus supports mysql http engineering linkedin com data replication open sourcing databus linkedins low latency change data capture system another open source cdc tool implemented php written part flexviews project http swanhart livejournal com html 5,recently academic project needed use cdc mysql tools available market able create poc using maxwell cdc unique feature maxwell parse ddl statements well link page maxwell http maxwells daemon io complied list cdcs https github com wushujames mysql cdc projects wiki 4,recommend take look debezium open source cdc platform amongst others also supports mysql use debezium stream changes apache kafka means embedded mode also use debezium connectors library java applications easily propagate data changes streaming apis kinesis etc disclaimer im debezium project lead 4,exact limit really hard determine ahead time one thing people underestimate high requirements index must fulfill becomes candidate used query efficient nonclustered index offers great selectivity returns small percentage total rows selectivity given sql servers query optimizer likely ignore index ideally cover query return teh columns required query create index index columns includes another handful columns included columns thus cover query chances query optimizer use index also means code always using select fetch columns likelihood indices used goes quite dramatically actually im sure ton criteria well would believe two critical ones course always keep indices properly maintained reorganize rebuild make 0,always use following use master get dbname go kick users alter database dbname set single user rollback immediate go prevent sessions establishing connection alter database dbname set offline sometimes may take sometimes blocked youre one running active connection database check query windows might database context include open dialogs object explorer intellisense long running jobs etc im done making changes databases config simply alter database dbname set online alter database dbname set multi user though sometimes thing need database requires database online sometimes leave single user mode alter database dbname set online go use dbname make changes im ready users connect 0,dont backup restore use sql server snapshots takes lot disk space store sparse file size files youve snapshotted rolling back hundreds times faster available sql server enterprise sql server developer editions 0
restoring old backup latest mysql release old database mysql server four years old uninstall mysql server install latest release restore,depends might see performance benefits might see worse performance example new feature might mean planner chooses different kind join crucial query impact change real world application predicted certainty planners make educated guesses definite calculations planning migration one version rdbms another need adequate planning time test performance change separate test environment mirrors production closely possible positive side often new releases improve database performance way give false sense security unless customer unimportant 4,jack douglas mentions really depends whatever make sure test putting production environment mileage vary depending storage engines use features use check changes mysql idea new changed instance innodb received performance boosts myisam default may tables side note always read process updating make sure upgrade goes smoothly pay attention specific conflicts might arise upgrading 4,imho comes mysql definitely see much better performance innodb past releases dirty pages innodb buffer pool linger long dirty pages flushed robustly also luxury creating multiple innodb buffer pools see innodb buffer pool instances innodb buffer pool size reduce thread locking amongst buffer pools make sure set innodb thread concurrency let innodb storage engine decide best handle thread allocation caveat migration mysql make sure mysqldump databases except mysql schema cleaner crisper way migrate user grants mysql schema fact two options accomplishing option use mk show grants dumps mysql grants sql statements completely portable mysql instance option run commands personal emulation 5,firewall database server exposed internet firewall block access servers apart ports required typically would http https web server external service requires access sql server permit access specific ip addresses required firewall occur via vpn connection openly exposed create new administrator account disable default sa preferably switch using mixed mode authentication windows accounts 0,rdbms fall scale depending configured scaled application utilizes thinks youve got two questions one first dbms capable persisting game data efficient way subjective imho second scale dbms perform 1000s users many online services found combination mysql memcached provide excellent performance scale however comes point solution falls however might exactly need online services sandwiching nosql solution architecture experience couchbase find useful 0,im answering question almost years late innodb file formats conceived time innodb independent mysql server example mysql could run two different versions innodb reason would want run barracuda could reduce flexibility downgrading mysql failed upgrade want move back version read newer format technical reasons antelope better mysql innodb file format option deprecated since mysql innodb longer independent thus innodb use mysql rules upgrades backwards compatibility required confusing setting required mysql default switched barracuda dynamic since currently supported releases mysql read format safe move away antelope still offer downgrade mysql server antelope tables upgraded barracuda dynamic next table rebuild optimize table 0
sql server agent error new shop error started show repeatedly event type error event source sqlserveragent event category alert engine,sounds like youve got sort corruption problem disk sql agent logs get written id check disk corruption well drive case talking application log sql agent also writes 6,could nothing important according kb however id schedule chkdsk drive next boot makre sure backups order 4,want reclaim space ibdata dump restore choice rolando points also probably best performance however want cut losses lose 150gb harddrive simply enable innodb file per table cnf restart server table issue alter table disable keys alter table engine innodb alter table enable keys problem large tablespaces take would suggest setting slave live db run conversion slave either shut master slave copy new dataspace master promote slave master caught going difficulty making change downtime 0,tunned little query shrink log requested set nocount select use name char13 char10 dbcc shrinkfile mf name truncateonly char13 char10 char13 char10 sys master files mf join sys databases mf database id database id database id mf type desc log 0
enforce schema bound views reporting purposes need able query dependencies views underlying tables column level via sys sql expression dependencies,easy ddl trigger database create trigger requireschemabinding database create view alter view declare object nvarcharmax declare schema nvarcharmax set object eventdata value event instance objectname nvarcharmax set schema eventdata value event instance schemaname nvarcharmax isnullobjectpropertyobject idquotename schema quotename object isschemabound begin raiserrornot today punk rollback transaction end 7,could use ddl trigger unfortunately schemabinding attribute eventdata structure construct object name check property using create trigger enforceviewbinding database create view alter view declare xml xml eventdata declare name sysname quotename xml value event instance schemaname sysname quotename xml value event instance objectname sysname objectpropertyexobject id name nv isschemabound begin raiserror views must specify schemabinding option rollback end 7,psql could connect server file directory server running locally accepting connections unix domain socket var run postgresql pgsql error generally means server running based dpkg output thread comments due postgresql main package somehow uninstalled since uninstall hasnt called purge option dpkg data configuration files still apt get install postgresql fix problem 0,postgresql terminal commands list databases available el defiant bin psql localhost username pgadmin list command stated simply psql pgadmin commands print terminal list databases name owner encoding collate ctype access privileges kurz prod pgadmin utf8 en us utf en us utf pgadmin pgadmin utf8 en us utf en us utf postgres postgres utf8 en us utf en us utf template0 postgres utf8 en us utf en us utf postgres postgres ctc postgres template1 postgres utf8 en us utf en us utf postgres postgres ctc postgres rows available databases psql commands list tables available specify database list tables database el defiant 0
parent child relationship name field implies relationship working harmonized system try catalog mysql database works example like live animals horses,many methods representing trees sql discussed bill karwin book recommend paper also interesting mysql specific see answer eldrith conundrum comparison different methods joe celko recommends one directed graph method suggest get hands copy book sql smarties subtome trees hierarchies sql could also google work vadim tropasko one key messages take much work solution much dependent usage patterns solutions suited one pattern another basically question broad methods better others particular issue interests subtrees others better obtaining level think members generation pedigree better moving subtrees think shifting department staff one line business another list goes unfortunately substitutes trudging available work done looking 4,describe variant materialized path see example https communities bmc com docs doc example say variant separator token instead using fixed number positions different level materialized path usually dont explicit parent child relationship represented instead encoded path string determine sub categories use like example ancestor categories need procedural part splits path string direct parent first ancestor direct children located using combination like length function root leaf predicates easy define well starting point would like add parent child relationship operations described answer referential integrity bit backward since already domain hand existing table adding separate relation table represent parent child relationship would something 5,detailed exhaustively page start stop pause resume restart sql server services question specifically asks recommended microsoft im inclined think counter productive discussion article details process using either command line powershell sql server management studio gui either database engine agent whether steps satisfactory would opinion dont want right answer always date stopping service prior power necessary recommended shutting server happens running sql services necessary windows kernel sends signal shutdown sql server fashion safe system wait complete speaking generally anything built ability safely shutdown shutdown manually stands reason microsoft applications follow api procedures tying preshutdown shutdown phases docs preshutdown assume theyre using 0,question normally install stand alone installarion yes absolutely availability groups meant ha dr solution stand alone instances replicas directly connected storage unlike fci required special installation procedure shared storage etc although answer already clear following summarized steps availability groups setup reference document might helpful especially production implementation summarized steps wsfc ready enable hadr sql service properties vai sql configuration manager replicas set database recovery full need added ag perform full backup user database begin creating availability group wizard vai ssms add replicas detailed ref endpoints configuration listener configuration optional read routing configuration optional following areas must consider avoid port conflict 0
performance degradation updating tables 10s millions records want update tables 10s millions records problem taking much time update process also,first question important use lot cpu time query bottlenecked resource could introduce enough additional disk access cpu time used per second would go would really improvement resource would prefer saturate understanding emphasized might help guide people providing answer find useful suggested comment query may run faster join rather correlated subquery something like update temp set customername user customername user user customerid temp customerid another important thing know whether want update rows table values already correct get big performance boost updating rows dont need add temp customername distinct user customername clause limit number rows updated statement vacuum analyze update avoid table 10,follow good advice kgrittn still performance issues may need perform update batches would still perform set based updates limit first whatever number works ive used 50000records dont match keep looping done 5,use cross apply unpivot columns rows select distinct probably jobrunid ut name ut type ut granularity dbo testupload cross apply select thingbname name thingbtype type thingbgranularity granularity union select thinganame name thingatype type thingagranularity granularity ut jobrunid jobrunid 0,ive read blog posts like theres quite inherited first sql server instance noticed many objects prefixed wanted start developing new ones less verbose approach singular naming convention lot easier possibly developers work object relational mapping one widely used prefixes tbl tbl tbl tbl even tablename tbl trying query work visual studio hell matter whole set table prefixed tbl please keep way whole database least end would definitely say matter personal preference also lot consistency go road lot people happy least keep along development hand wanted say take approach go non prefix singular name table make developer happy future important happiness 0
create index table mysql database use reason performance stability shouldnt,yes lock table youre adding index created table large may take awhile read row building index 15,note table using innodb plugin storage engine highly recommend secondary index almost surely case still read table non blocking select statements 17,according http www postgresql org message id blu0 smtp179b92c5102247cd961a4b3cf2a0 phx gbl temp counter files space used shows total temp files used since probably cluster creation reflect current space used temp files system example shows almost 700gb temp files used actual space taken temp files var lib pgsql data base pgsql tmp 53mb currently 0,dont see would useful connection id innodb transaction id shown show engine innodb status available using select trx id information schema innodb trx trx mysql thread id connection id available mysql possibly innodb plugin transactions dont become visible transaction actually kind interaction innodb 0
database could handle storage billions trillions records looking developing tool capture analyze netflow data gather tremendous amounts day capture billion,going put sql server would suggest table something like create table tcp traffic tcp traffic id bigint constraint pk tcp traffic primary key clustered identity11 tcp flags smallint bits tcp use smallint src int since less billion possible use int netxhop bigint use big integer ip address instead storing dotted decimal unix secs bigint src mask int assumption tos tinyint values see rfc prot tinyint values see rfc input int assumption doctets int assumption engine type int assumption exaddr bigint use big integer ip address instead storing dotted decimal engine id int assumption srcaddr bigint use big integer ip address 41,company work dealing similar amount data around tbs realtime searchable data solve cassandra would like mention couple ideas allow o1 search multi tbs database specific cassandra db though use db well theory shard data way single server reliably realistically hold volume data ready hardware faults whole node failures duplicate data start using many back end servers beginning use many cheaper commodity servers compared top end high performance ones make sure data equally distributed across shards spend lot time planning queries derive api queries carefully design tables important prolonged task cassandra design composite column key get access key o1 spend time 57,would recommend hbase store raw data one hbase tables depending need query hbase handle large data sets auto sharding region splits addition design row keys well get extremely fast even o1 queries note retrieving large data set still going slow since retrieving data operation since want query across field would recommend creating unique table example src address data table looks like timestamp1 data timestamp2 data want query data across starting mar mar range scan start stop rows specified imho row key design critical part using hbase design well able fast queries store large volumes data 5,used past power designer designer back days terrific reverse engineering facilities script generation everything youd need free free tool ive used bit past db designer could use small dbs errors back working complicated schema designs years ago im sure fixed use sql servers diagramming tool 0,found interesting tutorial using liquibase version oracle 0,alternative horse names solution simplest option use row array comparision array select city cities temps see sqlfiddle necessary unnest array work arrays well rowsets unlike using array operators operation benefit use index tree gin temps normalized design splits temps separate table foreign key reference cities may actually faster lots cities lots samples earlier suggested may want use gin array index array operators mistaken support desired operation made error testing made appear frequent updates id normalize another table btree index table theres small amount data wouldnt bother indexes id use temps 0
distinct combined somehow postgres way combining distinct neat way getting result select count select foo union select union select null,perhaps like select foo exists values null except select foo chk exists values null intersect select foo chk values aznull zfoo foo chk chk note null array also null compared way 7,looking grammar problem defined row array comparisons expression operator array expression distinct operator construct told comparison operators behavior suitable use distinct constructs since postgresql user defined operators may define operator function combo purpose create function distinct fromtext text returns bool select distinct language sql create operator procedure distinct fromtexttext leftarg text rightarg text precede select count select foo union select union select null foo anyarray null count row 13,operator building daniels clever operator create function operator combo using polymorphic types works type like construct make function immutable create function distinct fromanyelement anyelement returns bool language sql immutable select distinct create operator procedure distinct fromanyelementanyelement leftarg anyelement rightarg anyelement quick search symbolhound came empty operator seem use modules going use operator lot might flesh assist query planner like losthorse suggested comment starters could add commutator negator clauses assist query optimizer replace create operator create operator procedure distinct fromanyelementanyelement leftarg anyelement rightarg anyelement commutator negator add create function distinct fromanyelement anyelement returns bool language sql immutable select distinct create operator 10,ways perform data transformation access pivot function easiest use aggregate function case aggregate case version select personid maxcase optionid else end optiona maxcase optionid else end optionb maxcase optionid else end optionc personoptions group personid order personid see sql fiddle demo static pivot select select personid optionid personoptions src pivot countoptionid optionid optiona optionb optionc piv order personid see sql fiddle demo dynamic version two versions work great known number values values unknown want implement dynamic sql oracle use procedure create replace procedure dynamic pivot pop cursor sys refcursor sql query varchar21000 select personid begin select distinct optionid personoptions order 0,since old question dug anyway ill mention use built xquery support db2 regular expression matching something along lines select whatever users xmlcast xmlqueryfn matches user name aofdmep z0 sidbfkfpo integer xmlquery calls xquery matches function column user name result xml boolean xmlcast used convert sql data type 0,thought read lot topic broad topic configuration control change management strategy cmmi domain topic even companies accreditation cmmi sometimes version control databases question answered keeping mind following constraints keeper every ddl executed keeper people ability execute ddl statements need log changes done need compare vast differences database design done via external tool published database external tool ddl scripts source control even key point source control publish database need know instantaneous changes time time hourly daily defined server structure development test production good testing strategy answer true use external source control example embercadero database change management tool http www embarcadero com 0
wrong subquery work get subquery return lowest course weight lecturer currently returns lowest subquery wrong outer query question show lecturer,many many ways one slightly changes attempt select m1 moduleid m1 cwweight staffid dbo module m1 inner join dbo lecturer m1 moduleconvenor staffid m1 cwweight select minm2 cwweight dbo module m2 m2 moduleconvenor staffid 4,another approach easily extensible things like getting lowest highest min max etc select moduleid cwweight staffid moduleconvenor rn row number partition moduleconvenor order cwweight dbo module select moduleid cwweight staffid inner join dbo lecturer staffid staffid rn thing like pattern chance ties break adding clauses order example two modules share lowest weight would pick one higher moduleid partition moduleconvenor order cwweight moduleid desc 4,also use pivot assuming least sql server select algodata binnet newmoon titles pivot counttitle id pub id 0,also use trigger create trigger limittable yourtabletolimit insert declare tablecount int select tablecount count yourtabletolimit tablecount begin rollback end go 0
oracle startup error could open parameter file installed oracle 11g server ubuntu cant start server start server following error occurs,see two things wrong oracle tries open parameter file oracle home dbs directory format spfile oracle sid ora find tries open init oracle sid ora problems virtue fact oracle trying open parameter file named initxe ora sid must one point equal xe easy part try first since comments echo oracle sid yields nothing try setting export oracle sid xe try starting oracle comments show file named initxe ora really name file paste comment somehow lowercase output ls command reason point ubuntu linux case sensitive filesystem initxe ora initxe ora actually two different files comment correct file lower cased youll want 10,new installed oracle initxe ora parameter file u01 app oracle product xe dbs means need create xe db first run u01 app oracle product xe bin createdb sh script first 6,want add different databases require different strategies lets compare mysql innodb postgresql example innodb innodb tables basically tree index primary key extended include row information index entry physical order scans supported scans happen logical order means two things sequential scan innodb generates lot random disk primary key index must traversed regardless whether one using secondary index primary key lookups faster model approach case important index enough fields multi page tables typical rule index everything want filter postgresql postgresql uses heap files one table per file tables may many files tuples allocated heaps free space physical order scans supported logical order 0,different types cars instance general problem surfaces data modeling called generalization specialization er modeling superclass subclass object modeling object modeler uses inheritance features built object model solve problem quite easily subclasses simply extend superclass relational modeler faced problem design tables emulate benefits one would get inheritance simplest technique called single table inheritance data types cars grouped single table cars column car type groups together cars single type car belong one type column irrelevant say electric cars left null rows pertain electric cars simple solution works well smaller simpler cases presence lot nulls adds tiny bit storage overhead little bit retrieval 0
index used order trying get info mysql using index create inner join trying order end sql query select product inner,david spilletts answer correct points except encouraging suggestion heres way encourage almost versions force optimizer choose plan uses wanted index find rows perform join cant always used foreign key constraint assures case two queries produce identical results call technique first limit join select ps select productstore order storetitle limit ps inner join product productuuid ps productuuid order ps storetitle 5,real answer prefix indexes virtually useless referring key teststoretitle storetitle182 since index contains truncated values completely ordered list titles hence easily used order innodb limit bytes max utf8 varchar255 increased complex set steps get later set global innodb file format barracuda set global innodb file per table alter table tbl drop index teststoretitle add indexstoretitle row format dynamic compressed agree limit group join ypercube suggests solution mostly orthogonal one solution probably significantly faster since wont need scan anything 4,oracle schema contains tables constraints etc mysql create create database oracle create schema dont use db1 alter session set current schema db1 user schema get list schemas select username sys users 0,top script put following set nocount hide rows effected messages 0
query cause deadlock query cause deadlock update top1 system queue set statusid id internalid internalid select top internalid system queue,looks trying select update statement onto table select holding shared lock values inside ix system queue directionbystatus index update needs locks released get exclusive lock update primary key guess clustered also part ix system queue directionbystatus key value anyway guess query would succeed rare chance index values selecting updating conflicting deadlocking time execute assume would link explains deadlocks detail http sqlblog com blogs jonathan kehayias archive anatomy deadlock aspx 13,dont expect mark post answer sharing information sql server experts topic http sqlblog com blogs alexander kuznetsov archive reproducing deadlocks involving one table aspx http rusanu com readwrite deadlock 6,use sqlcmd mode ssms occasion typically migrating databases one environment another example need backup database production restore dev ill one script using sqlcmd mode switch servers ive used one thing another dont use often 0,mysql general query log fields yymmdd hh mm ss thread id command type query body timestamp appears time changes example would thread id connected client shown show processlist line represents query issued necessarily round trip client database queries issued within stored procedures stored functions also logged proc function running could issue many queries response single query client also mysql api library underlying languages connectors mysql supports multiple statement execution would another case multiple queries dont necessarily represent multiple round trips since multiple queries send database batch thats factor capability used calling application general log also directed write mysql general log 0
script contents sql server database batch file want able script schema given database sql file windows command prompt basically want,theres nothing built command line red gate sql compare sqlcompare s1 mysqlinstance db1 mydb mkscr mydb schema ssms scripting functions wrappers smo know mention could write powershell script use smo adapted code found simple talk post system reflection assembly loadwithpartialname microsoft sqlserver smo servername mysqlinstance databasename mydb sqlserver new object microsoft sqlserver management smo server servername sqldb sqlserver databases databasename options new object microsoft sqlserver management smo scriptingoptions options extendedproperties true options driall true options indexes true options triggers true options scriptbatchterminator true options filename script folder mydb schema sql options includeheaders true options tofileonly true transfer new object microsoft 5,feed squillmans answer show sample sqlps browse directory database get member membertype method looking script directories believe add pssnapin sql note hostname server sqlserver show object names scripted dir sqlserver sql sqlserver default databases jproco tables select name objects dir sqlserver sql sqlserver default databases jproco tables objects script look real server 4,wrote open source command line utility named schemazen much faster scripting management studio output version control friendly supports scripting schema data generate scripts run schemazen exe script server localhost database db scriptdir somedir recreate database scripts run schemazen exe create server localhost database db scriptdir somedir 7,currently available extensions amazon rds detailed database engine features chapter issue following command get available extensions supported show rds extensions 0,need prevent drop table user try deny delete object dbo table deny restricted user http msdn microsoft com en us library ms173724 aspx 0,found foreign key db must creating think happened accident click new foreign key context menu table primary key within management studio sql express already automatically creates foreign key referring see dont realize alter one instead adding new one stay simply click close button meaning would like cancel foreign key would still created saving table definition foreign key makes sense removed 0
one administrator connect time ms sql server error experimenting effect giving sql server small amount memory thought going recover configured,start sql server single user mode one administrator connect time whats probably happening service using sysadmin login connect reporting services sql server agent starting sql server single user mode option specify client application application connect look command line options lists client application name use option sqlcmd sql server management studio limit connections specified client application example sqlcmd limits connections single connection connection must identify sqlcmd client program use option starting sql server single user mode unknown client application taking available connection connect query editor management studio use microsoft sql server management studio query client application name case sensitive able connect 22,start sql instance minimal configuration mode net start mssqlserver connect instance cmd window change max memory setting sqlcmd server instance see prompt looks like default maximum memory setting may available viewing changing modify behavior use query sp configure show advanced options go reconfigure go sp configure max server memory go reconfigure go restart instance configuration manager 6,ive come declare date daterange empty day range daterange extreme value date begin select distinct generate series lowerrange coalesceupperrange interval day extreme value interval day date rangetest order loop day range daterangei begin isemptya day range else day range end exception data exception raise info day range end end loop uppera extreme value interval day daterangelowera null end raise info end still needs bit honing idea following explode ranges individual dates replace infinite upper bound extreme value based ordering start building ranges union fails return already built range reinitialize finally return rest predefined extreme value reached replace null get infinite 0,would say best bet either extended events trace unfortunately dont know enough extended events give advice give setting trace first trace pull among things hostname server workstation query came include since sometimes username doesnt really help frequently track something machine running second filter textdata able pull queries start first however many characters query looking however may find bit slow im sure carriage returns line feeds formatting may throw trace pull queries duration higher reads higher writes higher etc using information query able keep number false positive responses fairly low using types restrictions make trace fairly low overhead edit example script 0
primary key within json data postgresql table column called json type json within json natural key select json id id,able add constraint primary key must simple composite index may partial expression index index acts constraint functionally theyre much cant appear primary key table metadata used target foreign key constraint true unique constraints issue sql standard definitions primary key unique constraints allow expressions row matching predicates postgresql lists expression index partial index constraint breaking standard lying applications apps understand postgresqls features look index pgs catalogs theres also info information schema cant go listed constraint 5,explained craigs answer neither primary key unique used howeverhere two part solution work around enforce null use check constraint thus alter table add constraint id null check json id null enforce uniqueness create unique index thus create unique index id json id psql give indexes id unique btree json id text check constraints id null check json id text null see null uniqueness enforced test insert values id field error new row relation violates check constraint id null detail failing row contains id field test insert values id abc insert test insert values id abc error duplicate key value violates 6,quote joe celko find reference web wikipedia entry even see shirts conferences rows records lot people point pedantic jerk likes humble verbally abuse newbies admit comes across also met person even shared meal cant tell different real life persona online front even caught calling rows records embarrassed full backstory case say guys online character wrote standard fact authority dictates distinction tell something much cringes someone calls row record many colleagues also experts sql server world us camp believe right example itzik ben gan obvious sql server guru quote first lesson training kit exam querying microsoft sql server example incorrect terms 0,great discussion stackoverflow covers many approaches one prefer sql server use table valued parameters essentially sql servers solution problem passing list values stored procedure advantages approach make one stored procedure call data passed parameter table input structured strongly typed string building parsing handling xml easily use table input filter join whatever however take note call stored procedure uses tvps via ado net odbc take look activity sql server profiler notice sql server receives several insert statements load tvp one row tvp followed call procedure design batch inserts needs compiled every time procedure called constitutes small overhead however even overhead tvps 0
oracle empty string converts null insert empty string converted null insert test values row containing null query table use select,oracle treats null inserting conversion null merely interpretation null way word null interpreted null rtrimaa interpreted null demonstration using following table insert drop table t1 create table t1 c1 varchar210 insert t1 c1 values insert inserted null value c1 select row follows select c1 t1 add clause compare equality one values compared null result always unknown unknown evaluate false except operations unknown value produce unknown values following return rows clauses contain conditions never true regardless data select c1 t1 c1 select c1 t1 c1 null select c1 t1 select c1 t1 null null oracle provides special syntax retrieve rows particular 6,says select nvlit null value dual sql fiddle things gets converted null insert thats oracle varchar2 thing select test trying select test null isnt defined return nothing null doesnt like equality operator use null null ill add char datatype behaves differently padded 8,rights table element soon add element need add table would add application maintenance downside putting everything one table might run scaling issues could mitigated using partitioning materialized views virtual columns likely measures would necessary far table design oracle might suggest something like create sequence userroleid create table userrole userid number7 null roleid number7 null constraint userrole pk primary key userid roleid enable organization index create table permissions id number7 null elementid number7 null constraint userrole pk primary key id elementid enable organization index package code could use userroleid sequence populate id users table id roles table necessary permissions table could 0,ive come declare date daterange empty day range daterange extreme value date begin select distinct generate series lowerrange coalesceupperrange interval day extreme value interval day date rangetest order loop day range daterangei begin isemptya day range else day range end exception data exception raise info day range end end loop uppera extreme value interval day daterangelowera null end raise info end still needs bit honing idea following explode ranges individual dates replace infinite upper bound extreme value based ordering start building ranges union fails return already built range reinitialize finally return rest predefined extreme value reached replace null get infinite 0
alternative way compress nvarcharmax trying compress tables nvarcharmax fields unfortunately row page compression desire impact mb saved gb table also,page row compression compress blobs size large value data types sometimes stored separately normal row data special purpose pages data compression available data stored separately want compress blobs need store varbinarymax apply stream compression algorithm choice example gzipstream many examples search gzipstream sqlclr 15,potentially two ways accomplish custom compression starting sql server built functions compress decompress functions use gzip algorithm use sqlclr implement algorithm choose remus mentioned answer option available versions prior sql server going way back sql server gzip easy choice available within net supported net framework libraries code safe assembly want gzip dont want deal coding deploying use util gzip util gunzip functions available free version sql sqlclr library author decide use gzip whether code use sql please aware algorithm used net gzip compression changed framework version better see remarks section msdn page gzipstream class means using sql server r2 linked 10,short many indexes rule bit misleading think long given average database around reads higher reads need optimised insert read unique index example update read even write intensive database still reads poor quality indexing examples wide clustered indexes sql server especially non monotonic clustered indexed overlapping indexes eg cold cole cold cole colf many single column indexes also overlapping useful indexes useless queries includes covering eg single column indexes note quite typical indexes several times bigger actual data even oltp systems generally id start clustered index usually pk unique indexes constraints cant covering foreign key columns id look common queries see 0,could take look talend open studio development environment runs inside eclipse many different kinds database connectors transformations also given open source project build connectors transformations share users even commercial users talend 0
sync instance instance without losing data already instance like said comment im dba im still learning process heres scenario oracle,golden gate answer types problem oracle bought therefore oracle golden gate right look image taken oracle golden gate admin guide http download oracle com docs cd e18101 doc e17341 pdf also look following article http www dbasupport com oracle ora11g oracle replication streams vs goldengate shtml thanks tevo pointing licensing issue actually golden gate may expensive solution depending circumstances companys standing oracle cost downtime etc 5,firstly never need shut source server oracle lot techniques use depends data bit vague sorry let give example table insert sequence generated pk simply find max destination database insert select dblink another example destination read maybe solution could active dataguard conflicts manage logical standby goldengate fine technology spending money sure really need also consider shareplex 4,default built oracle answer would probably oracle streams stream propagate information within database one database another possible alternative would create materialized views may union data materialized views existing tables create views union materialized views could refreshed daily options already listed may also good fits depending requirements consider update comments indicate would like replicate packages well could integrate change management process changes made also made would need use different schema since want preserve originals hand requirement moving lobs well makes things difficult want use one technology handle requirements probably go datapump could setup pull tables blobs packages separate schema may still 5,oracle enterprise edition feature called transportable tablespaces used copy set tablespaces one oracle database another transport tablespaces across platforms versions limitations include source destination databases must use compatible database character sets long fence objects want copy tablespaces separate objects dont want copied use feature move many kinds objects one go data pump also needed metadata 4,jack demonstrated way go however feel room improvement place everything schema convenient testing test setup drop schema cascade create schema meta tables schema table name create table schmaschma id int schma text insert schma values create table tbltbl id int tbl text insert tbl values t1 t2 dummy tables used example query create table t1id int insert t1 values create table t2foo text insert t2 values text text old function original answer create replace function dynaquery oldint int col text type anyelement col anyelement returns setof anyelement func begin return query execute select quote ident col select schma schma schma 0,know old thread would say large degree snapshot isolation magic bullet eliminate blocking readers writers however prevent writers blocking writers way around experience additional load tempdb negligible benefits row versioning reducing blocked readers huge reference row versioning snapshot isolation method oracle used decades achieve isolation without blocking readers oracle dbs ive worked nearly years experience far less blocking issues sql server sql developers hesitant use snapshot isolation though theyve tested code databases use default setting 0,yes see risks naive count sql using row locking example im pretty sure inserts deletes use page locks least sql engine chooses lock type based several factors none factors include opinion blanket solutions like changing temp tables table variables generally bad ideas also table variables useful situations limitations performance issues prefer temp tables circumstances particularly table hold dozen rows would require vendor explain system experienced heavy compile locks degraded performance remember anytime look find heavy locks sort necessarily mean locks problem maxs comments spid also important additionally transaction model error processing could big issues system experiences deadlocks input data quality 0,solution one provided aaron bertrand comes building comma separated values bit different connecting coursemaster courseid values studentmaster course sql fiddle ms sql server schema setup create table dbo coursemaster courseid char2 coursename char3 create table dbo studentmaster rollno char5 name varchar10 address varchar20 course varchar100 insert dbo coursemaster values abc def ghi jkl mno pqr stu insert dbo studentmaster values ram ram address hari hari address jeff jeff address daisy daisy address query select sm rollno sm name sm address select cm coursename dbo coursemaster cm sm course like cm courseid xml path type valuesubstringtext varcharmax course dbo studentmaster sm 0
database backup data log files told data files backup operate extent level log file backup operates page level know file,dont worry extents pages full backup contains data pages pages written backup single point time full also contains log records needed redo undo changes earliest page latest transaction committed backup differential backup omits pages changed since last full backup log backup contains log records since last log backup 6,told data files backup operate extent level log file backup operates page level would say statement completely correct data file log file backup would operate page level differential backup scans differential bitmaps backs data file extents marked changed full backup bit puzzled backup distinguishes data log depends backup command give give full backup command sql server knows backup whole database transaction log make sure backup restored database consistent log reads portion transaction log backups changes made last log backup full backup 4,backup standby perfectly fine avoid cancelled statement conflict backup standby system need pause replication standby using select pg xlog replay pause run backup finished run select pg xlog replay resume resume replication keep mind running commands cause recovery lag slave might quite large depending database size also take account space wal segments take replayed slave pause may find useful administractive functions documentation instance check server actually recovery prior pausing select pg recovery 0,three dmvs use track tempdb usage sys dm db task space usage sys dm db session space usage sys dm db file space usage first two allow track allocations query session level third tracks allocations across version store user internal objects following example query give allocations per session select sys dm exec sessions session id session id db namedatabase id database name host name system name program name program name login name user name status cpu time cpu time milisec total scheduled time total scheduled time milisec total elapsed time elapsed time milisec memory usage memory usage kb user objects 0
change three hundred procedures need change procedures packages database due migration accomplish weekend migration one server exadata however database developed,id probably something like call dbms metadata get ddl get ddl object clob write code search replace clob use execute immediate execute newly modified clob something like search replace implements whatever logic need declare ddl clob begin select dba objects objects want change loop ddl dbms metadata get ddl object type object name owner search replace ddl file folder documents directory name execute immediate ddl end loop end 5,script procedures file search replace trivially dome sed script along lines file folder documents directory name note tested top head fiddle load stored procedures note youre frigging code base really test youre rather blind search replace production code could possibly go wrong 7,alternatively could calculate result using formula select dayyourdate iteration dbo yourtable cases operands division operation integers result also integer rounded one dayyourdate give yield still get result 0,database restore issued norecovery left pending state accessed logs differentials added database state since independent activity occurring recovery places database operational state add components backup set time 0
optimization database heavy updates software hardware situation postgresql database quite heavily updated time system hence bound im currently considering making,hardware upgrade number disks key performance yes hard disc even sas head takes time move want hugh upgrade kill sas discs go sata plug sata ssd enterprise level like samsung 843t result around thousand iops per drive reason ssd killers db space much cheaper sas drive phyiscal spinning disc keep iops capabilities discs sas discs mediocre choise start large get lot iops higher use database smaller discs would mean lot iops end ssd game changer regarding software kernel decent database lot iops flush buffers log file needs written basic acid conditions guaranteed kernel tunes could would invalidate transactional integrity partially 10,afford put pg xlog separate raid pair drives controller battery backed ram configured write back true even need use spinning rust pg xlog everything else ssd use ssd make sure super capacitor means persist cached data power failure general spindles means bandwidth 6,looks like already acceptable solution db mail set could potentially something like exec msdb dbo sp send dbmail recipients email domain com subject csv extract body see attachment query select col1col2col3 mydatabase dbo mytable attach query result file query attachment filename csv extract txt query result separator query result header emailing file somewhere anyway might save steps 0,im dba oracle sql server confusion due mismatch semantic extra level sql servers hierarchy theres note talk single instance database instances pun intended instances oracle one instance amount memory allocated sga pga processes spawned smonpmon dw0x system views monitor stuff created used dba views structure familiar oracle already mentioned database physical files typically moving rac makes difference glaring sql server instance works way bunch memory allocated system views sys monitor stuff instance system databases system system views msdb database also holds system related information typically thats find backups related data bit maintenance required least flush old backups records model aptly 0
unable backup database sql2005 database ms sql2005 backup whenever try create backup selected database sql mgm studio get error backup,ms kb article say happens full text catalog folder either deleted corrupted enable database full text indexing database restored microsoft sql server database backup therefore folder full text catalog database exist server restore database instance sql server running upgraded sql server however full text search service accessed upgrade database attached somewhere however specify incorrect location full text catalog folder attachment article also workaround section 4,copied database attached also copy full text data another location change locations full text indexes sounds like databases pointing full text location copied database another server didnt include full text data depending answered determine next steps take 4,azure sql directly supports azure sql database directly supports clearing proc cache current user database without hacks alter database scoped configuration clear procedure cache additional information following script shannon gowen used watch process step step run script user database master count number plans currently cache select count sys dm exec cached plans executing statement clear procedure cache current database means queries recompile alter database scoped configuration clear procedure cache count number plans cache cleared cache select count sys dm exec cached plans list available plans select sys dm exec cached plans 0,proper structure scenario subclass inheritance model nearly identical concept proposed answer heterogeneous ordered list value model proposed question actually quite close animal entity contains type race properties common across types however two minor changes needed remove cat id dog id fields respective entities key concept everything animal regardless race cat dog elephant given starting point particular race animal doesnt truly need separate identifier since animal id unique cat dog additional race entities added future fully represent particular animal meaning used combination information contained parent entity animal hence animal id property cat dog etc entities pk fk back animal entity differentiate 0
list primary keys tables postgresql query found queries one table wasnt able modify see tablename column type,something like select tc table schema tc table name kc column name information schema table constraints tc join information schema key column usage kc kc table name tc table name kc table schema tc table schema kc constraint name tc constraint name tc constraint type primary key kc ordinal position null order tc table schema tc table name kc position unique constraint 13,accurate answer select tc table schema tc table name kc column name information schema table constraints tc information schema key column usage kc tc constraint type primary key kc table name tc table name kc table schema tc table schema kc constraint name tc constraint name order missed kc constraint name tc constraint name part lists constraints 20,error harmless get rid think need break restore two commands dropdb postgres mydb pg restore create dbname postgres username postgres pg backup dump clean option pg restore doesnt look like much actually raises non trivial problems versions combination create clean pg restore options used error older pg versions indeed contradiction quoting manpage clean clean drop database objects recreating create create database restoring whats point cleaning inside brand new database starting version combination accepted doc says quoting manpage clean clean drop database objects recreating might generate harmless error messages objects present destination database create create database restoring clean also specified drop 0,answers provide settings type sqlite console via cli nobody mentions settings put rc file avoid type time save sqliterc mode column headers separator row nullvalue null note ive also added placeholder null values instead default empty string 0
done enhance performance multiple join aggregate queries typical star schema simulated mentioning two queries first query simply joins fact table,rarely need point benefit trying micro optimise star schema queries non clustered indexes laden included columns fact tables built scanned indexes youve created examples subset copies parent table scanned seeks minor performance improvements come scanning marginally fewer pages parent table given star schemas built support ad hoc query patterns viable create indexes support every possible enquiry create fact table clustered index date key majority typical fact table queries include time element clustering date key enables range scanning fact table rows add non clustered indexes foreign keys fact tables assist highly selective queries foreign keys dimension tables created nocheck prevent impact 8,addition marks excellent answer strategies add existing system exhaustive list course pre aggregated tables indexed views physically materialize results intermediate results query sql server end scanning much smaller indexes return full result set keeps project within database using technology youre familiar analysis services may worth looking plan support lot slicing dicing data analysis services built pre aggregate data automatically according parameters enter disadvantage probably totally new technology im expert area say learning curve powerful tool result caching lot rows coming back youre finding users running queries cache results invalidate cache new data loaded figure way selectively invalidate based new data 5,answer since refer website use index luke com consider chapter use index luke clause searching ranges greater less example matches situation perfectly two column index one tested equality range explains nice index graphics ypercubes advice accurate sums rule thumb index equality first ranges also good one column queries one column seems clear details benchmarks concerning related question working indexes postgresql composite index also good queries first field less selective column first apart equality conditions columns doesnt matter put column first likely receive conditions actually matters consider demo reproduce create simple table two columns 100k rows one one lots distinct values 0,always concerned system may migrated system doesnt support uniqueidentifier compromises dont know designer may known uniqueidentifier type things didnt know technically though shouldnt major concern 0
anybody use sqlcmd mode practice sqlserver introduced something called sqlcmd mode msdn link first glance mode adds variable subsitution command,use sqlcmd mode ssms occasion typically migrating databases one environment another example need backup database production restore dev ill one script using sqlcmd mode switch servers ive used one thing another dont use often 4,actively use deploying logshipping allows everything within single script connecting primary monitor secondary servers 4,use ola hallengrens scripts days support sql servers see http ola hallengren com download maintenancesolution sql scripts several awards quite widely used documentation quite good guidance backups database maintenance feature choose frequency backups types backups retention backups past used sql servers maintenance plans code olas scripts one month retention backups less backup retention period specified hours days lot flexibility one comment deleting files ideally backup perhaps days backups individual file way delete backup delete one backup one days worth backups 0,jack said plus need list registered values enum type enum support functions based jacks example select enum rangenull mood enum range sadokhappy thats simpler resilient unlikely changes future major postgres versions might break query dbfiddle 0
two queries logically equivalent two queries logically equivalent declare datetime datetime getdate query select mytable datediffday loginserttime datetime query select,whether two queries posted logically equivalent irrelevant shouldnt use either try steer away couple things whenever possible try avoid applying functions columns always good mostly better keep calculations constants columns destroy sargability render indexes columns useless case much prefer query especially logdatetime indexed might ever dont like shorthand date math recommend sure faster type try date data type get ugly error much better spell loginserttime dateaddday datetime 15,would use following sargeable query select mytable loginserttime dateaddday datetime reason believe result datetime documented even happens equivalent dateaddday datetime may break later release 8,equivalent records days ago current time day returned query comparing days using dateadd function take time part consideration function return comparing sunday monday regardless times demo declare mytable tablepk int loginserttime datetime insert mytable values dateaddhour castdateaddday cast getdate dateas datetime dateaddhour castdateaddday cast getdate date datetime declare datetime datetime getdate select mytable datediffday loginserttime datetime records select mytable loginserttime datetime record logical equivalent first query enable potential index usage either remove time part datetime set time select mytable loginserttime cast datetime date reason first query use index loginserttime column buried within function query compares column constant value enables optimizer 6,one google results wanted remove require ssl mysql user enforced require none usage grant usage dbname dbusername require none verify settings changed running show grants dbusername worked mysql 0,counting toast tables twice owner pg total relation sizereltoastrelid entry pg class use relkind reltype filter want also trying micromanage rdbms extent rarely pays 0,cant say size could try check last entry amount entries dump check current last entry database might help determine time import take till finishes size imported data might even get bigger uncompressed 7gb since indexes usually contained dumps get built insert sidenote also way speed import drop index import rebuild later helped several times speed things 0
perl vs ksh unix database administration realize somewhat subjective question looking community guidance company fairly new dbas used use db2,end post really important bit information regarding best suits needs database administrators lets start needs dba generally two realms operation system level maintenance database level maintenance whereby actions merely maintaining system better performance look dont get pedantic go two realms consider ksh korn shell one older unix operational shells used control server goal ksh manage server allow one things traditional bourne shell bourne shell people familiar older vintage allowed differences shell better support things like arithmetic arrays making easier program regards default shell aix perl perl high level general purpose programming language designed run top shell designed primarily reporting mind 6,ksh bash let script items sql plus quite complex stuff however shell scripting tends bit write sh derivatives arent really much good developing complex program logic running automated tasks theyre ok complex client side data manipulation good sed awk notwithstanding perl various oracle libraries dbd oracle available cpan somewhat better work involves complex processing client side type system bit sophisticated line text another option thats quite good python cx oracle 5,anonymous pl sql blocks dont start code begin end least exists sql function cant used pl sql like try something like set serveroutput declare number begin select count table rownum dbms output put linehas rows else dbms output put lineno rows end end yes using exists query also possible set serveroutput declare varchar210 begin select case exists select table rows else rows end dual dbms output put linec end note exists rownum version stop first row found thats point dont need read whole table index 0,alternative test attribs unpivot view provided jackpdouglas works versions 11g fewer full table scans create replace view test attribs unpivot select id name myrow attr cast decodemyrow1attr12attr23attr34attr4attr5 varchar22000 attr test attribs cross join select level myrow dual connect level final query used unchanged view 0
sort match like wondering implement sql get results sorted best match like predicate 100k articles database user call items part,isnt possible calculate relevance like predicate sql server previous questions believe platform youll want look full text search supports scoring ranking results relevance 10,since matches match like pattern order included simple thing assume shorter values column youre matching better matches theyre closer exact value pattern order lenitem nale asc alternatively could assume values match pattern appear earlier better matches order patindex user input item nale asc could combine two look earliest matches first within prefer shorter values order patindex user input item nale asc lenitem nale asc sophisticated full text indexing accounts matching single column gives decent enough results 18,would good idea would numeric60 take less bytes test select pg column sizeint4 pg column sizenumeric60 pg column size pg column size row performance table queried lot slower stored binary coded decimal arbitrary precision value 0,use sys triggers meta data table contains row object trigger change output mode text clicking toolbar button shown execute script use yourdbname go select go char10 char13 drop trigger quotenameobject schema nameo object id quotenamename sys sql modules inner join sys triggers object id object id copy output new sql server management studio window verify code performs actions expecting execute 0
search see mysql user exists system im finding user show grants username localhost im thinking perhaps username slightly different want,use mysql select hostuser user give mysql users server 7,thanks matthewhs answer managed compose query would work searching large number users use mysql select user host user user like user note user whatever component username need search may need sides dont know start username also merely note variable dont include actual query unless username includes 9,use identity column temporary table avoid cursor especially convenient older sql server versions dont support window functions create table variable variableno int identity null code varchar4000 null insert variable code select distinct variable simstg parameter left outer join sim variable variable code bucketref stgbucketno bucketref simbucketno code null declare curid smallint select curid isnullmax id sim variable bucketref simbucketno insert sim variable bucketref variableno code select simbucketno variableno curid code variable drop table variable 0,executing query failed following error index partition table reorganized page level locking disabled maintenance plan must attempting alter index reorganize online operation remove fragmentation pages order pages must locked moved possible page locks disabled way defragment without page locks lock entire partition possible reorganize online difference two locking schemes real world production consequences need grasp record page evaluate impact disallowing particular lock type unfamiliar sql server storage internals start anatomy record anatomy page put simply rows records rows stored pages 8kb alter permitted lock types disable page locks row table locks disable row locks page table locks disable table locks 0
insert performance increases load piece code performs inserts highly denormalized tables tables numbers columns ranging sql server r2 running windows,one possible reason four concurrent processes generate favourable pattern log flushes typically meaning log flush writes data case single executing process determine transaction log throughput flush size factor monitor sys dm os wait stats writelog logbuffer waits sys dm io pending io requests io performance performance monitor counters sys dm os performance counters log bytes flushed sec log flushes sec log flush wait time look internal limits reached sql server r2 maximum outstanding asynchronous log flush os per database bit versions bit also total size limit outstanding ios 3840kb information reading transaction log monitoring trimming transaction log fat diagnosing transaction 15,everything paulwhite says plus foreign keys place every insert require check done table referenced sounds like youre getting 360ms feels slow anyway checking tables massively helped data ram already rather load disk sounds like loading data ram significant part execution needs happen could also effective plan caching queries need compiled first time subsequent calls able avoid phase 12,try something like see understand concept select top rowid select rowid yourtable documentid documentid union select rowid yourtable employeeid employeeid union select rowid yourtable companyid companyid group rowid order count desc rowid asc 0,heres postgresql solution using built array functions alter table table add chk one null check array lengtharray removearray col1 text col2 text col3 text null 0
better store images blob url possible duplicate files database wondering theres good reason still use blob fields database couple years,reason use blobs quite simply manageability exactly one method back restore database easily incremental backups zero risk image meta data stored db tables ever getting sync also one programming interface run queries load save images dont need give remote clients filesystem access know grants apply images associated data plus one method storage management oracle might put everything asm use oracle lvm everything another application mix relational data serialized objects blob binary doesnt need images run query relation data bytes blob might file header example applications fact endless accessing blob slower accessing filesystem highly likely database misconfigured 17,dont use blobs mostly backup restore perspective dont want blob data slowing backups dont store full url however store filepath certain point build path one way people programs access files ftp http local directory nfs mounted directories course likely deal larger images people one datasets gets 700gb images compressed per day even thumbnails images still store externally 13,tell answers big depends factors might paying hosting charge file storage database storage file storage typically cheaper especially cloud services self hosted using sql server upcoming version codename denali extend filestream allow access via tsql file system also make sure stay sync access update delete either side keep everything organized research find whats important pick direction based choice 30,im big fan storing reference copy image database managability disaster recovery standpoint really way fly still lots things serve image filesystem applications putting much pressure database server things doesnt really want 8,working linux storing images filesystem database significant better performance see excerpt brad edigers book advanced rails 8,im much fan storing images database small app users seems like easy solution start scale makes things difficult preference start storing images folder web server keep path easily accessible configuration need quickly move dedicated optimized image server later might want move somewhere else think s3 akamai way much complicated change code expecting read database 5,smart move smart move depends specific case file location directly affecting url structure youre storing full file addresses database bad say bad move since trouble case somebody move rename directory application built way simply point files directory file access logic dynamic made smart move following reasons database storing need serve lot images per request increase response time application since files going sent synchronously network also add little processing server file storing usually cheaper database storage file storing unless restrictions image access determined user access given group images theres need processing access images kind file database storage possible access files using 4,quickest cheapest way find hitting databases use sp whoisactive http whoisactive com 0,general engine create execution plan based essentially number rows whether index expected number result rows intermediate rows form input query question exists plan encourages seek based plan table good choice table small table large index exists antijoin plan good choice table large table small index returning large result set however encouragement like weighted input strong often makes choice moot ignoring effect example returning different columns due addressed maxvernon answer 0,routinely increase decrease max server memory configuration option sql server instances running laptop lock pages memory enabled never caused problems memory always seems released quickly also stop start restart instances find need using sql server configuration manager keep open far checkpoint concerned configure depends version manual checkpoint duration target available since though undeniably effective later versions running manual checkpoint duration per database shutdown help spread load reduce recovery time startup managed checkpointing also reduce needed memory shrink requested fewer dirty pages write persistent storage particular benefit dropping clean buffers reducing max memory setting noticed using lock pages memory may increase 0,put way ive yet used lnnvl several years dba pl sql programmer used nvl2 occasion always look side true side wasnt point seems better readability perspective end using nvl decode case etc alternatively works assuming one good handle oracle handles nulls arithmetic point one may well use original query readability execution plan may take harder hit return rows startcommission current commission rows null either including select emp startcommission currentcommission startcommission currentcommission null null null null number always null hence return either records equal combined total null either fields null 0,first thing report person responsible network systems security company person throw network administrator person call cio cto right better yet demand face face explain situation first thing person block ip firewall buy little time much maybe minutes ip maps range ips reported whois net block entire ip range given whois prevent guy requesting new ip isp getting new ip minutes maybe mark storey smith says either add firewall move db dmz already firewall db dmz need immediate forensic check see intervening servers firewall compromised likely change admin password long complex passwords sa windows admin domain admins local admins review every 0,add index youll use temp table index twice query run maintain usual index tasks like uniqueness data loaded temp table already sorted create temp table clustered index sort data taking account sql servers feature temp tables reuse decide create index temp table try create table statement youll add index explicitly table creation prevent sql server reuse table next time 0,youre looking begin transaction alter table foo add varchar exec sp executesql nupdate foo set cast varchar alter table foo drop column commit 0
possible passwords configured per database per host cnf following cnf client password somepass password use every user host database connect,answered add section user host db connect using syntax cnf clienthost1 note client host1 user myuser password mypass database dbname host server location com users cnf utilize command line mysql defaults group suffix host1 64,answer correct unfortunately mysqladmin doesnt support defaults group suffix least version im using hence resorted using defaults file host cnf instead works mysql mysqladmin mysqldump 8,put clear passwords text files recommended since mysql use mysql config editor save passwords encrypted also provide different passwords different connections https dev mysql com doc refman en mysql config editor html 8,need order query results put order simple check article sql server architect conor cunningham pretty much sums topic seatbelt expecting order without order 0,good answer found execute following sql select sid serial username sql id sql child number optimizer mode hash value address sql text sqlarea sqlarea session sql hash value sqlarea hash value sql address sqlarea address username null output unreadable change linesize take set linesize sql work might need log sysdba sqlplus sysdba 0,seems though issue data types isnull fixed issue thanks ypercube research coalesce equivalent case statement using case expression1 null expression1 expression2 null expression2 else expressionn end paul white explains coalesce expression returns data type expression highest data type precedence isnullcheck expression replacement value returns type check expression avoid data type issues seems isnull appropriate function use dealing two expressions xml plan excerpts xml plan using case expression null select thefunction null scalaroperator scalarstring case else null end condition scalaroperator const constvalue scalaroperator condition scalaroperator const constvalue scalaroperator else scalaroperator const constvalue null scalaroperator else scalaroperator xml plan using case expression 0
identity primary key index become fragmented understand index fragmentation possible cases found databases non clustered example alter table dbo claimlineinstitutional,updates data already causes rows moved forward pointers added test get fragmentation 115k densely packed rows create table fragtest fragtestid int null identity primary key somestring varchar4100 null insert fragtest somestring values go insert fragtest somestring select f1 somestring fragtest f1 cross join fragtest f2 go insert fragtest somestring select f1 somestring fragtest f1 go select count fragtest select object id avg fragmentation percent page count sys dm db index physical stats2 object idtempdb fragtest null null null update fragtest set somestring replicateb fragtestid fragtestid select object id avg fragmentation percent page count sys dm db index physical stats2 object 8,logical fragmentation occurs logically next page different physically next page case leaf level index monotonically increasing key happen extent allocations index become interleaved extent allocations objects even without small amount fragmentation ensue due first page allocations coming mixed extents pages different index levels interleaved extents edit plus course effect updates page splits per gbns answer 4,looking clause row pointed table1 requires column type row pointed table1alias requires column columna conn perhaps multiple rows table1 id3 0,one issue covered accepted answer need value time formula possibly change instance consider commision example commission paid amount stored historical figure actually paid way calulate commisions could change next month frequently wont change actually paid must stored separately idea storing price customer actually paid product calculation discounts etc rather relying formula price table anything except initial calculation product price next month might price cutomer made order need historical record value point time always store value using formula inital calulation 0
postgresql create table error im new postgresql try create table database psql write create table mail user user char50 null,user reserved word may want try username instead http www postgresql org docs static sql keywords appendix html 7,use reserved words quoting create table mail user user char50 null domain char50 null password char50 null 5,collation right choice everything bit faster without locale since collation right anyway create database without collation meaning may pain provide collation many operations shouldnt noticeable difference speed default collation ad hoc collation though unsorted data collation rules applied sorting aware postgres builds locale settings provided underlying os need locales generated locale used related answer however craig already mentioned indexes bottleneck scenario collation index match collation applied operator many cases involve character data use collate specifier indexes produce matching indexes partial indexes may perfect choice mixing data table example table international strings create table string string id serial lang id int 0,way prefer put function utility database create synonym regular database way get best worlds one copy object maintain developer doesnt provide three four part names use utilitydb go create function dbo lastindexof go use otherdb go create synonym dbo lastindexof utilitydb dbo lastindexof go especially powerful clr functions since extra administrative overhead changing deploying way preferable using master marking system object isnt guaranteed forward portable id love know developer expects create function sys schema though understand maintaining multiple copies function databases difficult really maintaining single copy multiple copies really benefit exceptions client wants function handle nulls differently something case would 0
predicate applied index scans problems queries similar select counta dbo oinv t0 inner join dbo ocrd t2 t2 cardcode t0,looks like limitation sql servers ability imply predicates change t0 cardcode p2 t2 fathercard p3 t2 cardcode p2 t2 fathercard p3 predicate pushed scan t2 performance much better guaranteed join condition t2 cardcode t0 cardcode two equal change semantics example data added question option hash join hint original version cpu time ms elapsed time ms machine second version cpu time ms elapsed time ms im sure details implied predicate fails appears play part however ed predicate t0 cardcode p2 t2 fathercard p3 able applied scan t2 without problems 4,way change filter hash match without rewriting query enterprise edition indexed view used example create view dbo oinv ocrd schemabinding select inv cardcode crd fathercard count big cnt dbo oinv inv join dbo ocrd crd crd cardcode inv cardcode group inv cardcode crd fathercard go create unique clustered index cuq dbo oinv ocrd cardcode fathercard create nonclustered index dbo oinv ocrd fathercard include cnt query like declare p2 nvarchar50 nd20b5dd1 c729 4b7a a276 950ce7dcf128 p3 nvarchar50 n6a0dbeab fecb 40db 86c5 aafa612da691 select count big dbo oinv t0 join dbo ocrd t2 t2 cardcode t0 cardcode t0 cardcode p2 t2 fathercard 6,youre using psql command line tool issue command first pset format wrapped wrap long lines terminal window like test id text lorem ipsum dolor sit amet consectetur adipiscing elit mauris lorem also set number columns wrap pset columns change dots ellipses pset linestyle unicode info http www postgresql org docs current static app psql html 0,may able see object tables dont grants object check dba objects table need grants appropriately granted user select dba objects object name cot ntn pi doesnt produce output check raw oracle data dictionary tables query select name owner name object name decodeo type next object index table cluster view synonym sequence procedure function package package body trigger type type body table partition index partition lob library directory queue java source java class java resource indextype operator table subpartition index subpartition lob partition lob subpartition dimension context resource plan consumer group subscription location java data unknown type ctime mtime charo stime 0
best practices regarding lookup tables relational databases lookup tables code tables people call usually collection possible values given certain column,idn take mean identity sequence auto increment field take look note section misusing data values data elements first reference underneath figure course separate table sales persons reference using foreign key preferably simple surrogate key sales person id shown expert thinks deference surrogate keys really quite basic sql technique shouldnt cause problems day day sql appears error figure sales person salesdata surrogate key number text im inferring quote avoid costs temptation common novice database programmers commit error outlined section common lookup tables commonly called muck massively unified code key approach accident notably joe celko also sarcasticlly known otlt one true lookup 10,third approach advantages two options put actual code code table mean short character sequence captures essence full value unique given example may idn name democrats code dem code carried transactional tables foreign key short intelligible somewhat independent real data incremental changes name would suggest code change republicans decamp en masse however change code may necessary attendant problems surrogate id would incur style termed abbreviation encoding recommend celkos writing google books holds several examples search celko encoding examples letter encodings countries letter encoding gbp usd eur currency codes short self explaining changing iso 10,windows simply stopping postgresql service running postgresql windows x64 exe top existing works uninstall necessary course backup recommended clear explicit documentation update procedure windows absent note documentation link provided dezso moved current manual https www postgresql org docs current static upgrading html postgresql release notes typically document migration tips appendix example appendix release notes https www postgresql org docs current static release html section release subsection migration version https www postgresql org docs current static release html idm46428658121200 best source windows installer information enterprise db forums posting found addressed question april whats right way upgrade new version postgresql https web 0,alternative test attribs unpivot view provided jackpdouglas works versions 11g fewer full table scans create replace view test attribs unpivot select id name myrow attr cast decodemyrow1attr12attr23attr34attr4attr5 varchar22000 attr test attribs cross join select level myrow dual connect level final query used unchanged view 0
sp msforeach db need use use keyword sp msforeachdb undocumented sp designed run sql every database server instance appear need,look sp helptext entry sp msforeachdb weird friend sp msforeachtable exec sys sp helptext objname nsp msforeachdb exec sys sp helptext objname nsp msforeachtable youll see theyre wrappers sp msforeach worker exec sys sp helptext objname nsp msforeach worker build valid list things dont actually loop meaningful way rate aaron bertrands sp foreachdb much better piece code doesnt skip databases etc 5,procedure perform use command way procedure works replaces every command database prefix run use foodb go exec sys sp msforeachdb nselect sys objects total rows system also get number resultsets show objects foodb issue command way order get command execute context individual database exec sys sp msforeachdb nselect sys objects total rows system case execute command database replaced database name select master sys objects select tempdb sys objects call system function doesnt support database prefix typically requires use command first way differently could exec sys sp msforeachdb nselect db namedb id simply exec sys sp msforeachdb nselect one reason works 11,keep following mind caring updating statistics copied rebuilding indexes vs updating statistics benjamin nevarez default update statistics statement uses sample records table using update statistics fullscan scan entire table default update statistics statement updates index column statistics using columns option update column statistics using index option update index statistics rebuilding index example using alter index rebuild also update index statistics equivalent using fullscan unless table partitioned case statistics sampled applies sql server later statistics manually created using create statistics updated alter index rebuild operation including alter table rebuild alter table rebuild update statistics clustered index one defined table rebuilt reorganizing 0,certainly difference version mongodump vs mongodb server vs use docker rescue docker run rm pwd workdir workdir mongo mongodump server database workdir dump docker use precise version tools cli without install even specify alias bashrc alias mongodump docker run rm pwd workdir workdir mongo mongodump note folder dump saved need write permissions mongodb user container write mounted volume achieved manually setting permissions dump folder running command example would mkdir dump dump completed permissions modified back normal sudo chmod dump 0
pivot rows multiple columns sql server instance linked server oracle server table oracle server called personoptions contains following data personid,prefer pivot query manually may use pivot well select personid maxcase optionid end optiona maxcase optionid end optionb maxcase optionid end optionc personoptions group personid 5,would equivalent sql server syntax based reading oracle docs nullif pivot appear format sql server kin challenge pivot list needs static unless make query dynamic itzik demonstrates idea translated sql personoptionspersonid optionid select union select union select union select union select union select select personid nullifp optiona nullifp optionb nullifp optionc personoptions po pivot countpo optionid optionid 9,ways perform data transformation access pivot function easiest use aggregate function case aggregate case version select personid maxcase optionid else end optiona maxcase optionid else end optionb maxcase optionid else end optionc personoptions group personid order personid see sql fiddle demo static pivot select select personid optionid personoptions src pivot countoptionid optionid optiona optionb optionc piv order personid see sql fiddle demo dynamic version two versions work great known number values values unknown want implement dynamic sql oracle use procedure create replace procedure dynamic pivot pop cursor sys refcursor sql query varchar21000 select personid begin select distinct optionid personoptions order 20,create function like create replace function public get messages timestamp time timestamp time timestamp returns table recipient varchar timestamp timestamp begin return query select distinct recipient recipient timestamp messages left join identities recipient name timestamp time time order recipient timestamp desc end language plpgsql use function like table select get messages timestamp2015 0,make use xml query columns might table build xml columns per row cross apply extract value using values function query id known get table directly col1 col2 might get using xml select id tx valuecol1 text int col1 tx valuecol2 text int col2 cross apply select xml path type txx sql fiddle 0,best box solutions ive found use combination slow query log sucks compared profiler running wireshark port really sucks compared profiler wont work youre encrypting connections theres also show full processlist like reduced combination sys dm exec sessions sys dm exec requests little sys dm exec sql text thrown 0
help choose raid level combination sql server instance going rebuild one ibm server scratch server dedicated sql server instance running,vote option bear mind raid means protection logs matter yes also benefit simplicity sql server docs say optimized parallelism use kb kb stripe size usually good go controller default imo 6,notes os binaries 40gb disks doesnt leave many options want backups let raid controller choose stripe size depends raid level micro optimisation youll see conflicting data log file write speed determines throughput allow free space times biggest table inside mdf index rebuilds raid idiotic server data value id go one raid array 5,id go either two raid1 volumes three mirrors raid10 striping across two sets three mirrors rationale two mirrors little period server get proper consistency checks often month quite likely bad blocks matter long mirror still intact one disk fails likely wont able recover one hence three mirrors raid6 bad idea database setup writes tend small turns read modify write operations background whether two raid1 volumes one raid10 better depends application likely need entire space go raid10 otherwise id suggest one volume system indexes one data pages go two volumes check controller allows extending raid1 raid10 later case need expand 4,paul rightly mentioned answer fundamental reason scalar udfs could executed using parallelism however apart implementation challenges another reason forcing serial froid paper cited paul gives information quoting paper section currently sql server use intra query parallelism queries invoke udfs methods designed mitigate limitation introduce additional challenges picking right degree parallelism invocation udf instance consider udf invokes sql queries one figure query may use parallelism therefore optimizer way knowing share threads across unless looks udf decides degree parallelism query within could potentially change one invocation another nested recursive udfs issue becomes even difficult manage approach froid described paper result parallel plans 0,stating workaround seems little premature say database contains tables views export tables views schema sql create statements run target database could also export actual data something like csv format import target database perhaps even write something etc export import actual data wouldnt need spend penny new instances sql server dont ultimately need 0,best solution found desc table name command information list mysql tables command gives description one database table exactly trying find 0
acceptable circular foreign key references avoid acceptable circular reference two tables foreign key field situations avoided data inserted example opinion,acceptable circular foreign key references would impossible insert data without constantly dropping recreating constraint fundamentally flawed model every domain think example think domain relationship account contact requiring junction table fk references back account contact create table account id int primary key identity name varchar50 create table contact id int primary key identity name varchar50 create table accountcontact accountid int foreign key references accountid contactid int foreign key references contactid primary keyaccountidcontactid 6,since using nullable fields foreign keys fact construct system works correctly way envision order insert rows accounts table need row present contacts table unless allow inserts accounts null primarycontactid order create contact row without already account row present must allow accountid column contacts table nullable allows accounts contacts allows contacts account perhaps desirable perhaps said personal preference would following setup create table dbo accounts accountid int null constraint pk accounts primary key clustered identity11 accountname varchar255 create table dbo contacts contactid int null constraint pk contacts primary key clustered identity11 contactname varchar255 create table dbo accountscontactsxref accountscontactsxrefid int null constraint 12,file system useful looking particular file operating systems maintain sort index however contents txt file wont indexed one main advantages database another understanding relational model data doesnt need repeated another understanding types txt file youll need parse numbers dates etc file system might work cases certainly 0,heres list recommended uses taken utf encoding variable length encoding huge benefit scenarios also make things worse others unfortunately little use utf8 encoding given data compression clustered columnstore indexes available across editions sql server scenario truly benefits utf encoding one following conditions true data mostly standard ascii values either might small amount varying range unicode characters would found single bit code page might exist bit code page column currently otherwise would nvarcharmax meaning data fit nvarchar4000 lot data column set columns gb stored nvarchar performance would negatively impacted making table clustered columnstore table due table used data typically bytes desire 0
finding fixing innodb index corruption encountered new issue yesterday one mysql slave dbs runs ec2 aws db created snapshot another,may easiest solution however would love clarify things making snapshot running mysql instance could affect one file respsonsible secondary index manipulation ibdata1 system tablespace ibdata1 home classes innodb information data pages innodb file per table disabled index pages innodb file per table disabled data dictionary included list tables tablespace ids double write buffer provides checksum info prevent data corruption insert buffer changes secondary indexes redo logs undo logs pictorial representation pivotal classes would worry double write buffer insert buffer live snapshot either properly written introduce data corruption flush tables read lock halt writes ibdata1 one would think wrote subject used 7,thanks rolando michael responses close loop heres answer came original questions determine tables similar index corruption use check table ran mysqlcheck relevant innodb tables find ones index corruption whats efficient way fix corrupt indexes use optimize table rebuid innodb table corrupt indexes causes complete table rebuild fixes corruption 13,data domains use deduplication compression features dont always play well sql servers compression thats main point whether problematic depends many factors youre compressing backups sql server data domain able deduplicate data efficiently data travel network youre compressing backups data domain deduplicate less efficiently sysadmin likely push uncompressed route another possible issue higher amount time restores since dd apply magic data making available end one way know sure test take backups without dd significant amount time take least number backups normally keep online say month without compression restore backups test server regularly compare backup restore times without dd draw conclusions tested 0,bum getting sore sitting fence going throw answers hope get voted oblivion please offer constructive criticism mail address min cn want track local domain email addresses max rfc amount code validate email actually insane lets assume valid may want abstract email address communication method easily list methods communicate user gender gender change time could track important follow http en wikipedia org wiki iso iec known0 male1 female2 applicable9 addresses noram gon na take cheap way stick north american addresses convenient abstract countries divisions cities counties mostly due taxation taxes apply many levels point tax rate abstract geographic area golden geographicarea 0
sql server linked server performance remote queries expensive two database servers connected via linked servers sql server 2008r2 databases linked,connecting remote resource expensive period one expensive operations programming environment network io though disk io tends dwarf extends remote linked servers server calling remote linked server needs first establish connection query needs executed remote server results returned connection closed takes time network also structure query way transfer minimum data across wire dont expect db optimize write query would select remote data table variable temp table use conjunction local table ensures data needs transferred query running easily sending 6m rows remote server order process except clause 6,plan moment looks like optimal plan dont agree assertion answers sending 6m rows remote server plan looks though rows returned remote query performing index seek local table determine whether matched pretty much optimal plan replacing hash join merge join would counterproductive given size table adding intermediate temp table adds additional step doesnt seem give advantage 9,yeehaw declare mytable table mycolumn varchar100 insert mytable mycolumn values test select substringmt mycolumn ca lenmt mycolumn mytable mt cross apply select values patindex mycolumn ca 0,try ps ef grep ysql identify process id strace cp pid leave seconds minute tell process spending time could waiting disk seen read write dominate 0
single drive vs multiple drives generally bottleneck rdbms mysql user performance disk access ssd provides great performance compared conventional spindle,could depend great deal storage engine myisam think would great idea make data contiguous want would benefit queries involving bulk operations large range scans toolset compressing repairing myisam improve table format also make access little better imho concurrent inserts could thrive disk environment innodb completely different story innodb file per table disabled data scattered throughout ibdata1 thus scattered amongst multiple disks could run optimize table innodb table make data index pages contiguous could cause two problems makes ibdata1 grow deteriorate performance time dont forget four types data reside ibdata1 table metadata table data pages index data pages mvcc data constant 7,yes multiple drives provide better throughput thanks wonderful technology call raid database youll want raid striped mirrors 4n drives would doublen write speed quadruplen sequential read throughput seek times stay roughly though youll lose half space mirroring 5,agree burden justification ones requiring access typically environments consulted access production systems small environment support person access backups etc support support indirect access dedicated support developer production data big thing need access hook keeping everything running smoothly answer finance guys question something working cant always work even day old data case hand access worse typically consultant tend avoid getting sort access unless needed since working financial databases last thing want accused entering invoices hand dont need access shouldnt dont really buy sensitive data argument since developer probably hook making sure handled correctly hard verify without looking actually stored bug report 0,table clustered index index table data otherwise heap type table rebuild clustered index index fact space wouldnt counted data non clustered index result partially used pages merged full form insert data index clustered otherwise index order leaf pages created needed ever one partial page one end enter data index order page needs split data fit right place end two pages approximately half full new row goes one time happen lot consuming fair amount extra space though extent future inserts fill gaps non leaf pages see similar effect actual data pages far significant size also deletes may result partial pages remove 0
number row value expressions insert statement exceeds maximum allowed number row values one insert script written following insert tablename column1,hard coded restriction exists good reasons related possible excessive query plan compilation time workaround listing values clauses cte inserting cte recommend break insert statement values clauses lines workaround use alternative data loading method bcp bulk insert fundamental issue may addressed future version sql server see also martin smiths great answer stack overflow recommended workaround declare table c1 int error insert c1 values cte workaround error recommended declare table c1 int cte c1 select values vv insert c1 select c1 cte 25,use following syntax insert tablename column1 select union select union theres restriction number inserted rows 11,quickest dirtiest way clear slave info mysql instance add skip slave start etc cnf mysqld service mysql stop rm var lib mysql master info var lib mysql relay service mysql start remove skip slave start etc cnf would necessary according mysql documentation reset slave mysql unlike case mysql earlier reset slave change replication connection parameters master host master port master user master password retained memory means start slave issued without requiring change master statement following reset slave thus replication info still memory mysql restart way go 0,reading post later result web search indeeded described sql server bol doh could find earlier behavior client connections failover availability group failover occurs existing persistent connections availability group terminated client must establish new connection order continue working primary database read secondary database failover occurring server side connectivity availability group may fail forcing client application retry connecting primary brought fully back online availability group comes back online client application connection attempt connect timeout period client driver may successfully connect one internal retry attempts error surfaced application case 0
run large script many inserts without running memory question script around thousand insert select statements try run get error message,sure getting memory error easier approach export data spreadsheet delimited format csv use data import wizard ssms insert data 9,maximum batch size sql server network packet size nps nps usually 4kb works mb would mean insert statements would average kb doesnt seem right maybe extraneous spaces something unusual first suggestion would put go statement every insert statement break single batch insert statements separate batches easier digest careful one inserts fails may hard time finding culprit might want protect transaction add statements quickly editor good search replace let search replace return characters like macro facility second suggestion use wizard import data straight excel wizard builds little ssis package behind scenes runs wont problem 17,bulk insert bcp seem appropriate options insert statements need stick insert statements would consider options use transactions wrap batches statements one minimize impact log batch begin transaction insert dbo tablea select insert dbo tablea select insert dbo tablea select commit transaction go begin transaction insert dbo tablea select insert dbo tablea select insert dbo tablea select commit transaction go instead individual insert statements use union statements time insert dbo tablea select union select union select go insert dbo tablea select union select union select go ive left error handling brevity point would never try send single batch individual statements sql 14,follow steps identify much space want add database storage allocation open windows explorer right click disk drive database files exist select properties check much disk space available decide much want allocate database suggestion leave least disk space free house database files disk os sub suggestion dont rebuild migrate data disk youre screwing leave least pure data disk numbers estimates think actual percentage suggestions update storage allocation database open ssms click view tab select object explorer expand databases folder right click database trying bulk insert select properties click files list option select page area left properties window find database files row 0,theoretically shouldnt anything take mirror maintenance bring back whole purpose mirroring survive something like occurs accidentally must survive recover fine occurs intentionally follow erics advise set partner suspend necessary wouldnt mostly im interested see fail fails job conditionning side harm setting suspend continue take log backups principal mirror principal knows last lsn confirmed mirror going truncate log past point 0,two major things master handle replication setup disk master write every completed sql transaction binary logs network slave connected master must play traffic cop master writes sql statement recent binary log master polls db connections emanating slaves slave db connection master following master receives request oldest sql statement binary log since slaves last entry relay logs master sends following slaves io thread master log filename master log position sql statement masters log filename position busy master slaves seconds behind master simply marvelous either metrics become noticeable may need certain things scale db server place binary logs separate disk faster disk 0
matriculation number good primary key trying design grade assignment database facing following dilemma student table following characteristics matriculation number know,would go surrogate key approach im familiar matriculation number expect large enough surrogate integer might preferred alternative larger pk means larger indexes means less performance reads writes plus space usage space referring tables go surrogate key 4,currently work field case surrogate key used software probably deals much wider range scenarios extra flexibility may mean lot future student numbers overall may change quite frequently depending numbers assigned distribution authority new student enrolled number unique identifier may internally assigned student authority actually generated number student asynchronously case may necessary use one field database crushes primary key idea immediately student numbers change bad news lots reasons remember primary key table propagated many parts system many associated rows performance rarely concern single student data operations depending indexes set using student number alternate key may slight performance hit however least experience 4,regarding window aggregates sum avg sumsalesytd partition territoryid order datepartyymodifieddate msdn page clause transact sql rather hidden remark general remarks order specified entire partition used window frame applies functions require order clause rows range specified order specified range unbounded preceding current row used default window frame applies functions accept optional rows range specification example ranking functions accept rows range therefore window frame applied even though order present rows range means code results sumsalesytd partition territoryid order datepartyy modifieddate range unbounded preceding current row sum calculated row getting rows territoryid year part modifieddate less equal year part row often called cumulative 0,answer xxd nice small files fast example script im using xxd home user myimage png tr tmp image hex echo create table hexdump hex text delete hexdump copy hexdump tmp image hex create table bindump binarydump bytea delete bindump insert bindump binarydump select decodehex hex hexdump limit update users set image select decodehex hex hexdump limit id psql mydatabase 0
explain execution plan researching something else came across thing generating test tables data running different queries find different ways write,constant scans way sql server create bucket going place something later execution plan ive posted thorough explanation understand constant scan look plan case compute scalar operators used populate space created constant scan compute scalar operators loaded null value theyre clearly going used loop join effort filter data really cool part plan trivial means went minimal optimization process operations leading merge interval used create minimal set comparison operators index seek details whole idea get rid overlapping values pull data minimal passes although still using loop operation youll note loop executes exactly meaning effectively scan addendum last sentence two seeks misread plan 13,constant scans produce single memory row columns top compute scalar outputs single row columns expr1005 expr1006 expr1004 null null bottom compute scalar outputs single row columns expr1008 expr1009 expr1007 null concatenation operator unions rows together outputs columns renamed expr1010 expr1011 expr1012 null null null expr1012 column set flags used internally define certain seek properties storage engine next compute scalar along outputs rows expr1010 expr1011 expr1012 expr1013 expr1014 expr1015 null null true null false last three columns defined follows used sorting purposes prior presenting merge interval operator expr1013 scalar operator4 expr1012 null expr1010 expr1014 scalar operator4 expr1012 expr1015 scalar operator16 expr1012 29,type logging would looking gaining application audit table tracks application user application command result among things along another audit table tracks individual table changes provides logging need per user could also set connection pool application control connections per application first blush think trying manage many users burden outweighs possible benefits 0,reindex rebuild db running take offline rebuild index database offline database online rebuild index widely accepted parameter rebuilding fragmentation rebuild fragmentation lies reorganize use script maintenance plan ola hallengren script script time would suggest create 0
creating persisted computed column function working programmers database solution want add computed column mimic old keys older queries procedures systems,read persisted computed columns bol related indexes computed columns restrictions expression use persisted computed column expression must deterministic persisted specified 5,im sure think need function computed column add new column table default value index however want create table dbo whatever id int alter table dbo whatever add yourmom uniqueidentifier default newsequentialid create index ix whatever dbo whatever yourmom since updated question lets address truly awful idea im going simplify example little bit create table dbo whatever id int primary key create table dbo ennui id int primary key meh int go create function dbo badidea notguido int returns int schemabinding returns null null input begin declare int select select id dbo ennui meh notguido return end go alter table dbo 7,still dont understand needs column table never mind persisted one create table valued function cross apply query actually needs since old key never change doesnt need computed persisted anyway really want old key live multiple places sounds like people shouldnt making kind decision already made kind decision lookup trigger populate write time static column table still highly recommend table valued function facilitate write trigger way handles multi row operations without write loop call scalar valued function every row show similar things really question lead developer doesnt like bad slow painful row row create function dbo getidbyguid guidkey uniqueidentifier returns int 8,pointed even technically possible alternate throw raiserror likely wouldnt want actually nifty ability parameterless throw throw error using message number msg instead msg isnt difference throw batch aborting raiserror important behavioral difference demonstrated test setup drop proc throw drop proc raiserror go create procedure throw set nocount begin try select dividebyzero end try begin catch throw end catch select aa go create procedure raiserror set nocount begin try select dividebyzero end try begin catch raiserrortest yo return typically end catch block using raiserror end catch select bb go test exec throw select cc returns results tab dividebyzero empty result set 0,must go innodb file per table need cleaning innodbs current infrastructure seen many db hosting clients setup mysql leave innodb default state causes system tablespace better known ibdata1 grow wildly even switched innodb file per table ibd file would extracted ibdata1 ibdata never shrink example table called mydb mytable inside ibdata1 taking 2gb extract following step add etc cnf mysqld innodb file per table step service mysql restart step alter table mydb mytable engine innodb make file var lib mysql mydb mytable ibd unfortunately 2gb space occupied table change reclaimed wrote past posts cleanup innodbs infrastructure innodb store databases one 0,edit tokenandpermstore looks normal size unlikely factor next place look probably spinlock stats also mentioned chat determine problem persists remove explicit drop statements let tables fall scope instead edit youre 2005sp2 im starting wonder might tokenandpermuserstore issue would explain problem alleviated restart instance problem prevalent servers large numbers database run following query add output question select sumsingle pages kb multi pages kb securitytokencachesizekb sys dm os memory clerks name tokenandpermuserstore quick hack fix create job monitors size cache runs dbcc freesystemcache tokenandpermuserstore exceeds certain size iirc ill check tomorrow update ive set limit 100mb servers caused problem sp3 introduces mechanism 0
mysql auto increment limited primary keys know mysql limits auto increment columns primary keys first thought performance restriction since probably,would want auto increment column primary key want column auto increment definition storing meaningful data column case storing non meaningful information makes sense special case want synthetic primary key case lack information benefit risk someone ever come along future want change data attribute entity changed multiple auto increment columns table seems even odder two columns would data theyre generated algorithm populated time suppose could come implementation possible slightly sync enough concurrent sessions cant imagine would ever useful application 9,interesting question different databases unique approaches providing auto increment mysql one auto increment key generated uniquely identify row table lot explanation behind implementation depending datatype auto increment values fixed length datatype bytes max tinyint max unsigned tintint max int max unsigned int postgresql internal datatype serial used auto increment larger ranges allowed using bigserial oracle schema object called sequence create new numbers simply summoning nextval function postgresql also mechanism nice url provides dbs specify http www w3schools com sql sql autoincrement asp concerning question really want multiple auto increment columns single table emulate two reasons must emulate mysql accommodates one 4,fact auto increment attribute limited primary key used old versions definitely probably still mysql manual versions since reads like one auto increment column per table must indexed default value indeed auto increment column table primary key makes sense different topic though also mention auto increment column always integer type technically floating point type also allowed unsigned signed type wastes half key space also lead huge problems negative value inserted accident finally mysql later defines type alias serial bigint unsigned null auto increment unique 8,decided see happens actually click create catalog ssms host steps performed basic premise backup exists program files microsoft sql server dts binn ssisdbbackup bak part catalog creation backup restored make ssisdb pathway program files going protected prevent accidental files error encountering indicates person running ssms access file program files microsoft sql server dts binn assuming youre using runas something like launch ssms try opening windows explorer internet explorer navigate folder get might click yes something like show files know im restricted area propose uac affecting ability install ssisdb case close ssms instances right click ssms choose run administrator prevent authorization 0,nope heres simple test select coalesce1 select runs fine select coalescenull select throws error second condition evaluated exception thrown divide zero per msdn documentation related coalesce viewed interpreter easy way write case statement case well known one functions sql server mostly reliably short circuits exceptions comparing scalar variables aggregations shown aaron bertrand another answer would apply case coalesce declare int select case else min1 end generate division zero error considered bug rule coalesce parse left right 0,use import export wizard move data databases right click database want export choose tasks export data wizard guide process youre right though wont able backup restore moving sql server r2 sql server express 0
