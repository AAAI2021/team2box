running sp askbrent expertmode powershell trying run sp askbrent powershell using invoke sqlcmd capture output variable query exec saidba monitoring,brent correct expertmode turns multiple result sets diagnostics wait stats file stats perfmon counters want one result set dont turn expertmode want multiple result sets application case posh cant consume youll need log tables thats parameters come outputdatabasename name database want write must already exist outputschemaname schema tables live like dbo must already exist outputtablename table name top result set diagnostics parameter also requires two table doesnt already exist created outputtablenamefilestats outputtablenameperfmonstats outputtablenamewaitstats rest result sets written table doesnt already exist created totally optional pass none want work differently describe need ill see come way get hope helps comment actually 10,discount brents answer although give good ones training times powershell execute queries return multiple datasets using invoke sqlcmd built currently would two options either net native code system data sqlclient using trusty smo tend chose smo simply event want include server properties something else smo access context sp askbrent went ahead spent minutes building output html report main points interest script handle multiple datasets execute query executewithresults method available following namespaces microsoft sqlserver management smo server microsoft sqlserver management smo database see example using database namespace msdn script using server namespace work either way review msdn article executewithresults return dataset 7,speed things could try adding primary key cspec rowid dont scan every iteration change cte temp table suitable pk see next point add index sourcetable1 match cte clause currently pk scanned meaning sourcetable1 rows scanned every iteration million rows sourcetable2 marketid also index wouldnt worry scanned understand query plans show lot scans poor indexes operations target table indexing appears ok another observation uniqueidentifier varchar bad choices clustered indexes pks wide increasing overhead collection comparisons least edit another observation thanks marian clustered index wide generally every non clustered index points clustered index means huge nc index could probably achieve result reordering 0,smart move smart move depends specific case file location directly affecting url structure youre storing full file addresses database bad say bad move since trouble case somebody move rename directory application built way simply point files directory file access logic dynamic made smart move following reasons database storing need serve lot images per request increase response time application since files going sent synchronously network also add little processing server file storing usually cheaper database storage file storing unless restrictions image access determined user access given group images theres need processing access images kind file database storage possible access files using 0
database redesign opportunity table design use sensor data collection background network approximately sensors data points collect minute intervals data points,think partitioning table big reason indexes giant table even one index generated lot cpu load disk perform index maintenance executing inserts updates deletes wrote earlier post back october table partitioning would big help one excerpt past post partitioning data serve group data logically cohesively class performance searching partition need main consideration long data correctly grouped achieved logical partitioning concentrate search time separating data id possible many rows data may never accessed reads writes major consideration locate ids frequently accessed partition less frequently accessed ids reside one big archive table still accessible index lookup blue moon query read entire post later 5,interesting sensors produce kind data make sense put table amount data see youd worried performance days usual amount time produce graph could two tables main sensor data table stores data little want slack days ago today everything older goes archive table could help reduce size table reports begin generated hopefully majority gb data archive table main table archiving job scheduled run nightly maybe also consider building separate reporting database stores data structure better generating reports tables designed closely match querying maybe pre calculate aggregate values would otherwise take long time generate possible populate main database regular nightly basis course need 5,issue stems fact entity type missing model consider following erd note ive added intersection entity type department classification new entity type position provides information implicit model particular department give set jobs various classifications adding position model explicit entity advantages avoids problem concerned worker potentially assigned departments classifications different companies gives locus predicates might applicable position salary grade etc allows record fact position exists even workers currently position quite possibly useful information note avoid problem position defined department classification different companies ive expanded keys department classification good reasons read length todd everetts answer beware model presumes simplification specifically assumes position recorded 0,kernel paramter configured properly oracle database installation guide 11g release linux part number e24321 configuring kernel parameters linux 0
use truncate drop system work lot stored procedures sql scripts make use temporary tables using tables good practice drop many,truncate drop almost identical behavior speed truncate right drop simply unnecessary note wrote answer sql server perspective assumed would apply equally sybase appears entirely case note first posted answer several highly rated answers including accepted answer made several false claims like truncate logged truncate rolled back truncate faster drop etc thread cleaned rebuttals follow may seem tangential original question leave reference others looking debunk myths couple popular falsehoods pervasive even among experienced dbas may motivated truncate drop pattern myth truncate logged therefore rolled back myth truncate faster drop let rebut falsehoods writing rebuttal sql server perspective everything say equally applicable 131,testing truncate drop vs drop directly shows first approach actually slight increased logging overhead may even mildly counter productive looking individual log records shows truncate drop version almost identical drop version except additional entries operation context allocunitname lop count delta lcx clustered sys sysallocunits clust lop count delta lcx clustered sys sysrowsets clust lop count delta lcx clustered sys sysrscols clst lop count delta lcx clustered sys sysrscols clst lop hobt ddl lcx null null lop modify row lcx clustered sys sysallocunits clust lop hobt ddl lcx null null lop modify row lcx clustered sys sysrowsets clust lop lock xact 52,try sql alter authorization schema yourschemaname dbo go drop user theuseryouwanttodelete go cant drop principal schema owner alter authorzation changes owned schema used yourschemaname obviously substitute owned schema database dbo likewise change ownership whatever principal need environment allow drop previously schema owning user example purposes used theuseryouwanttodelete thatll non owner want drop 0,user logs theyre assigned security token includes information group membership token persists user logs point discarded even make changes group membership ad mean time changes make take effect next time user logs receives new security token reproduce scenario assigning permissions file system example ad behaviour sql server behaviour 0
storage engines work oracle http en wikipedia org wiki database engine mentions database engines aka storage engines storage engines used,oracle like mysql like rdbmss comes built storage engine exchanged another 17,completeness whilst jacks answer technically true possible use data cartridges expand vanilla oracle offerings fact company called coppereye released new indexing method patented available utilizing functionality see old press release patent makes fascinating reading 6,outer join clause outer table turns inner table meaning rows predicate evaluated make effectively clause outer join makes inner join would try fe violationdate fe violationdate fj feeduedate fj feeduedate fe violationdate fe violationdate fj feeduedate fj feeduedate 0,coalesce internally translated case expression isnull internal engine function coalesce ansi standard function isnull sql performance differences arise choice influences execution plan difference raw function speed miniscule 0
copy postgresql data one pc another migrating server application existing system another system unfortunately existing system also database server data,perhaps easiest way full dump old server pipe result straight new server like pg dump old server ip username dbname psql localhost username dbname superuser default postgres user superuser mightve created others update case move data different server versions use pg dump latest version likely pg dump new server 12,could dump database using pg dump restore new server using psql heres couple commands link create backup pg dump mydb db sql copy db sql new server specific command depends os go new server createdb mydb utf8 dont specify utf8 encoding always psql mydb db sql answering johnp answered fine answer assumes pg hba conf edited allow remote connections postgres conf edited listen network 14,going allow external partner employee org access server like credentials belong db owner couple databases concern along service account multiple instances outside entity escalate permissions quite easily given outside entity dbo risk opened could hack elevate sysadmin dbo escalation next concern user may local admin access duration doesnt matter easily add without already login sql server sysadmin change sa password threat continues article add sysadmin another example add sysadmin based exploits would use different service account password instance make difficult outside entity move one server next 0,normalizing database schema limit redundancy tables divided smaller tables established relations one one one many many many process single fields original table appear multiple normalized tables instance database blog could look like unnormalized form assuming unique constraint author nickname author nickname author email post title post body dave dave com blah bla bla dave dave com stuff like sophie oph ie lorem ipsum normalizing would yield two tables author author nickname author email dave dave com sophie oph ie post author nickname post title post body dave blah bla bla dave stuff like sophie lorem ipsum author nickname would primary 0
trim whitespace spaces tabs newlines im sql server need clean whitespace start end columns content whitespace could simple spaces tabs,might want consider using tvf table valued function remove offending characters start end data create table hold test data coalesceobject iddbo trimtest begin drop table dbo trimtest end create table dbo trimtest sampledata varchar50 null insert dbo trimtest sampledata select char13 char10 char9 char13 char10 test char13 char10 go create tvf coalesceobject iddbo stripcrlftab begin drop function dbo stripcrlftab end go create function dbo stripcrlftab val nvarchar1000 returns results table trimmedval nvarchar1000 null begin declare trimmedval nvarchar1000 set trimmedval case right val char13 right val char10 right val char9 left case left val char13 left val char10 left val char9 4,anyone using sql server newer use trim built function example declare test nvarchar4000 set test nchar0x09 nchar0x09 nchar0x09 nchar0x09 content nchar0x09 nchar0x09 nchar0x09 nchar0x09 nchar0x09 select trimnchar0x09 nchar0x20 nchar0x0d nchar0x0a test please note default behavior trim remove spaces order also remove tabs newlines cr lfs need specify characters clause also used nchar0x09 tab characters test variable example code copied pasted retain correct characters otherwise tabs get converted spaces page rendered anyone using sql server older create function either sqlclr scalar udf sql inline tvf itvf sql inline tvf would follows create alter function dbo trimchars originalstring nvarchar4000 charstotrim nvarchar50 returns 7,understanding correct sql server records every operation changes data transaction log rollback change data also records transaction log well statement run write data transaction log also reserve data transaction log case statement needs rolled back true rollback transaction information written log plenty ways see action quick demo query ill use see written log select count transaction count sumdatabase transaction log bytes used used bytes sumdatabase transaction log bytes reserved reserved bytes sys dm tran database transactions database id table create table tlogdemo fluff varchar1000 begin transaction query uses minimal logging insert tlogdemo tablock select replicatea master spt values t1 cross 0,11g oracle documentation sys system users following administrative user accounts automatically created install oracle database created password supplied upon installation automatically granted dba role sys account perform administrative functions base underlying tables views database data dictionary stored sys schema base tables views critical operation oracle database maintain integrity data dictionary tables sys schema manipulated database never modified user database administrator must create tables sys schema sys user granted sysdba privilege enables user perform high level administrative tasks backup recovery system account perform administrative functions except following backup recovery database upgrade account used perform day day administrative tasks oracle strongly recommends 0
tsql return wrong value power2 select power2 returns instead seems digits precision rounding 17th even making precision explicit select powercast2,online documentation power float expression arguments float expression expression type float type implicitly converted float implication whatever pass first parameter going implicitly cast float53 function executed however always case case would explain loss precision conversion float values use scientific notation decimal numeric restricted values precision digits value precision higher rounds zero hand literal type numeric declare foo sql variant select foo select sql variant property foo basetype go column name numeric dbfiddle multiply operator returns data type argument higher precedence appears sp1 precision retained select version go column name microsoft sql server sp1 kb3182545 x64 br oct br copyright microsoft 17,result exactly representable float real matter problem arises precise result converted back numeric type first power operand database compatibility level introduced sql server rounded float numeric implicit conversions maximum digits compatibility level much precision possible preserved conversion documented knowledge base article sql server improvements handling data types uncommon operations take advantage azure sql database need set compatibility level alter database current set compatibility level workload testing needed new arrangement panacea example select power10 ought throw error stored numeric maximum precision overflow error results compatibility result digits 14,starter thing current count store variable query like select count subject current isolation level concurrent pending transactions depending isolation level query see see rows inserted deleted pending uncommitted transactions way answer count rows visible current transaction note even touch even thorny subject concurrent transactions start end count mention rollbacks 0,relational algebra projection means collecting subset columns use operations projection list columns selected query optimiser step projection manifest buffer spool area description containing subset columns underlying table operator logical view based columns used subsequent operations view projection equates list columns selected query underlying view 0
sql server uses 1gb ram normal exact uses kb occasionally shoots query takes long total system memory 3gb running winxp,sql server express edition limited 1gb ram buffer pool non express editions default limited unless configured set max either case usage typically decrease unless memory pressure forces service restarted multiple instances definitely impact performance especially memory allocations exceed physical memory contention slow things concurrent queries inactive databases impact disk space consume 8,likely nothing wrong database sql server reserves large amount memory purpose caching disk reads among things server scenarios absolutely common sql server take tens even hundreds gigabytes whats ordinary sql server taking little memory thats youre using sql server express edition 4,clear solves problem theres redundancy since physical hardware os kernel mysql binaries maybe different disks storage controller etc reason reporting db offload queries oltp db kit wheres extra power coming something else trying get setup one conceivable use would segregate users somehow perhaps would thought could done grant 0,like need wrap code create procedure syntax remove go statements begin transaction commit transaction go create procedure dbo assignusertoticket updateauthor varchar100 assigneduser varchar100 ticketid bigint begin begin transaction save transaction mysavepoint set updateauthor user1 set assigneduser user2 set ticketid begin try update dbo tblticket set ticketassignedusersamaccountname assigneduser ticketid ticketid insert dbo tblticketupdate ticketid updatedetail updatedatetime usersamaccountname activity values ticketid assigned ticket assigneduser getdate updateauthor assign commit transaction end try begin catch trancount begin rollback transaction mysavepoint rollback mysavepoint end end catch end go also note added try catch statement block allow performing rollback transaction statement case error occurs probably need 0
getting select return constant value even zero rows match consider select statement select query id players username foobar returns column,select col1 col2 col3 query id players username foobar union select null null null exists select players username foobar alternative might faster second subselect required qid query id values select qid query id qid left join players useranme foobar write compact representation select qid query id values qid query id left join players useranme foobar think explicit cte readable although always eyes beholder 21,expecting one zero rows back would also work select maxcol1 col1 maxcol2 col2 query id players username foobar return one row values null except query id row found 6,convert implicit occurring collation column match parameters collation parameter converted columns collation explain collation coercion rules triggers conversion implicit collation column coercible default parameter parameter converted columns collation explicit different collations collation conflict error would result 0,select name ig income group el educational level country staffname left join country income group id left join incomegroup ig educational level ig id left join educationallevel el country el id 0
determinate trigger direct inserts inserts via stored procedure tablex modified two ways client direct inserts client uses stored procedure inserts,cant determine available inserted deleted tables could probably cheat gawdawful bloody hack define view extra column doesnt matter proc insert update assign one value column direct updates supply different non existent value create instead trigger view update logic based source flag create view dbo vwtablex select fake source column dbo tablex go create trigger inserthack dbo vwtablex instead insert begin set nocount exists select inserted fake source column begin perform logic proc sourced data insert dbo tablex select col1 col2 everything fake column inserted fake source column end exists select inserted fake source column fake source columns null begin perform 5,use set context info stored procedure read trigger context info trigger reset exit 4,operations log file havent committed first place operations committed uncommitted written log commit ensures log entries made durable flushed disk nothing prevents uncommitted entries flushed either log block fills another transaction commits thus forcing flush every transaction rollback must analyze transaction entries generate compensating actions every insert delete every delete insert every update update reverses data back compensating actions course logged database recovered must rollback transaction committed log must analyze log figure uncommitted transactions generate compensating actions actions belonging uncommitted transactions online recovery write compensation actions log standby recovery write compensating actions alternative stream thus allowing log applied form master 0,pivot data first apply filter order pivot mysql use aggregate function case expression select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id convert data rows columns see sql fiddle demo id views maxviews null null null null null null data column format apply filter select select id castmaxcase meta key views meta value end unsigned views castmaxcase meta key maxviews meta value end unsigned maxviews posts left join meta id post id group id src views maxviews see sql fiddle 0
invest time change column type char36 uuid million rows database already didnt know postgresql uuid data type designed schema one,im postgres person stretch imagination based know sql server rows fit onto data page better performance going reading data disk typically expensive operation thus going 36ish1 byte wide field byte guid seems straight forward cost savings fewer reads incur faster return results course assumes guid uuid satisfies business needs table uuid satisfies would bigint thatd shave storage costs another bytes per row edit1 character data postgres additional storage cost short strings bytes byte overhead anything longer bytes second respondent came byte cost byte field also option string compression perhaps wont cost full cant tell final cost would fundamentals remain anything 6,would consider changing uuid type char36 takes bytes uuid takes youll save bytes per row equate mb day gb year plus indexes depending hardware isnt much could adds improvement opportunities like also see constraint schema ensures interaction id actually right format using right type give well like however using bigint would save even even better performance unlikely application large bigint id column wont work 13,reusing identity value general discouraged either value used entirely internally case actual value immaterial also used externally case reusing value likely going lead misidentification take obvious case invoice purchase order number might easily come identity column exposed externally would never want reuse precisely reason refer specific transactions would want get confused resolving issues big hassle companies merge acquired creating problems purpose wise 0,go far course problem may bit art isnt pure science main product analysis reporting system regard quite detail records initially designed lots joins common id child records found denormalized couple fields could cut lot joins could take away lot performance headaches knew created normalized design started using profiled actual performance hundreds millions rows across dozens tables end story profiled couldnt know sure going work us liked idea normalizing could update easily end actual performance deciding factor thats advice profile profile profile 0
effective way compress store sql server backup ive testing different methods compressing storing sql server backups using sql server r2,ive testing different methods compressing storing ms sql backups using ms sql r2 enterprise edition im wondering effective compression algorithm long term storage backups outside sqls internal compression algorithms since using sql r2 enterprise edition must leverage data compression row page compression data level backup compression taking backup minimize disk footprint backups use master go exec sp configure backup compression default reconfigure override backup compression uses cpu cycles compress data leaves server vast majority scenarios compressed backups faster uncompressed backups note use open source tools need uncompress database backup file start restore process self receive sql database backup gb compressed 13,terms backup compression couple years ago make comparison backup compression options provided red gates sql backup questss litespeed sql server ideras sqlsafe benchmarking three products differences typical backup maximum compression spread three time taken somewhat wider spread backup size red gate coming top compression vs idera quest order 8,declare table sysname npersonal information schema sysname ndbo create table sz dbname nvarchar255 fullname nvarchar768 rows sysname reserved sysname data sysname index size sysname unused sysname declare sql nvarcharmax ndeclare nvarchar512 select sql nif exists select quotenamename sys tables inner join quotenamename sys schemas schema id schema id name table name schema begin set quotenamename quotename schema quotename table insert szfullname rows reserved data index size unused exec quotenamename sys sp spaceused update sz set dbname name dbname null end sys databases database id state read exec sys sp executesql sql table sysname schema sysname table schema select database dbname 0,quite common query pardon pun people running queries perform full table scans fts poster feels system make use indexes basically boils explanation given tables small optimiser say worth bother going index lookup fetching data instead ill slurp data pick need perform fts edit answer txsings comment mvcc multi version concurrency control database traverse every record count given moment thats example count much expensive mysqls innodb rather myisam excellent explantion postgresql available guy wrote post major contributor postgresql thanks dezso leading post 0
positive sum items negative return one needing find way sum positive values num return sum positive numbers individual row negative,try select salesid sumnum num num group salesid union select salesid num num want sum values one row must create maxvalue minvalue function use summaxvalue0 num summinvalue0 num described max function sql server takes two values like math max net 26,works select salesid sumnum group salesid case num else id end assumptions id starts hence use salesid else salesid id would work well considered positive number hence zero positive negative although seems make sign unnecessary helps remember case forgotten handled sum individual row sum positive numbers means sum strictly positive numbers needed must tested real data indexes table scan performances may little better cases absence index seems smaller impact query test data set count create table id int identity01 salesid intnum decimal164 insert besalesid num select castrand int rand rand go 24,another option one ive learned comes sqlcmd documentation need set codepage sqlcmd match file encoding case utf codepage youd want sqlcmd mssqlserver08 dp0 aqualogydb sql dp0 databasecreationlog log 0,long variables involved datatypes compatible sql variant case basically lob datatypes clr types user defined datatypes use select case coalesce cast startdate sql variant enddate statecode countycode producername taxid farm null yes else end necessary cast one arguments coalesce returns data type expression highest data type precedence sql variant high datatype precedence beaten user defined data types would prevent method working anyway personally dont find understandable finding conjunction individual null results though 0
calculating percentage row total sum apologies bad title wasnt sure would good title currently simplified view data im working agent,return agents commissions commission percentages use analytic function analytic clause partition whole table select agent commission commission sumcommission commission commissions learned ren nyffenegger ratio report function tightens syntax using package store commission sum would involve pl sql specifically excluded indicating want sql solution since already using functions assume intention exclude pl sql case package solution may help depends application works session first created calls function package get commission implicit call packages constructor could get sum store could reference stored sum get commission function would sum course soon call function different session sum would calculated also calling function every agent would 9,youre looking analytical function ratio report select agent roundratio reportcommission comm commissions 23,could try following query sumcommission calculated total commission select sumcommission total agents select agent name commission commission total commission agents total commission 5,need use function oracles sql select listaggsignbitand43 power2level within grouporder level desc bin dual connect power2 level result bin found deadly snippet fiddle number substitute column choice probably possible using recursive ctes thats bit pay grade reverse process use snippet input select reverse1000 dual select sumto numbersubstrxlevel1 power2level output input connect level lengthx result output dbfiddle recursive cte might trick substitute column kicks found another function work tweaks older versions databases dont recursive ctes excellent orafaq site select decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue decodebitandvalue bin number select value dual result bin fiddle observant among notice code remove leading 0,null mean value known row moment added known future example finishdate running project value applied row example escapevelocity black hole star opinion usually better normalize tables eliminating nulls case want allow nulls column yet want one null allowed kind relationship two tables perhaps simply change column null store instead null special value like known never appear solve uniqueness constraint problem may possibly unwanted side effects example using mean known apply skew sum average calculations column calculations take account special value ignore 0,using object identifier type regclass simplified considerably list child tables parent schema foo select inhrelid regclass child optionally cast text pg inherits inhparent schema foo regclass table name provide cast regclass optionally schema qualified search path decides visibility similarly returned table names schema qualified escaped automatically necessary safe fast convenient btw display source table per row row retrieved table select tableoid regclass source schema foo condition 0
warning sign select operator mean comparing execution plan stored procedures second one get warning sign marked red arrow mean,query planner warnings actual execution plan perhaps estimated one would expect see warnings section listing planner engine concerned common warnings operation spills data disk wanting index statistics unavailable common warning apparently missing indexes ssms gets information shows green text statements search showplan warning query plan warnings number articles detail places pick plan warnings via extended events see articles pop searching query warning extended events allows monitor problems running applications output included use set showplan xml set showplan give information ssms tools sql sentrys plan explorer analyse show warnings 5,right click warning properties see warnings 6,situation prefer limit drops using view layer create copy materialized view suffixed new also use data performance make sure indexes also created suffix dependant objects discovered via drop cascade create view upon new materialized view provide layer abstraction need change one place alter existing dependencies instead refer new view refreshing data needed beforehand drop original materialized view indices dependants alter materialized view indices drop suffix restore original names eg create table test myfield int insert test values create materialized view mv test select myfield test create view test select myfield mv test select test create materialized view mv test new 0,try alter sequence foo fee restart alter sequence foo fee restart http msdn microsoft com en us library ff878572 aspx 0
sql find records prefixes string table column named prefix storing aaaa application long string eg aaaabbbbccccdddd want select rows particular,first way using like wrong string like pattern string first pattern second furthermore prefixes starting wildcard needed something like work select table aaaabbbbccccdddd ilike prefix small proof table prefix values abcde abcd abce bcde select table abcdefghijkl ilike prefix prefix abcde abcd 7,another option would create prefixes selected word test equality ilike strings table select prefix select aaaabbbbccccdddd text word generate series0 lengthw word lateral select prefix table prefix leftw wordg prefix ilike leftw wordg might efficient big tables many length word equality checks using index seeks performing full index table scan need case insensitive check ilike used commented code improvement case abcdefghijkl ilike prefix answer prefix column used left ilike right side query performing column ilike aaabbbccdd checks arbitrary number aaabbbccdd ilike column checks ways number different ilike conditions one case number length searching word number rows table 6,personally find alias expression easier read comprehend reason troubleshooting select statement lengthy expressions probably want find expression via column name way around quick find expression application sees alias2 select alias1 long expression aggregates multiple column references long expression aggregates multiple column references alias2 preference may different true advantage using one except subjective taste reasons important thing pick one way consistently unless flip coin able defend choice come someone likes way write code dba fussy prepared rewritten ive blogged one thing feel even stronger use single quotes around alias names column alias alias column one form deprecated difficult read many newbies 0,using aarons link even finer adaption process would recommend function view query report size per row per table including indexes create function dbo getcolumnsize typename sysname max length int precision int returns int begin return select case typename tinyint smallint int bigint numeric precision decimal precision real float case precision else end money smallmoney time timestamp date smalldatetime datetime datetime2 datetimeoffset char max length varchar max length nchar max length nvarchar max length binary max length varbinary max length bit end end select schemaname objectname sumceilingbytes ceilingcountdistinct columnname rowsize floorpower230 power230 sumceilingbytes select name schemaname name objectname name columnname name 0
dba would go transitioning oracle sql server im oracle dba also sybase experience major architectural conceptual differences two rdbms platforms,ive swapped working oracle sql server past years wrote blurb going way number idiomatic architectural differences various bits terminology get used differently vendor developer dba communities surrounding product physical architecture sql server organises various things bit differently oracle one two key concepts direct analogues oracle database separate item sql server user permissions schemas name spaces storage youre familiar sybase work much databases sybase due common origins product filegroups roughly equivalent table spaces although local database schema distinct concept database user sql server although users default schema mvcc works somewhat differently sql server relatively recent feature maintaining different copies row locks 49,main product works sql server oracle differences work around might good keep mind date time handling different different precisions different set functions work empty strings nulls oracle sql server handling character encoding unicode different sql server normal varchar unicode nvarchar columns mixed database oracle decide database level kind encoding use 8,named pipes protocol useful application designed around netbios lan based protocols named pipes provides easy access remote procedure calls rpc within single security domain thus advantageous applications usually tcp protocol good practice dont care network 0,column order matter column orders match example insert items ver select items item id dont match could example insert items veritem id item group name select items item id relying column order bug waiting happen change number columns also makes sql harder read good shortcut explicitly list columns table inserting query using source data eg insert items ver item id name item group select item id name item group items item id dbfiddle 0
find iteration day week month date dimension table need add new column define iteration day week within month second mon,actually surprisingly easy using day doesnt matter name day always hold true select case dayyourdate dayyourdate dayyourdate dayyourdate else end occurance yourtable case short circuits 5,alternatively could calculate result using formula select dayyourdate iteration dbo yourtable cases operands division operation integers result also integer rounded one dayyourdate give yield still get result 4,noted system web unsupported library order reference system web need make call create assembly seems like tried reference location system web dll copy paste different location sql server try locate dependent assemblies location words reference location system web dll dependent libraries living directory work fine working example able add system web assembly well assembly create assembly system web windows microsoft net framework64 v4 system web dll permission set unsafe go create assembly systemwebtest sqlserver systemwebtest dll permission set safe go see client messages assemblies sql server loads take note sql server displays following warning registering fully tested sql server hosted 0,pretty broad ill give general answer ctes unindexable use existing indexes referenced objects constraints essentially disposable views persist next query run recursive dedicated stats rely stats underlying objects temp tables real materialized tables exist tempdb indexed constraints persist life current connection referenced queries subprocedures dedicated stats generated engine far use different use cases large result set need refer put temp table needs recursive disposable simplify something logically cte preferred also cte never used performance almost never speed things using cte disposable view neat things speeding query isnt really one 0
set names attributes creating json row json possible rename default f1 f2 f3 names using row json function columns row,something like select bla name1 otherbla name2 select row jsonr course achieved select row jsonr select bla name1 otherbla name2 found former readable part construct rows structure fly 9,common table expression allows specify aliases explicitly cte columns datacol1col2colacolb values 12fredbob select row jsondata data different dezsos example doesnt use col alias col select list aliases column names cte table alias ive used values expression subquery use select whatever like point whatever column aliases provided assumed subquery overridden cte definition specifying column name list thing subquery instead using alias select row jsondata values 12fredbob datacol1col2colacolb doesnt work row expression directly cast row concrete type alias regress select row12fredbob xabcd error syntax error near line select row12fredbob xabcd 17,select id select row json select first name last name first last age customers want without performance impact verbose id first last age fisrt name john last name smit 23,use json build object select json build objectid data customer id first name data first name last name data last name json data 9,wait stats numbers server anything youll likely kind waits appear also definition must one wait highest percent doesnt mean anything without kind normalization server days im reading output task manager correctly means wait seconds cxpacket per second overall addition since youre sql server cxpacket waits include benign parallel waits actionable waits see making parallelism waits actionable details would jump conclusion maxdop set incorrectly based presented would first measure throughput actually problem cant tell depends workload oltp system might measure transactions per second etl might measure rows loaded per second problem system performance needs improved would check cpu times experience problem 0,im unsatisfied answer couldnt manage get flow distinct operator along results guaranteed correct however alternative get good performance along correct results unfortunately requires nonclustered index created table approached problem trying think combination columns could order get correct results applying distinct minimum value updateid per objectid along objectid one combination however directly asking minimum updateid seems result reading rows table instead indirectly ask minimum value updateid another join table idea scan updates table order throw rows updateid isnt minimum value rows objectid keep first rows based description data distribution shouldnt need throw many rows data prep put million rows table rows 0,put clear passwords text files recommended since mysql use mysql config editor save passwords encrypted also provide different passwords different connections https dev mysql com doc refman en mysql config editor html 0,query syntactically correct sql even table name column reason scope resolution query parsed first checked whether table name column since doesnt table checked would throw error neither tables name column finally query executed select table name select name table results query would give every row table subquery select name table select name table table single column name value many rows table table rows query runs select table name name name name select table name name select table name null table empty query return rows thnx ughai pointing possibility fact dont get error probably best reason column references prefixed table name 0
polling way updating apps data database application needs data freshly updated database possible case way getting data besides timer based,service broker sql server sorry im sure rdbms 5,oracle use built dbms alert package facilitate dbms alert supports asynchronous notification database events alerts appropriate use package database triggers application notify whenever values interest database changed suppose graphics tool displaying graph data database table graphics tool reading graphing data wait database alert waitone covering data read tool automatically wakes data changed user required trigger placed database table performs signal signal whenever trigger fired 10,another oracle solution weve developed applications using dotnet framework microsoft take advantage database change notification feature oracle conjunction odp net oracle data provider dotnet using database actually notifies dotnet application new data arrived allowing us avoid constant polling link reference oracle tutorial hope helps dont know rdbmss 5,certain database vendors also provide integrated message buses app simply subscribe oracle advanced queueing ibm db2 mqseries called websphere mq sybase rtms alternative would route data database first place via message bus like tibco rv simply branch stream going db one going application use caching layer like coherence app db 7,listen notify postgresql http www postgresql org docs current static sql notify html database notify static channel name static message function trigger perform pg notifydynamic channel name dynamic message database client listen channel name note lack quotes listen client receive postgresql process id channel name message value standard jdbc driver postgresql doesnt like notifications however use https github com impossibl pgjdbc ng driver purpose 7,recovery conf file add line tells postgres failover master slave add trigger file file trigger create file given path nodes change file dont include anything trigger find additional information streaming replication hand may possible make automatically created tricks using monitoring tools making fail manual better 0,dual table exactly one row following sql statement show select dual dual2 table rows insert one see behavior expression oracle evaluate without actually using data table evaluate every row like would normal column expression row result returned two rows get twice 0,think index table contents ordered list pointers positions file aka offsets say millions records stored table rather search table matching criteria much faster reference ordered list matches stack pointers specific matching rows perfect example index tables primary key field typically id field want row id much faster ask index pointer data scan data source position heres obvious use indexing create table activity log id int unsigned null auto increment activity type id smallint unsigned null datetime created datetime keyactivity type id primary keyid create table activity log date key activity log id int unsigned null date created key int unsigned 0,comments agree extended ascii really bad term actually means code page maps characters code points range beyond standard code point range defined ascii sql server supports many code pages via collations non ascii characters stored varchar long underlying collation supports character character stored varchar char columns sql server collation code page greater query bellow list select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name subset also support character column collation need one following support select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name 0,may referring federated database system 0
count set difference union fairly simple question cant seem find answer im working unions differences would like perform count results,go select count select tbl1 id tbl1 except select tbl2 id tbl2 union select tbl3 id tbl3 5,another way result select count select id tbl1 except select id tbl2 except select id tbl3 4,maybe use lpad rpad create temporary table tt category int unsigned insert ttcategory values select lpad category tt granted wouldnt work youd replace single null value idea least 0,according docs article compression new pages allocated heap part dml operations use page compression heap rebuilt rebuild heap removing reapplying compression creating removing clustered index would seem align youre seeing seems like youre actually getting compression table rebuild could try loading data uncompressed table see still average rows per page decreases remains youre getting compression rebuild necessary could also add clustered index table prevent table uncompressed low compressed bulk loading data 0
empty columns take space table table holds basic info title date fields theres one field called comments varchar4000 time leave,think separate table would better improve page density reduce fragmentation especially dont always populate field data page holds around bytes rows say bytes rows bytes long rows page rest page wasted space db takes likely never hold data add data long field record mostly full page likely overrun page result pointer page rest record empty pages pointers lead poor performance normalize field 10,predictable performance avoid high variation rows per page would lean storing data related table especially populated small percentage time especially retrieved queries rows value null contribute space overhead minimal important one page might fit two rows next page fit rows really impact statistics might better splitting stored separately doesnt impact operations core table 9,takes minimal space used one bit null bitmap two bytes length zero null overhead minimal optimisation premature know issue keep one table break kiss introducing outer joins add overhead querying data see https stackoverflow com questions come limits bytes per row per varchar nvarchar valu 12,regarding window aggregates sum avg sumsalesytd partition territoryid order datepartyymodifieddate msdn page clause transact sql rather hidden remark general remarks order specified entire partition used window frame applies functions require order clause rows range specified order specified range unbounded preceding current row used default window frame applies functions accept optional rows range specification example ranking functions accept rows range therefore window frame applied even though order present rows range means code results sumsalesytd partition territoryid order datepartyy modifieddate range unbounded preceding current row sum calculated row getting rows territoryid year part modifieddate less equal year part row often called cumulative 0,create another table log inserts updates deletes using dml triggers lets say table want track create another table say tbl row stats create dml triggers table inserts updates deletes whenever three actions happen table insert row tbl row stats respective row id table action took place later select using group clause see number times action took place per row tracking selects add application code stored procedure whenever select done application corresponding insert tbl row stats done 0,short answer qualified yes number rows grows precise schema datatypes operations choose grows importance much normalize data depends operations plan perform stored data datapoints table particular seems problematic planning comparing nth point given spectra mth storing separately could mistake datapoints stand alone make sense context associated spectra dont need primary key foreign key spectra nth column index column suffice define inter intra spectrum operations must perform figure cheapest way accomplish equality thats needed may denormalized possibly pre calculated statistical metadata assist operations absolutely need sql access individual datapoints ensure reduce size row bare minimum number fields smallest datatype possible largest 0
check constraint array column verify length im playing postgres table validation rules trying set check constraint array column idea allow,array lengthwords return null specified array dimension exists use cardinality instead create table words table id serial primary key words varchar20 check cardinalitywords insert words table words values results error new row relation words table violates check constraint words table words check detail failing row contains 5,crucial point may aware quoting manual noted check constraint satisfied check expression evaluates true null value bold emphasis mine using cardinality fixes case like horse explains explicitly disallowing empty arrays would cheaper check words solutions still allow words null might want add null constraint either way aware null always passes check constraints general solution disallow null expressions check expression true 6,rolandmysqldba given right hint answer question problem seems lie query results given back fields read somehow database dropped indexes primary key inserted new index alter table newbb innopost add index threadid visible dateline index threadidvisibledatelineuseridattachipaddress link explains happens covering index queried fields query postidattach extracted key saves checking real data using hard disk queries run seconds thanks lot help edit actual underlying problem solved circumvented technique innodb needs serious fixing area 0,smaller value work cases takes varchar24000 work cases little choice use larger value creating separate table values coordinating inserts selects etc would add complexity would obliterate memory performance benefits extending field 0
computed columns computed values computed columns determined value retrieved value changed time im guessing novice question since im finding anything,depends define computed column persisted computed column calculated stored data inside table define column persisted calculated query run please see aarons answer great explanation proof pinal dave also describes detail shows proof storage series sql server computed column persisted storage 19,easy prove create table computed column uses scalar user defined function check plans function stats update select see execution gets recorded lets say function create function dbo mask varchar32 returns varchar32 schemabinding begin return select xx substring len xxxx end go table create table dbo floobs floobid int identity11 name varchar32 maskedname convertvarchar32 dbo maskname constraint pk floobs primary keyfloobid constraint ck name check lenname go lets check sys dm exec function stats new sql server azure sql database insert select select name execution count sys dm exec function stats inner join sys objects object id object id database id 34,wrote case closed ill reopen much gone wrong database design test setting create table patient patient id int primary key site held int null create table messageq messageq id varchar primary key varchar patient id int null references patient message body varchar null create index patient site idx patientsite held create index messageq patient id idx patientpatient id insert patient values insert messageq values m1 aaa1 m2 aaa2 m3 aaa3 m4 aaa4 m5 aaa5 m6 aaa6 m7 aaa7 m8 aaa8 m9 aaa9 m10 bbb10 major points simplify names better readability dont use non descriptive column names like entity id replaced 0,would say depends install sql server default ones seen case insensitive run select serverpropertysqlsortordername get back nocase iso would mean searches case insensitive using upper would give results would add bit work cpu 0
export security related information sql server database guys probably know sql server provide box solution export security related statements declared,im sorry havent response since yesterday heres least starting point try pulling pieces need always read discussion threads regrettably unable find script endorsed big names recognize test thoroughly schema object server column level permissions often missing havent actually used theyre set starting points particular order list permission databases automated permissions auditing powershell sql part permission scripting databases script db level permissions v2 stored procedure script user permissions security queries database level security queries server level 8,idera sql permissions extractor seems product youre looking script server object permissions free also commercial edition called sql secure features feature comparison two editions found 6,database homes installed properly central inventory list central inventory windows located program files oracle inventory linux unix platforms location central inventory found etc orainst loc inventory contentsxml inventory xml list installed homes xml format inventory version info saved saved minimum ver minimum ver version info home list home name oragi12home1 loc opt oracle grid12102 type idx home name oradb12home1 loc opt oracle base product db12102ee type idx home name oradb11g home1 loc opt oracle base product db11204ee type idx home list inventory parse list find detailed information opatch example opt oracle grid12102 opatch opatch lsinventory oh opt oracle grid12102 details 0,profile database usage load identify bottlenecks due missing indexes due many indexes choose proper index require good knowledge specific database indexing techniques 0
understanding cleanuptime ola hallengrens sql server scripts relation full backups log backups trouble understanding exactly expect cleanuptime option ola hallengren,cleanuptime always specified specific backup job example create full backup job differential backup job transaction log backup job cleanuptime always relates extension job lets take look full backup example full backup create full backup job normally add one following parameters databases databases get backed really relevant example directory directory store backups backuptype full differential tlog cleanuptime much hours worth backups keep fileextensionfull extension backup backup job place create full backup according schedule defined job lets assume following job runs fileextensionfull set bak directory set sqlbackup cleanuptime set hours look maintenancesolution sql file find description parameter set cleanuptime null time hours 15,upvoted hot2uses answer covers question detail want share easy way test stuff might help helped fully understand script works install backup script dependencies test instance tested local computer alter script search replace hh minute references find hh script dealing cleanup time allows quickly run backups various types full diff log see effects execution retention minutes hours run full diff log backup test database note files created individual folders used note cleanuptime minute due alter script hours minutes exec dbo databasebackup databases test directory olabackuptest backuptype full verify cleanuptime cleanupmode backup exec dbo databasebackup databases test directory olabackuptest backuptype diff verify 8,opened case microsoft regarding issue microsoft confirmed bug also affects sql server planning release fix sql server service pack released time writing answer microsoft releases service pack sql server users bypass issue disabling lock partitioning via trace flag note issue applies machines processors information lock partitioning thanks microsoft support prompt helpful update bug fixed sql server cumulative update sql server sp 0,really depends much data changing lets say table columns also indexes diff column values columns changing even data columns changing columns indexed may better deleting inserting columns changing lets say part non clustered indexes may better updating records case clustered index updated indexes updated research find comment sort redundant sql server internally separate mechanism performing update place update ie changing columns value new original row place update delete followed insert place updates rule performed possible rows stay exactly location page extent bytes affected chnaged tlog one record provided update triggers updates happen place heap updated enough space page updates also 0
effect execution plans table variable primary key read great deal differences temporary tables table variables sql server experimenting switching mostly,since declaring primary key table variable implicitly creates index key columns fact way index table variable prior sql server presence definitely effect resulting query plans optimizer make use primary key index appropriate see action running short script execution plan enabled table scan change clustered index seek primary key index declare t1 table id int null data varchar50 null insert t1 values aaaaa bbbbb ccccc ddddd eeeee select t1 id primary key index declare t2 table id int null primary key clustered data varchar50 null insert t2 values aaaaa bbbbb ccccc ddddd eeeee select t2 id whether declaring primary key instead 6,query optimizer going make choices differently theres primary key constraint yes might estimating one row understanding estimates incorrect different knowing table contains unique values certain plan space explorations require key example good general rule thumb provide much information data query task optimizer key say explicitly declaring key add much cost beyond little keyboard work cases personally rarely use table variables lack statistics including distribution density cardinality information separate considerations provides less information optimizer equivalent temporary table experience much table variable plans adapt well changing circumstances time use table variable special reasons sure always adequate query optimization point view enough information 7,possible solution create server logon trigger would check app name created following server trigger local instance tried connect via odbc connection rejected would modify needs create trigger trggetappname server logon app name like microsoft sql server management studio app name like net sqlclient data provider begin rollback end word caution logon triggers logon trigger effectively prevent successful connections database engine users including members sysadmin fixed server role logon trigger preventing connections members sysadmin fixed server role connect using dedicated administrator connection starting database engine minimal configuration mode 0,sql injection attacks untrusted input directly appended queries allowing user effectively execute arbitrary code illustrated canonical xkcd comic thus get situation userinput getfromhtml robert drop table students query select students studentname userinput stored procedures general good defenses sql injection attacks incoming parameters never parsed stored procedure dbs programs dont forget precompiled queries count stored procedures look like following create stored procdure foo select students studentname program desires access calls foouserinput happily retrieves result stored procedure magical defense sql injection people quite able write bad stored procedures however pre compiled queries stored database program much difficult open security holes understand sql 0
solution assigning unique values rows finite collaboration distance table created populated following code create table dbo examplegroupkey int null recordkey,iterative sql solution performance comparison assumes extra column added table store super group key indexing changed setup drop table exists dbo example create table dbo example supergroupkey integer null default groupkey integer null recordkey varchar12 null constraint iexample primary key clustered groupkey asc recordkey asc constraint ix dbo example recordkey groupkey unique nonclustered recordkey groupkey index ix dbo example supergroupkey groupkey supergroupkey asc groupkey asc insert dbo example groupkey recordkey values archimedes newton euler euler gauss gauss poincar ramanujan neumann grothendieck grothendieck tao able reverse key order present primary key extra unique index required outline solutions approach set super group 7,recursive cte method likely horribly inefficient big tables rcte anchor select groupkey recordkey cast castgroupkey varchar10 varchar100 groupkeys cast castrecordkey varchar10 varchar100 recordkeys lvl example union recursive select groupkey recordkey case groupkeys like caste groupkey varchar10 castr groupkeys caste groupkey varchar10 varchar100 else groupkeys end case recordkeys like caste recordkey varchar10 castr recordkeys caste recordkey varchar10 varchar100 else recordkeys end lvl rcte join example recordkey recordkey groupkeys like caste groupkey varchar10 groupkey groupkey recordkeys like caste recordkey varchar10 select row number order groupkeys supergroupkey groupkeys recordkeys rcte exists select rcte lvl lvl groupkeys like castc groupkey varchar10 lvl lvl groupkey 6,problem following links items puts realm graphs graph processing specifically whole dataset forms graph looking components graph illustrated plot sample data question question says follow groupkey recordkey find rows share value treat vertexes graph question goes explain groupkeys supergroupkey seen cluster left joined thin lines picture also shows two components supergroupkey formed original data sql server graph processing ability built sql time quite meagre however helpful problem sql server also ability call python rich robust suite packages available one igraph written fast handling large graphs millions vertices edgeslink using igraph able process one million rows minutes seconds local testing1 compares 10,theres little bit overhead transaction log file written disk even changes occurred db databases sql server compression turned backups transaction logs written periods inactivity generally kb per trn file overhead transaction logs going contain changes made since last trn file written amount total data wont vary significantly frequently write files less data loss risk experts recommend run log backups every minute yes really generally try run every minutes business hours bulk activity going every minutes peak times systems real activity work hours system hour operation 0,many perhaps database products allow define derived table calculated column explicitly assigned alias words let select select expression instead would something like select select expression somealias reason assign name expression automatically contrast oracle oracle fine derived table calculated column without explicit alias column default name problem name basically name expression oracle applies certain transformations original expression using name particular unquoted identifiers including names functions uppercased spaces removed thus expression count default name column becomes count expression default name note presence special symbols example prescribes enclose name quotation marks able use reference words derived table like select count must write select 0,similar setup recently encountered messages logs using dell compellent san things check receiving messages helped us find solution review windows performance counters disks warning messages pointing specifically disk avg read time disk avg write time disk read bytes sec disk write bytes sec disk transfers sec avg disk queue length averages many database files one drive averages skew result mask bottle neck specific database files check query paul randal returns average latency file dmv sys dm io virtual file stats case average latency reported acceptable underneath covers many files ms average latency check timings pattern happen frequently certain time night 0
install sql server management studio locally install sql server management studio desktop access database sql server instance find installer google,download install sql server management studio using links provided scott hanselmans website links various ssms versions x86 x64 well links various sql server express versions way also use latest ssms manage database 12,want shiny new monthly releases management studio include handy check updates mechanism negates need full setup program download since retrieves components need well release cycle completely independent sql server go read use new web installer much longer applies want use previous versions currently sp2 sp1 theyve made easier find download see page much still apply well legal copy sql server able install management studio media wherever need manage sql server said express version management studio since service pack fully functional product absolutely differences except download free instead finding licensed media would recommend downloading version use older version support sooner philosophy 75,problem see dynamic allocation resources leads unpredictable performance reporting query cpus available yesterday ran minutes today took considerably longer also see latency guest waits cores become available jonathan kehayias gives practical warnings subscribing cpu memory frankly trust experience advice typical vm admin offense suspect lot direct experience combination accidental dba day virtualization considerations 0,firstly generate script unix prompt echo select concatdrop table table name information schema tables table name like prefix mysql user root password blah batch drop sql replace prefix batch option suppresses fancy formatting mysql default produce runnable sql script review script looks ok run drop sql mysql prompt 0
delete low priority row visibility large myisam table mysql 5windows xp x64 run delete low priority queries delete low priority,docs specify low priority server delays execution delete clients reading table affects storage engines use table level locking means delete low priority statement begin processing read locks finished another read lock comes delete low priority statement begins wait delete low priority statement lock though acquire table lock run complete killed normal table lock situations write requests given higher priority read requests example write lock going read request goes read queue another write request comes second write request execute read requests using low priority situation reversed read lock going write comes wait another read comes write gets lock second read execute 5,rows remain visible test shows delete prevent rows visible subsequent queries test illustrates table lock taken immediately normal delete query waits delete finishes returns zero count testbed long running query create database stack use stack create table table id int auto increment primary key varchar val varchar10 insert table varchar val select hello select union select union select union select union select union select union select union select union select union select s1 select union select union select union select union select union select union select union select union select union select s2 select union select union select union select 4,oracle provides nvl scenario isnull equivalent ms sql server could disguise view make code clearer 0,talked similar issue worked case setting sql server dependent disk drivers youll want test make sure works setup trick 0
displaying estimated execution plan generates cxpacket pagelatch sh latch ex access methods dataset parent waits im running microsoft sql server,appears request actual execution plan triggered stats updates since mention happens mornings imagine theres overnight process lot modifications tables involved thus sql server uses stats create plan hit modification threshold executes automatic stats updates part operation xml estimated plan shared see close together update dates stats morning lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 lastupdate 06t09 large busy tables seems likely based sampling percentages surprising stats updates taking lot horsepower 13,see long estimated plan times ssms one following order likelihood query optimizer decided needed create update statistics size estimated plan large say mb simply takes ssms long time display query compilation actually took long time due cpu usage looking good enough plan server extreme duress example might wait gateway become available various bugs lead extremely long running compile times situation answer almost certainly sql server updating creating statistics clues size query plan small query plan relatively simple low cost compile cpu significantly lower compile time new contributor josh darnell also pointed good clue last updated time statistics xml sql server 9,thought read lot topic broad topic configuration control change management strategy cmmi domain topic even companies accreditation cmmi sometimes version control databases question answered keeping mind following constraints keeper every ddl executed keeper people ability execute ddl statements need log changes done need compare vast differences database design done via external tool published database external tool ddl scripts source control even key point source control publish database need know instantaneous changes time time hourly daily defined server structure development test production good testing strategy answer true use external source control example embercadero database change management tool http www embarcadero com 0,feature want could exist principle would efficient current database structures see max vernons answer sql server would either maintain set diff maps compare current db contents full backup specify base applications deduplicate large files make two full backups changed data actually stored like diff custom base exdupe example nice thing works set backup files fact starting 3rd full backup file pay incremental differential space usage space usage difference previous backup file first deduplicating storage similar behavior feature describe exist feature consumes budget causing features present one apparently make far enough priority list im sure would good seems like fairly esoteric 0
whats difference cte cte column names shown using common table expressions msdn define cte expression name column name cte query,nearly answer one questions already msdn page line directly quote explains basic syntax structure cte expression name column name cte query definition list column names optional distinct names resulting columns supplied query definition emphasis added would mean would need specify column names situations would work test table noname cast function select cast1 char1 dbo casttochar1 select test table would test table select noname cast1 char1 cast dbo casttochar1 function select test table would distinct names columns test table select cast1 char1 dbo casttochar1 select test table 23,ultimately column needs valid name assign two ways column list cte foo select col tab select foo cte using original column names aliases cte select col tab select col cte alias column list column list aliases cte foo bar select col1 valid outer select col2 colx valid outer select tab select foo bar cte similar definition view derived table specify list column names column list lots complex calculations easier spot name theyre scattered source code easier got recursive cte assign two different names column original name aliases assign alias calculation want must rename column 7,anecdotally prefer name columns inside cte instead inside cte xxx as1 clause since youll never inadvertently mismatch names vs column contents take instance following example mycte select mt mt myschema mytable mt select mycte mycte mycte display shows contents column heading contents column heading realization never specify column names xxx clause instead like mycte select alias1 mt alias2 mt myschema mytable mt select mycte alias1 mycte alias2 mycte removes doubt column definitions totally unrelated side note always specify schema name referencing object names end statements semi colon 10,based post appears clear message rdbms lookup time optimizations replaced hardware makes io time negligible absolutely true ssd database servers combined high actual ram makes io waiting significantly shorter however rdbms indexing caching still value even systems huge io boon io bottlenecks poorly performing queries caused bad indexing typically found high workload applications poorly written applications key value rdbms systems general data consistency data availability data aggregation utilizing excel spreadsheet csv file method keeping data base yields guarantees ssd doesnt protect primary server become unavailable reason network os corruption power loss ssd doesnt protect bad data modification ssd doesnt make 0,since using standard edition cant use tde options using encryption keys instance database level sql server two kinds keys symmetric asymmetric symmetric keys use password encrypt decrypt data asymmetric keys use one password encrypt data called public key another decrypt data called private key sql server two primary applications keys service master key smk generated sql server instance database master key dmk used database also encryption column level creating master key encryption along create certificate create symmetric key example done described encrypt column data reference sql server database encryption keys database engine drive level using bitlocker drive encryption data protection 0,odds statistics date one instances causing crappy execution plans generated 0
possible take database offline backup using sql job scenario generate backup database sql server restore new server sql server taking,yoy couple options situation turn applications change data database set database single user mode back something like alter database dbname set single user rollback immediate backup database dbname disk locationandfilename restore redirect app new server 5,possible restore read db new server yes example create database readonlydb go alter database readonlydb set read backup database readonlydb disk share readonly bak destination server already read writeonline database name entirely sure mean could restore database different name would like remember remove read property afterwards restore database readonlydb2 disk share readonly bak move readonlydb datalocation readonlydb2 mdf move readonlydb log loglocation readonlydb log2 ldf stats go alter database readonlydb2 set read write could also replace existing database replace keyword possible restore offline db new server possible sql server removes handles database files able access offline alter database readonlydb set 11,think itzik ben gan ken henderson books may usable websites one favorite resource sqlservercentral com question day concerned sql may help improve knowledge 0,recommend use dbcc shrinkfile check available space available file using query answer link determine used free space within sql database files one data file one log file itll probably file file pass file number shrinkfile command necessary recommend small batches maybe gb time data file gb data gb following dbcc shrinkfile1 go dbcc shrinkfile1 go dbcc shrinkfile1 go would leave certain amount space free dont go way gb leave maybe gb free work operations within file shrink file block usually gets suspended operations working data file shrinking large chunk batch may take long time complete part unused space log file 0
unable create filtered index computed column previous question mine good idea disable lock escalation adding new calculated columns table creating,unfortunately sql server ability create filtered index filter computed column regardless whether persisted connect item open since please go ahead vote maybe microsoft fix one day aaron bertrand article covers number issues filtered indexes 20,create index whereclause possible creates filtered index specifying rows include index filtered index must nonclustered index table creates filtered statistics data rows filtered index filter predicate uses simple comparison logic reference computed column udt column spatial data type column hierarchyid data type column comparisons using null literals allowed comparison operators use null null operators instead source msdn 4,although create filtered index persisted column fairly simple workaround may able use test ive created simple table identity column persisted computed column based identity column use tempdb create table dbo persistedviewtest persistedviewtest id int null constraint pk persistedviewtest primary key clustered identity11 somedata varchar2000 null testcomputedcolumn persistedviewtest id persisted go created schema bound view based table filter computed column create view dbo persistedviewtest view schemabinding select persistedviewtest id somedata testcomputedcolumn dbo persistedviewtest testcomputedcolumn convertint next created clustered index schema bound view effect persisting values stored view including value computed column create unique clustered index ix persistedviewtest dbo persistedviewtest viewpersistedviewtest id 21,dont rely behaviour ssms historically buggy poorly documented also sometimes changes version version easiest way sure click script button top dialog compare output documentation example click script button adding several files ssms version comes sql server like see exact script used backup database mydatabase disk ng location file1 bak disk ng location file2 bak noformat noinit name ndatabasename full database backup skip norewind nounload stats go refer documentation backups see script creates striped backup mirrored backup would need mirror clause like backup database mydatabase disk ng location file1 bak mirror disk ng location file2 bak go dont know version ssms 0,anything dbo avoided default sql server also descriptive like default names since preknown makes hackers life much easier although theyre point theyre trying figure schema name youre probably already borked work use schemas divy database logical sections assign permissions schemas instance may inventory system database main tables might inv schema import anything database staging schema would used part import process system stored procedures users dont need access put sp schema 0,imagine may lot backup jobs full backups differential backups transaction log backups imagine need move backups local disk san use backup devices need create devices paths new disk backup scripts written use full paths need rewrite every job every job step change paths scenario need use backup devices example use custom scenario dynamically changes backup path file name example adding date time folder file name 0
justify using nolock hint every query ever justify using query hint seeing nolock every single query hits busy server point,discussed https stackoverflow com questions using nolock hint ef4 https stackoverflow com questions happen result using nolock every select sql sever define busy high volumes 50k new rows per second large aggregates etc dont see need get dodgy data 11,pick battles battles like cant easily system every dml hinted rowlock hint irrespective modifying one row several thousand rows showed several examples really hurts performance system already working resistance change note convinced enough use going forward though nolock place recommend good references showcasing troubles using microsoft sql server development customer advisory team blog previously committed rows might missed nolock hint used itzik ben gan sql magazine clustered index scans part iii itkiz ben gan sqlpass org beware nolock hint 17,explain colleagues importance understanding isolation levels show examples nicest easiest explanation found little kendras poster isolation levels ask think need nolock hint dont use set transaction isolation level statements ask exactly situation want fix maybe deadlocks blocking etc dont want hold locks might consider snapshot isolation level asking clear picture 9,general depends one case cte nicer derived table need reference several times query silly example select xy select xy select maxx vs cte xy select xy select xy cte select maxx cte different database engines treat multiple references cte different ways sql server cte generally fully evaluated reference 0,find default directory depending install instance red hat var log mysql query time time seconds start recording done startup runtime log slow queries var log mysql mysql slow log long query time 0,according documentation also cant start pg reserved looks fairly freeform 0
regenerating cube relational schema visual studio analysis services cube project company received outside contractor im trying get developers work local,thats works dsv data source view generated defining tables queries want use cube flow create one ds data sources defining connect source databases create dsv data source view adding tables named queries defining get data ds create dimensions cubes defining data dsv needs look multidimensional model process dimensions cubes load data source system technically suppose could recreate source systems schema analyzing xml dsv file contains data types table names queries could theoretically map back recreate source databases would require lot manual work writing something parse xml dsv contains named queries would still need analyze sql queries reconstruct underlying table depending 8,time need use schema generation top development never generated schema previously generated sql schema script objects ssms provide script along ssas project developers new developer would deploy sql scripts edit data source ssas project point database straightforward approach believe 4,answer first question tools default walk id index fetch data write disk yes tools similarly impact working set would generally recommend running secondary preferably hidden secondary possible ill echo stennie comments recommend backup methods dealing large amounts data second question assume looking mongodump equivalent fields option mongoexport dump specific fields query option used filter results used projection select fields returned feature request tracked tools yet scheduled stennie also mentioned option write custom exporter fits needs would still recommend running secondary protect working set 0,isnt performance gain recoverabily gain made file corruption happens system tables database lost keep user data separate file group groups restore files keeping rest database online restore assuming enterprise edition state cant say would benefit multiple file groups system objects primary filegroup however kick junk saying autoshrink enabled 0
mysql set utc time default timestamp set timestamp column whose default value current utc time mysql uses utc timestamp function,specify utc timestamp default specify automatic properties use default current timestamp update current timestamp clauses also insert utc timestamp values like though table create table test ts timestamp null default current timestamp update current timestamp insert query would like insert utc timestamp insert test ts values utc timestamp 4,go along ypercubes comment current timestamp stored utc retrieved current timezone affect servers timezone setting default time zone option retrieval allows retrieval always utc default option system system time zone set may may utc mysql select global time zone session time zone global time zone session time zone system system row set sec mysql select current timestamp current timestamp row set sec set dynamically mysql set session time zone query ok rows affected sec mysql select global time zone session time zone global time zone session time zone system row set sec permanently cnf mysqld variables default time zone restart 49,solution trigger delimiter create trigger update utc insert table row begin set new field utc timestamp end delimiter every new inserted row timestamp utc 4,cant reference alias clause order sql server parses statement many discussions stackoverflow couple examples give background https dba stackexchange com questions select clause listed first queries parsed way disallows use column aliases clauses alternative would select dv name maxhb dateentered de devices dv inner join heartbeats hb hb deviceid dv id group dv name maxhb dateentered 0,great explanation richard wanted add links official documentation topics written sql server much concepts remain today understanding avoiding blocking understanding locking sql server edit additions five ways fight blocking video fresh video kendra little published today dba detective troubleshooting locking blocking rodney landrum identify blocking problems sql profiler brad mcgehee well known sql server authors mvps 0,lose one seconds worth transactions default value helps keep innodb acid compliant according mysql documentation innodb flush log trx commit value innodb flush log trx commit log buffer written log file per second flush disk operation performed log file nothing done transaction commit value default log buffer written log file transaction commit flush disk operation performed log file value log buffer written file commit flush disk operation performed however flushing log file takes place per second also value note per second flushing guaranteed happen every second due process scheduling issues default value required full acid compliance achieve better performance setting 0
oracle import problem caused different character sets im trying import oracle export oracle xe get following messages import xe fehlerhaft,dont choice character set xe change suit database trying import would practical migrate source database export import work character set conversion might mean text columns non ascii characters wont look import rows rejected long new character set case converting utf8 mean possible single byte character grow conversion theory may need increase column size export adjust target schema import data separate step see possible data truncation problems 4,actual ddl using create table could use nls length semantics parameter set char rather default byte varchar25 allocated enough space store characters database character set potentially bytes rather bytes could allow character unfortunately changing nls length semantics probably wont terribly helpful youre relying import process create table dump file inherently add char byte keyword would actually issue statement create table bdata artikel key varchar23 byte null name varchar260 byte null abkuerzung varchar25 byte null 8,use solution already mentioned another stackoverflow post ref https stackoverflow com mysql alter table add column exist set dbname database set tablename tablename set columnname colname set preparedstatement select select count information schema columns table name tablename table schema dbname column name columnname select concat alter table tablename add columnname int11 prepare alterifnotexists preparedstatement execute alterifnotexists deallocate prepare alterifnotexists 0,first thing thought setup db servers identical hw os configs installed mysql percona mariadb get fourth server installed monyog eval version lasts days register db servers monyog use performance metrics charts monyog set charts monyog use sysbench db servers discretion basic outline believe use monyog right box check testimonial monyogs website look name page eventaully convinced company purchase utlimate version outright testimonial watched happened caveat get monyog db servers sysbench ready get best day usage monyog update edt keep innodb buffer pool small default innodb buffer pool size 8m keep binary logs disabled include log bin cnf metrics measured innodb 0
shrinking database insert update process archiving wherein take backup current database restore xxx archive database database contains previous data insert,instead backup restore consider moving data across new presized database creating new indexes data keep file size small indexes unfragmented also make sure compression turned 4,database used reporting thinking shrinking database one time sensible shrink ok disk space cheap increases index fragmentation side effect shrink operation advisable shrink using dbcc shrinkfile maintenance window minimal activity shrink database chunks using script options consider recovering space another option would script database bcp bcp data take backup restore route make sure enable backup compression source server instant file initialization source destination server help cut restore time process change recovery model simple backup database move database pseudo simple state planning shrink database sure related shrinking database shrink reorg take backup since going use database reporting keep database read mode 4,key lookup required get referenced query assume used calculate comp columns referenced query plan used seek t2 also query use index t2 t1 optimizer decided scanning clustered index cheaper scanning filtered nonclustered index performing lookup retrieve values columns explanation real question optimizer felt need retrieve index seek would expect read comp column using nonclustered index scan perform seek index alias t2 locate top record query optimizer expands computed column references optimization begins give chance assess costs various query plans queries expanding definition computed column allows optimizer find efficient plans optimizer encounters correlated subquery attempts unroll form finds easier reason find 0,unfortunately design taken bol page revert database database snapshot limitations restrictions reverting unsupported following conditions database must currently one database snapshot plan revert read compressed filegroups exist database files offline online snapshot created alternative could drop first snapshot db basis understand seems limiting look way snapshots sparse files based original data files reverting specific snapshot would invalidate snapshots anyway base data files would changed revert operation limitation annoying doesnt look unreasonable 0
varchar datatype allow unicode values table varchar column allowing trademark copyright unicode characters shown create table varcharunicodecheck col1 varchar100 insert,trademark registered symbols unicode characters wrong strings contain ascii characters simple test shows characters ascii extended ascii ascii codes declare varcharunicodecheck table col1 varchar100 insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany insert varcharunicodecheck col1 values mycompany select rightcol1 1as last char asciirightcol1 last char ascii varcharunicodecheck clearly see characters byte encoded yes pure ascii characters extended ascii show real unicode character trademark code binary representation declare table uni ch nchar1 ascii ch char1 insert values select unicodeuni ch unicode asciiascii ch ascii castuni ch varbinary10 uni ch 15,comments agree extended ascii really bad term actually means code page maps characters code points range beyond standard code point range defined ascii sql server supports many code pages via collations non ascii characters stored varchar long underlying collation supports character character stored varchar char columns sql server collation code page greater query bellow list select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name subset also support character column collation need one following support select collationpropertyname codepage code page name description sys fn helpcollations collationpropertyname codepage order name 7,definition varchar says allows non unicode string data trademark registered symbols unicode characters definition contradicts property varchar datatype answers incorrect think would help point confusion base terminology emphasized two words quote question example confusion sql server documentation speaks unicode non unicode data talking characters speaking byte sequences represent certain characters primary difference unicode types nchar nvarchar xml deprecated evil ntext non unicode types char varchar deprecated evil text types byte sequences store non unicode types store one several bit encodings unicode types store single bit unicode encoding utf little endian answers mentioned characters stored bit non unicode encoding depends code 4,yes one bit column table storage uses byte bit columns stored byte next free respect also bit per column storage need null bitmap rounded next byte data pages contains bit columns irrespective whether allow null exception nullable columns added later metadata change via alter table row yet updated 0,right click green text showing suggested index select missing index details open new tab ssms index create script 0,file doesnt mean generate bind token first session somehow publish join transaction another session eg spid trancount rollback go begin transaction select sys objects declare bind token varchar255 exec sp getbindtoken bind token output declare bind token bin varbinary128 cast bind token varbinary128 set context info bind token bin rollback another session declare bind token varchar255 select castcontext info varchar255 sys dm exec sessions session id exec sp bindsession bind token go select 0
stored procedure null parameter within clause want use parameter within clause value provided strongly typed dataset trying moment get right,select table1 table1 url like parameter1 table1 id parameter2 parameter3 null table1 id2 parameter3 take look example change clause nested clause specifying initial expression well parameter3 null demand nested expression true parameter3 null 12,ive always fan dynamic sql approach type problem find provides optimal balance complexity versus quality query plan following code define base query whatever would need add filters provided parameter null create procedure dbo getdata parameter1 varchar256 parameter2 varchar256 parameter3 int null begin set nocount declare basequery nvarcharmax nselect dbo table1 paramlist nvarcharmax p1 varchar256 p2 varchar256 p3 int whereclause nvarcharmax parameter1 null begin set whereclause whereclause url p1 end parameter2 null begin set whereclause whereclause id p2 end parameter3 null begin set whereclause whereclause id2 p3 end set basequery basequery whereclause execute sp executesql basequery paramlist p1 parameter1 p2 parameter2 12,pull using mysqldump catch ship data may cost associated shipping data example lets says want rename mydb ourdb step create new database mysql create database ourdb step get schema without triggers mysqldump hrdshost uuser ppassword skip triggers mydb tmp schema sql step get triggers mysqldump hrdshost uuser ppassword skip routines triggers mydb tmp triggers sql step generate script insert select across tables foreign key constraints etl data script tmp datatransfer sql echo etl data script echo set old character set client character set client etl data script echo set old character set results character set results etl data script echo 0,strictly speaking difference input query optimizer two forms input tree iso select name total suminv quantity production product production productinventory inv inv productid productid group name option recompile querytraceon querytraceon input tree iso select name total suminv quantity production product join production productinventory inv inv productid productid group name option recompile querytraceon querytraceon see clause predicate tightly bound join using modern syntax older syntax logical cross join followed relational select row filter query optimizer almost always collapses relational select join optimization meaning two forms likely produce equivalent query plans actual guarantee 0
get max serial according sum corresponding amount table data follows amount need query find maxs according sumamount sumamount need result,select mins select sumamount order running total mytable running total 4,another way works versions prior use recursive cte tested rextester com ct select top amount running total amount tablex order union select amount amount previous total select amount previous total ct running total rn row number order ct join tablex ct ct running total rn select ct running total option maxrecursion limit recursion ctes anchor union alls first leg selects first row iteration ctes recursive part second leg selects next row computes running total sum row number trick subquery overcome limitation top allowed recursive parts ctes 6,short sentence collation defines sorting comparing collation determines rules sql server uses compare sort character data rules language locale aware may also sensitive case accent kana width collation suffixes identify dictionary rule insensitivity cs case sensitive ci case insensitive accent sensitive ai accent insensitive ks kana sensitive binary collations identified suffixes bin binary bin2 binary code point sensitive regards different collations certainly demand workarounds avoid resolve collation conflict errors kill performance due known non sargable expressions dealing different collations nightmare thats recommendation pick one stick references collation hell questions sql server collations shy ask collation mean 0,working linux storing images filesystem database significant better performance see excerpt brad edigers book advanced rails 0
relating executioninstanceguid ssisdb release sql server integration services ssis delivered ssisdb catalog tracks operations packages among things default package execution,much comment trying something msdn page system table catalog executions get execution id bigint unique identifier id instance execution article ssis view connection manager information past executions understand ssis provides new system variable serverexecutionid use inside packages custom logging notifications good variable include direct pointer catalog views use find connection string information catalog executions contains one row per execution filter execution id sample query declare execution id bigint execution id serverexecutionid goes select package name start time end time status emc package path castemc property value varchar1000 connection string catalog executions join catalog event messages em execution id em operation 5,created ssis project using deployment model consisting single package package added ole db connection manager pointed tempdb dropped script task onto canvas also turned explicit logging using ole db connection manager captured oninformation event scr fire info configured script task grab two parameters system executioninstanceguid system serverexecutionid admit point noticed second variable marians answer inside task raise information events get values recorded logged explicit table dbo sysssislog free logging catalog operation messages public void main bool fireagain true string description string empty string variable string empty string value string empty variable system serverexecutionid value dts variables variable value tostring description 9,dont fancy worried scared restarting sql server make sure dont long running transactions best restart sql server using console shutdown command low minimum activity period also called maintenance window minimize impact business dr setup dont want best failover restart passive secondary node clean shutdown sql server occurs scenarios stop sql server using services console shutting server running shutdown command ssms situations sql server cleanly shutsdown databases terminates service involves commiting rolling back transactions writing dirty pages disk writing entry transaction log improper shutdown sql server shutdown nowait pulling power cable server access killing sqlserver exe task manager dirve failure sql 0,agree using auto features best instead running database wide cases per table tuning necessary dont quite agree design choice postgres tie together vacuum analyze seen several instances databases lot insert update little delete never get analyze done start perform badly solution go tables get used heavily subject large queries set auto analyze settings tables something getting analyzed every day get per table settings gui auto vacuum tab see analyze settings set independently vacuum settings end reloptions table seen query select relname reloptions pg class reloptions null sample value agressive analyze might autovacuum enabled trueautovacuum analyze threshold 10autovacuum analyze scale factor 0
two counts single table single query similar questions answers forums think problem simpler two quesries eg select count agent select,use case statement scenario select count total agents sumcase active else end active agents agent sample execution sample data create table agent id int active char10 insert agent active values select count total agents sumcase active else end active agents agent 6,couple others ways addition arulkumars answer sample data create temp table foox values truefalsefalsetruefalse using filter prefer method syntactically simpler sum method work way select count count total count filter count foo count total count row group grouping sets want results pivoted explain analyze select count foo group grouping sets true null count true total rows really want fun false sounds cool 5,one valid reason move another mysql flavour performance latest mariadb versions drop replacemants mysql respectively included major improvements query optimization read site mariadb examples improvements features index condition pushdown cost based choice range vs index merge subquery materialization subquery cache new joins methods like hash join several features affect performance statements subqueries derived tables views details found optimizer feature comparison matrix page additional features like microsecond support time datatypes improvements replication binary logging previous versions also features found mainstream mysql like persistent virtual columns course migration server mysql mariadb examined tested whether features useful databases applications whether difference performance big 0,create master master setup follows create second master masterb masterb acts slave logtable create logtable new innodb run insert logtable new select logtable psuedocode masterb sends replication mastera logtable new mastera finishes syncing swap tables 0
reasonable mark columns one primary key table representing movies fields id pk title genre runtime released tags origin downloads database,id column advantage comes uniqueness want need enforce uniqueness whatever combination attributes never going enforced adding meaningless id advantage shows ever get point youd need new table needs foreign key one case included id use one fk new table dont think free lunch downside approach youll likely find writing joins mere purpose fetching information could perfectly well part new table made 4,table definition looks reasonable columns null unique constraint work expected except typos minor differences spelling may rather common afraid consider horses comment alternative functional unique index option would functional unique index similar dave commented would use uuid data type optimize index size performance cast array text immutable due generic implementation indexing array full text search hence need little helper function declare immutable create replace function movie uuid title text runtime int2 released int2 genres text tags text origin text returns uuid language sql immutable faking immutable select md5 title runtime text released text genres text tags text origin text uuid 4,imagine group friends conversation turns movies someone asks think three musketeers respond one additional information would need absolutely certain thinking movie directors name production studio year released one stars names combination two answer question however would think genre would good candidate one reason genre much subjective criteria three musketeers action drama adventure comedy action adventure romantic comedy often see movie listed different genres even allow multiple genres user may select entirely different one listed actual movie looking even runtimes differ especially theater vcr dvd ray versions need hard objective attributes change one media release another unfortunately exclude name movie movies 6,open transaction almost consequence simple begin transaction wait nothing wait bit longer commit worst hold bytes status values big deal programs actual work within transaction another matter point transaction sure several facts within database true simultaneously despite users writing database concurrently take cannonical example transferring money bank accounts system must ensure source account exists sufficient funds destination account exists debit credit happen neither happens must guarantee transactions happen perhaps even two accounts system ensures taking locks tables concerned locks taken much peoples work see controlled transaction isolation level lot work good chance transactions queued waiting objects hold locks reduce systems 0,query run fairly infrequently example report building table fly probably better1 query run frequently temp table required performance potentially problem table cheap build temp table long database fast enough may get away however need keep eye performance table doesnt totally date subject relatively frequent reporting activity periodic rebuild probably best way go table expensive build needs date may need manage denormalised structure either maintained indexed view triggers rather complicated places additional burden write operations extreme cases large data volumes may need hybrid approach historical data queried denormalised structure optimised performance current data queried live application extreme cases get low latency 0,introduction transactions savepoints scenario creating new user basic process could following begin insert users username password values xxx set userid last insert id insert profile pictures user id profile picture path values userid somewhere filesystem jpg commit whatever reason process end middle would enough issue rollback query somewhere instead commit changes would undone one basic tasks accomplished transactions mentioned savepoints rolando might come handy example one inserts row profile pictures first tries copy image desired location fail one wants undo second insert first copying picture fails reason cancel whole user registration one define savepoint inserting users inserting profile pictures begin 0
use unique key via combinations table fields take look following sqlfiddle http sqlfiddle com dacb5 create table contacts id int,try create table contacts id int auto increment primary key name varchar20 network id int network contact id int unique key network id network contact id 9,change tables definition adding unique key constraint combination two columns create table contacts id int auto increment primary key name varchar20 network id int network contact id int constraint network id contact id unique unique key network id network contact id also check answer bill karwin differences insert ignore replace insert duplicate key update 6,assuming talking data encrypted sql server keys way find columns key name function return name key used encryption particular value return null isnt anything encrypted known key 3rd party simple encrypted knowlegde test every column see contains least one row varbinary value returns key name functionality key name create test database create database test encr go change context use test encr go possible encrypt different rows different keys ill create keys demo create symmetric key create symmetric key symmetrickey1 algorithm aes encryption password password01 go create second key create symmetric key symmetrickey2 algorithm aes encryption password password02 go create table 0,restating question storing users passwords simplest answer still store passwords alternative method use databases built system manage storage method good databases may still better whatever might otherwise avoiding storage really avoiding coding storage also might better compromise database get copies files access tables directly theres little point using brilliant password management system someone access system level table containing passwords hashed passwords also access files directly bother cracking passwords already data like encrypt data easily either user needs able access user level encryption wont work well really seem asking use application level authentication scheme database already one security ignoring possibility could 0
find duration dataset dataset following structure target polltime value null null null null null null null would like build resultset,pointed better solved gaps islands select dense rank partition target order polltime dense rank partition target value order polltime grp schema table select target minpolltime start maxpolltime finish datediffmi minpolltime maxpolltime duration group target grp maxvalue null order start original source code getting status change table 4,sqlserver op commented uses sqlserver query used first tried convert new idea came converted select target polltime value id row number partition target order polltime isnull case value null else end table1 select t1 target startdate t1 polltime enddate coalescet3 polltime t1 polltime t1 value block t1 id sumt2 isnull t1 inner join t2 t1 target t2 target t2 id t1 id left join t3 t1 target t3 target t3 id t1 id group t1 target t1 polltime t1 value t1 id t3 id t3 polltime select target startdate min startdate enddate maxenddate durationmin datediffmi minstartdate maxenddate value null group 4,following alternative sql server r2 solution efficient relatively nulls right supporting indexes available gaps islands method may faster table large minimizing sorting costs nulls form high proportion data sample data required indexes create table dbo table1 target char1 null polltime datetime null value varchar5 null primary key target polltime optional helpful filtered index find null rows required order create unique index fi1 dbo table1 target polltime include value value null sample data insert dbo table1 target polltime value values null null null null null null null solution step find start end dates select t1 target calc startorend calc polltime groupid 4,postgresql different documentation create view documentation create materialized view mention replace keyword seems shortcut aside dropping dependent objects rebuilding one recommend two small things use drop materialized view blabla cascade get list dependent objects drop recreation dependent object one transaction 0,past ive wanted restrict access database use restricted user mode however doesnt work users members db owner role realize recommended role users sometimes end standardized disabling login maintenance windows didnt worry users elevated access example alter login user1 disable dont forget enable done example alter login user1 enable 0,im reaching think least one dangerous scenario restore database filetable files network default specifically sql server could restore virus wont anything course virus doesnt suddenly become sentient users try access file could infected hey said reaching im envisioning scenario outside hacker wants get malware door sends email bob accounting saying heres file sqlserver filetableshare myvirus exe point gone past firewalls without detection internal antivirus anti malware tools 0
delete related records multi key merge sql server suppose something like source table variable values leftid int null rightid int,separate delete operation mind delete dbo mapping exists select values leftid leftid exists select values leftid leftid rightid rightid outline left anti semi join exists pattern often outperform left join null pattern sure overall goal clarity performance judge work better requirements matched source option youll look plans qualitatively plans runtime metrics quantitatively know sure expect merge command protect race conditions would happen multiple independent statements better make sure true changing merge dbo mapping holdlock target dan guzmans blog post personally would without merge unresolved bugs among reasons paul white seems recommend separate dml statements well heres added schema prefix always 6,filter rows need consider target table cte use cte target merge select leftid rightid customvalue mappings exists select values leftid leftid merge using values leftid leftid rightid rightid matched target insert leftid rightid customvalue values leftid rightid customvalue matched update set customvalue customvalue matched source delete 9,try ps ef grep ysql identify process id strace cp pid leave seconds minute tell process spending time could waiting disk seen read write dominate 0,site ive consulted oracle swore toad interface queries dbadmins mostly developers 0
merge output better practice conditional insert select often encounter exists insert situation dan guzmans blog excellent investigation make process threadsafe,using sequence use next value function already default constraint id primary key field generate new id value ahead time generating value first means dont need worry scope identity means dont need either output clause additional select get new value value insert dont even need mess set identity insert takes care part overall situation part handling concurrency issue two processes exact time finding existing row exact string proceeding insert concern avoiding unique constraint violation would occur one way handle types concurrency issues force particular operation single threaded way using application locks work across sessions effective bit heavy handed situation like frequency 8,updated answer response srutzky another albeit minor issue serializable transaction output clause approach output clause present usage sends data back result set result set requires overhead probably sides sql server manage internal cursor app layer manage datareader object simple output parameter given dealing single scalar value assumption high frequency executions extra overhead result set probably adds agree reasons use output parameters prudent mistake use output parameter initial answer lazy revised procedure using output parameter additional optimizations along next value srutzky explains answer create procedure dbo namelookup getset byname vname nvarchar50 vvalueid int output begin set nocount set xact abort set 7,put two lines cnf mysqld general log general log file users ugrad linehanp mydb logfile txt log queries server source php phpmyadmin careful though enabling general log place heavy load server used sparingly short periods debugging documentation available fro disable enable general query log change log file name runtime use global general log general log file system variables set general log disable log enable set general log file specify name log file general log general log synonyms 0,try sql alter authorization schema yourschemaname dbo go drop user theuseryouwanttodelete go cant drop principal schema owner alter authorzation changes owned schema used yourschemaname obviously substitute owned schema database dbo likewise change ownership whatever principal need environment allow drop previously schema owning user example purposes used theuseryouwanttodelete thatll non owner want drop 0
best practices backing mysql db ive recently discovered production web servers run mysql backed regularly im used backing sql server,would recommend setting dedicated replica use backup let perform backup tasks without impacting primary add complexity architecture youll want monitor replication lag ensure everything working actual process couple options without third party tools snapshots taken using mysqldump command assuming youre using innodb mysqldump databases single transaction databases sql depending data size may preferable shut mysql backup data files directly replica restarted replay events primary received duration youre using mysql enterprise mysqlbackup utility incremental backups taken enabling binary log replica obviously records events mutate data youll need combine snapshots 7,best practices take mysql server backup mysql replication setup replication mysql setup master slave server read writes db could go slave server advantage replication take backup slave server without interrupting master server application continue work master without downtime using mysql dump data set small realize small relative term qualify lets say 10gb mysqldump probably work great easy online flexible things mysqldump backup everything certain databases tables backup ddl optimize dump faster restore make resultant sql file compatible rdbmses many things however important options related consistency backup favorite options single transaction option gives consistent backup tables using innodb storage engine non 29,logically identical exists closer antisemijoin youre asking generally preferred also highlights better cant access columns used filter opposed available null values many years ago sql server ish left join quicker hasnt case long time days exists marginally faster biggest impact access join method complete join filtering constructing joined set memory using exists checks row doesnt allocate space columns plus stops looking finds row performance varies bit access general rule thumb exists tends little faster id less inclined say best practice factors involved 0,clear asking believe group one misunderstood concepts sql ill add answer anyhow may may help understanding concept group assume table like create table year int null parameterno int null mark int null primary key year parameterno insert year parameterno mark values would select year summark group parameterno mean grouping parameterno means two groups apply aggregate function sum year come play could mean group group hardly useful result another possibility randomly pick one row group like group group results unpredictable may get different result data query sql92 requires columns select clause part group clause want select year summark would add least 0
figuring physical size group tables sql server group tables want know physical size disk tables plus indexes easier way gui,use sp spaceused stored procedure example execute sp spaceused person person execute sp spaceused person address gives hope youre looking 8,looking get size information tables database use query select type desc indexsize obj name name index name indexsize reserved mb indexsize used mb indexsize row count indexsize object id indexsize index id select quotenameobject schema nameddps object id quotenameobject nameddps object id obj name sumddps reserved page count reserved mb sumddps used page count used mb sumrow count row count ddps object id ddps index id sys dm db partition stats ddps group ddps object idddps index id indexsize join sys objects indexsize object id object id join sys indexes indexsize object id object id indexsize index id index id 9,alternative horse names solution simplest option use row array comparision array select city cities temps see sqlfiddle necessary unnest array work arrays well rowsets unlike using array operators operation benefit use index tree gin temps normalized design splits temps separate table foreign key reference cities may actually faster lots cities lots samples earlier suggested may want use gin array index array operators mistaken support desired operation made error testing made appear frequent updates id normalize another table btree index table theres small amount data wouldnt bother indexes id use temps 0,building temporary indexes etl jobs necessarily bad practice index builds fairly quick might efficient relatively small incremental updates large tables sounds like case caveat expect tables grow substantially time work tables etl may well ok tables fact tables accumulate large data volumes next years index rebuilds may get slower time staging data dropping indexes loading make bulk loads staging much quicker may well need add indexes staging tables order support queries supplying etl process 0
sql server implementation longest common substring problem sql server implementation longest common substring problem solution checks rows column sql server,done rather easily sqlclr user defined aggregate uda aggregate operates set rows would able operate rows subset based condition optional group wanting operate separate sets rows whether depends planning result one project research wont repeated probably best make small console app read rows process accordingly however need use returned value within database centered process sqlclr fine assuming follow two recommendations mentioned following tricks used reduce memory usage implementation pseudocode section need find creative way dealing situation multiple results considered longest common substring common substrings tie first place using example wikipedia page shows matches strings abab baba returns bab aba perhaps 7,solomon probably right clr solution shows heres sql version play create test data sql creates 470k rows test duplicates object idtempdb strings null drop table strings select name string lena name stringlength strings sys columns sys columns name like refer set nocount nulls mean longest common string existsselect strings string null begin return end need know number rows sample length shortest string longest common substring longer shortest string set declare totalrows int declare minlen tinyint declare result varchar50 select minlen minstringlength totalrows countdistinct string strings raiserrornmaximum possible length total distinct rows d00 minlen totalrows nowait check backwards longest possible string 4,one mentioned thus suggestion take look massively sharded mysql solutions example see highly regarded tumblr presentation concept instead one extra large database use many small ones holding parts original data thus scale horizontally instead trying improve vertical performance googles bigtable gfs also using cheap horizontally scalable nodes store query petabytes data however troubles need run queries different shards anyone interested made hello world sharding application ago discussed blog post used ravendb details irrelevant idea 0,need pass refcursor procedure use output parameter quick procedure test create replace procedure passenger details passenger details sys refcursor begin open passenger details select test test full name age alien nationality foo category name airline name wobble class type dual end passenger details test sql plus sql variable mycursor refcursor sql exec passenger details mycursor pl sql procedure successfully completed sql print mycursor full name age natio cat airl class test test alien foo name wobble sql 0
whether create separate tables different product types im process designing database im second thoughts initial design decisions product types follows,would suggest start correct relational model option typical usage model leads toward denormalising areas dont afraid discussing colleague last week schema designs often considered something set stone ever change strange considering refactoring accepted practice every layer application refactoring database schema still viewed impractical interface database well designed theres nothing stopping adapting schema learn systems usage patterns 7,design decision would probably go option modified option first option one thing like clarity product table affords one big table field determine type relation isnt clear another indexing strategy would always require type field listed since types index cardinality extremely low select product table type basically full table scan anyway option create parent table holds columns types share create product type table individual columns one extra link parent table create link table product option model option etc links respective keys reciprocal links model option option model go ahead create tables well add clarity joins anyone looking downside complexity making sure 8,92k 4m still records id suggest first option throw want deleted database let database sort give database chances itll efficiently parcel time id recommend chunking application time critical youre worried slowing case solution hours taking records table id recommend dropping indexes delete recreating indexes followed optimize table records table index rebuild overhead isnt worth faster keep indexes date deleting records said youve done delete running optimize table might bad idea dont peak hours though 0,appears greatest per group problem get maximum seats per engine results select engines maxseats max seats planes group engines using derived table join back source get rows matching maximums select engines manufacturer model es max seats planes inner join select engines maxseats max seats planes group engines es engines es engines seats es max seats 0
encryption data log backup files using always encrypted sql server implement always encrypted feature sql server mdf files ldf files,whole entire data log file doesnt get encrypted specific fields encrypt encrypted yes encrypted data log backup files sql server never sees unencrypted values docs always encrypted microsoft explains always encrypted allows clients encrypt sensitive data inside client applications never reveal encryption keys database engine sql database sql server isnt great wording means data encrypted sql server engine doesnt even key decrypt thus name far sql server engine concerned data always encrypted 4,documentation states opening paragraph driver encrypts data sensitive columns passing data database engine automatically rewrites queries semantics application preserved similarly driver transparently decrypts data stored encrypted database columns contained query results implied encrypted columns encrypted therefore database logs backups entirely encrypted setup always encrypted specify columns encrypted data columns encrypted data encrypted prior sent sql server data encrypted inside data file log file inside backups taken database data columns encrypted always encrypted visible plain text inside mdf ldf perhaps compressed inside backup files database transparent database encryption encrypt entire database mdf ldf backups taken database entirely encrypted sql server instance 4,possible restore sql server database sql server without restoring r2 first answer aaron bertrand details alternate options get data sql database https dba stackexchange com 0,norecovery specifies roll back occur allows roll forward continue next statement sequence case restore sequence restore backups roll forward recovery default indicates roll back performed roll forward completed current backup recovering database requires entire set data restored roll forward set consistent database roll forward set rolled forward far enough consistent database recovery specified database engine issues error source restore sequence understanding norecovery recovery 0
lots indexes mysql vs mongodb migrating ive never used mongodb read lot think going good project also lots experience mysql,given description strongly suggest using mongodb would necessarily bad choice although believe case reasons pure technical ones points caught eye data modeling trying use mongodb relational data model adaptations almost always leads tears misery except trivial use cases better end story worse end loosing money potentially big time reason sql identify entities attributes relations bang head wall hours get upper left beyond joins right get questions derived use cases answered avoiding data redundancy like devil holy water data modeling mongodb works different identify use cases questions derived model data way questions answered efficient way since bit abstract let give example 7,deem multiple important aspects need consider deciding tool going employ develop project primary objective manage pertinent data quite valuable organizational asset reliable manner achieve said objective way technical means supported sound theory regard worth mentioning success determined database depend database management system dbms choice also number factors logical model physical implementation settings qualified administration since considering sql platform tentative dbms fact suggests intention implement relational database focus respect throughout present answer although dr codd turing award recipient published seminal paper relational model large shared data banks back really consider exceptional work remains unparalleled state art solidly based first order logic 5,index used optimize group order uses different columns sorting use index index would help database would able read rows table sort order collate nocase index help use different collation query add normal index use group seriesname collate nocase allowed using offset clause pagination efficient database still group sort rows begin stepping better use scrolling cursor note guarantee dbid dlstate values come specific row sqlite allows non aggregated columns aggregated query bug compatibility mysql 0,one time another ive worked databases mention unfortunately found doesnt take long syntax deviate flavours anything simplest select insert update delete gets categories suggest quickly get vendor specific ive always pretty much port sql one platform another although going back years amazed different sql server teradata sql even update joins 0
sql server updating fields huge table small chunks get progress status large 100million row table need update couple fields log,answering second part print output loop long running maintenance procedures sys admin sometimes run run ssms also noticed print statement shown ssms whole procedure finishes im using raiserror low severity declare vartemp nvarchar32 set vartemp convertnvarchar32 getdate raiserror nyour message current time vartemp nowait im using sql server standard ssms complete working example run ssms declare varcount int declare vartemp nvarchar32 varcount begin set vartemp convertnvarchar32 getdate raiserror nyour message current time vartemp nowait print vartemp waitfor delay set varcount varcount end comment raiserror leave print messages messages tab ssms appear whole batch finished seconds comment print use raiserror messages 4,aware question answered related question explicit transactions needed loop sake completeness address issue part suggestion linked answer since suggesting schedule via sql agent job million rows dont think form sending status messages client ssms ideal though ever need projects agree vladimir using raiserror nowait way go particular case would create status table updated per loop number rows updated thus far doesnt hurt throw current time heart beat process given want able cancel restart process weary wrapping update main table update status table explicit transaction however feel status table ever sync due cancelling easy refresh current value simply updating manually count 12,note tested server lying around preeeettty sure though work though caution noted comments erny note high cpu load due operations may expected pretty much time using temporary tablespace time form exclusive locks table vacuuming happen client queries simply wait lock acquired access table question dont need close existing connections one thing aware though moving table vacuum full need wait exclusive lock first first obviously need additional storage st phane mentions comments needs least twice big table question vacuum full full copy lucky dynamically add disk machine worst case attach usb disk risky slow though next mount new device make available 0,typical user select insert delete update create temporary tables execute first four pretty obvious though may also set read users select create temporary also handy typically harmless temporary tables help optimizing queries breaking smaller faster parts limited executing connection automatically dropped closed execute depends type system stored routines would like users access make sure also know security definer invoker definition stored routines case make sure apply specific schemas avoid using grant select insert update delete user host also grants privileges mysql system tables effectively allowing user create new accounts upgrade set privileges instead grant select insert update delete schema user 0
retrieving row count without using count function wrote query shoots syntax error would select maxrow select row number overorder id,actually three problems query first maxrow return string row second subquery needs alias try like select maxrow select row number overorder id desc row users userquery third problem count much better way expertly described answer gbn note also row reserved keywords list avoided well 13,theres another variant without scanning table using system tables select si rowcnt sys objects join sysindexes si si id object id type desc user table si indid heap clustered index name users compare plans durations time youll see faster variant 6,want get exact count rows efficient manner count ansi standard look scalar expressions states count give row count table intended optimised start count specified result cardinality row number function isnt practical option isnt counting function row number run badly add rows show bad sum1 may optimised count internally id never use rowcount require rows returned first select huge unnecessary overhead live approximate sql server use sys dm db partition stats marians answer date since sql server added dmvs select total rows sumst row count sys dm db partition stats st object nameobject id mytable index id see info https stackoverflow 21,assumptions clarifications need differentiate infinity open upper bound upperrange null either way simpler way null vs infinity postgresql range types since date discrete type ranges default bounds per documentation built range types int4range int8range daterange use canonical form includes lower bound excludes upper bound types like tsrange would enforce possible preventing adjacent overlapping entries exclude postgresql solution pure sql ctes clarity select range coalescelowerrange infinity startdate maxcoalesceupperrange infinity order range enddate test select lagenddate order range startdate null step select countstep order range grp select daterangeminstartdate maxenddate range group grp order subqueries faster less easy read select daterangeminstartdate maxenddate range 0,actually wrote article type calculation basically use following code find 3rd friday month date range use tempdb set nocount object iddbo null drop table dbo create table date datetime year smallint quarter tinyint month tinyint day smallint 1st 366th day year week tinyint 1st 54th week year monthly week tinyint 1st 2nd 3rd 4th 5th week month week day tinyint mon tue wed thu fri sat sun go use tempdb populate table day week defined mon tue wed thu5 fri sat sun c0 select values11 dc c1 select c0 cross join c0 c2 select c1 cross join c1 c3 select 0,quoting brent ozars article sql server virtualization question regarding virtualization san recommendation setting block size sql server related settings including ntfs allocation unit size raid stripe size partition offset check san vendor documentation see right cases oltp databases decent shape 64k ntfs allocation unit size raid stripe size 1mb partition offset yes default youll safe 64k block size see also storage documentation maybe specify preferable unit database server great information blog san storage best practices sql server 0
switch schemabinding view without recreating switch schemabinding view without recreating,yes good use schemabinding always sometimes remove change dependent object alter view alter view myview remove schemabinding select go 11,wont alter view allow get done create view would create view schemabinding select stmt go lose clause alter view viewname select stmt go see alter view msdn 8,looking around hours created stored proc hope helps someone create procedure viewremoveschemabinding viewname varcharmax begin declare positionshemabinding int declare command nvarcharmax select command object definitionobject id viewname set positionshemabinding charindexwith schemabinding command positionshemabinding begin schema binding present lets remove set command stuff command charindexwith schemabinding command lenwith schemabinding set command replace command create view alter view execute sp executesql command end end put schemabinding create procedure viewaddschemabinding viewname varcharmax begin declare positionshemabinding int declare command nvarcharmax declare objectname varcharmax select command object definitionobject id viewname objectname object nameobject id viewname set positionshemabinding patindex schemabinding command positionshemabinding begin schema binding present 4,state codes meaning account locked user id valid undocumented user id valid undocumented login used disabled incorrect password invalid password related sql login bound windows domain password policy enforcement see kb925744 login valid server access failed login valid permissioned use target database password expired initial database could found login valid database unavailable login permissioned detailed information available aaron bertrands blog 0,foreign key cant made conditional question business rule appears employee work one one physical store given super type store two sub types suggested physical online physical store may staffed one employees employee must assigned one one physical store physical stores two sub types brick mortar kiosk three direct sub types kiosk online brick mortar hides property possessed every store whether found physical location design relies human understand semantics inherent sub type names understand online stores dont employees readily apparent declared schema code form trigger must written express understanding way dbms enforce developing testing maintaining trigger impact performance much difficult solution 0,use join create populate new table one go select dbo newtable dbo tablewithidentity left join dbo tablewithidentity condition right side matches thus prevent duplication left side rows outer join left side rows eliminated either finally join identity property eliminated selecting left side columns therefore produce exact copy dbo tablewithidentity data wise identity property stripped said max vernon raised valid point comment worth keeping mind look execution plan query notice source table mentioned execution plan instance eliminated optimiser optimiser correctly establish right side join needed plan reasonable expect future version sql server may able figure identity property need removed either since 0
sql server agent jobs availability groups im looking best practice dealing scheduled sql server agent jobs sql server availability groups,within sql server agent job conditional logic test current instance serving particular role looking availability group select ars role desc sys dm hadr availability replica states ars inner join sys availability groups ag ars group id ag group id ag name youravailabilitygroupname ars local primary begin server primary replica something end else begin server primary replica optional something end pull current role local replica primary role whatever job needs primary replica else block optional handle possible logic local replica isnt primary course change youravailabilitygroupname query actual availability group name dont confuse availability groups failover cluster instances whether instance primary secondary 40,im aware two concepts accomplish prerequisite based thomas stringers answer created two functions master db two servers create function dbo svf agreplicastate availability group name sysname returns bit begin exists select ag name sys dm hadr availability replica states ars inner join sys availability groups ag ars group id ag group id ars local ars role desc primary ag name availability group name return return end go create function dbo svf dbreplicastate database name sysname returns bit begin exists select adc database name sys dm hadr availability replica states ars inner join sys availability databases cluster adc ars group id 9,rather per job basis checking every job state server deciding continue ive created job running servers check see state server primary enable job step targeting database ag server secondary disable job targeting database ag approach provides number things works servers databases ag mix dbs ags anyone create new job worry whether db ag although remember add job server allows job failure email remains useful jobs failure emails right viewing history job actually get see whether job actually ran something primary rather seeing long list success actually didnt run anything secondary script checks database field proc executed every mins server added 14,yes nested loop join run bottom inner input row outer input using examples data change query full outer join without join condition machine result plan merge join select full outer join 0,big roughly kind drives behind laptops 5400rpm drives database tiny incurring lot autogrow events suspect kind configuration problem ran following code seconds win7 vm 7200rpm drives create table dbo test1 id int identity11 text varchar2000 timestamp datetime null defaultgetdate go insert dbo test1 text select sure long text supposed go questions machine vm application connecting via tcp ip shared memory named pipes activity going box triggers table resource governor implemented etc checked sys dm exec requests profiler see long individual inserts actually taking else sending server blocking waiting might occurring 0,purpose subquery understand select rows latest related entry billing pricequotestatus qualifying name incorrect 2nd query immediately clear modification didnt produce equivalent output 1st query picks latest row billing pricequotestatus checks whether name qualifies name adjustmentpaymentbillable 2nd query backwards check row qualifying name last one also doesnt make sense compute aggregate exists semi join dont want equivalent consequently get rows 2nd query incorrect time range predicate mess inefficient possibly incorrect least ticking bomb pq date applied time zone pst 02t00 timestamp 03t22 timestamptz column date applied type timestamptz construct time zone pst converts type timestamp shifts time offset hard coded time 0
set get custom database variables using pgadmin iii right click database navigate variables tab put variable name value property database,add variable end postgresql conf like customized options custom variable classes general list custom variable class names general application version v1 restarted add general application version manually pgadmin show drop least otherwise use like postgresql variable update version newer postgresql onwards dont set custom variable classes anymore one set whatever variable want limitation seems still two parts set something bla error unrecognized configuration parameter something set thing something bla set guess avoid collision builtin parameters 8,add dezsos answer variables changed select set configclass name value valid transaction boolean read select current settingclass name see link info http www postgresql org docs static functions admin html 8,ideally would create database load bit sample data measure size extrapolate far accurate method estimating size database years want compute database size would generally start figuring many rows fit single block simplicity well assume rows never deleted updates never change size row well also assume compression used otherwise things get bit complicated calculate size data row fixed size data types date char thats size type variable size data types number varchar2 thats average size data column couple bytes additional overhead pretty safely ignore theyre going swamped errors estimating size actual data subsequent estimate number rows per block expect row bytes 0,depends add column require adding data rows quite quick example adding int char requires physical row movements adding nullable varchar default shouldnt unless null bitmap needs expand need try restored copy production get estimate creating new table copying renaming may take longer add indexes keys billion row table changed billion row tables took second add nullable column say take backup first 0
mysql benchmarking tools ive heard long time ago tool helps tweek mysql settings better performance cant seam find aware use,benchmarking tuning tool imho theres tool specific latter unless super generic usage need identify usage pattern tune database hosts accommodate youre write heavy different configuration read heavy scenario bottom line tuning follows applications usage benchmarking use sysbench heres example blog added heres beef config tools changes versions vs vs lot tweaks dont jive really need savvy dba evaluate whats going host load storage traffic application specific requirements theres lot go optimal configuration tool may get part way may leave something include something may cause failure buffers flushing plugins threading config tool may provide false confidence implementing correct config 6,think monyog handle request monyog mysql monitor advisor mysql dba box helps mysql dbas manage mysql servers tune mysql servers fix problems mysql database applications monyog finds problem sql monitors advisors well suggests parameter use mysql system variables editing cnf ini file helps fine tune mysql server many features dba would like day day activites like trend report dashboard server config management snmp smtp alerts etc details refer http www webyog com en monyog feature list php 4,correct different grains must mixed fact table reserve balance end month sum payments end month grain one facts semi additive type fact additive define tables grain describing see grain monthly claim snapshot makes fact table periodic snapshot fact table article kimball example additive semi additive facts fact table example periodic snapshot semi additive facts data warehouse toolkit page best practice transactional fact table reflect every change reserve payments adjustments lowest atomic level deal claims often atomic level claim sub claim insurance company may term generally sub claim represent different party claim payments reserves party example may payments insured payments insured 0,start sql instance minimal configuration mode net start mssqlserver connect instance cmd window change max memory setting sqlcmd server instance see prompt looks like default maximum memory setting may available viewing changing modify behavior use query sp configure show advanced options go reconfigure go sp configure max server memory go reconfigure go restart instance configuration manager 0
sql server changes execution plan part previous question sql server changes execution plan using sql server developer edition sql server,likely explanation sessions different settings sql server various session settings affect execution plan selected results values settings depend connect sql server since different tools set options different ways connect like sql server management studio allow override defaults well example image reproduced erland sommarskogs definitive article topic slow application fast ssms understanding performance mysteries whole thing well worth reading definitely read section titled default settings make sure settings value connections get execution plans maximum compatibility features like indexed views ensure settings follows many settings maintained backward compatibility strongly recommended set shown table use tool sets right way automatically books online references 11,discount difference mvc app checked query executing using sql profiler similar problem recently turns query executed mvc app using entity framework executing sql statement sp executesql causing sql server use different execution plan compared running pure sql management studio changed use stored procedure rather linq 4,would instead recommend official method reproduce convenience change number size innodb log files mysql earlier use following instructions procedure use depends value innodb fast shutdown determines whether bring system tablespace fully date shutdown operation innodb fast shutdown set stop mysql server make sure shuts without errors ensure information outstanding transactions redo log copy old redo log files safe place case something went wrong shutdown need recover tablespace delete old log files log file directory edit cnf change log file configuration start mysql server mysqld sees innodb log files exist startup creates new ones innodb fast shutdown set set innodb fast 0,let start saying hate alter evil imho say current table schema create table table love id int unsigned null auto increment value varchar40 date created date primary keyid engine myisam charset utf8 heres path recommend create new table object replace old one create table table love new id int unsigned null auto increment value varchar40 date created date primary keyid engine innodb charset utf8 insert rows old table name new table insert table love new idmy valuedate created select idmy valuedate created table love smoke test migration select count table love new select count table love select ida valuea date 0
nested view good database design read somewhere long time ago book states allow nested view sql server sure reason cant,sometimes nested views used prevent repeating aggregates lets say view counts messages groups userid might view counts number users messages kind thing effective base view indexed view dont necessarily want create yet another indexed view represent data slightly different grouping youre paying index maintenance twice performance probably adequate original view nested views youre select changing ordering top seems would better encapsulated stored procedure parameters inline table valued functions bunch nested views imho 26,regardless platform following remarks apply nested views harder understand debug table column view column refer lem dig levels view definitions make harder query optimizer come efficient query plan see anecdotal evidence compare shows optimizer often smart enough correctly unpack nested views select optimal plan without compilation cost measure performance cost comparing view query equivalent one written base tables hand nested views let centralize reuse aggregations business rules abstract away underlying structure say database developers ive found rarely necessary example using nested views centralize reuse certain business definitions eligible student valid use nested views maintaining tuning database weigh cost keeping removing 47,later versions sql seem better optimizing use views views best consolidating business rules eg work telecom product database product assigned rateplan rateplan get swapped rates rateplan get activated deacitvated rates increased modified make easy make nested views 1st view joins rateplans rates using whatever tables needed returning necessary data next levels views would need 2nd views isolate active rateplans active rates customer rates employee rates employee discount business vs residential customer rates rateplans get complicated point foundation view ensures overall business logic rateplans rates joined together properly one location next layer views give us focus specific rateplans types active inactive 7,real issue nested views real issue proliferation nested views developers layer additional tweaks existing views found queries nested view layers actually joined one views definition tendency take easy way rather analyze solve problem root issue 4,paul white explained excellent lucid manner reason behind sql server behaviour running servers memory also huge thanks swasheck first spotting issue opened case microsoft suggested problem resolved using trace flag t2335 startup parameter kb2413549 using large amounts memory result inefficient plan sql server describes details trace flag cause sql server generate plan conservative terms memory consumption executing query limit much memory sql server use memory configured sql server still used data cache query execution consumers please ensure thoroughly test option rolling production environment 0,found answer yesterday help friend mine log via ssms user windows login attempting use delete old login add windows login able transfer ownership job properly sql able get user data windows right world 0,dont put nulls warehouse marts warehouse well normalized least bcnf therefore exclude nulls nulls might preserved staging tables exist data sources shouldnt needed warehouse marts designed support presentation tools user queries nulls get way things never displayed make user queries complex error prone especially foreign key columns frequently subject joins 0,database id indicated error message database affected tempdb one method fixing type corruption tempdb simply restart sql server instance database ids follow recommendations may need restore backup however try member sysadmin role execute dbcc page look metadata indexid value heap clustered index need restore backup rebuild non clustered index run dbcc command let us know find check https www mssqltips com sqlservertip using dbcc page examine sql server table index data details dbcc page command 0
ssis purpose flagging package entry point package visual studio designer right click ssis package designate entry point package search found,theres one behaviour think packages entry point package set executed catalog create execution see http msdn microsoft com en us library ff878034 aspx 8,side note default scope parameters configuration page integration services catalogs called entry point packages project probably want disable flag child packages parameters doesnt show clutter configuration dialog 6,using sql server express addition answers trying connect using port ensure use comma syntax colon syntax myservername1433 instancename wrestling couple hours following suggestions still connecting used instead connected right away sql server management studio remotely first step success 0,daniels answer focuses cost reading individual rows context putting fixed size null columns first table helps little putting relevant columns first ones query helps little minimizing padding due data alignment playing alignment tetris columns help little important effect mentioned yet especially big tables additional columns obviously make row cover disk space fewer rows fit one data page kb default individual rows spread pages database engine generally fetch whole pages individual rows matters little whether individual rows somewhat smaller bigger long number pages read query fetches relatively small portion big table rows spread less randomly whole table supported index result roughly 0
sql join syntax ms sql taught mssql classes join two tables select firsttable join secondtable id id professional life came,queries second type fall call sql antipattern check nice book written bill karwin second query almost resembles cartesian join whose clause evaluated fly first cleaner order execution better managed could compare ways getting explain plan seeing execution time creating third type refactors query get explain plan running time see indexes evaulated three query styles better going join syntax generate result sets left joins right joins may far different maybe desirable inner joins 5,posting good explanation differences ansi sql complaince queries produce result find always good idea explicitly state joins much easier understand especially queries contain non join related evaluations clause explicitly stating joins also saves inadvertently producing cartesian product rolandomysqlsba alluded 2nd query whatever reason forgot include clause query would run without join conditions return result set every row firsttable matched every row secondtable addition returning want make mistake like large tables hundreds thousands even millions rows cause performance issues database attempts fulfill query 5,developer centric dba centric ive heard good things rhino etl https github com ayende rhino etl 0,recovered mysql ibd frm files using mysql utilites mariadb generating create sqls get create sqls frm file must use https dev mysql com doc mysql utilities en mysqlfrm html shell mysqlfrm server root pass localhost t1 frm port way may create sqls create tables create tables database alter table xxx discard tablespace discard tables want replace ibd files copy ibd files mysql mariadb mariadbs data path first try use mysql restrore database crashes immediately stops tablespace id broken error error hy000 got error storage engine used mariadb succesfully recovered data alter table xxx import tablespace run statement mariadb warns file 0
identify blocking send alert need create alert notify query blocked seconds example someone transaction open table forgets run commit rollback,sure easier way built one way would first configure blocked process threshold seconds sp configure show advanced options go reconfigure go sp configure blocked process threshold go reconfigure go set event notification blocked process report see answer example code event notifications activation procedure could use sp send dbmail send email 4,liked martins suggestion using event notifications followed example link put together server youll need either put email address call sp send dbmail modify procedure read email addresses config table sort also adjust blocked process threshold liking result concise report message info blocked blocking processes error handling really basic ends conversation receives error message end dialog message make sure database mail configured system allow use sp send dbmail use msdb go exec sp configure show advanced options reconfigure go exec sp configure blocked process threshold reconfigure go create queue blockedprocessqueue create service blockedprocessservice queue blockedprocessqueue http schemas microsoft com sql notifications 7,please look architecture innodb picture percona cto vadim tkachenko rows deleting written undo logs file ibdata1 growing right duration delete according mysqlperformanceblog coms reasons run away main innodb tablespace lots transactional changes long transactions lagging purge thread case reason would occupy one rollback segment along undo space since deleting rows rows must sit ibdata1 delete finished space logically discarded diskspace shrink back need kill delete right kill delete query rollback deleted rows instead create table tablename new like tablename insert tablename new select tablename columnname like rename table tablename tablename old tablename new tablename drop table tablename old could done 0,unfortunately optimize views like one time trick would use query divide two non overlaping queries use union merge results optimizer would choose one resultset another condition isbig specified otherwise returns everything obviously harder mantain select stateabbreviation isbig tblstates stateabbreviation ak tx union select stateabbreviation isbig tblstates stateabbreviation ak tx 0
tell insert certain table slow know insert sql table slow number reasons existence insert triggers table lots enforced constraints checked,things look reduce batch size something smaller like didnt say large row size try turning io stats see much io fk lookups taking waiting caused insert happening master dbo sysprocesses lets start see go 10,brad examine wait stats query sql2000 could use dbcc sqlperf waitstats syntax get details 7,try using set statistics io set statistics profile statistics io useful telling tables amount table scans logical reads physical reads use three focus part query plan needs tuning statistics profile primarily return query plan tabular format look io cpu columns costing amount query table scan temp table vs sort insert clustered key etc 5,say looking analyzing performance query maybe helps analyze query execution plan check index scans table scans usage convert implicit functions sql data types parallelism run query set statistics io set statistics time see execution time read write io insert check waittime sysprocesses session spid run profiler select standard template select following performance statistics repeated plan compiled many times good rpc completed sql batchcompleted sql batchstarting add column rowcounts see exactly number rows batch filter results see query last collect page life expectancy counter windows perfmon min sql low memory also collect disk counters disk queue length disk time data files 6,single table use sp spaceused mytable tables database use sp msforeachtable follwoing create table temp table name sysname row count int reserved size varchar50 data size varchar50 index size varchar50 unused size varchar50 set nocount insert temp exec sp msforeachtable sp spaceused select table name row count count col count data size temp inner join information schema columns table name collate database default table name collate database default group table name row count data size order castreplacea data size kb integer desc drop table temp 0,aside using third party tool run problem lot deal sqlserver2005 doesnt support meta data however suggest cos used bit loved redgates sqlprompt product incredibly handy however cost money theres tradeoff someone workaround would love hear 0,based context previous question sql query combinations without repitition think looking way find combinations users include name id result set following script demonstrates one way achieve sample data declare users table userid integer username nvarchar50 insert users userid username values ntom nann ndina nmark load source data table set make easier find combinations working table find combinations declare combination table item id tinyint identity11 primary key nonclustered item nvarchar500 null item value integer null bit value convert integer power2 item id persisted unique clustered add user details working table insert combination item item value select username userid users use fact 0,nice ear author kalen suspects enforcement sort minimum row length anything padded course cases possible find phantom byte tinyint bit well varchar1 char1 wont increase beyond move smallint char2 increase move say char3 essentially point efficiencies gain choosing data types wisely point edge cases rules dont hold due factors storage layer edit hope concrete information wanted let know author internals book currently thinks shes certain 0
make sqlcmd return errorlevel sql script fails im running sqlcmd batch file wondering make return errorlevel something goes wrong backup,msdn sqlcmd utility page http msdn microsoft com en us library ms162773 aspx raiserror used within sqlcmd script state raised sqlcmd quit return message id back client example raiserror50001 could wrap backup database command try catch block raise appropriate error message sqlcmd would return batch file instance consider following errorleveltest sql file error exit begin try creates error backup restore operations allowed database tempdb backup database tempdb end try begin catch declare msg nvarchar255 set msg error occurred error message raiserror end catch following bat file first line wrapped readability needs single line echo program files microsoft sql server tools 10,use option sqlcmd specifies sqlcmd exits returns dos errorlevel value error occurs value returned dos errorlevel variable sql server error message severity level greater otherwise value returned http msdn microsoft com en us library ms162773 aspx 55,tried backup restore make backup sql server use restore db r2 instance end restore upgraded automatically different sbs cant make backup able transfer also logins could get help following kb articles transfer logins passwords instances sql server transfer logins passwords instances sql server sql server ps posted initially comment never saw sbs close enough 0,4th way use except aka minus rdbms supports give execution plan exists declare nodatewanted date select id nodatewanted tarih kimlik except select id nodatewanted siparis tarih nodatewanted youll add date filter constant check rows row pull data course 0
merge two select queries different clauses one table services need merge two select queries different clauses example select regn region,treat two current queries resultsets tables join select firstset region firstset openservices firstset dfc secondset closedyesterday select regn region countcallid openservices sumcase descrption like dfc else end dfc oscl status group regn order openservices desc firstset inner join select regn region countcallid closedyesterday oscl datediffday closedate getdate group regn order closedyesterday desc secondset firstset region secondset region order firstset region prettiest bit sql ive ever written hopefully youll see works understand maintain suspect better performing query would single select oscl grouped regn three counters separate sumcase statements akin currently dfc single table scan depending indexes schema 15,building michaels suggestion select regn region sumcase status else end openservices sumcase status description like dfc else end dfc sumcase datediffday closedate getdate else end closedyesterday oscl group regn order openservices desc 6,ibm db2 would use following select tabschematabname syscat columns colname column name note db2 column names upper case unless defined inside double quotes something upper case supply exact casing column name well 0,want know best convenient way migrate whole data includes security permission users memberships databases best method migrate data objects related particular database using backup restore method perhaps since want migrate whole database new instance backup user system databases restore new sql server instance backup database current server copy new server restore backup system database restore new instance backup restore system databases first backup restore system databases backing restoring system databases would provide logins jobs packages stored msdb new instance new server would also include complete securables permissions sql server mainstream support highly advisable upgrade latest sql server version stay make 0
using identity column increment need run sql server logging db main tables seperate datacentres writing time idea restoring db new,wont cause problems sql server lets create table decrement id integer identity0 test int insert decrement test select number numbers select top id test decrement order id asc go id test good idea long term might different problem others may end confused ie think order query upside normal happens someone else restores database reseeds identity normal way youve got overlapping ids possible modify schema site column use site id composite key 7,problems may rise setting following link martin smiths comment negative values identity column may cause issues applications database designers make identity columns start min value rather another issue related values negative decreasing identity also clustered key table tree structures efficient traversed left right lower higher values inserts done right higher side key ever increasing property matters ever clustered key table see blog post kimberly tripp best properties clustered keys especially ever increasing decreasing key inserting data always wrong left side index causing fragmentation index effects may critical case think mind identity also chosen clustered key martin suggests clustered index also 7,going backwards feels wrong two data centers could also implement identity ranges unless cycle identity values alarming rate reason cant data center create table dbo table id int identity11 primary key data center create table dbo table id int identity10000000001 primary key would allow generate billion well values data center danger collision data center could add check constraint data center prevent overlapping values depending prioritize errors vs duplicates could also implement recurring job periodically checks close lower bound data center youre concerned youll really generate billion values either data center apps lifetime never mind billion enough two alternatives give room 9,right click database tasks look export data selected database preconfigured data source next screen define destination change flat file destination go later use template ssis need often 0,dont put nulls warehouse marts warehouse well normalized least bcnf therefore exclude nulls nulls might preserved staging tables exist data sources shouldnt needed warehouse marts designed support presentation tools user queries nulls get way things never displayed make user queries complex error prone especially foreign key columns frequently subject joins 0,definitely create non clustered index update metadata tested sql really tested production system create table int identity11 null int null constraint primary key clustered asc go create nonclustered index nc asc go insert values fun part dbcc ind give us database pages table non clustered index stored find pagepid indexid pagetype following dbcc traceon3604 sure allowed dbcc page pagepid tableresults notice null bitmap header lets alter table alter column int null really impatient try run dbcc page command fail lets check allocation dbcc ind page moved magic changing nullability column affect storage non clustered indexes cover column metadata needs updated 0
mysql return json standard sql query read json objects json object type want select return json necessarily want store json,took bit figuring used postgresql things much easier consult fine manual functions create json values theres json array function much use really least case answer question select return json two ways rather painful either use hack see db fiddle use one new mysql supplied json functions ironically appears even hack hack mysql fiddle answers use mysql group concat function post helped might want set group concat max len system variable bit default paltry first query imagine messy ddl dml bottom answer select concat better result best result select group concat json separator better result select concat name field name field 10,converting row json doesnt sound like want aggregate json state want equivalent row json suggest checking much simpler json object select json object name field name field address field address field contact age contact age contact aggregating json side note need aggregate resultset json upcoming mysql json arrayagg return result set single json array json objectagg return result set single json object 14,sql server index scan since thinks cheaper seeking required row likely sql server correct given choices setup aware sql server may actually range scan index opposed scanning entire index provide ddl tables along indexes may may able help make much less resource intensive side note never ever use date literals like instead declare orderstartdate datetime2 feb declare orderenddate datetime2 feb use declare orderstartdate datetime2 27t00 declare orderenddate datetime2 28t00 aarons post may help clarify 0,key preserved means key value goes table giving counter examples may help understand concept better example1 view contains aggregation suppose following view structure groupid averagesalary example values comes one rows try update averagesalary view database way find rows update example2 view shows values one table view shows values person person contact detailsidpersonidcontacttypecontactvalue table example rows 11emailddd example com 11phone898 join table show business friendly information view personidnamelastname phone1email1 would like update phone1 email1 personid maps two different rows may rows example view database way find rows update note restrict view sql makes clear find rows update may work two example 0
configure aws aurora separate write read operations want migrate database instance aws rds mysql aurora doubt replication aurora management write,afaik youre right aws rds aurora mysql fork support automatic transparent read write splitting http docs aws amazon com amazonrds latest userguide chap aurora html order way thats completely transparent application would need intermediate proxy application would always connect proxy proxy would packet inspection examine incoming query determine read write gets forwarded master read get forwarded replicas aware notable implications means proxy needs understand mysql protocol needs inspect packet query determine rw ro needs forward query appropriate backend mysql instance likely needs keep track connection maintaining map front end connections app proxy backend connections mysqld instances front end connection would 10,wanted point aws updated cluster read endpoint load balancing case anyone runs google https aws amazon com blogs aws new reader endpoint amazon aurora load balancing higher availability 9,short many indexes rule bit misleading think long given average database around reads higher reads need optimised insert read unique index example update read even write intensive database still reads poor quality indexing examples wide clustered indexes sql server especially non monotonic clustered indexed overlapping indexes eg cold cole cold cole colf many single column indexes also overlapping useful indexes useless queries includes covering eg single column indexes note quite typical indexes several times bigger actual data even oltp systems generally id start clustered index usually pk unique indexes constraints cant covering foreign key columns id look common queries see 0,design decision would probably go option modified option first option one thing like clarity product table affords one big table field determine type relation isnt clear another indexing strategy would always require type field listed since types index cardinality extremely low select product table type basically full table scan anyway option create parent table holds columns types share create product type table individual columns one extra link parent table create link table product option model option etc links respective keys reciprocal links model option option model go ahead create tables well add clarity joins anyone looking downside complexity making sure 0
fill factor setting performance ive read typically indexing recommendation leave fill factor logic behind seems new data may inserted table,default fill factor zero identical per product documentation fill factor value percentage server wide default means leaf level pages filled capacity trying find opitmal value fill factor well researched problem described scenario much done unless table doesnt use default values 4,considered updates change row new data fit space row would moved new page rule filling pages percent reduce space table uses consequentially give slight performance improvement 4,use sql profiler production time done correctly filtering get back small amount data server risk minimal tracing everything would useless 0,data likely long outlive application code rule critical data useful time like foreign key constraints help keep integrity data must database otherwise risk losing constraint new application hits database multiple applications hit databases including might realize important data rule data imports reporting applications may able use data layer set main data entry application frankly chances bug constraint much higher application code experience personal opinion based years dealing data experience hundreds different databases used many different purposes anyone doesnt put contraints database belong eventually poor data sometimes bad data point unusable especially true financial regulatory data needs meet certain criteria auditing 0
nvl stand nvl stand im talking oracle informix perhaps others function used filter non null values query results similar coalesce,quite simply null value function substitutes nulls given resultset column value given second parameter 19,null value logic according http www abbreviations com term references found support phils null value supposition havent found definitive origin 8,prevention steps could make sure one one sysadmin access restored database put db single user mode restore completed check code inside stored procedures functions triggers inside database perform dbcc checkdb make sure integrity issues check users used access database remove start allowing access restricted specific objects checked like shawn said code execute unless stored procedure seems vbalid exec another malicious code reason checking code inside one putting multi user mode 0,might want select insert products discounts product id discount amount discount description values convertvarchar30 product id top product products left outer join products discounts pd product id pd product id top ten pd product id null generates statements top products dont product discount need excel copy paste results new query window ssms run said youll likely want automate procedure could accomplished using either cursor like declare cmd nvarcharmax declare cur cursor local forward static select insert products discounts product id discount amount discount description values convertvarchar30 product id top product products left outer join products discounts pd product id pd 0
replace space space one column table like id propinsi kota aceh denpasar aceh banda aceh sumatera asahan table many rows,try show without space select trimkota yourtable change data update yourtable set kota trimkota trim function different replace replace substitutes occurrences string trim removes spaces start end string want remove start use ltrim instead end use rtrim 16,run new query mysql select replacekota table name show result looks like trimming spaces column update update table name set kota replacekota save 4,dont believe allow run ddl access tables could make role make member db ddladmin db datareader db datawriter roles 0,use sql server extended events capture sql statements execute sql server management studio includes xevent profiler item object explorer every connected sql server version higher right click tsql session launch session aware capturing sql statements across entire server negatively affect performance youd likely want short period time stopping session 0
limit maximum number rows table configuration table sql server database table ever one row help future developers understand id like,two constraints would create table dbo configuration configurationid tinyint null default rest columns constraint configuration pk primary key configurationid constraint configuration onlyonerow check configurationid need primary key unique constraint two rows id value check constraint rows id value arbitrarily chosen combination two almost opposite constraints restrict number rows either zero one fictional dbms current sql implementation allows construction allows primary key consisting columns would solution create table dbo configuration configurationid needed rest columns constraint configuration pk primary key columns 52,could define id computed column evaluating constant value declare column unique create table dbo configuration id cast1 tinyint bit columns constraint uq configuration id unique id 24,anyone using sql server newer use trim built function example declare test nvarchar4000 set test nchar0x09 nchar0x09 nchar0x09 nchar0x09 content nchar0x09 nchar0x09 nchar0x09 nchar0x09 nchar0x09 select trimnchar0x09 nchar0x20 nchar0x0d nchar0x0a test please note default behavior trim remove spaces order also remove tabs newlines cr lfs need specify characters clause also used nchar0x09 tab characters test variable example code copied pasted retain correct characters otherwise tabs get converted spaces page rendered anyone using sql server older create function either sqlclr scalar udf sql inline tvf itvf sql inline tvf would follows create alter function dbo trimchars originalstring nvarchar4000 charstotrim nvarchar50 returns 0,use bcp utility bcp select col1col2col3 mydatabase dbo mytable queryout mytable csv servername argument specifies character output opposed sqls native binary format defaults tab separated values changes field terminator commas specifies windows authentication trusted connection otherwise use myusername mypassword doesnt export column headers default need use union headers use sqlcmd sqlcmd servername select col1col2col3 mydatabase dbo mytable mydata csv use powershell link article end using might want append notype export csv extractfile get rid unnecessary column output file 0
way find changed password login trying find changed password login sql server r2 already checked default trace log event default,article help set advance event happened past didnt kind auditing mechanism set still hope though lets say create login flooberella password nx check policy information default trace eventclass audit addlogin event however change password using either methods alter login flooberella password ny exec sp password ny nz nflooberella events captured default trace obvious security reasons possible anyone access default trace figure someone elses password want make easy even find password changed polling frequency events example reveal certain properties security strategy else relies information still log also relies using undocumented dbcc command system database may wish back master restore elsewhere get 11,longer comment posting answer select top10 transaction id begin time transaction name transaction sid suser sname transaction sid fn dblognull null operation lop begin xact transaction id begin time transaction name transaction sid 00002b12 event session startup null 00002b13 dbmgr startupdb null 00002b14 addguestusertotempdb null 00002b15 dbmgr startupdb null 00002b16 dbmgr startupdb null 00002b17 dbmgr startupdb null 00002b18 dbmgr startupdb null 00002b19 dbmgr startupdb null 00002b1a autocreateqpstats 0x010500000000000515000000a065cf7e784b9b5fe77c877084b65600 00002b1b test ack 0x010500000000000515000000a065cf7e784b9b5fe77c877084b65600 rows affected 4,backup types dependent sql server recovery model every recovery model lets back whole partial sql server database individual files filegroups database table level backup created option workaround taking backup sql server table possible sql server various alternative ways backup table sql sql server bcp bulk copy program generate table script data make copy table using select save table data directly flat file export data using ssis destination explaining first one rest might knowing method backup sql table using bcp bulk copy program backup sql table named person contact resides sql server adventureworks need execute following script sql table backup developed 0,sql server real problem avoiding logging deleted rows proposal set second database simple recovery mode copy whole table select sem copy original truncate original insert original select sem copy target server sql server version move data backup restore cases take bulkcopy option edit although time learn partitioned tables seems option enterprise edition 0
rely reading sql server identity values order tl dr question boils inserting row window opportunity generation new identity value locking,best expect identities consecutive many scenarios leave gaps better consider identity like abstract number attach business meaning basically gaps happen roll back insert operations explicitly delete rows duplicates occur set table property identity insert gaps occur records deleted error occurred attempting insert new record rolled back update insert explicit value identity insert option incremental value transaction rolls back identity property column never guaranteed uniqueness consecutive values within transaction values must consecutive transaction use exclusive lock table use serializable isolation level consecutive values server restart reuse values use identity values create separate table holding current value manage access table number assignment 5,inserting row window opportunity generation new identity value locking corresponding row key clustered index external observer could see newer identity value inserted concurrent transaction yes allocation identity values independent containing user transaction one reason identity values consumed even transaction rolled back increment operation protected latch prevent corruption extent protections specific circumstances implementation identity allocation call cmedseqgen generatenewvalue made user transaction insert even made active locks taken running two inserts concurrently debugger attached allow freeze one thread identity value incremented allocated able reproduce scenario session acquires identity value session acquires identity value session performs insert commits row fully visible session performs 26,paul white answered absolutely correct possibility temporarily skipped identity rows small piece code reproduce case create database testtable create database identitytest go use identitytest go create table dbo identitytest id int identity c1 char10 create clustered index ci dbo identitytest id dbo identitytestid perform concurrent inserts selects table console program using system using system collections generic using system data sqlclient using system threading namespace identitytest class program static void mainstring args var insertthreads new list thread var selectthreads new list thread start threads infinite inserts var insertthreads addnew threadinfiniteinsert insertthreads start start threads infinite selects var selectthreads addnew threadinfiniteselectandcheck selectthreads 7,go far course problem may bit art isnt pure science main product analysis reporting system regard quite detail records initially designed lots joins common id child records found denormalized couple fields could cut lot joins could take away lot performance headaches knew created normalized design started using profiled actual performance hundreds millions rows across dozens tables end story profiled couldnt know sure going work us liked idea normalizing could update easily end actual performance deciding factor thats advice profile profile profile 0,sounds like need sparse columns filtered indexes go option fully supported documented features exactly scenario sql server database engine uses sparse keyword column definition optimize storage values column therefore column value null row table value requires storage cant imagine xml solution performing well scenario huge overhead redundant metadata slow query 0,cant something like first without order since query could give different order dont specify lets try way select select table order putyourorderfieldhere limit name like john limit 0
foreign keys link using surrogate natural key best practice whether foreign key tables link natural key surrogate key discussion ive,neither sql relational model disturbed foreign keys reference natural key fact referencing natural keys often dramatically improves performance youd surprised often information need completely contained natural key referencing key trades join wider table consequently reduces number rows store one page definition information need always completely contained natural key every lookup table term lookup table informal relational model tables tables table us postal codes might rows look like ak alaska al alabama az arizona etc people would call lookup table big systems unusual find tables one candidate key also unusual tables serve one part enterprise reference one candidate key tables serve 14,main reason support surrogate keys natural keys often subject change means related tables must updated put quite load server years using variety databases many topics true natural key often fairly rare things supposedly unique ssn things unique particular time become non unique later things like emails addresses phone numbers may unique used different people later date course things simply dont good unique identifier like names people corporations avoiding joins using natural key yes speed select statements dont need joins cause places still need joins slower int joins generally faster also probably slow inserts deletes cause performance problems updates key changes 5,good news mysqls functionality finally fixed changelog previously control mysql interrupted current statement one exited mysql control interrupts current statement one cancels partial input line otherwise exit 0,checking variables context execute following query select name context pg settings name variable name case wal keep segments context sighup means requires server reload use pg ctl reload shell prompt select pg reload conf psql database client latest version types context internal context means modified compilation time postmaster means service restart needed others session backend specific 0
lob data slow table scans questions rather big table one columns xml data average size xml entry kilobytes columns regular,presence xml field causes table data located lob data pages fact table pages lob data merely xml column table effect presence xml data certain conditions causes portion rows data stored row lob data pages one maybe several might argue duh xml column implies indeed xml data guaranteed xml data need stored row unless row pretty much already filled outside xml data small documents bytes might fit row never go lob data page correct thinking lob data pages cause slow scans size also sql server cant scan clustered index effectively theres lot lob data pages table scanning refers looking rows course 11,correct thinking lob data pages cause slow scans size also sql server cant scan clustered index effectively yes reading lob data stored row leads random io instead sequential io disk performance metric use understand fast slow random read iops lob data stored tree structure data page clustered index points lob data page lob root structure turn points actual lob data traversing root nodes clustered index sql server get row data sequential reads get lob data sql server go somewhere else disk guess changed ssd disk would suffer much since random iops ssd way higher spinning disk considered reasonable table structure 10,data types changed time variable length columns removed indexes defragmented often never rebuilt lot rows deleted lot variable length columns updated significantly good discussion 0,create custom aggregate function oracle database provides number pre defined aggregate functions max min sum performing operations set records pre defined aggregate functions used scalar data however create custom implementations functions define entirely new aggregate functions use complex data example multimedia data stored using object types opaque types lobs user defined aggregate functions used sql dml statements like oracle database built aggregates functions registered server database simply invokes aggregation routines supplied instead native ones user defined aggregates used scalar data well example may worthwhile implement special aggregate functions working complex statistical data associated financial scientific applications user defined aggregates feature 0
enforce schema bound views reporting purposes need able query dependencies views underlying tables column level via sys sql expression dependencies,easy ddl trigger database create trigger requireschemabinding database create view alter view declare object nvarcharmax declare schema nvarcharmax set object eventdata value event instance objectname nvarcharmax set schema eventdata value event instance schemaname nvarcharmax isnullobjectpropertyobject idquotename schema quotename object isschemabound begin raiserrornot today punk rollback transaction end 7,could use ddl trigger unfortunately schemabinding attribute eventdata structure construct object name check property using create trigger enforceviewbinding database create view alter view declare xml xml eventdata declare name sysname quotename xml value event instance schemaname sysname quotename xml value event instance objectname sysname objectpropertyexobject id name nv isschemabound begin raiserror views must specify schemabinding option rollback end 7,psql could connect server file directory server running locally accepting connections unix domain socket var run postgresql pgsql error generally means server running based dpkg output thread comments due postgresql main package somehow uninstalled since uninstall hasnt called purge option dpkg data configuration files still apt get install postgresql fix problem 0,postgresql terminal commands list databases available el defiant bin psql localhost username pgadmin list command stated simply psql pgadmin commands print terminal list databases name owner encoding collate ctype access privileges kurz prod pgadmin utf8 en us utf en us utf pgadmin pgadmin utf8 en us utf en us utf postgres postgres utf8 en us utf en us utf template0 postgres utf8 en us utf en us utf postgres postgres ctc postgres template1 postgres utf8 en us utf en us utf postgres postgres ctc postgres rows available databases psql commands list tables available specify database list tables database el defiant 0
create index table mysql database use reason performance stability shouldnt,yes lock table youre adding index created table large may take awhile read row building index 15,note table using innodb plugin storage engine highly recommend secondary index almost surely case still read table non blocking select statements 17,according http www postgresql org message id blu0 smtp179b92c5102247cd961a4b3cf2a0 phx gbl temp counter files space used shows total temp files used since probably cluster creation reflect current space used temp files system example shows almost 700gb temp files used actual space taken temp files var lib pgsql data base pgsql tmp 53mb currently 0,dont see would useful connection id innodb transaction id shown show engine innodb status available using select trx id information schema innodb trx trx mysql thread id connection id available mysql possibly innodb plugin transactions dont become visible transaction actually kind interaction innodb 0
wrong subquery work get subquery return lowest course weight lecturer currently returns lowest subquery wrong outer query question show lecturer,many many ways one slightly changes attempt select m1 moduleid m1 cwweight staffid dbo module m1 inner join dbo lecturer m1 moduleconvenor staffid m1 cwweight select minm2 cwweight dbo module m2 m2 moduleconvenor staffid 4,another approach easily extensible things like getting lowest highest min max etc select moduleid cwweight staffid moduleconvenor rn row number partition moduleconvenor order cwweight dbo module select moduleid cwweight staffid inner join dbo lecturer staffid staffid rn thing like pattern chance ties break adding clauses order example two modules share lowest weight would pick one higher moduleid partition moduleconvenor order cwweight moduleid desc 4,also use pivot assuming least sql server select algodata binnet newmoon titles pivot counttitle id pub id 0,also use trigger create trigger limittable yourtabletolimit insert declare tablecount int select tablecount count yourtabletolimit tablecount begin rollback end go 0
index used order trying get info mysql using index create inner join trying order end sql query select product inner,david spilletts answer correct points except encouraging suggestion heres way encourage almost versions force optimizer choose plan uses wanted index find rows perform join cant always used foreign key constraint assures case two queries produce identical results call technique first limit join select ps select productstore order storetitle limit ps inner join product productuuid ps productuuid order ps storetitle 5,real answer prefix indexes virtually useless referring key teststoretitle storetitle182 since index contains truncated values completely ordered list titles hence easily used order innodb limit bytes max utf8 varchar255 increased complex set steps get later set global innodb file format barracuda set global innodb file per table alter table tbl drop index teststoretitle add indexstoretitle row format dynamic compressed agree limit group join ypercube suggests solution mostly orthogonal one solution probably significantly faster since wont need scan anything 4,oracle schema contains tables constraints etc mysql create create database oracle create schema dont use db1 alter session set current schema db1 user schema get list schemas select username sys users 0,top script put following set nocount hide rows effected messages 0
query cause deadlock query cause deadlock update top1 system queue set statusid id internalid internalid select top internalid system queue,looks trying select update statement onto table select holding shared lock values inside ix system queue directionbystatus index update needs locks released get exclusive lock update primary key guess clustered also part ix system queue directionbystatus key value anyway guess query would succeed rare chance index values selecting updating conflicting deadlocking time execute assume would link explains deadlocks detail http sqlblog com blogs jonathan kehayias archive anatomy deadlock aspx 13,dont expect mark post answer sharing information sql server experts topic http sqlblog com blogs alexander kuznetsov archive reproducing deadlocks involving one table aspx http rusanu com readwrite deadlock 6,use sqlcmd mode ssms occasion typically migrating databases one environment another example need backup database production restore dev ill one script using sqlcmd mode switch servers ive used one thing another dont use often 0,mysql general query log fields yymmdd hh mm ss thread id command type query body timestamp appears time changes example would thread id connected client shown show processlist line represents query issued necessarily round trip client database queries issued within stored procedures stored functions also logged proc function running could issue many queries response single query client also mysql api library underlying languages connectors mysql supports multiple statement execution would another case multiple queries dont necessarily represent multiple round trips since multiple queries send database batch thats factor capability used calling application general log also directed write mysql general log 0
primary key within json data postgresql table column called json type json within json natural key select json id id,able add constraint primary key must simple composite index may partial expression index index acts constraint functionally theyre much cant appear primary key table metadata used target foreign key constraint true unique constraints issue sql standard definitions primary key unique constraints allow expressions row matching predicates postgresql lists expression index partial index constraint breaking standard lying applications apps understand postgresqls features look index pgs catalogs theres also info information schema cant go listed constraint 5,explained craigs answer neither primary key unique used howeverhere two part solution work around enforce null use check constraint thus alter table add constraint id null check json id null enforce uniqueness create unique index thus create unique index id json id psql give indexes id unique btree json id text check constraints id null check json id text null see null uniqueness enforced test insert values id field error new row relation violates check constraint id null detail failing row contains id field test insert values id abc insert test insert values id abc error duplicate key value violates 6,quote joe celko find reference web wikipedia entry even see shirts conferences rows records lot people point pedantic jerk likes humble verbally abuse newbies admit comes across also met person even shared meal cant tell different real life persona online front even caught calling rows records embarrassed full backstory case say guys online character wrote standard fact authority dictates distinction tell something much cringes someone calls row record many colleagues also experts sql server world us camp believe right example itzik ben gan obvious sql server guru quote first lesson training kit exam querying microsoft sql server example incorrect terms 0,great discussion stackoverflow covers many approaches one prefer sql server use table valued parameters essentially sql servers solution problem passing list values stored procedure advantages approach make one stored procedure call data passed parameter table input structured strongly typed string building parsing handling xml easily use table input filter join whatever however take note call stored procedure uses tvps via ado net odbc take look activity sql server profiler notice sql server receives several insert statements load tvp one row tvp followed call procedure design batch inserts needs compiled every time procedure called constitutes small overhead however even overhead tvps 0
oracle empty string converts null insert empty string converted null insert test values row containing null query table use select,oracle treats null inserting conversion null merely interpretation null way word null interpreted null rtrimaa interpreted null demonstration using following table insert drop table t1 create table t1 c1 varchar210 insert t1 c1 values insert inserted null value c1 select row follows select c1 t1 add clause compare equality one values compared null result always unknown unknown evaluate false except operations unknown value produce unknown values following return rows clauses contain conditions never true regardless data select c1 t1 c1 select c1 t1 c1 null select c1 t1 select c1 t1 null null oracle provides special syntax retrieve rows particular 6,says select nvlit null value dual sql fiddle things gets converted null insert thats oracle varchar2 thing select test trying select test null isnt defined return nothing null doesnt like equality operator use null null ill add char datatype behaves differently padded 8,rights table element soon add element need add table would add application maintenance downside putting everything one table might run scaling issues could mitigated using partitioning materialized views virtual columns likely measures would necessary far table design oracle might suggest something like create sequence userroleid create table userrole userid number7 null roleid number7 null constraint userrole pk primary key userid roleid enable organization index create table permissions id number7 null elementid number7 null constraint userrole pk primary key id elementid enable organization index package code could use userroleid sequence populate id users table id roles table necessary permissions table could 0,ive come declare date daterange empty day range daterange extreme value date begin select distinct generate series lowerrange coalesceupperrange interval day extreme value interval day date rangetest order loop day range daterangei begin isemptya day range else day range end exception data exception raise info day range end end loop uppera extreme value interval day daterangelowera null end raise info end still needs bit honing idea following explode ranges individual dates replace infinite upper bound extreme value based ordering start building ranges union fails return already built range reinitialize finally return rest predefined extreme value reached replace null get infinite 0
sync instance instance without losing data already instance like said comment im dba im still learning process heres scenario oracle,golden gate answer types problem oracle bought therefore oracle golden gate right look image taken oracle golden gate admin guide http download oracle com docs cd e18101 doc e17341 pdf also look following article http www dbasupport com oracle ora11g oracle replication streams vs goldengate shtml thanks tevo pointing licensing issue actually golden gate may expensive solution depending circumstances companys standing oracle cost downtime etc 5,firstly never need shut source server oracle lot techniques use depends data bit vague sorry let give example table insert sequence generated pk simply find max destination database insert select dblink another example destination read maybe solution could active dataguard conflicts manage logical standby goldengate fine technology spending money sure really need also consider shareplex 4,default built oracle answer would probably oracle streams stream propagate information within database one database another possible alternative would create materialized views may union data materialized views existing tables create views union materialized views could refreshed daily options already listed may also good fits depending requirements consider update comments indicate would like replicate packages well could integrate change management process changes made also made would need use different schema since want preserve originals hand requirement moving lobs well makes things difficult want use one technology handle requirements probably go datapump could setup pull tables blobs packages separate schema may still 5,oracle enterprise edition feature called transportable tablespaces used copy set tablespaces one oracle database another transport tablespaces across platforms versions limitations include source destination databases must use compatible database character sets long fence objects want copy tablespaces separate objects dont want copied use feature move many kinds objects one go data pump also needed metadata 4,jack demonstrated way go however feel room improvement place everything schema convenient testing test setup drop schema cascade create schema meta tables schema table name create table schmaschma id int schma text insert schma values create table tbltbl id int tbl text insert tbl values t1 t2 dummy tables used example query create table t1id int insert t1 values create table t2foo text insert t2 values text text old function original answer create replace function dynaquery oldint int col text type anyelement col anyelement returns setof anyelement func begin return query execute select quote ident col select schma schma schma 0,know old thread would say large degree snapshot isolation magic bullet eliminate blocking readers writers however prevent writers blocking writers way around experience additional load tempdb negligible benefits row versioning reducing blocked readers huge reference row versioning snapshot isolation method oracle used decades achieve isolation without blocking readers oracle dbs ive worked nearly years experience far less blocking issues sql server sql developers hesitant use snapshot isolation though theyve tested code databases use default setting 0,yes see risks naive count sql using row locking example im pretty sure inserts deletes use page locks least sql engine chooses lock type based several factors none factors include opinion blanket solutions like changing temp tables table variables generally bad ideas also table variables useful situations limitations performance issues prefer temp tables circumstances particularly table hold dozen rows would require vendor explain system experienced heavy compile locks degraded performance remember anytime look find heavy locks sort necessarily mean locks problem maxs comments spid also important additionally transaction model error processing could big issues system experiences deadlocks input data quality 0,solution one provided aaron bertrand comes building comma separated values bit different connecting coursemaster courseid values studentmaster course sql fiddle ms sql server schema setup create table dbo coursemaster courseid char2 coursename char3 create table dbo studentmaster rollno char5 name varchar10 address varchar20 course varchar100 insert dbo coursemaster values abc def ghi jkl mno pqr stu insert dbo studentmaster values ram ram address hari hari address jeff jeff address daisy daisy address query select sm rollno sm name sm address select cm coursename dbo coursemaster cm sm course like cm courseid xml path type valuesubstringtext varcharmax course dbo studentmaster sm 0
database backup data log files told data files backup operate extent level log file backup operates page level know file,dont worry extents pages full backup contains data pages pages written backup single point time full also contains log records needed redo undo changes earliest page latest transaction committed backup differential backup omits pages changed since last full backup log backup contains log records since last log backup 6,told data files backup operate extent level log file backup operates page level would say statement completely correct data file log file backup would operate page level differential backup scans differential bitmaps backs data file extents marked changed full backup bit puzzled backup distinguishes data log depends backup command give give full backup command sql server knows backup whole database transaction log make sure backup restored database consistent log reads portion transaction log backups changes made last log backup full backup 4,backup standby perfectly fine avoid cancelled statement conflict backup standby system need pause replication standby using select pg xlog replay pause run backup finished run select pg xlog replay resume resume replication keep mind running commands cause recovery lag slave might quite large depending database size also take account space wal segments take replayed slave pause may find useful administractive functions documentation instance check server actually recovery prior pausing select pg recovery 0,three dmvs use track tempdb usage sys dm db task space usage sys dm db session space usage sys dm db file space usage first two allow track allocations query session level third tracks allocations across version store user internal objects following example query give allocations per session select sys dm exec sessions session id session id db namedatabase id database name host name system name program name program name login name user name status cpu time cpu time milisec total scheduled time total scheduled time milisec total elapsed time elapsed time milisec memory usage memory usage kb user objects 0
change three hundred procedures need change procedures packages database due migration accomplish weekend migration one server exadata however database developed,id probably something like call dbms metadata get ddl get ddl object clob write code search replace clob use execute immediate execute newly modified clob something like search replace implements whatever logic need declare ddl clob begin select dba objects objects want change loop ddl dbms metadata get ddl object type object name owner search replace ddl file folder documents directory name execute immediate ddl end loop end 5,script procedures file search replace trivially dome sed script along lines file folder documents directory name note tested top head fiddle load stored procedures note youre frigging code base really test youre rather blind search replace production code could possibly go wrong 7,alternatively could calculate result using formula select dayyourdate iteration dbo yourtable cases operands division operation integers result also integer rounded one dayyourdate give yield still get result 0,database restore issued norecovery left pending state accessed logs differentials added database state since independent activity occurring recovery places database operational state add components backup set time 0
optimization database heavy updates software hardware situation postgresql database quite heavily updated time system hence bound im currently considering making,hardware upgrade number disks key performance yes hard disc even sas head takes time move want hugh upgrade kill sas discs go sata plug sata ssd enterprise level like samsung 843t result around thousand iops per drive reason ssd killers db space much cheaper sas drive phyiscal spinning disc keep iops capabilities discs sas discs mediocre choise start large get lot iops higher use database smaller discs would mean lot iops end ssd game changer regarding software kernel decent database lot iops flush buffers log file needs written basic acid conditions guaranteed kernel tunes could would invalidate transactional integrity partially 10,afford put pg xlog separate raid pair drives controller battery backed ram configured write back true even need use spinning rust pg xlog everything else ssd use ssd make sure super capacitor means persist cached data power failure general spindles means bandwidth 6,looks like already acceptable solution db mail set could potentially something like exec msdb dbo sp send dbmail recipients email domain com subject csv extract body see attachment query select col1col2col3 mydatabase dbo mytable attach query result file query attachment filename csv extract txt query result separator query result header emailing file somewhere anyway might save steps 0,im dba oracle sql server confusion due mismatch semantic extra level sql servers hierarchy theres note talk single instance database instances pun intended instances oracle one instance amount memory allocated sga pga processes spawned smonpmon dw0x system views monitor stuff created used dba views structure familiar oracle already mentioned database physical files typically moving rac makes difference glaring sql server instance works way bunch memory allocated system views sys monitor stuff instance system databases system system views msdb database also holds system related information typically thats find backups related data bit maintenance required least flush old backups records model aptly 0
done enhance performance multiple join aggregate queries typical star schema simulated mentioning two queries first query simply joins fact table,rarely need point benefit trying micro optimise star schema queries non clustered indexes laden included columns fact tables built scanned indexes youve created examples subset copies parent table scanned seeks minor performance improvements come scanning marginally fewer pages parent table given star schemas built support ad hoc query patterns viable create indexes support every possible enquiry create fact table clustered index date key majority typical fact table queries include time element clustering date key enables range scanning fact table rows add non clustered indexes foreign keys fact tables assist highly selective queries foreign keys dimension tables created nocheck prevent impact 8,addition marks excellent answer strategies add existing system exhaustive list course pre aggregated tables indexed views physically materialize results intermediate results query sql server end scanning much smaller indexes return full result set keeps project within database using technology youre familiar analysis services may worth looking plan support lot slicing dicing data analysis services built pre aggregate data automatically according parameters enter disadvantage probably totally new technology im expert area say learning curve powerful tool result caching lot rows coming back youre finding users running queries cache results invalidate cache new data loaded figure way selectively invalidate based new data 5,answer since refer website use index luke com consider chapter use index luke clause searching ranges greater less example matches situation perfectly two column index one tested equality range explains nice index graphics ypercubes advice accurate sums rule thumb index equality first ranges also good one column queries one column seems clear details benchmarks concerning related question working indexes postgresql composite index also good queries first field less selective column first apart equality conditions columns doesnt matter put column first likely receive conditions actually matters consider demo reproduce create simple table two columns 100k rows one one lots distinct values 0,always concerned system may migrated system doesnt support uniqueidentifier compromises dont know designer may known uniqueidentifier type things didnt know technically though shouldnt major concern 0
perl vs ksh unix database administration realize somewhat subjective question looking community guidance company fairly new dbas used use db2,end post really important bit information regarding best suits needs database administrators lets start needs dba generally two realms operation system level maintenance database level maintenance whereby actions merely maintaining system better performance look dont get pedantic go two realms consider ksh korn shell one older unix operational shells used control server goal ksh manage server allow one things traditional bourne shell bourne shell people familiar older vintage allowed differences shell better support things like arithmetic arrays making easier program regards default shell aix perl perl high level general purpose programming language designed run top shell designed primarily reporting mind 6,ksh bash let script items sql plus quite complex stuff however shell scripting tends bit write sh derivatives arent really much good developing complex program logic running automated tasks theyre ok complex client side data manipulation good sed awk notwithstanding perl various oracle libraries dbd oracle available cpan somewhat better work involves complex processing client side type system bit sophisticated line text another option thats quite good python cx oracle 5,anonymous pl sql blocks dont start code begin end least exists sql function cant used pl sql like try something like set serveroutput declare number begin select count table rownum dbms output put linehas rows else dbms output put lineno rows end end yes using exists query also possible set serveroutput declare varchar210 begin select case exists select table rows else rows end dual dbms output put linec end note exists rownum version stop first row found thats point dont need read whole table index 0,alternative test attribs unpivot view provided jackpdouglas works versions 11g fewer full table scans create replace view test attribs unpivot select id name myrow attr cast decodemyrow1attr12attr23attr34attr4attr5 varchar22000 attr test attribs cross join select level myrow dual connect level final query used unchanged view 0
search see mysql user exists system im finding user show grants username localhost im thinking perhaps username slightly different want,use mysql select hostuser user give mysql users server 7,thanks matthewhs answer managed compose query would work searching large number users use mysql select user host user user like user note user whatever component username need search may need sides dont know start username also merely note variable dont include actual query unless username includes 9,use identity column temporary table avoid cursor especially convenient older sql server versions dont support window functions create table variable variableno int identity null code varchar4000 null insert variable code select distinct variable simstg parameter left outer join sim variable variable code bucketref stgbucketno bucketref simbucketno code null declare curid smallint select curid isnullmax id sim variable bucketref simbucketno insert sim variable bucketref variableno code select simbucketno variableno curid code variable drop table variable 0,executing query failed following error index partition table reorganized page level locking disabled maintenance plan must attempting alter index reorganize online operation remove fragmentation pages order pages must locked moved possible page locks disabled way defragment without page locks lock entire partition possible reorganize online difference two locking schemes real world production consequences need grasp record page evaluate impact disallowing particular lock type unfamiliar sql server storage internals start anatomy record anatomy page put simply rows records rows stored pages 8kb alter permitted lock types disable page locks row table locks disable row locks page table locks disable table locks 0
better store images blob url possible duplicate files database wondering theres good reason still use blob fields database couple years,reason use blobs quite simply manageability exactly one method back restore database easily incremental backups zero risk image meta data stored db tables ever getting sync also one programming interface run queries load save images dont need give remote clients filesystem access know grants apply images associated data plus one method storage management oracle might put everything asm use oracle lvm everything another application mix relational data serialized objects blob binary doesnt need images run query relation data bytes blob might file header example applications fact endless accessing blob slower accessing filesystem highly likely database misconfigured 17,dont use blobs mostly backup restore perspective dont want blob data slowing backups dont store full url however store filepath certain point build path one way people programs access files ftp http local directory nfs mounted directories course likely deal larger images people one datasets gets 700gb images compressed per day even thumbnails images still store externally 13,tell answers big depends factors might paying hosting charge file storage database storage file storage typically cheaper especially cloud services self hosted using sql server upcoming version codename denali extend filestream allow access via tsql file system also make sure stay sync access update delete either side keep everything organized research find whats important pick direction based choice 30,im big fan storing reference copy image database managability disaster recovery standpoint really way fly still lots things serve image filesystem applications putting much pressure database server things doesnt really want 8,working linux storing images filesystem database significant better performance see excerpt brad edigers book advanced rails 8,im much fan storing images database small app users seems like easy solution start scale makes things difficult preference start storing images folder web server keep path easily accessible configuration need quickly move dedicated optimized image server later might want move somewhere else think s3 akamai way much complicated change code expecting read database 5,smart move smart move depends specific case file location directly affecting url structure youre storing full file addresses database bad say bad move since trouble case somebody move rename directory application built way simply point files directory file access logic dynamic made smart move following reasons database storing need serve lot images per request increase response time application since files going sent synchronously network also add little processing server file storing usually cheaper database storage file storing unless restrictions image access determined user access given group images theres need processing access images kind file database storage possible access files using 4,quickest cheapest way find hitting databases use sp whoisactive http whoisactive com 0,general engine create execution plan based essentially number rows whether index expected number result rows intermediate rows form input query question exists plan encourages seek based plan table good choice table small table large index exists antijoin plan good choice table large table small index returning large result set however encouragement like weighted input strong often makes choice moot ignoring effect example returning different columns due addressed maxvernon answer 0,routinely increase decrease max server memory configuration option sql server instances running laptop lock pages memory enabled never caused problems memory always seems released quickly also stop start restart instances find need using sql server configuration manager keep open far checkpoint concerned configure depends version manual checkpoint duration target available since though undeniably effective later versions running manual checkpoint duration per database shutdown help spread load reduce recovery time startup managed checkpointing also reduce needed memory shrink requested fewer dirty pages write persistent storage particular benefit dropping clean buffers reducing max memory setting noticed using lock pages memory may increase 0,put way ive yet used lnnvl several years dba pl sql programmer used nvl2 occasion always look side true side wasnt point seems better readability perspective end using nvl decode case etc alternatively works assuming one good handle oracle handles nulls arithmetic point one may well use original query readability execution plan may take harder hit return rows startcommission current commission rows null either including select emp startcommission currentcommission startcommission currentcommission null null null null number always null hence return either records equal combined total null either fields null 0,first thing report person responsible network systems security company person throw network administrator person call cio cto right better yet demand face face explain situation first thing person block ip firewall buy little time much maybe minutes ip maps range ips reported whois net block entire ip range given whois prevent guy requesting new ip isp getting new ip minutes maybe mark storey smith says either add firewall move db dmz already firewall db dmz need immediate forensic check see intervening servers firewall compromised likely change admin password long complex passwords sa windows admin domain admins local admins review every 0,add index youll use temp table index twice query run maintain usual index tasks like uniqueness data loaded temp table already sorted create temp table clustered index sort data taking account sql servers feature temp tables reuse decide create index temp table try create table statement youll add index explicitly table creation prevent sql server reuse table next time 0,youre looking begin transaction alter table foo add varchar exec sp executesql nupdate foo set cast varchar alter table foo drop column commit 0
best practices regarding lookup tables relational databases lookup tables code tables people call usually collection possible values given certain column,idn take mean identity sequence auto increment field take look note section misusing data values data elements first reference underneath figure course separate table sales persons reference using foreign key preferably simple surrogate key sales person id shown expert thinks deference surrogate keys really quite basic sql technique shouldnt cause problems day day sql appears error figure sales person salesdata surrogate key number text im inferring quote avoid costs temptation common novice database programmers commit error outlined section common lookup tables commonly called muck massively unified code key approach accident notably joe celko also sarcasticlly known otlt one true lookup 10,third approach advantages two options put actual code code table mean short character sequence captures essence full value unique given example may idn name democrats code dem code carried transactional tables foreign key short intelligible somewhat independent real data incremental changes name would suggest code change republicans decamp en masse however change code may necessary attendant problems surrogate id would incur style termed abbreviation encoding recommend celkos writing google books holds several examples search celko encoding examples letter encodings countries letter encoding gbp usd eur currency codes short self explaining changing iso 10,windows simply stopping postgresql service running postgresql windows x64 exe top existing works uninstall necessary course backup recommended clear explicit documentation update procedure windows absent note documentation link provided dezso moved current manual https www postgresql org docs current static upgrading html postgresql release notes typically document migration tips appendix example appendix release notes https www postgresql org docs current static release html section release subsection migration version https www postgresql org docs current static release html idm46428658121200 best source windows installer information enterprise db forums posting found addressed question april whats right way upgrade new version postgresql https web 0,alternative test attribs unpivot view provided jackpdouglas works versions 11g fewer full table scans create replace view test attribs unpivot select id name myrow attr cast decodemyrow1attr12attr23attr34attr4attr5 varchar22000 attr test attribs cross join select level myrow dual connect level final query used unchanged view 0
sp msforeach db need use use keyword sp msforeachdb undocumented sp designed run sql every database server instance appear need,look sp helptext entry sp msforeachdb weird friend sp msforeachtable exec sys sp helptext objname nsp msforeachdb exec sys sp helptext objname nsp msforeachtable youll see theyre wrappers sp msforeach worker exec sys sp helptext objname nsp msforeach worker build valid list things dont actually loop meaningful way rate aaron bertrands sp foreachdb much better piece code doesnt skip databases etc 5,procedure perform use command way procedure works replaces every command database prefix run use foodb go exec sys sp msforeachdb nselect sys objects total rows system also get number resultsets show objects foodb issue command way order get command execute context individual database exec sys sp msforeachdb nselect sys objects total rows system case execute command database replaced database name select master sys objects select tempdb sys objects call system function doesnt support database prefix typically requires use command first way differently could exec sys sp msforeachdb nselect db namedb id simply exec sys sp msforeachdb nselect one reason works 11,keep following mind caring updating statistics copied rebuilding indexes vs updating statistics benjamin nevarez default update statistics statement uses sample records table using update statistics fullscan scan entire table default update statistics statement updates index column statistics using columns option update column statistics using index option update index statistics rebuilding index example using alter index rebuild also update index statistics equivalent using fullscan unless table partitioned case statistics sampled applies sql server later statistics manually created using create statistics updated alter index rebuild operation including alter table rebuild alter table rebuild update statistics clustered index one defined table rebuilt reorganizing 0,certainly difference version mongodump vs mongodb server vs use docker rescue docker run rm pwd workdir workdir mongo mongodump server database workdir dump docker use precise version tools cli without install even specify alias bashrc alias mongodump docker run rm pwd workdir workdir mongo mongodump note folder dump saved need write permissions mongodb user container write mounted volume achieved manually setting permissions dump folder running command example would mkdir dump dump completed permissions modified back normal sudo chmod dump 0
pivot rows multiple columns sql server instance linked server oracle server table oracle server called personoptions contains following data personid,prefer pivot query manually may use pivot well select personid maxcase optionid end optiona maxcase optionid end optionb maxcase optionid end optionc personoptions group personid 5,would equivalent sql server syntax based reading oracle docs nullif pivot appear format sql server kin challenge pivot list needs static unless make query dynamic itzik demonstrates idea translated sql personoptionspersonid optionid select union select union select union select union select union select select personid nullifp optiona nullifp optionb nullifp optionc personoptions po pivot countpo optionid optionid 9,ways perform data transformation access pivot function easiest use aggregate function case aggregate case version select personid maxcase optionid else end optiona maxcase optionid else end optionb maxcase optionid else end optionc personoptions group personid order personid see sql fiddle demo static pivot select select personid optionid personoptions src pivot countoptionid optionid optiona optionb optionc piv order personid see sql fiddle demo dynamic version two versions work great known number values values unknown want implement dynamic sql oracle use procedure create replace procedure dynamic pivot pop cursor sys refcursor sql query varchar21000 select personid begin select distinct optionid personoptions order 20,create function like create replace function public get messages timestamp time timestamp time timestamp returns table recipient varchar timestamp timestamp begin return query select distinct recipient recipient timestamp messages left join identities recipient name timestamp time time order recipient timestamp desc end language plpgsql use function like table select get messages timestamp2015 0,make use xml query columns might table build xml columns per row cross apply extract value using values function query id known get table directly col1 col2 might get using xml select id tx valuecol1 text int col1 tx valuecol2 text int col2 cross apply select xml path type txx sql fiddle 0,best box solutions ive found use combination slow query log sucks compared profiler running wireshark port really sucks compared profiler wont work youre encrypting connections theres also show full processlist like reduced combination sys dm exec sessions sys dm exec requests little sys dm exec sql text thrown 0
help choose raid level combination sql server instance going rebuild one ibm server scratch server dedicated sql server instance running,vote option bear mind raid means protection logs matter yes also benefit simplicity sql server docs say optimized parallelism use kb kb stripe size usually good go controller default imo 6,notes os binaries 40gb disks doesnt leave many options want backups let raid controller choose stripe size depends raid level micro optimisation youll see conflicting data log file write speed determines throughput allow free space times biggest table inside mdf index rebuilds raid idiotic server data value id go one raid array 5,id go either two raid1 volumes three mirrors raid10 striping across two sets three mirrors rationale two mirrors little period server get proper consistency checks often month quite likely bad blocks matter long mirror still intact one disk fails likely wont able recover one hence three mirrors raid6 bad idea database setup writes tend small turns read modify write operations background whether two raid1 volumes one raid10 better depends application likely need entire space go raid10 otherwise id suggest one volume system indexes one data pages go two volumes check controller allows extending raid1 raid10 later case need expand 4,paul rightly mentioned answer fundamental reason scalar udfs could executed using parallelism however apart implementation challenges another reason forcing serial froid paper cited paul gives information quoting paper section currently sql server use intra query parallelism queries invoke udfs methods designed mitigate limitation introduce additional challenges picking right degree parallelism invocation udf instance consider udf invokes sql queries one figure query may use parallelism therefore optimizer way knowing share threads across unless looks udf decides degree parallelism query within could potentially change one invocation another nested recursive udfs issue becomes even difficult manage approach froid described paper result parallel plans 0,stating workaround seems little premature say database contains tables views export tables views schema sql create statements run target database could also export actual data something like csv format import target database perhaps even write something etc export import actual data wouldnt need spend penny new instances sql server dont ultimately need 0,best solution found desc table name command information list mysql tables command gives description one database table exactly trying find 0
single drive vs multiple drives generally bottleneck rdbms mysql user performance disk access ssd provides great performance compared conventional spindle,could depend great deal storage engine myisam think would great idea make data contiguous want would benefit queries involving bulk operations large range scans toolset compressing repairing myisam improve table format also make access little better imho concurrent inserts could thrive disk environment innodb completely different story innodb file per table disabled data scattered throughout ibdata1 thus scattered amongst multiple disks could run optimize table innodb table make data index pages contiguous could cause two problems makes ibdata1 grow deteriorate performance time dont forget four types data reside ibdata1 table metadata table data pages index data pages mvcc data constant 7,yes multiple drives provide better throughput thanks wonderful technology call raid database youll want raid striped mirrors 4n drives would doublen write speed quadruplen sequential read throughput seek times stay roughly though youll lose half space mirroring 5,agree burden justification ones requiring access typically environments consulted access production systems small environment support person access backups etc support support indirect access dedicated support developer production data big thing need access hook keeping everything running smoothly answer finance guys question something working cant always work even day old data case hand access worse typically consultant tend avoid getting sort access unless needed since working financial databases last thing want accused entering invoices hand dont need access shouldnt dont really buy sensitive data argument since developer probably hook making sure handled correctly hard verify without looking actually stored bug report 0,table clustered index index table data otherwise heap type table rebuild clustered index index fact space wouldnt counted data non clustered index result partially used pages merged full form insert data index clustered otherwise index order leaf pages created needed ever one partial page one end enter data index order page needs split data fit right place end two pages approximately half full new row goes one time happen lot consuming fair amount extra space though extent future inserts fill gaps non leaf pages see similar effect actual data pages far significant size also deletes may result partial pages remove 0
matriculation number good primary key trying design grade assignment database facing following dilemma student table following characteristics matriculation number know,would go surrogate key approach im familiar matriculation number expect large enough surrogate integer might preferred alternative larger pk means larger indexes means less performance reads writes plus space usage space referring tables go surrogate key 4,currently work field case surrogate key used software probably deals much wider range scenarios extra flexibility may mean lot future student numbers overall may change quite frequently depending numbers assigned distribution authority new student enrolled number unique identifier may internally assigned student authority actually generated number student asynchronously case may necessary use one field database crushes primary key idea immediately student numbers change bad news lots reasons remember primary key table propagated many parts system many associated rows performance rarely concern single student data operations depending indexes set using student number alternate key may slight performance hit however least experience 4,regarding window aggregates sum avg sumsalesytd partition territoryid order datepartyymodifieddate msdn page clause transact sql rather hidden remark general remarks order specified entire partition used window frame applies functions require order clause rows range specified order specified range unbounded preceding current row used default window frame applies functions accept optional rows range specification example ranking functions accept rows range therefore window frame applied even though order present rows range means code results sumsalesytd partition territoryid order datepartyy modifieddate range unbounded preceding current row sum calculated row getting rows territoryid year part modifieddate less equal year part row often called cumulative 0,answer xxd nice small files fast example script im using xxd home user myimage png tr tmp image hex echo create table hexdump hex text delete hexdump copy hexdump tmp image hex create table bindump binarydump bytea delete bindump insert bindump binarydump select decodehex hex hexdump limit update users set image select decodehex hex hexdump limit id psql mydatabase 0
explain execution plan researching something else came across thing generating test tables data running different queries find different ways write,constant scans way sql server create bucket going place something later execution plan ive posted thorough explanation understand constant scan look plan case compute scalar operators used populate space created constant scan compute scalar operators loaded null value theyre clearly going used loop join effort filter data really cool part plan trivial means went minimal optimization process operations leading merge interval used create minimal set comparison operators index seek details whole idea get rid overlapping values pull data minimal passes although still using loop operation youll note loop executes exactly meaning effectively scan addendum last sentence two seeks misread plan 13,constant scans produce single memory row columns top compute scalar outputs single row columns expr1005 expr1006 expr1004 null null bottom compute scalar outputs single row columns expr1008 expr1009 expr1007 null concatenation operator unions rows together outputs columns renamed expr1010 expr1011 expr1012 null null null expr1012 column set flags used internally define certain seek properties storage engine next compute scalar along outputs rows expr1010 expr1011 expr1012 expr1013 expr1014 expr1015 null null true null false last three columns defined follows used sorting purposes prior presenting merge interval operator expr1013 scalar operator4 expr1012 null expr1010 expr1014 scalar operator4 expr1012 expr1015 scalar operator16 expr1012 29,type logging would looking gaining application audit table tracks application user application command result among things along another audit table tracks individual table changes provides logging need per user could also set connection pool application control connections per application first blush think trying manage many users burden outweighs possible benefits 0,reindex rebuild db running take offline rebuild index database offline database online rebuild index widely accepted parameter rebuilding fragmentation rebuild fragmentation lies reorganize use script maintenance plan ola hallengren script script time would suggest create 0
mysql auto increment limited primary keys know mysql limits auto increment columns primary keys first thought performance restriction since probably,would want auto increment column primary key want column auto increment definition storing meaningful data column case storing non meaningful information makes sense special case want synthetic primary key case lack information benefit risk someone ever come along future want change data attribute entity changed multiple auto increment columns table seems even odder two columns would data theyre generated algorithm populated time suppose could come implementation possible slightly sync enough concurrent sessions cant imagine would ever useful application 9,interesting question different databases unique approaches providing auto increment mysql one auto increment key generated uniquely identify row table lot explanation behind implementation depending datatype auto increment values fixed length datatype bytes max tinyint max unsigned tintint max int max unsigned int postgresql internal datatype serial used auto increment larger ranges allowed using bigserial oracle schema object called sequence create new numbers simply summoning nextval function postgresql also mechanism nice url provides dbs specify http www w3schools com sql sql autoincrement asp concerning question really want multiple auto increment columns single table emulate two reasons must emulate mysql accommodates one 4,fact auto increment attribute limited primary key used old versions definitely probably still mysql manual versions since reads like one auto increment column per table must indexed default value indeed auto increment column table primary key makes sense different topic though also mention auto increment column always integer type technically floating point type also allowed unsigned signed type wastes half key space also lead huge problems negative value inserted accident finally mysql later defines type alias serial bigint unsigned null auto increment unique 8,decided see happens actually click create catalog ssms host steps performed basic premise backup exists program files microsoft sql server dts binn ssisdbbackup bak part catalog creation backup restored make ssisdb pathway program files going protected prevent accidental files error encountering indicates person running ssms access file program files microsoft sql server dts binn assuming youre using runas something like launch ssms try opening windows explorer internet explorer navigate folder get might click yes something like show files know im restricted area propose uac affecting ability install ssisdb case close ssms instances right click ssms choose run administrator prevent authorization 0,nope heres simple test select coalesce1 select runs fine select coalescenull select throws error second condition evaluated exception thrown divide zero per msdn documentation related coalesce viewed interpreter easy way write case statement case well known one functions sql server mostly reliably short circuits exceptions comparing scalar variables aggregations shown aaron bertrand another answer would apply case coalesce declare int select case else min1 end generate division zero error considered bug rule coalesce parse left right 0,use import export wizard move data databases right click database want export choose tasks export data wizard guide process youre right though wont able backup restore moving sql server r2 sql server express 0
