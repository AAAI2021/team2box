2 what version control methodologies help teams of people track database schema changes
5 what are the differences between nosql and traditional rdbms over the last few months nosql has been frequently mentioned in the technical news what are its most significant features relative to traditional rdbms at what level physical logical do the differences occur where are the best places to use nosql why
14 what we earn and what we lost with this migration what should expect as drawbacks after the migration is it really unnecessary to change the applications in any situation
20 have symfony application with an innodb database that is 2gb with tables the majority of the size of the database resides in single table 2gb am currently using mysqldump to backup the database nightly due to my comcast connection oftentimes if am running dump manually my connection to the server will timeout before the dump is complete causing me to have to rerun the dump currently run cron that does the dump nightly this is just for dumps that run manually is there way to speed up the dumps for the connection timeout issue but also to limit the time the server is occupied with this process btw am currently working on reducing the size of the overall database to resolve this issue
21 are there any techniques or tools to work with sqlite on medium size traffic concurrency db environment
29 im looking for beginner and intermediate level sql puzzles that can point trainees at for practice im aware of http sqlzoo net which is great resource is there anything else out there that you could suggest
33 just as the title says where can see it are there any config options for it like how many ms would determine if query is slow or not
36 ive heard long time ago that there is this tool that helps you tweek mysql settings for better performance but cant seam to find it am aware that can use ab for apache to simulate high traffic and it will generate me slow log however if it crashes already happened and it was in production mode dont know why it crashed and if it can be tweeked from the config
43 how to create an index to filter specific range or subset of the table in mysql afaik its impossible to create directly but think its possible to simulate this feature example want to create an index for name column just for rows with status active this functionality would be called filtered index in sql server and partial index in postgres
46 use indexes like most developpers do mostly on well index but im sure there is lot of subtle way to optimize database using index im not sure if it is specific to any implementation of dbms my question is what are good examples of how to use index except for basic obvious cases and how does dbms optimize its database when you specify an index on table
47 database files that are built using sql are not compatable with is there work around
48 is there way to traverse tree data in sql know about connect by in oracle but is there another way to do this in other sql implementations im asking because using connect by is easier than writing loop or recursive function to run the query for each result since some people seem to be confused by the phrase tree data will explain further what mean is with regards to tables which have parent id or similar field which contains primary key from another row in the same table the question comes from an experience where was working with data stored in this way in an oracle database and knew that the connect by isnt implemented in other dbmss if one were to use standard sql one would have to create new table alias for each parent one would want to go up this could easily get out of hand
56 ive been running an auto index tool on our ms sql database modified script originating from microsoft that looks at the index statistics tables automated auto indexing from the stats now have list of recommendations for indexes that need creating edit the indexes described above take information from the dmvs that tell you what the database engine would use for indexes if they were available and the scripts take the top recommendations by seeks user impact etc and put these in table edit above partially taken from larry colemans answer below in order to clarify what the scripts are doing as am new to database admin and having had quick search around the net am reluctant to take the plunge and blindly add the recommended indexes however not being experienced in the field am looking for some advice on how to determine whether the recommendations are necessary or not do need to run the sql profiler or is it better to examine the code that queries the tables and do you have any other advice
59 im interested mainly in mysql and postgresql but you could answer the following in general is there logical scenario in which it would be useful to distinguish an empty string from null what would be the physical storage implications for storing an empty string as null empty string another field any other way
62 based on traversing tree like data in relational database using sql question would like to know how the way regularly used to describe tree like data on relational databases considering physical implications im assuming that the rdbms has not special features to handling that other than regular sql ansi or common available features in doubt im always interested on mysql and postgresql and eventually sqlite
63 in mysql is it better to always allow nulls unless you know field is required or always use not null unless you know field will contain nulls or doesnt it matter know in some dbmss they say to use not null as much as possible because allowing nulls requires an extra bit or byte per record to store the null status
64 what is good way to migrate db changes from development to qa to production environments currently we script the change in sql file and attach that to tfs work item the work is peer reviewed when the work is ready for testing then the sql is run on qa the work is qa tested when the work is ready for production then the sql is run on the production databases the problem with this is that it is very manual it relies on the developer remembering to attach the sql or the peer reviewer catching it if the developer forgets sometimes it ends up being the tester or qa deployer who discovers the problem secondary problem is that you sometimes end up needing to manually coordinate changes if two separate tasks change the same database object this may just be the way it is but it still seems like there should be some automated way of flagging these issues or something our setup our development shop is full of developers with lot of db experience our projects are very db oriented we are mainly net and ms sql shop currently we are using ms tfs work items to track our work this is handy for code changes because it links the changesets to the work items so can find out exactly what changes need to include when migrating to qa and production environments we are not currently using db project but may switch to that in the future maybe that is part of the answer am very used to my source control system taking care of things like this for me and would like to have the same thing for my sql
81 like to get the latest executed statements within my database along with performance indicators as such like to know which sql statements were most cpu disk intensive
82 found sqlplusinterface is rather outdated its quite nice to have some commands or keywords at disposal but for example no arrow up key for the previous history entry is available what is good replacement extension for sqlplus could be gui or better so it stays useful via ssh command line utility sql plus is the main command line tool to operate with the oracle database
86 currently use phpmyadmin for most of my database administration it works well enough but im interested in other tools that might work more quickly efficiently etc what are the best and or alternative admin tools for mysql on nix box
93 ive been wanting to use joins for while but im having trouble visualizing the output so know how to put it to use lets say have tables create table cities id int unsigned primary key auto increment city tinyblob create table users id int unsigned primary key auto increment username tinyblob city int unsigned foreign key city references cities id if my application is to run an sql query to get users profile data how would use join to get the city associated with users record and how would the outputted record appear
94 looking at the structure of most php mysql based websites ive seen it appears that its not terribly difficult to discern the database password if you dig bit as theres invariably setup or configuration file someplace that stores the information for logging into the db other than the basic precaution of making sure that my databases privileges are appropriately restricted for remote requests what options are available that could implement in my own projects to protect this information
111 for example database that is 3nf is always 1nf and 2nf does this hold for and
123 the dominant topologies of data warehouse modelling star snowflake are designed with one to many relationships in mind query readability performance and structure degrades severely when faced with many to many relationship in these modelling schemes what are some ways to implement many to many relationship between dimensions or between the fact table and dimension in data warehouse and what compromises do they inflict with regards to necessary granularity and query performance
126 recently one of our asp net applications displayed database deadlock error and was requested to check and fix the error managed to find the cause of the deadlock was stored procedure that was rigorously updating table within cursor this is the first time ive seen this error and didnt know how to track and fix it effectively tried all the possible ways know and finally found that the table which is being updated doesnt have primary key luckily it was an identity column later found the developer who scripted database for deployment messed up added primary key and the problem was solved felt happy and came back to my project and did some research to found out the reason for that deadlock apparently it was circular wait condition that caused the deadlock updates apparently take longer without primary key than with primary key know it isnt well defined conclusion that is why im posting here is the missing primary key the problem are there any other conditions which cause deadlock other than mutual exclusion hold and wait no preemption and circular wait how do prevent and track deadlocks
127 is it true that stored procedures prevent sql injection attacks against postgresql databases did little research and found out that sql server oracle and mysql are not safe against sql injection even if we only use stored procedures however this problem does not exist in postgresql does the stored procedure implementation in postgresql core prevent sql injection attacks or is it something else or is postgresql also susceptible to sql injection even if we only use stored procedures if so please show me an example book site paper etc
135 know that those letters mean extract transform and load but when used it at first thought that during the transform phase could do plenty of different joins on data that ive extracted from data sources later on realized that doing join on different etl is not that handy so what do we do in transform phase calculate and output the result string transformation should input data sources only be csv xml or plain file if joins are not that handy should we only do high level transformation within an etl thank you
144 when want column to have distinct values can either use constraint create table t1 id int primary key code varchar10 unique null go or can use unique index create table t2 id int primary key code varchar10 null go create unique index t2 on t2code columns with unique constraints seem to be good candidates for unique indexes are there any known reasons to use unique constraints and not to use unique indexes instead
152 one of the practices that ive seen about being performed by dbas in my organization is to treat full database export using tools like exp expdp as backup would this be good practice what would be the advantages of using rman over this approach
153 am new to sql and wanted to know what is the difference between those two join types select from user inner join telephone on user id id select from user left outer join telephone on user id id when should use one or the other
194 assuming production oltp system with predominantly innodb tables what are the common symptoms of mistuned misconfigured system what configuration parameters do you most commonly change from their defaults how do you spot potential bottlenecks before there is problem how do you recognize and troubleshoot active problems any anecdotes detailing specific status variables and diagnostics would be appreciated
197 somewhat know the answer to this question already but always feel as though there is more need to pick up on the topic my basic understanding is that generally speaking single index that just includes all the fields you might be querying sorting on at any given time isnt likely to be useful yet have seen this type of thing as in someone thought well if we just put all this stuff in an index the database can use it to find what it needs without having ever seen an execution plan for some of the actual queries being run imagine table like so id int pk uid name varchar50 customerid int foreign key datecreated datetime might see single index including the name customerid and datecreated fields but my understanding is that such an index would not be used in query like for example select id name customerid datecreated from representatives where customerid order by datecreated for such query it seems to me that better idea would be an index including the customerid and datecreated fields with the customerid field being first this would create an index that would have the data organized in such way that this query could quickly find what it needs in the order that it needs another thing see perhaps as frequently as the first is individual indexes on each field so one each on name customerid and datecreated fields unlike the first example this type of arrangement seems to me sometimes to at least be partially useful the querys execution plan may show that at least its using the index on the customerid to select the records but its not using the index with the datecreated field to sort them know this is broad question because the specific answer to any particular query on any particular set of tables is usually to see what the execution plan says its going to do and otherwise take the specifics of the tables and queries into account also know that it depends on how often query might be run as opposed to the overhead of maintaining particular index for it but suppose what im asking is as general starting point for indexes does the idea of having specific indexes for specific frequently pulled queries and the fields in the where or order by clauses make sense
201 which databases are recommended to be used as embedded databases to store data within an application the embedded database may or may not synchronize back to larger system database
232 im designing database with multiple lookup tables containing possible attributes of the main entities im thinking of using or character key to identify these lookup values rather than an auto incrementing integer so that when store these attribute ids on the main tables ill see meaningful values rather than just random numbers what are the performance implications of using character field as primary key rather than an integer im using mysql if that matters edit these lookup tables have new records added infrequently they are manually maintained and the character based keys are manually created as well heres an example cuisines id description chnse chinese italn italian mxicn mexican
239 are there any guidelines or rules of thumb to determine when to store aggregate values and when to calculate them on the fly for example suppose have widgets which users can rate see schema below each time display widget could calculate the average user rating from the ratings table alternatively could store the average rating on the widget table this would save me from having to calculate the rating every time display the widget but then id have to recalculate the average rating each time user rated widget ratings widgets widget id widget id user id name rating avg rating the column in question
252 title sums it up im reasonably well off with php and comfortable getting it to do what need it to with values pulled from mysql however in the interest of expanding my knowledge ive been wondering if perhaps it would be more efficient technique in the long run are they equivalent due to being roughly the same amount of processing in the end or are there cases where one presents distinct advantage over the other
255 does collation have any influence over query speed does the size of table change depending of the collation if want to build website that must support all possible languages lets take for google which would be the recommended collation will need to store characters such as my searches over the website will have to return something for the th ng input it must be case insensitive as well how do know which is the best choice to make which collation better suits this case
262 from time to time consumers of my database processes will ask for an estimate of when given task will be done while feel like know how to read an explain in most database engines have trouble trying to translate this to ask me again in minutes does anyone know good rule of thumb to use for any particular database realize this isnt going to be hard and fast rule but even being able to give ballpark figure could be useful in some instances
264 ive being reading around reasons to use or not guid and int int is smaller faster easy to remember keeps chronological sequence and as for guid the only advantage found is that it is unique in which case guid would be better than and int and why from what ive seen int has no flaws except by the number limit which in many cases are irrelevant why exactly was guid created actually think it has purpose other than serving as primary key of simple table any example of real application using guid for something guid uniqueidentifier type on sql server
270 have an asp net mvc app which works with database under sql server r2 express edition there is need to perform regular task on updating some records in the database unfortunately the express edition lacks sql agent what approach would you recommend
283 am interested to know what methods other people are using to keep track of changes made to the database including table definition changes new objects packages changes etc do you use flat files with an external version control system triggers other software
287 years ago it was common to write where exists select from some table where some condition last year noticed that many sql scripts switched to using the number instead of the star where exists select from some table where some condition just on so saw this oracle example where exists select null from is this common pattern with oracle and which are the performance arguments to use something like this
299 oracle is deprecating os authentication according to the oracle database security guide which says be aware that the remote os authent parameter was deprecated in oracle database 11g release and is retained only for backward compatibility in addition most security information and tools consider os external authentication to be security problem am trying to understand why this is the case here are some advantages see of os authentication without os authentication applications must store passwords in variety of applications each with their own security model and vulnerabilities domain authentication already has to be secure because if it is not then database security just slows down access to the database but cannot prevent it users that only have to remember one domain password can be made to create more secure domain passwords more easily than they can be made to create even less secure database passwords as the number of different databases they must connect to increases
309 am testing my application need some code that stable simulates the deadlock on database site sql script if possible thank you added reproducing deadlocks involving only one table
310 ive got mysql server with database of approximately tables taking up 4gb the vast majority of these tables all but are myisam this has been fine for the most part dont need transactions but the application has been gaining traffic and certain tables have been impacted because of table locking on updates thats the reason of the tables are innodb now the conversion on the smaller tables 100k rows dont take long at all causing minimal downtime however few of my tracking tables are approaching million rows is there way to speed up an alter table engine innodb on large tables and if not are there other methods to convert minimizing downtime on these write heavy tables
320 raid redundant arrays of inexpensive disks comes with different configurations raid raid what is the recommended raid configuration that should set up and use when installing an oracle database the database will mainly be used as data warehouse
321 according to the restrictions on stored routines and triggers dynamic sql cannot be used restriction lifted for stored procedures in version and later why is this limitation in place and why lift it for procedures but not functions or triggers
322 would like to build distributed system need to store data in databases and it would be helpful to use an uuid or guid as primary key on some tables assume its drawbacks with this design since the uuid guid is quite large and they are almost random the alternative is to use an auto incremented int or long what are the drawbacks with using uuid or guid as primary key for my tables will probably use derby javadb on the clients and postgresql on the server as dbms
337 how could change the sql server r2 express default collation for the whole server and particular database is there way to do it using visual interface of sql server management studio in the server properties window and in the corresponding database properties window this property is not available for editing
338 the server is oracle database 11g enterprise edition release 64bit is there an easy quick way to change the sids of the test databases on the server dropping recreating of the database is an option for me but im looking for something requiring less time the other option to assign names in the clients tnsnames ora is prone to errors because they are not administrated centrally compared with the time to drop create database on sql server the amount of time required to create new oracle database is excessively greater further on sql server you can rename sql server instances usually you rename the server where sql server is running and have some problems until you rename the server too
351 have an instance that really has nothing on it its merely been installed it was intended for one project but that never actually got done on this server the project was done in duplicate on another server so since have the instance want to rename it can this be done how would do so further investigation googling says its not really possible additional consultation with my coworkers says might as well install 2k8 instance and move on
358 in sql server in this case how can quickly shrink all the files both log and data for all databases on an instance could go through ssms and right click each and choose tasks shrink but im looking for something faster scripted some create database scripts and forgot they had ballooned sizes for defaults and dont need quite that much space reserved for these files on this project
364 ok to start with screwed up when created the databases using create script roughly like so artificial linebreaks and names paths for wrapping purposes create database example on primary name nexample data filename nj sqlserver2008 mssql instance example mdf size 446046kb maxsize unlimited filegrowth log on name nexample log filename nj sqlserver2008 mssql instance example ldf size 664505kb maxsize 2048gb filegrowth go because scripted it out from an existing development database and just wanted to get something going screwed up when didnt change my sizes to something reasonable like 4096kb and so now cant shrink the logfile below roughly 600mb know where went wrong but how do fix it easily
381 what is the meaning of dop in the context of sql server
385 ive got single database of about 5gb running on server with 8gb ram the vast majority of the tables are myisam about 3gb but im soon going to be converting some of them to innodb its going to be slow process focusing on the most write intensive tables at first is there anything wrong with running dedicated server where both types of storage engines exist
390 the question refers to the number parameter in this msdn documentation if you dont you can create multiple stored procedures in sql server differentiated by number and drop them with single drop create procedure dbo stored proc1 as select go create procedure dbo stored proc1 as select go exec stored proc1 returns go exec stored proc1 returns go drop stored proc1 drops both go wonder if this feature is used by anybody for something useful or if it is just historic curiosity
394 if have table with single column of sensitive data and want to grant broad use of the table without exposing that one column know that can create view that gives them access to all the non sensitive columns however postgresql allows you to grant column level permissions in the form of grant select col1 coln on table to role are there other engines which provide this capability
401 found new title called sql server denali in the drop down list on msdn but didnt find much information about it whats new this documentation is for preview only as see top new features of sql server denali does anyone have more detailed information about new features or significant bug fixes in this release im hoping someone has used or tested it new features sequences extended filestream thanks to eric humphrey offset fetch order by clause memory manager changes lag and lead over partition by order by clause thanks to gbn aarons list
405 there are two ways to connect to oracle as administrator using sqlplus sqlplus sys as sysdba sqlplus system manager these accounts should be uses for different purposes suppose which tasks are these two schemas meant for when should use one or the other among them
409 see from the thread which tools and technologies build the stack exchange network that the database specifications are database sql server r2 running microsoft windows server enterprise edition x64 while this does not necessarily mean that oracle wont do this job it does indicate that it is not as good for this part how far is it true what are the reasons
411 does an known account name like sa pose security threat to database when using windows authentication on sql server does it impose the same password policyif it was set to say account lockout after times
431 some statements like create table insert into etc take semicolon at the end create table employees demo employee id number6 first name varchar220 last name varchar225 constraint emp last name nn demo not null while others like set echo on connect system manager go through without the semicolon as well what is the reasoning behind this how can decide myself where to put the semicolon and where not
434 from the thread what is the difference between sys and system accounts in oracle databases understand that sys schema is supposed to be used only by the database but still an administrator can change the password for the schema can the database connect without the password if not how does it connect even after the password is changed
435 sql desc dual name null type dummy varchar21 sql select from dual sql find it really strange if there is no column named in dual how does the select statement work also why dont see the same behaviour when create my own dual table sql create table dual2dummy varchar21 table created sql desc dual2 name null type dummy varchar21 sql select from dual2 no rows selected sql
440 is it possible and how to convert huge myisam table into innodb without taking the application offline it requires to insert couple of rows into that table every second but it is possible to suspend it for about minutes obviously alter table engine innodb will not work therefor had the plan to create new table with the innodb engine and copy the content into it and in the end suspend the application log thread and rename table unfortunately even doing the copying in small batches of rows generates significant lag after some time edit existing rows are never changed this table is used for logging
442 sqlserver introduced something called sqlcmd mode msdn link at first glance this mode adds variable subsitution from command line batch files and some escaping to os commands is this feature used in your environments production or test
445 luckily this isnt current issue for me as in my current workplace were pretty well supplied with dba knowledge in the team however in small development teams its not uncommon in my experience for one of the team to be nominated as the de facto dba for small team theres rarely enough work for full time dba and or there are dbas but the production dbas dont want to get involved with supporting the dev test environments so tester or programmer will be expected to pick up that role and ive seen people getting suddenly landed with hey joes leaving so youre now our new dba no we have no training budget youll have to teach yourself have fun what resources can you recommend for someone who finds themselves suddenly in the position of having to get up to speed with development dba role what basic tasks should they aim to tackle first just to keep things ticking over
461 the title doesnt make too much sense but couldnt think better title for this problem have the following tables projects id name customers id id project name payments id id customer date sum when users enters the system he will have access to certain project now want to list all the payments for that project and it should be pretty easy select from payments where id customer in select id from customers where id project my question is if it isnt better to add column id project to payments table this way the queries will be easier and faster
467 due to the size of transaction log file it grew up to gb had to delete it because the disk was running out of space know this is not good practice but anyway after that the performance is very bad does anyone know whether it is possible to improve performace and how in this situation
468 realy rarely use triggers so met problem at first time have lot of tables with triggers or more for every table would like to know and change the order of firing triggers for every table is it possible to get this information added here is good enoght article on mssqltips have found
482 know that an insert on sql table can be slow for any number of reasons existence of insert triggers on the table lots of enforced constraints that have to be checked usually foreign keys page splits in the clustered index when row is inserted in the middle of the table updating all the related non clustered indexes blocking from other activity on the table poor io write response time anything missed how can tell which is responsible in my specific case how can measure the impact of page splits vs non clustered index updates vs everything else have stored proc that inserts about rows at time from temp table which takes about seconds per 10k rows thats unacceptably slow as it causes other spids to time out ive looked at the execution plan and see the insert clustered index task and all the index seeks from the fk lookups but it still doesnt tell me for sure why it takes so long no triggers but the table does have handful of fkeys that appear to be properly indexed this is sql database
486 for my job work on javaee application with postgresql as the database although we have sysadmin for our productions servers who also manages our database servers we have no full time dba which makes me wonder if there are any would imagine any full time dedicated dba would work exclusively with oracle database am overlooking something or am correct in assuming that there are no dedicated postgres dbas ps im just asking this out of sheer curiosity pps wanted to tag this question with dba but apparently that would be new tag could someone make this for me
505 have decent amount of data in database have well formed tables and good relationships between them with some redundancy in my data but how far should go with normalization are there performance drawbacks to too much normalization
511 want to search for string in the names of the columns present in database working on maintenance project and some of the databases deal with have more than tables so im looking for quick way to do this what do you recommend
530 we have an off the shelf application that uses microsoft sql database within this application we pick and choose various selection criteria for each report this application then runs these reports believe we have query plan issue the first report we run each day runs very fast minutes any report we run after the first report takes over an hour each night we run scheduled task that stops and starts sql server agent and sql server there are approximately other databases within this one instance of sql server no other databases have performance issues only the one off the shelf product mentioned earlier is there way to clear out all query plans that sql server currently has in memory how can do this without impacting or so users that rely on other databases on the same server
545 im contemplating setting up master slave replication for my database the slave server will be used for redundancy and possibly reports server however one of the biggest issues im running into is that were already maxed out on power at our datacenter so adding another physical server is not an option our existing database server is fairly under utilized as far as cpu load averages never really get above on quad core so the leading idea is to toss in some new drives and double the memory from 8gb to and run second mysql instance on the same physical machine each instance would have separate disks for the database is there anything wrong with this idea edit more info ive luckily never had anything bad enough happen to take down the server but am trying to plan ahead we of course have nightly backups that we could recover from but figured having the redundant data on separate disks would provide quicker solution if the master servers drives failed obviously not if the entire machine goes out as for the reporting aspect any tables we would report off of are myisam so doing expensive reads on the same tables that are being written to can bog down the server my assumption was having slave server to report off of wouldnt affect the main server as long as we threw enough ram at it since cpu load hasnt been an issue yet
550 and if can any reason performance stability why shouldnt
561 what are the best resources for learning how to operate and administer mongodb there are plenty of resources on developing against it which actually creates problem when youve got plenty of development competence and really need to filter out that noise for developers who cant afford to hire dba yet we need to get reasonably good at keeping our mongo cluster stable performant etc beyond the faqs and quickstart guides are there any other good repositories of info good blogs to follow etc
567 how do find out if procedure or function exists in mysql database and is there any discovery option like show procedures like show tables
573 weve been looking into using ssds with oracle to speed up our test migration runs it currently takes hours to complete migration run depnding on the volume of data were obviously doing lots of performance tweaking too weve number of cheap linux boxes were using for various runs and analysis the cost of ssds direct from dell is prohibitive was wondering if anyone has experience of using consumer ssds such as the crucial micron ones realise trim support would be an issue on linux using centos has anyone used them on windows to counter this
581 what mean is the following if creating an index on table with rows takes time will creating an index on the same table with ntake approximately time what im trying to achieve is to to estimate the time it takes to create the index on the production database by creating the same index on the much smaller test database
586 in postresql im trying to create view which will look just like an existing table but have different column names this works create or replace view gfam nice builds as select family tree family tree id as family tree family tree name family tree family tree description from gfam family tree the above makes duplicate of the family tree table but the following attempt fails create or replace view gfam nice builds as select family tree family tree id as family tree family tree name family tree family tree description from gfam family tree error cannot change name of view column family tree id how can rename columns
598 in this one project am working on need to set particular field to be unique not problem but if the field is null want the constraint to be ignored in sql server use filtered index as shown below but this is not available in earlier versions of sql create unique nonclustered index user username iuc on user pinnr where username is not null but dont think this is available in sql server in fact this blog post indicates there is workaround using trigger to check for uniqueness does anyone have an example of this or maybe an alternative unfortunately upgrading to sql server is not an option for this particular client
606 with sql sever database which is on remote hosted dedicated server can work using sql server management studio installed either on the same server or on my local computer the first case should work using remote desktop and this makes the work little bit slower in the second case need to open additional port in servers firewall but will have more comfortable user experience what is the recommended practice of these two
612 have table with at least million records in it these rows were created by custom app that reads several sharepoint site collections and stores the item urls in the table now since we read the site collections in serial manner first few thousands of rows belong to first site collection next few thousands belong to second site collection and so on have another app that reads this table in sequential manner however this way end up sending http requests to the same site collection for longer time know could get random results from the table in my second app but that is not an option cannot change the way the second app works now the question is how can take all rows in the table shuffule them and store it back in the table update sql server r2 is my database server
629 ive got process that grabs bunch of records 1000s and operates on them and when im done need to mark large number of them as processed can indicate this with big list of ids im trying to avoid the updates in loop pattern so id like to find more efficient way to send this bag of ids into ms sql server stored proc proposal table valued parameters can define table type just an id field and send in table full of ids to update proposal xml parameter varchar with openxml in proc body proposal list parsing id rather avoid this if possible as it seems unwieldy and error prone any preference among these or any ideas ive missed
647 am looking at rolling out cms system that will require the creation of around tables within the primary mysql database of the system the database will be the data store for several hundred small website front ends that might draw modest load of around 150k unique viewers per month but this might have to scale on short notice im looking for some advice around what kind of hardware should be used to be cost effective but also to have the ability to scale if the need arises would also like some advice around the software configuration should the mysql setup be clustered or just straight forward mysql with high number of open files any advice will be greatly appreciated
651 posted the same on stackoverflow please let me know if have to delete one im working on db2 database and as far as can see regexp is not supported without additional libraries so cannot implement something similar to what is explained in this article bringing the power of regular expression matching to sql do you know if can emulate with sql statement regular expression like this aofdmep z0 sidbfkfpo edit in the above hypothesis found that its acceptable for my case where like predicate where user name not like but its unsafe and doesnt cover other cases where dont have fixed char that can match
653 apologies for the bad title wasnt sure what would be good title for this this is currently simplified view of the data im working with agent commission smith neo morpheus need to calculate the percentage of the total commission each agent is responsible for so for agent smith the percentage would be calculated as agent smiths commission sumcommission so my expected data would be agent commission commission smith neo morpheus have function returning the commission for each agent have another function returning the percentage as commission sumcommission the problem is that sumcommission gets calculated for each and every row and given that this query would be run on data warehouse the data set would be rather large currently its just under records and quite honestly bad approach imo is there way of having the sumcommission not calculate for every row being fetched was thinking something on the lines of part query the first part would fetch the sumcommission into package variable type and the second part would refer to this pre calculated value but im not sure how can accomplish this am limited to using sql and im running on oracle 10g r2
659 have user getting an ora indicating that the password will expire within six days ran the following alter profile default limit password life time unlimited but when try to log in as the user the message is still there executing this select from dba profiles where resource name like password life time shows that the values was really changed to unlimited
667 im working on application that uses dynamic query to do select statement based on user input after discussing security with dbas they want me to convert my dynamic select statement into stored procedure have built dynamic sql using mssql but can not figure out how to convert it to oracle sql create procedure getcustomer firstn nvarchar20 null lastn nvarchar20 null cusername nvarchar10 null cid nvarchar15 null as declare sql nvarchar4000 select sql firstname lastname username userid from customer where if firstn is not null select sql sql and firstname like firstn if lastn is not null select sql sql and lastname like lastn if cusername is not null select sql sql and username like cusername if cid is not null select sql sql and userid like cid exec sp executesql sql firstname nvarchar20 lastname nvarchar20 cusername nvarchar10 cid nvarchar15 firstn lastn cusername cid please note that want to prevent sql injection do not want to just add string together have built separate class for creating this dynamic query for my application in net have almost lines of code to handle everything and prevent sql injection but dbas have told me that they want stored procedures so they can control input and output
668 is there way to audit logins to mysql id like to be able to create username for each employee and thereby create an audit trail of logins however googling has turned up no good results the more we can audit the better at the very least it would be nice to know who logged in when it would be even better to see who executed what query when the logs are there mostly to tell clients we have them since there is potentially sensitive information in the database obviously being able to audit the queries executed by each user and when would also give us the ability to better pinpoint who is the cause of an security issue if one should arise
682 im not finding user by show grants for username localhost im thinking that perhaps the username is slightly different so want to do wildcard search of all the database users how can do this update for clarification as im thinking maybe ive not been clear by wildcard meant wanted to be able to search for only part of the users name or for all users whos name matches pattern on the system needed this on there were 5k users and although thats not performance loss huge dont want to look through them one at time either
688 want to replicate the contents of mysql database to ms sql server database is this possible can anyone outline the steps required in order to achieve this thanks
692 is there any performance hit when doing select across another db on the same physical machine so have databases on the same physical machine running within the same sql instance for instance in somstoreproc on this db run select somefields from the other db dbo sometable so far from what have read on the internet most people seem to indicate no
696 prior to oracle was using custom aggregate function to concatenate column into row added the listagg function so am trying to use that instead my problem is that need to eliminate duplicates in the results and dont seem to be able to do that here is an example create table listaggtest as select rownum num1 decoderownum12to charrownum num2 from dual connect by rownum select from listaggtest num1 num2 duplicate what want to see is this num1 num2s here is listagg version that is close but doesnt eliminate duplicates select num1 listaggnum2 within group order by null over num2s from listaggtest have solution but its worse than continuing to use the custom aggregate function
710 manage an application which has very large nearly 1tb of data with more than million rows in one table oracle database back end the database doesnt really do anything no sprocs no triggers or anything its just data store every month we are required to purge records from the two of the main tables the criteria for the purge varies and is combination of row age and couple of status fields we typically end up purging between and million rows per month we add about million rows week via imports currently we have to do this delete in batches of about rows ie delete comit delete commit repeat attempting to delete the entire batch all at one time makes the database unresponsive for about an hour depending on the of rows deleting the rows in batches like this is very rough on the system and we typically have to do it as time permits over the course of week allowing the script to run continuously can result in the performance degradation that is unacceptable to the user believe that this kind of batch deleting also degrades index performance and has other impacts that eventually cause the performance of the database to degrade there are indexes on just one table and the index data size is actually larger than the data itself here is the script that one of our it people uses to do this purge begin loop delete from tbl raw where dist event date to date date mm dd yyyy and rownum exit when sql rowcount commit end loop commit end this database must be up and weve only got day maintenance window once year im looking for better method for removing these records but ive yet to find any any suggestions
712 want to be able to run query to get the crucial information about databases status want the query to be able to tell what whether or not the database is in good state this is the query that inherited for this check select name as suspectdb databasepropertyname nissuspect as suspect databasepropertyname nisoffline as offline databasepropertyname nisemergencymode as emergency has dbaccessname as hasdbaccess from sysdatabases where databasepropertyname nissuspect or databasepropertyname nisoffline or databasepropertyname nisemergencymode or has dbaccessname if that query returns any results the assumption being made is that the database is in suspect or potentially bad state is there better way to do this
728 do you know if there is system product technique which would allow to wrap one or more relational databases into one object oriented virtual database or could we call it multi database object oriented proxy let me illustrate the principle so basically the pink thing in the middle hereafter object proxy would contain definitions of business entities and their mappings to data in multiple relational databases applications would request or insert update delete data from the object proxy which would magically synchronize with underlying relational databases is that possible does such system exist
736 possible duplicate files in the database or not was wondering if theres any good reason to still use blob fields in database couple of years ago worked with db with bunch of images in it the db was very slow and couldnt see any good reason to keep the images inside the db so got the images out and stored the filenames instead was this smart move what would you do in my place
742 am currently running script which performs dbcc indexdefrag on every table in sql server database one table at time using dbcc dbreindex instead of indexdefrag is not an option due to space constraints and uptime requirements have noticed that it takes long time for certain tables to be defragmented for instance if examine the sys dm exec requests dynamic management view can see that the following indexdefrag is currently churning away on the clustered index of table that has table id of dbcc indexdefrag know that it will be long time before the defragmentation process completes leaving aside the fact that the script currently running will eventually defragment all the tables is there any harm in me manually running another dbcc indexdefrag on the clustered index of another table while the current command executes will both tables actually be defragmented at the same time if do this
756 on production sql server am seeing intermittent enormous spikes in data traffic up to 200mbit which is causing network io waits which in turn cause query timeouts how can find out what queries are returning big result sets
760 have table with four columns that are all non nullable and the data is such that all four are needed to distinguish unique record this means that if were to make primary key it would need to comprise all columns queries against the table will almost always be to pull back single record all columns will be filtered in the query since every column will need to be searched does having primary key benefit me at all besides enforcing uniqueness of records
763 came across view in our database today where the first statement in the where clause was where shouldnt this return true for every record why would someone write this if it isnt filtering any records
771 understand that there may be difference in meaning or intent between the two but are there any behavioral or performance differences between clustered primary key and clustered unique index
788 were using postgresql v8 there are tables involved employee and emaillist table employee column1 column2 email1 email2 column5 column6 table emaillist email tables are joined in such way that if either employee email1 or employee email2 do not have matching entry those rows will be returned select employee email1 employee email2 e1 email is not null as email1 matched e2 email is not null as email2 matched from employee left join emaillist e1 on e1 email employee email1 left join emaillist e2 on e2 email employee email2 where e1 email is null or e2 email is null column email which is varchar256 of emaillist table is indexed now the response time is seconds table count statistics currently employee has got records emaillist has got records and both tables are expected to grow in future is it good idea approach to index varchar column this question immediately strike on my mind because of the reason that weve not indexed varchar column before in our application experts advice suggestion on this are highly appreciated with this current query and index the response time of seconds is reasonable or is there any scope for further tuning what are other users real time experience opinion based on this kind of table size and response time note my actual requirement use case is explained in detail here
790 let us review this dba exchange oracle question for sql server this is sauces code after little formatting create procedure getcustomer firstn nvarchar20 null lastn nvarchar20 null cusername nvarchar10 null cid nvarchar15 null as begin declare sql nvarchar4000 select sql firstname lastname username userid from customer where if firstn is not null select sql sql and firstname like firstn if lastn is not null select sql sql and lastname like lastn if cusername is not null select sql sql and username like cusername if cid is not null select sql sql and userid like cid exec sp executesql sql firstname nvarchar20 lastname nvarchar20 cusername nvarchar10 cid nvarchar15 firstn lastn cusername cid end go sauce mentioned in his second note that he had lines of code to prevent sql injection my feeling is that he neednt do this to shield his stored procedure my question is this sql server procedure immune against sql injection by itself is it as satisfying solution with respect to performance
808 ive heard mention of statistics that sql server keeps by default what are they tracking and how can use this information to improve my database
817 know that in sql management studio can right click table trigger key and script object as is there way to do this programmatically given an objects name if so is there way to find all objects primary keys foreign keys triggers associated with given table and script all of them programmatically
818 as developer use sql profiler quite often its good debugging tool both to track what my code is doing and to analyse performance problems but ive always used it on my development environment and in very controlled way start my application and get it into specific state start trace on the profiler perform specific sequence of actions on my application stop the trace and examine the results can the sql profiler be practically used in an in production environment my first concern is that it would degrade the performance my second concern is that because its in production you arent triggering the interesting actions itself you would have to leave the profiler running for long period then analyse the results would the result set become too unwieldy taking up too much disk space and being too hard to query does anyone use the sql profiler in production
845 was wondering if there was way can execute sql loader script from sql plus we are using oracle 10g
854 we have very large database 6tb whose transaction log file was deleted while sql server was shut down we have tried detaching and reattaching the database and undeleting the transaction log file but nothing has worked so far we are currently running alter database dbname rebuild log on name dbname filename logfilepath but given the size of the database this will probably take few days to complete questions is there difference between the command above and the following one dbcc checkdb dbname repair allow data loss should we be executing repair allow data loss instead its worth noting that the data is derived from other sources so the database can be rebuilt however we suspect it will be much quicker to repair the database than to reinsert all the data again update for those keeping score the alter database rebuild log command completed after around 36hrs and reported warning the log for database dbname has been rebuilt transactional consistency has been lost the restore chain was broken and the server no longer has context on the previous log files so you will need to know what they were you should run dbcc checkdb to validate physical consistency the database has been put in dbo only mode when you are ready to make the database available for use you will need to reset database options and delete any extra log files we then ran dbcc checkdb took about 13hrs which was successful lets just say that weve all learnt the importance of database backups and granting project managers access to the server
858 recently found out that mysql doesnt support rollback of ddl such as alter table being used to postgresql that struck me as odd but friend of mine told me that even oracle doesnt allow it are there technical reasons for not supporting it is it simply an uninteresting feature for them edit just found this comparison it looks like there are many dbmses that do support transactional ddl
861 im trying to optimize the following statement vi castmonthgetdate as nvarchar castyeargetdate as nvarchar cast number as varchar the statement produces value like vi1 if the number parameter is would like to optimize this in terms of removing redundant cast statements and providing an efficient way to concatenate the strings and integers
872 need to optimize the following query select things omitted articles blogpost id articles id as articleid from blogposts join articles on articles blogpost id blogposts id where blogposts deleted and blogposts title like de and blogposts visible and blogposts date published now order by blogposts date created desc limit explain select gives me the following result id select type table type possible keys key key len ref rows extra simple articles all blogpost id null null null using temporary using filesort simple blogposts eq ref primary primary articles blogpost id using where why does it first take the articles and then the blogposts is it because blogposts have more entries and how can improve the query so that the articlepost can use an index update an index is set on blogposts date created removing the blogposts title like condition and the date published now doesnt do anything when remove the articles id as articleid it can use the blogpost id index on articles sounds strange to me someone knows why because actually need it the new explain looks like this id select type table type possible keys key key len ref rows extra simple articles index blogpost id blogpost id null using index using temporary using filesort simple blogposts eq ref primary primary articles blogpost id using where
888 in ms sql server whats the best way to update an updated by updated on set of fields on table ive seen it done it triggers in the code etc the upside ive seen to triggers is that it all happens in the same place on the downside there are occasions when an administrator has to bulk fix table and doesnt want to obliterate the username time of the last user update note dont want timestamp want the windows user id and human readable date time of the last change looking for what you recommend and why its good choice
907 have some standard sql that run against multiple databases on single server to help me diagnose problems select so name so type maxcase when sc text like remote then else end as relevant servername as server db name as dbname from sysobjects so with nolock join syscomments sc with nolock on so id sc id where sc text like emote group by so name so type order by so type so name how can execute this against all databases on single server besides manually connecting to one at time and executing
916 have table with identity column say create table with id id int identity11 val varchar30 its well known that this select into copy from with id from with id results in copy from with id with identity on id too the following stack overflow question mentions listing all columns explicitly lets try select id val into copy from with id from with id oops even in this case id is an identity column what want is table like create table without id id int val varchar30
917 from what can tell there are three possible ways of backing up your sql server database full backup differential backup log shipping what are the pros and cons of each strategy and in what situations should they be employed
940 how do determine if table exists in sql server database in sql server
955 have query where want the resulting records to be ordered randomly it uses clustered index so if do not include an order by it will likely return records in the order of that index how can ensure random row order understand that it will likely not be truly random pseudo random is good enough for my needs
956 like many others we upgraded our instance of mysql from to since then weve been running into an error that repeats itself when we perform certain actions attempting to view or run stored procedure attempting to insert into table after adding trigger for on insert actions executing select lenghtdescription id from table with the misspelling when length was spelled correctly the error did not display the error in question column count of mysql proc is wrong expected found the table is probably corrupted this has been reported as bug basically lack of backwards compatibility but there appears to be some confusion as to how to resolve the issue mysqls documentation says the solution is to dump your stored procedures again after the upgrade to then restore them again the bug report mentioned earlier shows that this has dubious results the bug report suggests running mysql upgrade as possible solution albeit with some concerning warnings however it has been reported elsewhere that this solution doesnt always work am in the unfortunate circumstance of not having test box under which to test possible solutions as our development box is pending an os upgrade to allow us to install the versions of mysql we want to install am hesitant to try any solution on our production environment unless it has been verified as working by people who have encountered this issue what is the correct way to resolve this issue outside of waiting for an official patch or solution from mysql while losing our stored procedures is not ideal it is also route we are willing to take to resolve the issue provided that there are no better alternatives and it will solve the issue cleanly
973 am new to oracle databases and would like to develop data dictionary and er diagrams for our existing databases do you have any tips scripts tools for doing so
983 in any given database that hold user records in the form of an unique auto increment field for the sake of the example inter user messages what to do when the time comes and it approaches the max signed or unsigned number of the current datatype bit int im guessing that the database server would overflow when it tries to assign the number to the next entry so how to avoid that happening without changing the datatype for the sake of the question and keep adding records what would you do why would use ints and not for example varchars it has been several days since ive asked myself this hypothetical question and would like to know what professional would do
1014 does someone who has worked extensibly sql wise with at least two top db products such as oracle sql server informix sybase db2 teradata know how different the db vendors sql dialects are from each other since come from an oracle background am especially interested in analytical functions model clauses hierachical queries start with connect by comes to mind any other feature that is more than the usual select from where probably the question boils down to if and to what extent these features are regulated by an ansi standard practically id like to know if there are rule of thumbs that would indicate if and how can take an sql dml statement that runs on one database and let it run on an other database
1018 example have more than tables starting with prefix dp and about starting with ex question how can drop all tables starting dp in one query
1021 im using expressmaint and windows scheduled tasks to create weekly full and daily differential backups of some ms sql server r2 express databases weekly fulls expressmaint local sqlexpress all db backup reports ru weeks rv backup data bu weeks bv daily diffs expressmain local sqlexpress all dif backup reports ru weeks rv backup data bu days bv when come to restore these to certain point have to restore each backup individually is there any way can chain series of backups into single restore that will be re played in the correct order when try this get the error an exception occurred while executing transact sql statement or batch microsoft sqlserver connectioninfo additional information the media loaded on foo bar fullbackup bak is formatted to support media families but media families are expected according to the backup device specification restore headeronly is terminating abnormally microsoft sql server error for help click http go microsoft com fwlinkprodname microsoft sql server prodver evtsrc mssqlserver evtid linkid edit am using the restore dialog from the tasks menu when you right click on database from there select restore from device and add the bak files wish to restore from if only add single bak file im ok if add multiple files receive the error above
1031 currently whenever write query that is adding columns that can contain nulls resort to wrapping each field in isnull or coalesce such as coalescescore10 coalescescore20 is there better way to handle this or is this the standard practice
1038 need to move whole bunch of large millions of rows tables from one sql2008 database to another originally just used the import export wizard but all the destination tables were missing primary and foreign keys indexes constraints triggers etc identity columns were also converted to plain ints but think just missed checkbox in the wizard whats the right way to do this if this were just couple of tables would go back to the source script out the table definition with all indexes etc then run the index creation portions of the script on the destination but with so many tables this seems impractical if there wasnt quite so much data could use the create scripts wizard to script out the source including data but 72m row script just doesnt seem like good idea
1040 what are main differences between database administrators and software engineers to what extent should software engineer know the details of the underlying database where is the border between these two professions
1043 remember from the stackoverflow podcasts that fog creek use database per customer for fogbugz assume that means the fogbugz on demand servers have 10s of thousands of databases we are just starting to develop web app and have similar problem to solve lots of customers with their own isolated data what problems should expect with using database per customer how can solve them my initial thoughts advantages of database per customer simpler database schema simpler backups you can backup each customer in turn without it really impacting on other customers makes it easy to export given customers data better cache performance write to one of the more active tables only impacts that single customer that performed the write easier to scale across hardware for example when we need to go from to servers we just move half our customers to the new server disadvantages can mysql cope with databases would performance suck changes to the schema can be hard to replicate out across all the databases we would really really have to have an automated plan for this such as versioning the schema and script that understands how to take database from one version to another doing anything that is common to all our customers might be awkward or impossible similar to above but any analytics we want to perform across all our customers might be impossible how should we track usage across all customers for example
1044 im huge fan of lot of the redgate tools however at my current workplace they are very paranoid over anything interacting with live servers in particular affecting performance so my question is specifically on their sql search tool what impact does it have on server im assuming that there will be initial indexing what does it interact with idea on impact on server can it be customised to not index live servers subsequent connections refreshes how is it determining if there is change to be re indexed and what performance load impact that will have on server
1083 have table with the following texts and the key word am searching for is search so have written query select id textvalue from dbo searchlike where textvalue like search how can modify the query so that only the records having search in the text is returned and it shouldnt be taking lsearch as per the image first three records only to be returned
1099 astor data greenplum and gridsql all allow massive parallel processing of sql queries they are also all built around postgresql technology is this just because of licensing issues or are there other reasons to me it seems like the myisam not being acid complient and therefore not running into the same issues with mvcc like seen here as postgresql is far better suited for building high performance data warehouses after all olap load does not require transactions as far as can see
1121 know the question might sound too stupid but never understood this part sql plus works with both sql and pl sql how do know whether some code is sql or pl sql if my code has for loop is it not sql anymore pl sql is an extension for sql to have loops conditionals etc then any sql code is by default pl sql code isnt it so is there demarcation between sql and pl sql two examples of differentiating sql and pl sql that triggerred this question what is the difference between these two create table statements https stackoverflow com questions oracle 11g varray of objects
1126 am working with team that is trying to implement an eav system they have decided to split the attribute value tables out by type and they are debating using different tables for different size ranges of varchar ex table up to varchar10 table varchar11 to varchar500 table varchar501 to varcharmax have always been under the impression that varchar was only going to use the size that it needed do you know if this is going to have any gains in performance and would it be worth the extra coding logic that would be needed
1141 latest version of management studio the one that ships with sql server finally has transact sql intellisense feature however out of the box it only works with sql server instances is there some workaround for this asked this question while ago at stackoverflow and unfortunately nobody knew such trick maybe will have more luck with this problem here
1144 ive inherited database which contains several procedures which are lines long with complex nested sub selects going up to or levels deep in places desperately need to refactor them for my own sanity but how can begin to do this with any level of confidence that they still work the same would write unit tests if this were net do you recommend similar approach
1166 if use database name with dot in it in sql server or something like myapp sales will this cause any problems
1170 is it possible to somehow do this with t1 as select as seq nothing as some type from dual union all select as seq nothing as some type from dual union all select as seq something as some type from dual union all select as seq something as some type from dual union all select as seq something as some type from dual union all select as seq something as some type from dual t2 as select as compare type from dual union all select as compare type from dual select t2 t1 from t1 t2 where case t2 compare type when then t1 some type like nothing else t1 some type not like nothing end know that my where is clause is not correct any help would be great in knowing if this type of statement is possible dont want to write dynamic sql if have to will write different sql statements thanks
1183 am making database for an accounting sales type system similar to car sales database and would like to make some transactions for the following real world actions salesman creates new product shipped onto floor itempk car make year price salesman changes price salesman creates sale entry for product sold salespk itemforeignkey price sold salesman salesman cancels item for removed product salesman cancels sale for cancelled sale the examples have found online are too generic like this is transaction would like something resembling what am trying to do to understand it anybody have some good similar or related sql examples can look at to design these do people use transactions for sales databases or if you have done this kind of sql transaction before could you make an outline for how these could be made my closed as not real question thread so far on stack overflow need example sql transaction procedures for sales tracking or financial database latest update user will send new inputs changes and cancellations from application application data products on display this is the parent node which has child nodes saleschild node of products on display product custom featureschild node of products on display product price current status child node of products on display app will package that data into xml format and then execute some sql stored procedures with transactions holding together the xml to table conversions into the sql tables designed with the same parent child node structure using something like what is described by the answers to this related question on stack overflow https stackoverflow com wish there was book on designing multi user sales databases and the stored procedure transactions that will be used by the related user apps from scratch app xml database please let me know if you know of good one or chapter of book
1201 oracle database backup and recovery basics 10g release says that archived redo logs are the key to successful media recovery back them up regularly but wonder why backups of archive logs is so important would it be possible to do point in time recovery just using regular rman full and incremental datafile backups
1206 am designing baseball simulation program and have run into problem with designing the boxscore schema the problem have is that want to track how many runs are scored in each inning the way do this in the actual program is to use dynamic array that grows for each inning played for those unfamiliar with the game of baseball games are usually nine innings long unless the game is tied still at the end of the 9th inning baseball games therefore have an undetermined length which means cannot design the database to only have columns for the runs scored each inning well technically innings teams one idea have had is to serialize the array and encode it as base64 before storing it in the database however do not know if this good technique to use and was wondering if anyone has better idea in case it matters the database am developing around is postgresql any suggestions are greatly appreciated thanks
1215 typically when see sql that uses something like select from employees where epmloyeetypeid in select id from type where name emp replace the where with this select from employees inner join type on id epmloyeetypeid and name emp is it possible to do the same with the inverse in case its is not in like below instead of an in clause insert into subscriptionsprojectid recordtypecid ntid active added lastupdate updateby select projectid recordtypecid ntid getdate getdate ntid from check chk where chk activestatus and not exists select subscriptionid from subscriptions where projectid projectid and ntid ntid and recordtypecid chk recordtypecid additional considerations can do this insert into subscriptionsprojectid recordtypecid ntidactive added lastupdate updateby select projectid recordtypecid ntid1 getdate getdate ntid from check chk left join subscriptions subs on subs recordtypecid chk recordtypecid and ntid ntid and projectid projectid and chk activestatus and subs subscriptionid is null
1229 how do you calculate mysql max connections what do you take into consideration
1230 im dev who inherited mostly functioning box doing most of what need except for the machine name is still that of the old dev we name it username dt or username lt for ease of id on the network and want to rename it from old username to my username naturally this will affect sql as well so thought would ask for more experienced advice on what need to do before rename my machine know there are some sp sprocs to be run but when do run them do need to restart my box before or after and do need certain level of privilege will it destroy any existing windows based auth on the box those accounts are all ad accounts anyways
1245 this question is inspired by the comment posted to the latest serverfault blog post are you guys still using linq to sql know can use sql server profiler and or the totracestring method to see the generated queries and analyze them myself however am looking for opinions from people with hands on experience with administering databases accessed by applications utilizing the entity framework are entity framework queries common cause of performance problems can linq queries be optimized in such cases or is raw transact sql the only solution
1261 so im fairly new to tuning innodb im slowly changing tables where necessary from myisam to innodb ive got about 100mb in innodb so increased the innodb buffer pool size variable to 128mb mysql show variables like innodb buffer variable name value innodb buffer pool size row in set sec when went to change the innodb log file size value example my cnf on mysqls innodb configuration page comments to change the log file size to of the buffer size so now my my cnf looks like this innodb innodb buffer pool size 128m innodb log file size 32m when restart the server get this error innodb initializing buffer pool size 0m innodb completed initialization of buffer pool innodb error log file ib logfile0 is of different size bytes innodb than specified in the cnf file bytes error plugin innodb init function returned error error plugin innodb registration as storage engine failed so my question is it safe to delete the old log files or is there another method to change the innodb log file size variable
1281 have started to learn pgadmin iii to manage postgresql database but it wasnt an easy to use application if create or have created table with pgadmin iii how can add auto increment functionality on column id that has type integer
1282 am doing type db sql mysql and the console output is nothing see the ibdata1 file growing but dont know how many gb it will grow to would like to see progress indicator of some kind any ideas
1283 before have used phpmyadmin to manage an mysql database but now would like to manage postgresql database the postgresql database is on server but not webserver so dont use php are there any good and free tools for managing postgresql database have tried with pgadmin iii but it was far from intuitive application to use compared to phpmyadmin that have used before what are postgresql dbas usually using do they use graphical tools like pgadmin iii or is it mostly command line tools
1285 am trying to learn postgresql administration and have started learning how to use the psql command line tool when log in with psql username postgres how do list all databases and tables have tried and ds but nothing is listed have created two databases and few tables with pgadmin iii so know they should be listed
1298 have table with the columns of type ntext and for some peculiar reasons the client wants to migrate the column type to nvarcharmax while doing so what all potential threat should be careful about is there any possibility of data loss for the nvarcharmax limitations
1301 when migrating tables coming from other dbmss to oracle one of the standard tasks is to replace all varcharn fields with varchar2n fields provided why does oracle call this datatype varchar2 and not just varchar like other dbmss
1305 the following two queries seem to be equivalent when executed in sql plus select from user tables select from user tables is there any difference between both versions
1326 ive been waiting now for hours for gb sql file to be imported with simple type site sql mysql command can see the ibdata1 is growing still currently nearly gb considering the triggers and stored procedures are at the end of the sql only think mysql should be adding data and key indexes the site sql was generated using this command from another server mysqldump databases site add drop database add create database add drop table single transaction triggers whats taking so long
1346 work at print shop that has decent sized archive of old print jobs currently to find job we have to search the smb share on win2000 server which looks through few hundred thousand files our job data is organized by year month customer name job and then the contents of the job are inside the last folder is there way to create database so that we can query job desc or job number basically just simple search that is faster than searching with windows search would be great tried the windows index service but it only finds doc file not folder names or pdfs in the future we are planning on making sql database where complete info will be entered when the job is created so it is easily queried in the future was hoping to throw together something that will make it easier to search older orders
1359 one of our corporate standards to is have separate filegroup file for user tables indexes this is set as the default so no need to qualify create table statements so it looks like this fileid system tables mdf fileid log ldf fileid user stuff ndf can anyone here help me understand the original justification why this was mandated ill come clean and state think its voodoo am wrong edit am aware of how to use filegroups for separation of indexes partitions archives as well as how to restore piecemeal this question is about the use of separate filegroup on the same volume for system tables only
1371 why does this query delete from test where id select id from select from test temp order by rand limit sometimes delete row sometimes rows and sometimes nothing if write it in this form set var select id from select from test temp order by rand limit delete from test where id var then it work correctly is problem in subquery
1410 have state machine which needs to push pop some file names for different users would traditionally use stacks as the choice of data structure but this needs to be done using database since dont have way to retain the data structure between incoming web requests was wondering what would be good way to implement the stack functionality using databases need to support pushfilename user push filename for the user popuser pop the top most filename for the user edit am prototyping an idea and so am using sqlite3 with python thanks
1423 am designing database and it has so many relationships among my tables and need book that teaches database design very well am looking for book where table relationships simple and complex has been covered extensively and maybe case studies in the book
1426 in sql server how do get the current date without the time part have been using getdate but would like it to have time of
1437 was looking for hierarchical database management system and the only one came across was ibms ims are there opensource systems that one can use
1457 so many times ive been brought in at the end of software development effort and been told something like okay weve got all this new code and it requires tables to change and data to be migrated it seems like every time its one off shoot from the hip best guess scenario feel like this is my weakest skill set as dba id like to get into some patterns for approaching managing and testing data migrations please clue me in to some best practices and or where can get learning material to help me get better in this area
1467 im new in postgresql try to create table in database in psql write create table mail user user char50 not null domain char50 not null password char50 not null but get error error syntax error at or near user line user char50 not null whats wrong how can fix it thank you
1485 is there sql command can run to determine the recovery model of my database want to know if its full recovery or not
1493 use sql server in my web application back end apparently iterate through all the records from the code whenever there is multiple insertion scenario have never tried the multple insertion using xml and think after reading many blogs about xml manipulation using sql server the process is pretty tideous so my question is is insertion via xml much efficient than the traditional insertion is the generic way in which can serialize class in manipulate xml in sql insert also read data as xml deserialize xml to usual object
1497 so have this audit table tracks actions on any table in my database create table track table id int16 unsigned not null userid smallint16 unsigned not null tablename varchar255 not null default tupleid int16 unsigned not null date insert datetime not null action char12 not null default classname varchar255 not null primary key id key userid userid key tableid tablenametupleiddate insert key actiondate actiondate insert engine innodb default charset latin1 and need to start archiving outdated items the table has grown to about 50million rows so the fastest way could delete the rows was to delete it table at time based on tablename this works pretty well but on some of the tables that are write heavy it wont complete my query deletes all items that have an associated delete action on tupleid tablename combination delete from track table where tablename sometable and tupleid in select distinct tupleid from track table where tablename sometable and action delete and date insert date subcurdate interval day let this run on my server for days and it never completed for the largest table the explain output if switch the delete to select id select type table type possible keys key key len ref rows extra primary track table ref tableid tableid const using where dependent subquery track table ref tableidactiondate tableid constfunc using where using temporary so million rows shouldnt take days to delete would think have my innodb buffer pool size set to 3gb and the server is not set to use one file per table what other ways can improve innodb delete performance running mysql on mac osx
1523 for example alter session set nls date format dd mon yyyy hh24 mi ss changes the date format for the session what is nls here
1527 wonder that this question hasnt already been asked google only has very few results for me that dont show high quality tool what are some open source also free is ok solutions for data warehouses and more specifically business intelligence tools what are your experiences with them have had course in my masters programm and we worked with ms business intelligence and mssql as data warehouse storage now want to get more into this topic with tools that are open are there any compareable tools for business intelligence mostly database independent and do you have any experience with them edit with marians comment to stephanies answer see that have formulated the question wrong am aware that dwh are just reporting optimized databases stephanies explanation was very clear on that am more interessted how to get the data into such optimised forms with what kind of bi software tools other techniques
1547 have database query which could result in big result set the client which displays the data receives the data over network so the idea was to minimize the amount of transferred data by retrieving only the first results from the database and sending them over to the client then will provide possibility to jump to the second page to retrieve the next results etc something similar what google offers the question is what is the effective way of implementing paging want to make sure that mssql uses cache as much as possible and that the same is not executed once again everytime change the paging there are more clients who are querying the database in the same time used sql engine ms sql my ideas were use prepared sql statemenst to ensure execution plan sharing use row count variable to retrieve only the needed rows but is it really the most effective way or do you think that it would be better to retrieve the whole result set and implement paging in the code which sends the data over to the client thank you for your tips regards tomas
1554 what are the best practices for running sql server in virtual machine my on line transaction activities are very low but there is high amount of data processing for the purpose of providing reporting data to multiple web sites
1570 an application needs to have data as more freshly updated from database as possible in such case is there any other way for getting the data besides of timer based requesting polling the database work with ms sql server and net applications entity framework but id like to get knowing about other types of databases as well
1577 given certain kind of wait how do you find which queries are causing those waits safely on production sql r2 server in this particular case am wondering about async network io
1584 our databases consist of lots of tables most of them using an integer surrogate key as primary key about half of these primary keys are on identity columns the database development started in the days of sql server one of the rules followed from the beginning was avoid creating clustered index based on an incrementing key as you find in these index optimization tips now using sql server and sql server have the strong impression that the circumstances changed meanwhile these primary key columns are perfect first candidates for the clustered index of the table
1592 in linux the default mysql grant tables can be created with the mysql install db script but that does not work on windows how can the default grant tables be installed on windows no not looking for the response that google results are packed full of about how they are automatically installed on windows because that is only the case with the installer distribution not the zip package besides that does not help after mysql is installed and the data directory is damaged or is being replaced or such
1632 am in middle of debate about whether it is better to make primary key out of an identity columns our out of udf that explicitly generates unique id am arguing for the identity column my partner is arguing for generating the values manually he claims by putting the udf on another table where we can have udf lock the resource increment an id table with one field called id value by use this as global unique identifier or have the table do an id when inserting that its simpler to move data between servers and or environments not having the identify constraint moving from one db where there is data to another similar db with lets say staging or dummy data for testing in non production we may want to pull all records from yesterday down to staging for testing which implementation makes more sense
1635 in his answer to which is better identity columns or generated unique id values mrdenny says when sql denali comes out it will support sequences which will be more efficient than identity but you cant create something more efficient yourself im not so sure knowing oracles sequences have either to create trigger for insert encapsulate each insert into call of stored procedure or pray that do not forget to properly use the sequence when do an ad hoc insert doubt that the advantages of sequences are so obvious
1637 what standard should follow when naming tables and views for instance is it good idea to put something like tbl at the beginning of table names should designate code lookup tables in some way like ct lut or codes are there any other dos donts im using ms sql server and have many databases with many tables so it would be nice to have something we can use as standard with some supporting rational
1654 should many to many table be indexed what kind of index would be best heres an example table create table user role userid int roleid int edit drachenstern added the table def based on the original comments and assumed ints
1656 am in new shop and the below error started to show up on repeatedly until event type error event source sqlserveragent event category alert engine event id description unable to read local eventlog reason the event log file is corrupted event type information event source sqlserveragent event category alert engine event id date time am user computer xxxxxxx description attempting to re open the local eventlog event type warning event source sqlserveragent event category alert engine event id description successfully re opened the local eventlog note some events may have been missed then the server shut down unexpectedly the shutdown stopped the events but my concerns are what was happening does anyone know what this means more importantly what kind of measures do we need to take to prevent this from happening
1677 would like to have good tool for designing the database schema with all the tables columns data types and relations today mostly do this with pen and paper but would like to do it in good design tool is there any good and maybe free database design tool
1692 in order to partition an existing non partitioned table it is possible to use either the exchange partition or dbms redefinition how to decide on which of those possibilities to choose for table partitioning does this depend on how many data reside in the table is one operation safer than the other
1694 have to store the ip address of all registered users in the database am wondering how many characters should declare for such column should support ipv6 as well if so what is the maximum length of ip address
1699 why is it not possible to achieve performance improvements with indexes only so that other techniques like table partitioning becomes necessary the question relates only to performance of course different partitions can be put into different tablespaces which has other effects that cannot be achieved with indices or in other words only performance wise is it possible to achieve same performance improvements with indices as with partitioning of tables
1705 up until recently have viewed the query cache as very important tool to improve query performance today was listening to podcast that discussed tuning the query cache to and using better memory caching solution such as memcache but they also mentioned that there are few cases in which query cache is helpful so general recommendation would be to have it enabled to on demand using select sql cache with query cache type config setting my question is assuming youve got caching solution like memcache in place what type of circumstances would make the query cache more optimal edit added link
1714 have sql server that has become unpredictable of late and im scratching my head as to why queries that executed in seconds are changing plans and taking minutes taking the time in full table scan or index spool now the first and most obvious thing is the statistics are obsolete causing the optimizer to get confused but am convinced this is not the case firstly because the underlying data isnt significantly changing adding one days data ontop of years data already in table and secondly because auto create statistics and auto update statistics are both true however the optimizer is getting confused running the sql in the tuning advisor gives me lots of multi column create statistics statements which do seem to fix it until the next bit of sql misbehaves any ideas of strategy can use to approach root causing this any why the normal statistics arent sufficient
1726 off late ive been facing lot of row lock contentions the table in contention seems to be particular table this is generally what happens developer starts transaction from oracle forms front end screen developer starts another transaction from different session using the same screen minutes in the front end seems unresponsive checking sessions shows row lock contention the solution that everyone throws around is to kill sessions as database developer what can be done to eliminate row lock contentions would it be possible to find out which line of stored procedure is causing these row lock contentions what would be the general guideline to reduce avoid eliminate such problems which coding if this question feels too open ended insufficient information please feel free to edit let me know ill do my best to add in some additional information the table in question is under lot of inserts and updates id say its one of the most busiest tables the sp is fairly complex to simplify it fetches data from various tables populates it into work tables lot of arithmetic operations occur on the work table and the result of the work table is inserted updated into the table in question the database version is oracle database 10g enterprise edition release 64bit the flow of logic is executed the same order in both the sessions the transaction isnt kept open for too long or at least think so and the locks occur during active execution of transactions update the table row count is larger than expected at about million rows also after tracing session found that couple of update statements to this table are not utilizing the index why is it so im not sure the column referenced in the where clause is indexed im currently rebuilding the index
1728 im selecting from table with long text columns id like to wrap long lines to maximum line length from select from test test id text lorem ipsum dolor sit amet consectetur adipiscing elit mauris lorem to test id text lorem ipsum dolor sit amet consectetur adipiscing elit mauris lorem
1732 have database model with user table and role table want to control the access rights to up to different elements the access can be granted to either role or single user below is the table definition of users roles and items create table users id serial not null primary key username character varying unique password character varying first name character varying last name character varying create table roles id serial not null primary key name character varying not null description character varying create table element id serial not null primary key name character varying not null description character varying now have two different ways of designing the rights one table with rights type column or rights tables one for each element want to control the access to what are the pros and cons of one rights table vs one rights table per element or is the more suitable way to do this
1742 this question is not about bytea oid blobs large objects etc have table containing primary key integer field and bytea field id like to enter data into the bytea field this can presumably be done by one of the pl languages and may look into doing this with pl python in the future as am still testing and experimenting would simply like to insert data from file on the server using standard sql statements am aware that only administrators with write permission on the server would be able to insert data in the way would like to im not concerned about that at this stage as users would not be inserting bytea data at present have searched the various stackexchange sites the postgresql archives and the internet generally but have not been able to find an answer edit this discussion from implies that what want to do is not possible how are bytea fields used then edit this similar question from remains unanswered solved the details provided here on the psycopg website provided the basis for solution ive written in python it may also be possible to insert binary data into bytea column using pl python dont know if this is possible using pure sql
1750 using sql server am performing huge delete from with no where clauses its basically equivalent to truncate table statement except im not allowed to use truncate the problem is the table is huge million rows and it takes over an hour to complete is there any way of making it faster without using truncate disabling or dropping indexes the log is already on separate disk any suggestions welcome
1755 which databases use real multidimensional indices is oracle ever using several indices to get data from tables or will it always take the one that seems to have the highest selectivity how about other dbms
1765 another sql server question have simple query that gives me the most cpu intensive sql since the counters were reset select top sumqs total worker time as total cpu time sumqs execution count as total execution count qs plan handle st text from sys dm exec query stats qs cross apply sys dm exec sql textqs plan handle as st group by qs plan handle st text order by sumqs total worker time desc question what exactly is the plan handle it doesnt appear to be hash of the plan like it is in oracle ask because want to be able to detect the situation in which the plan of statement has changes question once have plan handle am interested in the actual plan so do for example select from sys dm exec query plan 0x060006001f176406b8413043000000000000000000000000 in the query plan column get link that when click displays an xml document if save it on disk as whatever sqlplan can double click it in windows and it displays correctly in management studio surely there must be way to avoid this step question is there way to convert the xml back into textual format like in the old days of set showplan text want to be able to view them graphically but also automate diffing them in some meaningful way thanks
1767 in our current project it just happens too often that we need to extend columns by couple of characters from varchar20 to varchar30 and so on in reality how much does it really matter how good is this optimized what is the impact of just allowing or or even chars for normal input fields an email can only have chars so ok there is good limit there but what do gain if set it to because do not expect longer mail addresses than that usually our tables will not have more than rows and up to or such columns we use sql server now but it would be interesting to know how different dbs handle this issues in case the impact is very low as would expect it would help to get some good arguments backed up with links to convince my dba that this long field paranoia isnt really necessary in case it is im here to learn
1775 we have table that we use to store answers to questions we need to be able to find users that have certain answers to particular questions so if our table consists of the following data user id question id answer value sally pooch sally peach john pooch john duke and we want to find users who answer pooch for question and peach for question the following sql will obviously not work select user id from answers where question id and answer value pooch and question id and answer value peach my first thought was to self join the table for each answer we are looking for select user id from answers answers where user id user id and question id and answer value pooch and question id and answer value peach this works but since we allow an arbitrary number of search filters we need to find something much more efficient my next solution was something like this select user id countquestion id from answers where question id and answer value peach or question id and answer value pooch group by user id having countquestion id however we want users to be able to take the same questionnaire twice so they could potentially have two answers to question in the answers table so now im at loss whats the best way to approach this thanks
1792 am having performance issues on certain database queries that have large possible result sets the query in question have three ands in the where clause does the order of the clauses matter as in if put the asi event time clause first since that would remove the most of the results out of any of the clauses will that improve the run time on the query query select distinct activity seismo info from activity seismo info where activity seismo info asi activity id is not null and activity seismo info asi seismo id in and activity seismo info asi event time and activity seismo info asi event time order by activity seismo info asi event time desc explain of query id select type table type possible keys key key len ref rows extra simple act range act fi 1act fi act fi null using where using filesort using php mysql 51a 3ubuntu5 propel symfony
1805 have request like this one select estimateid creationuserid estimatestatusvalueid languageid locationid estimatoruserid filterunitsystemtypeid estimatenumber revisionnumber creationdate modificationdate projectdescription isbsdq closingdate closingtime closingupdatedon deadlinedate isreceived inclusion exclusion misc note workdeadlines comments validity planslocation plansreceivedfrom price from estimate estimates order by closingdate asc closingtime asc when run this query in ssms get executing time of 953ms but when run this query from linq query in my get executing time of 1813ms the linq query use the net sqlclient data provider and is issued against entityframework edmx file is this can be an issue does anybody knows why have big difference between execution times of those requests that are the same but execute from different context against the same database verified all execution plans of both request and they use the same index to satisfy their respective query to see the execution plan of the request use the sql profiler to trap the show plan xml event and compare it to the one of ssms and both are the same
1811 recently discovered that mysql has memory engine that wasnt aware of most of my database work is for hobby projects so learn what need as go it seems like this option should give me drastically improved performance so im wondering if there are any drawbacks that go with it the two that know of are need to have enough ram to hold the tables in question the tables are lost if the machine shuts down believe shouldnt be an issue since im using aws ec2 and can move to an instance type with more memory if needed believe can mitigate by dumping back to disk as needed what other issues are there can the memory engine ever give worse performance than either myisam or innodb think read something that indices are different with this engine is this something need to worry about
1814 we are doing full backup once week and daily diff between that the total db is 80gb the diffs start out under 1gb then gradually grow to approximately 5gb which is ok but then all of sudden jump to 40gb almost every week why would this happen how do investigate im reasonably sure that the actual db usage is uniform spikes
1847 what resources can you recommend for sql server developer wanting to learn oracle basics am looking for comprehensive whitepaper or blog post describing differences between these systems and answering questions like how to create an identity column or what data type is equivalent of float
1866 heard from podcast that there are no orms that have good solution for execution plan reuse it will lead to increased execution plan cache which affects the performance how does nhibernate handle execution plan are execution plan reused in nhibernate
1876 am starting to learn execution plans and am confused about how exactly hash match works and why it would be used in simple join select posts title users displayname from posts join users on posts owneruserid users id option maxdop as understand it the results of the top index scan become the hash able and each row in the bottom index clustered scan is looked up understand how hash tables work to at least some degree but am confused about which values exactly get hashed in an example like this what would make sense me is the the common field between them the id is hashed but if this is the case why hash number
1883 im using ubuntu server and have installed postgresql using apt get install postgresql would like to use the built in sha1 function but it seems that have to install pgcrypto first but dont know how to install it there is no pgcrypto if try to install it using apt get install pgcrypto and dont find any files starting with pgcrypto in my system tried find name pgcrypto how do install pgcrypto so can use the digestword to hashsha1 function in my database queries update im struggling to install pgcrypto on another ubuntu machine after installing the package using sudo apt get install postgresql contrib how do install it to my current postgresql database
1910 want to know why should use an int as lookup tables primary key instead of just using the lookup value as the primary key which in most cases would be string understand that using nvarchar50 rather than an int would use way more space if it is linked to table with many records on the other hand using the lookup value directly would basically save us doing join can imagine this would be big saving if the join is always required were working on web app so this counts quite bit what are the advantages of using int primary key specifically for lookup table other than it being the standard thing to do
1927 was trying to execute fairly large insert select in mysql with jdbc and got the following exception exception in thread main java sql sqlexception out of memory needed bytes at com mysql jdbc sqlerror createsqlexceptionsqlerror java since im not actually returning resultset object thought the java heap space shouldnt be an issue however tried to up it anyway and it did no good then tried to execute the statement in mysql workbench and got essentially the same thing error code out of memory needed bytes should have plenty of ram to complete these operations enough to fit the whole table im selecting from but im guessing there are various settings need to tweak to take advantage of all my memory im running an amazon ec2 high memory double extra large instance with windows server ami ive tried fiddling with the my ini file to use better settings but for all know might have made things worse heres dump of that file client port mysql default character set latin1 mysqld port basedir program files mysql mysql server datadir programdata mysql mysql server data character set server latin1 default storage engine innodb sql mode strict trans tablesno auto create userno engine substitution max connections query cache size 1024m table cache tmp table size 25g thread cache size myisam max sort file size 100g myisam repair threads myisam sort buffer size 10g key buffer size 5000m bulk insert buffer size 4000m read buffer size 8000m read rnd buffer size 8000m sort buffer size 1g innodb additional mem pool size 26m innodb flush log at trx commit innodb log buffer size 13m innodb buffer pool size 23g innodb log file size 622m innodb thread concurrency innodb file per table true join buffer size 4g max heap table size 10g so is this just matter of changing the above settings to work better for my environment if so what settings should use im the only one who ever uses this instance use it for my personal hobby project that involves statistical analysis of large datasets as such im free to let it consume all available resources for my own queries if this is not matter of changing those settings what is the problem thanks for any help you can offer for how to better configure everything
1942 how do switch off schemabinding for view without recreating it
1998 every day have csv file generated by script it has two columns column is name column is the size of their mailbox have years worth of these files would like to be able to import them into database we have sql in house or could install mysql or anything else for that matter want to be able to see growth patterns for these users over time basic reports which is another problem that ill solve at later time for now just want the data in db instead of hundreds of flat files what kind of db is good for this simple is best im not db guy what would you do this is mostly learning project for me
2009 im responsible for creating database on project we have fields that are rarely going to have value in every records and im trying to work out the best way to store this in the database as far as can see have options add column in the table for each extra value add linked table which references the original table and has records only where we need to store value use the xml data type in the original table and store all of the values in this are there any other options that ive not considered im trying work out the pros and cons of each method as far as can tell would be the easiest and would take the least amount of space but im struggling to find many resources for
2041 is there systematic way to force postgresql to load specific table into memory or at least read it from disk so that it will be cached by the system
2050 were looking to decommission sql server instance which has couple databases still remaining on it how can tell if they are still being used by users or web application found forum thread which had sql query you could run to retrieve the last query date it seems to work but want to know if this information is valid enough to drop databases is it if you have alternative methods that would help as well
2063 why does this query cause deadlock update top1 system queue set statusid id internalid where internalid in select top internalid from system queue where isoutgoing isoutgoing and statusid order by messageid asc internalid asc deadlock graph added keylock hobtid dbid objectname dbo system queue indexname pk system queue id lock5b25cc80 mode associatedobjectid owner list owner id processc6fe40 mode owner list waiter list waiter id processc7b8e8 mode requesttype wait waiter list keylock keylock hobtid dbid objectname dbo system queue indexname ix system queue directionbystatus id lock48cf3180 mode associatedobjectid owner list owner id processc7b8e8 mode owner list waiter list waiter id processc6fe40 mode requesttype wait waiter list keylock added thank you sankar for article that has solutions how to avoid this type of deadlock eliminate unnecessary columns from reader projection so he does not have to look up the clustered index add required columns as contained columns to the non clustered index to make the index covering again so that the reader does not have look up the clustered index avoid updates that have to maintain the non clustered index
2072 was using pentaho data integration even before pentaho bought it and call it that have the last free version went on their website recently to see if they had released another version only to find out my favorite open source etl is not much open anymore and not quite free does any of you know of alternatives in affordable easy to use etl tools
2084 my db files are growing very fast designer is not me and just weekly report makes gb growth and finally dont have enough space in the disk and number of un reported weeks counting up have some options to perform but need your recommendations shrink the database does shrinking gives the space to os detach and attach db from an external disk or network storage is network storage supported by sql server change raid config from to to double the disk size this is safest option but also dangerous of raid config change thanks for these great and helpful answers regarding to my question have checked the log file and of space is unused so shrunk it since the db is just used when we need some reports on time request not always need to access to db think performance wont be an issue for me
2109 for faster reporting and performance analysis we want to insert our web server logs into sql server this will allow us to see traffic patterns issues slowdowns in near real time we have daemon that listens for request response events from our load balancer and bulk inserts into the database however we get around gb of logs per day and we only need to keep about week around at least in this raw form what is the best way to store this data and the best way to delete old entries weve talked about storing each days data in its own table log would have all the entries for that day and then dropping the oldest table view could be created to span all the day tables for easy querying is the feasible
2124 want to find out what is causing the high sql compilations not re compilations am seeing in performance monitor counters here is my take on it if am seeing lot of sql compilations then it means that the queries on our system are not getting cached for following reasons many adhoc queries running queries which sql doesn cache update table1 set col1 string longer than characters where key column some int plans are timing out and being removed from the cache because cache is running out of space or plans are not being used long enough the only thing which goes near capturing cache inserts in profiler is stored procedures sp cacheinserts but it only looks after stored procedure cache so tried the following to get adhoc queries select cp refcounts when refcounts becomes plan is excluded from cache cp usecounts cp objtype st dbid st objectid st text qp query plan from sys dm exec cached plans cp cross apply sys dm exec sql text cp plan handle st cross apply sys dm exec query plan cp plan handle qp thought queries that caused the compiles should be the ones with objtype adhoc but this could also relate to re compilations now have to run profiler capture queries causing re compilations and then exculde it from the above list am going in the right direction is there single query can can use to achive just sql compilations without too much work resources which helped me in achiving the above knowledge http social msdn microsoft com forums en sqldatabaseengine thread 954b4fba 42e3 86e7 e5172abe0c83 http www sqlteam com forums topic asptopic id http technet microsoft com en nz library cc966425en us aspx http www sqlservercentral com forums topic914951 aspx any help is really appreciated
2137 what is quick way of finding out what is hitting your database especially when the log files are going crazy sql profiler if so how
2174 what do you do when you have database that is marked as suspect restore from the last backup please advise
2195 can anyone explain how the like operator is implemented in current database systems mysql or postgres or point me to some references that explain it the naive approach would be to inspect each record executing regular expression or partial string match on the field of interest but have feeling hope that these systems do something smarter
2296 what good way to make stored procs robust enough that they can scale very well and also contain error handling additionally whats the best way to handle multiple error scenarios in stored proc and have an intelligent feedback system that will return meaningful error information to the calling apps
2342 have read the definition of 1nf which is if each attribute of relation is atomic please tell me what is atomic
2357 so comment from this question mentions that there is slight difference in stored procedrues and stored funtions in postgresql the comment links to wikipedia article but some of this dont seem to apply that they can be used in select statement the syntax itself seem to be little bit confusing create function emp stamp returns trigger as emp stamp begin end emp stamp language plpgsql create trigger emp stamp before insert or update on emp for each row execute procedure emp stamp you create function but refer to it as procedure so whats the difference between these two
2361 have table that looks like this event id item id person id event date aug aug aug null aug aug aug aug so need to get the latest maxevent date non null person id per item id we have come up with fairly simple pl sql approach to this but were trying to get the job done with straight sql anyone have an idea just as side note the event id will not always be sequential like this we have two redundant db servers thanks ahead of time
2387 when try to drop database get the error cannot drop database dbname because it is currently in use however when run sp who2 there are definitely no sessions connected to this database ive also set the database to single user mode with rollback immediate why is this happening
2471 the answers and comments on the dba se version and programmers se version of the question what are the arguments against or for putting application logic in the database layer are very revealing about the divide between dbas and programmers in some workplaces what could dbas do differently to work better with programmers on issues like this should we study the tools and languages our programmers are using to understand their difficulties they face particularly when working with well designed databases encourage programmers to be better educated about databases and the advantages of having business logic at the database level change the way we define interfaces to our data such as by using more programmer friendly transactional apis eg for issues such as backwards compatibility
2511 seem to remember that on oracle there is difference between uttering select count from any table and select countany non null column from any table what are the differences between these two statements if any
2515 would like to duplicate database on the same server and keep it up to date either by running scheduled service once day or having sql server take care of this internally we dont need to transform data just copy from databasea to databaseb but never from databaseb to databasea would you set this up under the replication services or an ssis job what are the advantages and disadvantages of each the reason am doing is so can have staging application which read and writes to staging database but then every night any new data from the live database is pulled in to the staging database the live database is updated daily with new events and we want to ensure that the staging environment resembles the live environment as much as possible thanks greg
2524 in table such as user table where there will only be row with for any particular user why have an id field let alone as the primary key
2531 have small rows table called restrictions in my postgresql database where values are deleted and inserted on daily basis would like to have table called restrictions deleted where every row that is deleted from restrictions will be stored automatically since restrictions has serial id there will be no duplicates how do write such trigger in postgresql
2545 environment sql server sp3 have stored proc that takes an int as input want to cast char to an int during the call to the stored proc it seems cannot do that get syntax error before foo do not see it can someone help me find it please thank you very much create procedure testme test as int as begin select test end declare foo as char6 set foo 11test exec testme test castsubstring foo12 as int
2562 am parsing log file lines from legacy proprietary application into nice easy to query db the log lines have no unique integer id they do have unix timestamp as an character hex string sadly these timestamps are not always guaranteed to be unique there is also thus the varchar character hex id which when appended to the timestamp is unique tested this out with 400k records and just doing select on the table takes over seconds before go completely redesigning my table in some drastic way want to be sure that using this pk as opposed to an auto incrementing int is where my performance hit is have never really worked on table using something other than regular int pk am developer not dba am using innodb engine and few fk relations to some small tables mysql admin is showing the data length of the table at 150mb and the index length at 21mb with 380k rows as said im developer not dba but in my current situation dont really have one can bring in did some googling but found pretty wide array of answers that often delved into topics that just raised more questions for me im hoping someone here can give concise answer or at least point me to some more resources edit changed the column to char14 and removed one large text column that was somewhat superfluous this seems to have improved the time good deal and took the table size down to about 80mb but im still looking for suggestions
2569 in relation to most efficient way to return multiple aggregates in single stored proc have an email type application and want to select all the messages inbox for user the problem is that normalize the header part of the emails into the db such that the flat data goes into message table and the from to cc bcc get stored to another table whats the best way to select messages in full meaning denormalize the full message so that each record contains all the message pertinent fields including all the message table fields and any related records from the recipient table related to the message as per the pk fk relationship one thing am placing much weight on is the efficiency of the sql solution because this will be code that gets executed many many times over and will likely be the most run sql of the entire db for context here is view of my db schema
2572 would like to use integrated security with my internal application which is all on domain unfortunately ive never been able to get this to work well would like to assign an entire exchange active directory group role in sql server for read write access to certain tables that way wouldnt have to create an operator whenever someone is hired or delete an operator whenever someone is fired is this possible what steps would take to do this
2574 the tables in the schema are intended to be used read only and is only updated once each three month my question refers to performance maintenance backup restore export import what about the temporary tablespace is it better to use different one in this case
2599 im getting new core server for my sql server database would like to create files for my temp db and put each one on separate drive if one of these drives fails can sql server continue to function using the other files
2624 have table that stores images that range in size between kb each since the images are so small ive taken microsofts advice and not used the filestream data type the table is constructed simply create table screenshot id bigint not null data varbinarymax not null constraint pk screenshot primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary the table is heavily inserted into million records in the past week and rarely selected the key is using hilo algorithm so for the most part new rows are added at the end ive been having problems when lot of processes try to insert into this table because of locking and contention queries are timing out from waiting for locks should migrate this table to its own file group and drive how can improve the insert performance and decrease contention in this type of situation
2626 have heard that storing indexes on different filegroup and drive increases performance in database because the drive doesnt have to go back and forth between the index and the data to which the index refers have also heard that this is myth when is it advisable to store nonclustered indexes on separate filegroup and drive what perfmon profiler evidence would lead me to arrive at that conclusion does hardware play role in the decision whether raid san is used over single drive
2631 the site am working on has lot of backend cron activity so there are between and queries being executed every second of the day this can slow down page loads by second or more was thinking the best thing to do would be create two databases and synchronize them daily however do not know how to do this am worried synchronization will be slow block db access really dont like the idea of taking the site offline even for minutes at am so am wondering is separate databases the best solution for this problem or would something else be better how would synchronize the database without interfering too much with the user experience thanks very much btw running php mysql on could servers
2634 sometimes during snafu have to run kill query xxxxxxx twenty or thirty times any sort of kill all command am missing on account of how dont like typing
2650 have table which will potentially store hundreds of thousands of integers desc id key table field type null key default extra id key int16 no pri null from program have large set of integers id like to see which of these integers are not in the above id key column so far ive come up with the following approaches iterate through each integer and perform select count count from id key table where id key id key when count is the id key is missing from the table this seems like horrible horrible way to do it create temporary table insert each of the values into the temporary table and perform join on the two tables create temporary table id key table temp id key int16 primary key insert into id key table temp values select temp id key from id key table temp temp left join id key table as main on temp id key main id key where main killid is null drop table id key table temp this seems like the best approach however im sure there is far better approach havent thought of yet id prefer to not have to create temporary table and use one query to determine which integers are missing is there proper query for this type of search mysql
2662 on sql server machine have dbmail setup and correctly configured am able to use notify operator and exec msdb dbo sp send dbmail to queue emails but the problem is the queued mails never go through see emails in the queue when query select from msdb dbo sysmail unsentitems but if restart the sql server service all mails go through and receive emails in my inbox so far this is the only pattern could find so my question is is there way to troubleshoot dbmail read some steps on msdn but they are not of much help any urls to articles that could help are much appreciated
2677 when am looking to create some timestamp fields or other date time style fields what is the best way to name them should just put record timestamp
2678 how do show the binlog format on mysql server and if dont like it how do set it to xx permanently where xx is statement row or mixed
2684 have you ever had to justify not using query hint am seeing with nolock in every single query that hits very busy server it is to the point that the developers think it should just be on by default because they hate seeing it in their code thousands of times tried to explain that it is allowing dirty reads and they will end up with bad data eventually but they believe the performance tradeoff is well worth it their database is mess no wonder they have performance issues if you have clear example of how to present the case against this abuse of the nolock hint that would be appreciated
2687 am tasked with devising maintenance plan for our sql server databases know for backups want to do daily full database backup and transactional log backups every minutes my problem comes to figuring out which other tasks want to do and how often should do them so so far have this in mind correct me if there are any flaws in my thinking or better way to do this backup all tables full backup daily backup selected tables full backup hourly backup transaction logs every minutes check database integrity daily reorganize index daily update statistics daily shrink database weekly rebuild index weekly maintenance cleanup daily remembered reading some time ago when set up similar plan at another job that some of these tasks dont need to be run on daily basis or should not be run daily as to which ones it escapes me could use little guidance on creating better maintenance plan that will reduce data loss in an disaster but wont tax the system when running during peak hours and also increase the performance
2701 lnnvl is an oracle built in function that returns true for conditions evaluating to false or unknown and returns false for conditions evaluating to true my question is what would be the benefit of returning the opposite of the truth condition rather than just handling the null values for example suppose you have an emp table with startcommission and currentcommission columns which may contain nulls the following returns only rows with neither value null select from emp where startcommission currentcommission if you wanted to include rows where either commission is null you could do something like this select from emp where startcommission currentcommission or startcommission is null or currentcommission is null it would seem like function would exist to shorten this syntax but using lnnvl returns all the non equal records and all the records with nulls select from emp where lnnvlstartcommission currentcommission adding not to this only returns rows without nulls it seems to me that the desired functionality for this case would be to keep true conditions true false conditions false and have unknown conditions evaluate to true have really created low use case here is it really more likely to want to turn unknown into true true into false and false into true create table emp startcommission number32 currentcommission number32 insert into emp values nullnull insert into emp values null insert into emp values 2null insert into emp values
2710 disclaimer please bear with me as someone who only uses databases tiny fraction of his work time most of the time do programming in my job but every odd month need to search fix add something in an oracle database have repeatedly needed to write complex sql queries both for ad hoc queries and for queries built into applications where large parts of the queries where just repeated code writing such abominations in traditional programming language would get you in deep trouble yet have yet been unable to find any decent technique to prevent sql query code repetition edit 1st want to thank the answerers who provided excellent improvements to my original example however this question is not about my example its about repetitiveness in sql queries as such the answers jackp leigh so far do great job of showing that you can reduce repetitiveness by writing better queries however even then you face some repetitiveness that apparently cannot be removed this always nagged me with sql in traditional programming languages can refactor quite lot to minimize repetitiveness in the code but with sql it seems that there are no tools that allow for this except for writing less repetitive statement to begin with note that have removed the oracle tag again as would be genuinely interested whether theres no database or scripting language that allows for something more heres one such gem that cobbled together today it basically reports the difference in set of columns of single table please skim through the following code esp the large query at the end ill continue below create table to test queries create table test attribs id number primary key name varchar2300 unique attr1 varchar22000 attr2 varchar22000 attr3 integer attr4 number attr5 varchar22000 insert some test data insert into test attribs values alfred foobar insert into test attribs values batman foobar insert into test attribs values chris foobar insert into test attribs values dorothee foobar insert into test attribs values emilia barfoo insert into test attribs values francis barfoo insert into test attribs values gustav foobar insert into test attribs values homer foobar insert into test attribs values ingrid foobar insert into test attribs values jason bob insert into test attribs values konrad bob insert into test attribs values lucas foobar insert into test attribs values dup alfred foobar insert into test attribs values dup chris foobar insert into test attribs values dup dorothee foobar insert into test attribs values dup gustav foobar insert into test attribs values dup homer foobar insert into test attribs values dup ingrid foo insert into test attribs values martha bob create comparison view create or replace view ta selfcmp as select t1 id as id t2 id as id t1 name as name t2 name as name dup t1 attr1 as attr1 t1 attr2 as attr2 t1 attr3 as attr3 t1 attr4 as attr4 t1 attr5 as attr5 t2 attr1 as attr1 t2 attr2 as attr2 t2 attr3 as attr3 t2 attr4 as attr4 t2 attr5 as attr5 from test attribs t1 test attribs t2 where t1 id t2 id and t1 name t2 name and t1 name replacet2 name dup note this piece of horrible code repetition create comparison report compare 1st attribute select attr1 as different id id name name dup castattr1 as varchar22000 as val1 castattr1 as varchar22000 as val2 from ta selfcmp where attr1 attr1 or attr1 is null and attr1 is not null or attr1 is not null and attr1 is null union compare 2nd attribute select attr2 as different id id name name dup castattr2 as varchar22000 as val1 castattr2 as varchar22000 as val2 from ta selfcmp where attr2 attr2 or attr2 is null and attr2 is not null or attr2 is not null and attr2 is null union compare 3rd attribute select attr3 as different id id name name dup castattr3 as varchar22000 as val1 castattr3 as varchar22000 as val2 from ta selfcmp where attr3 attr3 or attr3 is null and attr3 is not null or attr3 is not null and attr3 is null union compare 4th attribute select attr4 as different id id name name dup castattr4 as varchar22000 as val1 castattr4 as varchar22000 as val2 from ta selfcmp where attr4 attr4 or attr4 is null and attr4 is not null or attr4 is not null and attr4 is null union compare 5th attribute select attr5 as different id id name name dup castattr5 as varchar22000 as val1 castattr5 as varchar22000 as val2 from ta selfcmp where attr5 attr5 or attr5 is null and attr5 is not null or attr5 is not null and attr5 is null as you can see the query to generate difference report uses the same sql select block times could easily be times this strikes me as absolutely brain dead im allowed to say this after all wrote the code but havent been able to find any good solution to this if this would be query in some actual application code could write function that cobbles together this query as string and then id execute query as string building strings is horrible and horrible to test and maintain if the application code is written in language such as pl sql it feels so wrong it hurts alternatively if used from pl sql or its like id guess there be some procedural means to make this query more maintainable unrolling something that can be expressed in single query into procedural steps just to prevent code repetition feels wrong too if this query would be needed as view in the database then as far as understand there would be no way other than to actually maintain the view definition as posted above actually had to do some maintenance on page view definition once that wasnt far off above statement obviously changing anything in this view required regexp text search over the view definition for whether the same sub statement had been used in another line and whether it needed changing there so as the title goes what techniques are there to prevent having to write such abominations
2716 in sql server how do you reduce the log files size without dbcc shrinking it know shrinking the log file will free up some space but will also cause fragmentation doing checkpoint in simple mode or backing up the transaction log in full mode should do the trick but it is not working in one of my scenarios any advice links will help in preventing log file fragmentation
2736 im trying to import an oracle export into oracle xe get the following messages import in xe fehlerhaft import done in we8mswin1252 character set and al16utf16 nchar character set import server uses al32utf8 character set possible charset conversion any ideas how can import this dump into oracle xe edit given table create table bdata artikel key varchar23 not null name varchar260 not null abkuerzung varchar25 not null get errors like this imp row rejected due to oracle error imp oracle error encountered ora value too large for column bdata artikel abkuerzung actual maximum column abl column aufbewahrungsl sung column afbl some rows are missing from the import
2743 say get resultset back from the following query select from sys database files for any given resultset want to be able to query the column names and types so can then create tables to store the results what is good way of performing this in sql
2758 have column created with type timestamp without time zone default now in postgresql database if select colums it has nice and readable format per default select created from mytable created but would like to get the timestamp in only milliseconds as long something like this select myformatcreated from mytable created how can get the timestamp column in only milliseconds from postgresql response to jack do get the same difference as you but if use timestamp with time zone can see that the error or difference is because gets time zone create table my table 2created timestamp with time zone create table insert into my table created values now insert select created extractepoch from created from my table created date part rows is the difference bug may be because of daylight saving times at the moment also interesting while using to timestamp to insert timestamp and insert into my table created values to timestamp0 insert insert into my table created values to timestamp1 insert select created extractepoch from created from my table created date part
2763 where would set the maximum time query will wait for lock in mysql before timing out
2770 am planing to have very soon few highly loaded postgresql databases have some expirience managing mysql databases with high load but now we have to use postgresql want to know what are the best tools for day to day database management and status reporting of course console is the best one but want to know about other options too all expirience is welcome
2784 ive table with multi column unique index on job id and keyword id would also need to add another index to job id if have frequent query which performs group by on that column at million rows it could take while this is why im asking instead of just doing
2790 have an audit coming up and was wondering what physical electronic and logical access controls an auditor would look for when auditing database for an erp system im really new to this process and any guidance would be appreciated
2796 unix timestamp is the number of seconds since midnight utc january how do get the correct unix timestamp from postgresql when comparing to currenttimestamp com and timestamp 1e5b de dont get the expected time from postgresql this returns the correct timestamp select extractepoch from now while this doesnt select extractepoch from now at time zone utc live in time zone utc what is the correct way to get the current unix timestamp from postgresql this returns the correct time and time zone select now now another comparison select now extractepoch from now extractepoch from now at time zone utc now date part date part row unix timestamp from the web sites
2804 would like to use default value for column that should be used if no rows is returned is that possible in postgresql how can do it or is there any other way can solve this something like this select maxpost id as max id default from my table where org id and if there is no rows with org id in the table want to return
2824 we are using tool that requires specific tables in our db2 database to have primary key defined is there way using select statement on the db to see if given table has one thanks
2868 am using the memory engine for all tables associated with particular mysql query because speed of access is paramount to my project for some reason have noticed that large amount of disk write still occurs is this because of windows swapping the ram to disk how can prevent this from happening edit here are my global variables mysql show global variables variable name value auto increment increment auto increment offset autocommit on automatic sp privileges on back log basedir program files mysql mysql server big tables off binlog cache size binlog direct non transactional updates off binlog format statement binlog stmt cache size bulk insert buffer size character set client latin1 character set connection latin1 character set database latin1 character set filesystem binary character set results latin1 character set server latin1 character set system utf8 character sets dir program files mysql mysql server share charsets collation connection latin1 swedish ci collation database latin1 swedish ci collation server latin1 swedish ci completion type no chain concurrent insert auto connect timeout datadir programdata mysql mysql server data date format datetime format default storage engine myisam default week format delay key write off delayed insert limit delayed insert timeout delayed queue size div precision increment engine condition pushdown on event scheduler off expire logs days flush off flush time foreign key checks on ft boolean syntax ft max word len ft min word len ft query expansion limit ft stopword file built in general log off general log file programdata mysql mysql server data fxmachine log group concat max len have compress yes have crypt no have csv yes have dynamic loading yes have geometry yes have innodb disabled have ndbcluster no have openssl disabled have partitioning yes have profiling yes have query cache yes have rtree keys yes have ssl disabled have symlink yes hostname fxmachine ignore builtin innodb off init connect init file init slave interactive timeout join buffer size keep files on create off key buffer size key cache age threshold key cache block size key cache division limit large files support on large page size large pages off lc messages en us lc messages dir program files mysql mysql server share lc time names en us license gpl local infile on lock wait timeout log off log bin off log bin trust function creators off log error programdata mysql mysql server data fxmachine err log output file log queries not using indexes off log slave updates off log slow queries off log warnings long query time low priority updates off lower case file system on lower case table names max allowed packet max binlog cache size max binlog size max binlog stmt cache size max connect errors max connections max delayed threads max error count max heap table size max insert delayed threads max join size max length for sort data max long data size max prepared stmt count max relay log size max seeks for key max sort length max sp recursion depth max tmp tables max user connections max write lock count min examined row limit multi range count myisam data pointer size myisam max sort file size myisam mmap size myisam recover options off myisam repair threads myisam sort buffer size myisam stats method nulls unequal myisam use mmap off named pipe on net buffer length net read timeout net retry count net write timeout new off old off old alter table off old passwords off open files limit optimizer prune level optimizer search depth optimizer switch index merge onindex merge union onindex merge sort union onindex merge inter engine condition pushdown on performance schema off performance schema events waits history long size performance schema events waits history size performance schema max cond classes performance schema max cond instances performance schema max file classes performance schema max file handles performance schema max file instances performance schema max mutex classes performance schema max mutex instances performance schema max rwlock classes performance schema max rwlock instances performance schema max table handles performance schema max table instances performance schema max thread classes performance schema max thread instances pid file programdata mysql mysql server data fxmachine pid plugin dir program files mysql mysql server lib plugin port preload buffer size profiling off profiling history size protocol version query alloc block size query cache limit query cache min res unit query cache size query cache type off query cache wlock invalidate off query prealloc size range alloc block size read buffer size read only off read rnd buffer size relay log relay log index relay log info file relay log info relay log purge on relay log recovery off relay log space limit report host report password report port report user rpl recovery rank secure auth off secure file priv server id shared memory off shared memory base name mysql skip external locking on skip name resolve off skip networking on skip show database off slave compressed protocol off slave exec mode strict slave load tmpdir windows temp slave net timeout slave skip errors off slave transaction retries slave type conversions slow launch time slow query log off slow query log file programdata mysql mysql server data fxmachine slow log socket mysql sort buffer size sql auto is null off sql big selects on sql big tables off sql buffer result off sql log bin off sql log off off sql low priority updates off sql max join size sql mode strict trans tablesno auto create userno engine substitution sql notes on sql quote show create on sql safe updates off sql select limit sql slave skip counter sql warnings off ssl ca ssl capath ssl cert ssl cipher ssl key storage engine myisam sync binlog sync frm on sync master info sync relay log sync relay log info system time zone eastern daylight time table definition cache table open cache thread cache size thread concurrency thread handling one thread per connection thread stack time format time zone system timed mutexes off tmp table size tmpdir windows temp transaction alloc block size transaction prealloc size tx isolation repeatable read unique checks on updatable views with limit yes version version comment mysql community server gpl version compile machine x86 version compile os win64 wait timeout
2895 somewhat new to using standard sql databases currently working with mysql mostly havent run across many usages of this as of yet when and why is it useful to have negative or rather signed keys indexing table
2905 know lot of database administrators and they are all over years old is all database administration like that mean is this about getting experience more than at least years or is being database administrator so hard
2918 h2 is single threaded database with good reputation regarding performance other databases are multi threaded my question is when does multi thread database become more interesting than an single thread database how many users how many processes what is the trigger anyone has experience to share summary the usual bottleneck is disk access ssds are fast but fragile failure procedure is must one long query on single thread system will block all others configuring multi threading system can be tricky multithreaded databases are beneficial even on single core systems
2967 got question about two databases in oracle 10g that have lets call them and have some information in various tables and want to get partial copy of some tables from and constantly check changes in and sync them in want to ask you about some method technique or maybe ideas knowing that cant make any change in just selects no triggers thank you help and patience for possible edits in advance additional information thanks for the answers dont know if its relevant but found the minus operator though im not sure if it works with sub table select
2973 have table items item id serial name varchar10 item group int and table items ver id serial item id int name varchar10 item group int now want to insert row into items ver from items is there any short sql syntax for doing this have tried with insert into items ver values select from items where item id but get syntax error error syntax error at or near select line insert into items ver values select from items where item now tried insert into items ver select from items where item id it worked better but got an error error column item group is of type integer but expression is of type character varying line insert into items ver select from items where item id this may be because the columns are defined in different order in the tables does the column order matter hoped that postgresql match the column names
2978 im writing script to populate some tables with data for testing would like to write something like the following but dont know how to do it im oracle 11g set enabled user id seq nextval pseudocode set disabled user id seq nextval pseudocode insert into users id usr name values enabled user id andrew insert into car car id car name usr id values carseq nextval ford enabled user id insert into users id usr name values disabled user id andrew insert into car car id car name usr id values carseq nextval ford disabled user id know could rearrange the queries and use the sequence currval reference but id prefer to have the id saved in properly named variables maybe should just wrap the script in declare begin end but im hoping there is more concise way to do it addition may it seems that in any case have to declare the variables in declare block so im trying with declare user id number100 begin insert into test user values user id andrew sysdate end but get the following error caused by java sql sqlexception ora line column pls encountered the symbol end of file when expecting one of the following at in is mod remainder not rem an exponent or or and or like like2 like4 likec between multiset member submultiset that points to the variable declaration im using java to load the script from file and running it using the oracle jdbc driver ojdbc14 jar on oracle 11g server the table test user has been created with create table test users id number10 not null name varchar2100 date ins date default sysdate primary key id
3005 installed postgresql on computer with mac os using the one click installer then try to access postgresql using the psql command but it doesnt seem to be available get this message psql bash psql command not found do have to install something more or how can configure postgresql so can use it on my computer
3042 sql server allows to use select statements similar to select firstname lastname city from dbo customers when try to execute such query on oracle database get the following error ora missing expression missing expression doesnt oracle support such queries
3056 sql server configuration manager is used to configure certain settings like connection protocols service start up etc is it possible to make these changes that is done in sql server configuration manager by using tsql statements or in the ssms
3093 ive recently finished project during which many db tables were created most of these tables contain temporary garbage and am looking for simple way to list all these tables is there way to list all db tables sorted according to their creation date
3102 would like to issue parameterized to oracle via sqlplus with all the the parameters provided in text file is there way to do this the query has more parameters than is reasonable for person to be prompted for and we would like to eliminate user error when entering the parameter for this the reason we need to issue parameterized query is that we are seeing huge difference in performance between straight sql statement and the same query issued as parameterized query in our application we would like to remove any application code from the chain and have the difference evident with only oracle tools
3110 on sql server if all the databases are in full mode with hourly transaction log backups is it possible to determine if rebuilding all indexes of database can grow log file of database and how much can it grow if there is no straight answer then any directions would be really appreciated thanks in advance
3115 want to know if certain postgresql table is part of any inheritance relationship is parent or child of other tables is there query for that where is the inheritance data stored
3134 about sql computing databases when we have two or more fields in table that together identify its records uniquely whats the proper way of calling them composite or compound keys ive seen on the web both uses so im not really sure
3139 we have an organic environment meaning people piled code on code for ten years with minimal oversight or documentation the server use has several databases which believe are no longer being used id love to delete them and leave just the three actually use at the reckless extreme could disable these databases and wait for someone to scream at the other could leave them running forever just in case what steps have you found valuable in identifying whether server is being used and how also what steps would you recommend to ensure that as one moves forward in disabling systems that they remain conveniently reversible for period of time rename objects rather than deleting them outright thanks
3140 how to create incremental number in oracle sql query without create any table have tried using with clause but failed to get the expected result am using oracle 10g here is the code that tryit seems not working with table3 as select years from dual where union all select t3 years from table3 t3 where and t3 years select years from table3 expected result want is
3148 would like to benchmark some sql queries agains my postgresql database is there any way can time sql queries using psql
3153 have database on sql server on windows server and want to move all of the data to mysql database on ubuntu server have tried using the sql server import and export wizard with the mysql odbc driver and it correctly accesses both databases but the xml files containing the specifications for type conversion did not exist and the specifications were too limited for me to correctly create them does anyone know either how to create the type conversion files or where to get better tool for transferring this data
3160 since varchar takes disk space proportional to the size of the field is there any reason why we shouldnt always define varchar as the maximum varchar8000 on sql server on create table if see anyone doing varchar100 should tell them no you are wrong you should do varchar8000
3204 recently database server with an important db broke some grub linux issue that id prefer not to solve can still access the filesystem is there chance to transfer the database by only moving some directories with content to an equal machine this is bit ubuntu with postgres edit on ubuntu the postgresql data directory is var lib postgresql main and not usr local pgsql data
3221 id like to limit the rows and columns that come back from the show table status command in mysql is there way to get this same information through select statement so can manipulate the results in normal way
3223 this is almost certainly the cause of my other question but thought it was worth separating the two as have hypothesis based on the following log that would love to have falsified or verified my hypothesis is that the other deadlock is actually result of the following queries with the original query hidden based on my understanding the innodb status only shows the most recent transactions is this correct based on the log have checked our code and found the following two queries executed in sequence db execute update people set iphone device id null where iphone device id and people id deviceid user people id have hard coded this query in this snippet to simplify things db execute update people set company id name dad password pass temp password null reset password hash null email redacted gmail com phone null mobile null iphone device id iphone device id blah iphone device time last checkin location lat lat location long lng gps strength picture blob id authority active date created last login panic mode battery level battery state unplugged where people id where db execute is basically doing executenonquery on system data dbcommand object so will this sequence of queries result in deadlock my hypothesis is that the two different field orders in the two queries causes the issue can wrap the two queries up in transaction to resolve the deadlock latest detected deadlock transaction transaction active sec os thread id starting index read mysql tables in use locked lock wait lock structs heap size row locks mysql thread id query id localhost famdev searching rows for update update people set iphone device id null where iphone device id iphone device id blah and people id waiting for this lock to be granted record locks space id page no bits index primary of table family people trx id lock mode locks rec but not gap waiting record lock heap no physical record fields compact format info bits len hex asc len hex 000002b8eedf asc len hex asc len hex 80000000000004c6 asc len hex asc dad len hex data0 asc data1 truncated sql null sql null len hex data2 asc redacted gmail com sql null sql null len hex data3 asc iphone data4 truncated len hex data5 asc len hex data6 asc len hex data7 asc len hex data8 asc len hex asc len hex data9 asc len hex data10 asc len hex asc len hex data11 asc jl len hex data12 asc len hex asc len hex data13 asc len hex data14 asc unplugged transaction transaction active sec os thread id starting index read thread declared inside innodb mysql tables in use locked lock structs heap size row locks undo log entries mysql thread id query id localhost famdev updating update people set company id name dad password pass temp password null reset password hash null email redacted gmail com phone null mobile null iphone device id iphone device id blah iphone device time last checkin location lat lat location long lng gps strength picture blob id authority active date created last login panic mode battery level battery state unplugged where people id holds the locks record locks space id page no bits index primary of table family people trx id lock mode locks rec but not gap record lock heap no physical record fields compact format info bits len hex asc len hex 000002b8eedf asc len hex asc len hex 80000000000004c6 asc len hex asc dad len hex data0 asc data1 truncated sql null sql null len hex data2 asc redacted gmail com sql null sql null len hex data3 asc iphone data4 truncated len hex data5 asc len hex data6 asc len hex data7 asc len hex data8 asc len hex asc len hex data9 asc len hex data10 asc len hex asc len hex data11 asc jl len hex data12 asc len hex asc len hex data13 asc len hex data14 asc unplugged waiting for this lock to be granted record locks space id page no bits index primary of table family people trx id lock mode locks rec but not gap waiting record lock heap no physical record fields compact format info bits len hex asc len hex 000002b8eedf asc len hex asc len hex 80000000000004c6 asc len hex asc dad len hex data0 asc data1 truncated sql null sql null len hex data2 asc redacted gmail com sql null sql null len hex data3 asc iphone data4 truncated len hex data5 asc len hex data6 asc len hex data7 asc len hex data8 asc len hex asc len hex data9 asc len hex data10 asc len hex asc len hex data11 asc jl len hex data12 asc len hex asc len hex data13 asc len hex data14 asc unplugged
3225 have running and quite loaded postgresql server need to take snapshots of the certain tables at the certain time midnight those tables are pretty loaded lot of updates and inserts what is the best way to do it edited table that have to snapshot has about records only of them are changing over the day added actually have one idea to add insert update trigger for each table that should be shapshoted and write all changes to another table that is partitioned on daily basis please tell me if it is god idea or not
3229 have vendor database from want to extract and transform certain information as dont want to change the structure of the original database due to licensing therefor wondering if can put together views from the vendor database but storing those in second sql server database together with some other replicated and transformed data the second database is supposed to live beside the first one on the same server is it possible to have views like this and if so do loose lot of performance when querying the views
3251 materialized viewmv log can be used to allow mv to do fast refresh which only modifies the data that has changed however various conditions prevent the mv from using the log and therefore require complete refresh oracle implemented an atomic complete refresh as delete and insert of every record it does this even if there are ultimately no changes to the data is there way to make this replication intelligent with regard to redo generation merge followed by delete requires querying the source twice would it be worth it to bulk collect the data to do bulk merge and delete is there better way update explored using global temporary table as staging area although they use less than half the redo they still use to much
3255 am trying to get an idea of the size of the hot data part of rather large table and was wondering if this could be done directly in mysql know that with the percona version of mysql can have access to figures like number of rows accessed per table but would actually need those data on per row basis row with id was read times row with id was read times where id is the auto increment column
3276 if have table with the columns id name created date and would like to add column use alter table my table add column email varchar255 then the column is added after the created date column is there any way can specify the position for the new column so can add it after name and get table like id name email created date
3281 have table create table names id serial name varchar20 want the last inserted id from that table without using returning id on insert there seem to be function currval but dont understand how to use it have tried with select currval as id from names id seq select currvalnames id seq select currvalnames id seq regclass but none of them work how can use currval to get the last inserted id
3289 we are migrating our database to new schema but want to validate the data has been moved correctly traditional data comparison tools can compare two databases for differences if the schemas are the same in our case there have been changes to the table designs but all the data from the old schema is in the new one its just been moved around bit and need to make sure it is correct we have tens of millions of rows so manual inspection is not an option are there any tools which could aid in this type of comparison if not are there any libraries frameworks which could help kick start the development of custom solution im happy to use database specific solution if necessary in this case for sql server my soluton im comparing the two data sets by creating view of each table on the old database with the same fields as the new database table then compare the data using the technique described here the shortest fastest and easiest way to compare two tables in sql server union im lucky in this migration as the overall table structure is similar to the old database with fields moved from one table to another dropped or added in the case of the dropped and added there is nothing to compare for the fields which have moved or been aggregated do calculations in the view to provide the correct information for comparison the union comparison shows me only the rows with differences so as soon as the data is correct get an empty result set
3323 this microsoft article how to determine the appropriate page file size for bit versions of windows server and or windows r2 provides guidance for calculating the page file size for bit windows and windows 2008r2 this no doubt works fine for general purpose servers im wondering what the guidance is for sql server 2008r2 running on windows r2 bit im presuming we want as little in memory data to be hitting the page file otherwise sql could be hitting the disk twice for data does sql server even allow data in memory to hit the page file ive hunted through sql server r2 books online for guidance but havent yet found any mention of page file use heres potential usage scenario given physical server with 64gb of ram is pagefile necessary for the entire 64gb of ram should we gear it up for 96gb of pagefile that does seem bit excessive for single file know conventional wisdom has been that windows couples pagefile to memory in an attempt to make swapping out apps easier on ram but is that true will less than 64gb pagefile hinder performance here
3343 have large sql server test database 9tb and want to reduced its size on the disk have been deleting unused tables and purging significant amounts of data but its size on disk it no reducing what should do ive looked at the shrink task in ssms but the options are bewildering do shrink the database or the files and with what options
3345 the thought occurred to me recently but have suspicion the idea might be insane couple benefits im perceiving are facilitate automated logging via triggers control mysql connections on per web application user level obviously this would yield me ridiculously long table of users but any penalties of that arent yet clear to me the user table is properly indexed for ologn single user lookups
3375 want to performance test aka bake off mysql server rpm against some other forks such as percona server mariadb and possibly some others im hoping that by asking this question can better understand the methodology behind setting up proper performance test planned on using sysbench to run my actual test but im open to anything what steps should be taking to ensure that the test results in apples to apples comparison and that only the rdbms is the variant where do get started how do evaluate results what advice can you give me
3394 first of all im developer not dba or sysadmin please be gentle im working on an application workflow where single user action will trigger complex changes in the database creating hundreds of records in some tables updating hundreds of records in others etc all in all about tables out of are touched by this action due to the complexity its very hard for me to manually revert all of the changes before can run another test during most of my development time can simply insert rollback statement near the end of the workflow but when get close to commiting my changes need to test the real thing have local copy of the production database to work with in my case dumping and restoring between tests is faster than writing script to undo all the changes its faster but its still slowing me down lot the restore takes around minutes on my ageing laptop is there any way can save snapshot of the current state of the database and then quickly restore it im guaranteed to be the only user on the system and have root access the database dump is 100mb when tared and gziped postgresql version is thanks in advance for any helpful ideas
3467 know mysql limits auto increment columns to primary keys why is this my first thought is that its performance restriction since there probably is some counter table somewhere that must be locked in order to get this value why cant have multiple auto increment columns in the same table thanks
3480 we run website that has 250mm rows in one table and in another table that we join it to for most queries has just under 15mm rows sample structures mastertable id userid created updated 15mm rows detailstable id masterid somecolumn 250mm rows usertable id role created username 12k rows we regularly have to do few queries against all these tables one is grabbing statistics for free users 10k free users select count1 from detailstable dt join mastertable mt on mt id dt masterid join usertable ut on ut id mt userid where ut role is null and mt created between date1 and date2 problem is this query will some times run long damn time due to the fact that the joins happens long before the where in this case would it be wiser to use wheres instead of joins or possibly where column in
3509 sql desc tab1 name null type id number name varchar21000 sql select from tab1 id name sql select from tab1 where id avgid select from tab1 where id avgid error at line ora group function is not allowed here sql the error is very clear and says that this cannot be done but dont see why the query makes perfect sense select all rows from tab1 whose id value is greater than the average
3512 am new to data mart design and need to clear few concepts have read up bit on dimension modelling where see that fact tables store foreign key references to dimension tables now suppose have phonenumber dimension table and phone extension dimension table these tables have different details because of which cannot combine them as understand both these dimension tables will have integer primary keys for better performance and the fact table will have its own integer primary key and also store foreign key references to these dimension tables but suppose have situation that not all phone numbers have phone extension related to them some phone numbers need not have an extension for phone numbers that have an extension the fact table would have foreign key references to both the dimension tables but how do capture the situation where there are only phone numbers and no extension to them and vice versa extension with no phonenumbers should capture such information with the phonenumber fk in the fact table having value and phone extension foreign key null or are such non related objects not recorded in fact tables also need to generate report of this data mart so do start by querying the fact table and retrieving the dimension key values or report straight from the dimension table thanks for your time in reading this appreciate any help
3519 need to implement basic personalized user functionality for my website is there standard structure for databases of this type like is it common practice to have all user info and data in single table with each user having his own row or should this info be split among different tables and linked together maybe for efficiency im not tremendously concerned with security at this point but ill obviously want to have password encryption before too long tried to find what was looking for on google but to no avail just let me know if the question needs further clarification or anything
3540 currently when executing the imp command with the following parameters file my dmp log my log fromuser myuser touser myuser the following errors occur imp oracle error encountered ora tns protocol adapter error imp import terminated unsuccessfully however tnsping successfully looks up the values in my tnsnames ora file my question is does the dump file created in 10g cause an issue on import into 11g or is this error something else that am unable to diagnose
3552 want to use syslog for logging mysql5 on ubuntu10 lts found information that output error log to syslog mysqld safe syslog but want to use syslog for logging general logs and slow query logs please advise me how to write config file could not find how to do that in the reference manual http dev mysql com doc refman en log destinations html
3605 have several work packages for each have setup up job in sql server agent sometimes should run all together what is the best way to run them all in given sequence im bit surprised that sql server agent is not able to include jobs as job steps to be executed did try
3628 ive been trying to use the sql server management studio 2008s built in solution explorer to manage project and after weeks of struggling with it ive decided dont really care for it its convenience features are actually quite hindrance unfortunately the place where work is ms shop and is pretty stuck in their ways want to go back to the command line crave vim ive installed local subversion repository with tortisesvn front end to replace the above mentioned solution explorer and have it running quite well have also installed vim and while still working on customizing it have it humming however the one thing that am yet to find solution to is how to run commands to sql server via the command line im used to mysql postgres command line prompt since that is what have used in the past and was using while going through my classes however cant seem to find way to execute my scripts via the command line in windows currently edit them in vim refresh the page in ssms and execute would prefer to not even touch smss unless needed ive seen posts suggesting sqlsharp but when went to the page get the feeling that isnt quite what im looking for edit really wish that it was possible to have answer check marks campbell has the technical right answer and ive tried it few times keep getting an error when try sqlcmd in researching that its how named pipes is operating even though have it set up and enabled it doesnt seem to work after trying that and not having any success tried the just copy and paste into management studio method but that was ridiculous so decided that maybe hardcode is correct too
3647 the following text is extract from oracle documentation start the instance and mount and open the database this can be done in unrestricted mode allowing access to all users or in restricted mode allowing access for database administrators only to mount database to previously started but not opened instance use the sql statement alter database with the mount clause as follows alter database mount you can read it directly from http download oracle com docs cd b19306 server b14231 start htm the first part gives me an impression that we are mounting the instance to the database but the to mount database to previously started but not opened instance part gives me an impression that we are mounting database to an instance which one understanding is right anyone can help to explain it please thanks
3653 in daily routine need to drop and do bulk load of database the problem is that have webapp relying on that database so cant drop the database just like this what would be good way to tackle this problem
3674 client has given me the data he wants visualised in my project but dont know what db format its in hes pretty unclear also have access to the table structures but dont know how to read it in its definitely not flat file there are several files for each dataset file dat biggest by far file id small file ind half the size of dat the first few bytes in the dat file are 6e in case magic numbers are used do these extensions ring any bells is there some software to determine the format
3682 would like some expert opinion on best practices when it comes to column naming the background is that according to wikipedia the following syntax select from employees join timesheets using employeeid is more efficient than select from employees join timesheets on employees employeeid timesheets employeeid however the join using syntax only works of all primary key columns have globally unique names thus wonder if this is considered the right thing to do personally always used to create tables with pk column id and foreign key column othertable id but that way its not possible to use using or natural join any links to design styles or best practice guides for table design would be appreciated too
3688 what does it mean that data is valid and accurate according to database integrity
3694 just took backup of sql server database the mdf and ldf files together total around gb but the bak file was only gb about smaller my first guess when one version of set of data is smaller than another version containing the same data would be data compression but compression usually yields much better compression ratio than especially for highly ordered data such as database tables also compressed data cant easily be compressed further but know that bak files can be compressed so if the data isnt being compressed and nothings being discarded because the whole point of making backup is to be able to restore it to an identical state afterwards then whats that thats unaccounted for
3696 im currently making an mmorpg game which could have few thousand players online at the same time probably not just wishful thinking first we wanted to use mysql but have heard it isnt fast enough for this scale which dbms is fast enough how much like sql server is it as have learned sql server in school
3710 this is my first post here and am by no means qualified as windows server or storage server or sql server administrator am mostly unix guy however tasked with job to investigate if our firm small software startup it infrastructure which includes windows server windows storage server and sql server used exclusively by finance hr sales legal and administration depts devs are all on unix what need to get some ideas around is how to protect the windows based infrastructure from insider attacks more precisely against rogue system administrators our current system administration role is played by temp in theory having access to all or most passwords login credentials he can do whatever he likes is there then some technology audit logs tamper proof backups etc that can be setup like onetime whose credentials are available only with an extremely trusted person maybe ceo himself and all other system admin tasks can be performed by this temp chap in worst case if the admin removes deliberately corrupts modifies certain files or alters database content such acts can be tracked down proven rectified understand that this is probably not pure dba question and pardon me for willfully posting this but didnt find more appropriate se forum but found quite few interesting windows server infrastructure discussions here
3737 our company currently hosts about databases per sql server instance databases and we see performance start to deteriorate database size ranges from 50mb to 60gb in the extreme with probably an average of maybe few gb what are some good metrics to collect and how would we get these to determine our bottleneck is it performance limitation of sql server to have too many databases is it all of the cached procedure plans for hundreds of stored procedures for hundreds of databases is it inefficient queries or combination what is likely to help us increase the number of databases per server the most all of the schemas are identical and the databases separate each customers data maybe refactoring everything so we can combine multiple customers into one db and filter by customer or optimizing our queries to the extreme or maybe its just sql server limitation
3751 how should represent latitude and longitude in postgres without using postgis the system am using does not allow sql passthrough so cannot use postgis
3754 have table in my oracle database where select pkcol count from mytable group by pkcol having count yields pkcol count trying to remove the duplicate rows delete mytable where pkcol yields ora index mytable pk mt or partition of such index is in usable state im using oracle dataaccess client oraclebulkcopy to fill the table as far as understand documentation from oracle primary key constraints had to be checked obviously they are not checked as found by doing the same bulkcopy two times in succession which ended in duplicates in all row now im only using it after deleting all rows and im using table with similar primary key as source as result expect no problems but embedded deep inside my ms build scripts end up with just duplicates out of rows guess that ignoring the primary key in the first place is clear bug no bulkcopy should be allowed to ignore primary key constraints edit meanwhile found that the conflicting rows where normally inserted by some script before bulkcopy was called the problem reduces to my known problem that bulkcopy doesnt check primary keys here
3760 have large table with varchar20 column and need to modify that to become varchar50 column typically performing an alter table adding tinyint on this particular table takes about minutes to complete so can really only do that on saturday or sunday night to avoid affecting the users of the database if possible would like to do this modification prior to then the column is also indexed which presume will make the alter table slower because it has to rebuild the index after modifying the column length the web app is set up in mysql replication environment slaves and one master recall once reading somewhere that one method is to first perform the alter table on each slave minimising impact on users then do this on the master but wont that then try to replicate the alter table command to the slaves so my question is what is the best way for me to modify this table with minimum disruption to my users edit the table is innodb
3768 what are the most common best practices on length and data type on common fields like first name last name address email sex state city country phone number etc
3794 have sql server database that has data file of some 2gb in size but the log file is over 8gb with pre databases could use the backup log and the truncate only option but this is no longer available with and later databases do have script that truncates the log file use mydatabase go alter database mydatabase set recovery simple with no wait dbcc shrinkfilemydatabase log alter database mydatabase set recovery full with no wait go this truncates the log file completely but my question is does this affect performance perform two full backups daily so the log should not really be necessary as far as data roll forward is concerned
3816 tablex can be modified in two ways client does direct inserts client uses stored procedure to inserts records how to determinate the way of clients call direct or stored proc in trigger of tablex thank you
3828 have database with varchar column that contains integers of varying length want to sort them so comes after not and 70a comes after was able do this with patindex cte and case statements in the where clause however was wondering if there was collation where this would be unecessary
3831 most of us will probably agree that using database indexes is good too many indexes and performance can actually be degraded as general rule which fields should be indexed which fields should not be indexed what are the rules for using indexes while striking balance between too many and not enough indexes in order achieve performance improvements not degradation
3849 wrote query below which shoots me syntax error why would it do so select maxrow from select row number overorder by id desc row from users error desc incorrect syntax near dont get it
3856 with sql server r2 when save the results as csv there are no headers can work around this by copying and pasting with the copy with headers or just grab the headers and paste them in the csv etc but is there better way to do this
3889 have the following in my my cnf client password somepass but this is not the password use for every user host database connect to is there some way to specify in the config different passwords for different things so dont have to type them in
3913 my company has many branch offices and each branch has db server some have sql server and some others have sql server the schema is the same for all these servers whenever want to do ddl like adding column etc do it either by connecting to each server one by one or by running the ddl script with osql through batch file is there built in tool to do ddl at once on several servers how about third party tools preferably free ones
3914 our company wants to use mysql for one software product this product is not open source is it legal to use it freely if not how about postgresql is this okay for our purpose
3972 have table with columns and when im trying to insert more columns get error code too many columns in this table have only rows for me the most important thing is the number of columns are there any limitations on the table want to create columns is that possible
3980 have stored procedure which accepts various varchar parameters the middle tier code which calls the procedure has not been consistent in terms of the values it submits for example sometimes parameter like transport description will be submitted as null and sometimes it will be an empty string in terms of the client an empty string simply means value was not entered should convert all the empty varchar parameters to null before inserting the data cant decide if this falls into the realm of best practice to preserve the integrity of the data or bad idea because im not representing what was actually submitted to the database should note that all inserts and updates are tracked in an audit log
3999 suppose have two queries that will be frequently run against my database select userid username usergender from users where userid user select userid username usergender from users where username like name should they be in two separate stored procedures or just single one which creates the statements dynamically using sp executesql if they are in two stored procedures then will need to modify both procedures if ever want to add or remove column in the select statement and if use dynamic sql then presumably am sacrificing small amount of performance is this case where maintainability and adherence to the dry principle dont repeat yourself by using dynamic sql would take precedence over the performance gain of stored procedure
4000 we have sql server database that we regularly transfer from our client site to ours this takes long time because we dont have direct connection and have to transfer the file over their web based file transfer application the database is currently about 10gb however we dont need all the data most of it is in audit tables and tables that hold calculated values that can be re generated have looked at creating filegroup to hold the audit tables and was hoping could just backup and restore the primary filegroup can backup fine but when restoring get an error saying that im not restoring it to the same database is it possible to restore part of database to different server using filegroups is there better way to do this
4018 have been trying to migrate database from sql server part of sbs to sql server r2 express edition naively tried using the sql server export and import wizard this has worked to degree but seemed to try and import views as tables and left stored procedures out altogether what is the least painful way of performing this sort of migration
4021 when do this copy mytable from my file csv with delimiter as csv on this rw peter peter file csv psql tells me this error could not open file my file csv for reading permission denied how can read my file thanks update it looks like something called apparmor is installed by default in ubuntu seems to have the same functionality as selinux mentioned in the comments update after removing apparmor still have the same problem selinux is not installed regarding the comment below about access the copy from is being run from superuser account it gives different error message if not and the file permissions copied above understand as readable by everyone update tried to get to the file under the postgres user it gets stuck at particular place in the tree drwxr peter peter phm postgres dexter home peter pypacks cd phm bash cd phm permission denied suppose will just put the file somewhere else but this is strange
4040 am mostly self taught when it comes to database designs am posing this question because have settled on this common structure but am wondering if it is the most efficient or industry standard method most databases design have user table and then persons activty is tracked in another table understand that the beauty of the database is to have these sorts of efficiencies but the activity table will gather many many events fairly quickly just from every user using it regularly thus becoming huge table fairly quickly with moderate user usage is this best practice to just let it grow in this way or is tier of tables or splitting to different tables based on dates or per amount of users or something else userdata activity id auto uint to many id auto uint username text userid uint email text timestamp time additional info type id to elsewhere additional info just would like to know of where can improve anything as to help me learn
4043 someone was running query on our sql server database remotely and their system crashed they have no backup of that query and want to see what was run on the server is it possible to find this query in log or in history somewhere
4106 in order to be confident in databases as entry level job candidate completed the oca certification now want to know what career paths follow this can one go into data mining bi or via this certification and heard dba is quite boring and risky for experimental people
4114 our old database is mysql server and is four years old if uninstall mysql server and install the latest release and restore the backup will gain performance benefits guess the new version release says that there are many performance benefits
4115 have never dipped my hands in mysql cluster and want to know how it can benefit me when is it best to use it and does it have performance benefits
4118 im granting the following rights to user grant super on to user host does this include the rights on dbname given by grant all on dbname to user host thank you
4124 have production mysql database that is running well but want to improve query performances have never used partitions and just going through the manuals have two tables that involve composite key on columns bill num bill date want to create partition on bill date the table consists records of four years want to know how new partitioned table will accommodate future years also want to know whether need to make changes to the table name in existing queries and replace the table name with the new partitioned table name
4129 have am having problems backing up my databases after an update have been poking around on my system trying to figure out why one query ran returned this result got error the user specified as definer cittool does not exist when using lock tables after some investigation it appears that the definer for these views is an old developer account that has been purged from the system the databases and views with this problem are used very infrequently and most being kept around for archival purposes there is about views with definer that no longer exists is there an easy way to change the definer to different account on everything at once is there way to get mysqldump to simply dump all the views out to file so could edit that file and recreate the views
4137 im having problem with permissions in oracle 10g im hoping someone can help me make sense of this have schema with table in it have granted select on that table to role grant select on user1 example table to example role then grant that role to user grant example role to user2 then user2 wants to create view on top of that table create or replace view user2 example view as select from user1 example table that throws an error however ora insufficient privileges why though if they have select permission via the role why can they not then create view on that object found that had to grant the object directly to the user before it would work grant select on user1 example table to user2 is there anyway not to have to do this wanted to use roles because have lot of tables and lot of users and dont want to have to maintain million different grants to individual users
4141 im not thinking of using it for the partition function just want to know if it is supported or not dont tell me why it is not good idea to use such columns on partitioned tables
4152 im in the process of designing database and im having second thoughts about my initial design decisions product types are as follows models parts replacement part kits and options option first design planned on having separate tables for the above product types id say about of the fields would be the same in each table created each product type as separate tables because of the associations need to create between them for instance model can have many options and option can have many models an option can also have many parts and part can have many options and so on option instead of having separate tables could create table called product that encompasses model part replacement part kits and options could have one field called type to differentiate between model options etc suppose down side is several fields would never be used left null for certain product types im guessing this is where not best practices would come into play option would greatly reduce the complexity of the db design also wouldnt have to worry about referencing bunch of tables when pulling out data for queries
4154 when sql server database in simple mode you dont have to care about the transaction log bakcups but in simple mode the transaction log seems to grow as it does in full mode does is truncate automagically at some time point or do have to truncate shrink it manually
4162 have the following stored procedure that works great except when set the orderby to ordernumber get the following error conversion failed when converting the nvarchar value sk11270 to data type int sk11270 is value in the ordernumber column which is nvarchar50 if run the identical query with any other orderby column it works fine im completely lost and my head hurts can anyone see anything obvious that would be causing this problem thanks in advance for any ideas rich this works sp jobs 120jobnumberasc97truetruetruetruetruetruetruetruetruefalsetruefalsetruefalse0 this doesnt sp jobs 120ordernumberasc97truetruetruetruetruetruetruetruetruefalsetruefalsetruefalse0 procedure dbo sp jobs pagenumber int pagesize int filterexpression varchar500 orderby varchar50 orderdirection varchar50 customerid int shownotset bit showplaced bit showproofed bit showreproofed bit showapproved bit showontime bit showlate bit showproblem bit showcompleted bit showdispatched bit showunapproved bit showclosed bit showreturned bit shownostock bit userid int with recompile as begin with keys as select top pagenumber pagesize row number over order by case when orderdirection asc then case when orderby jobnumber then p1 jobnumber when orderby ordernumber then p1 ordernumber when orderby custid then p1 custid when orderby status then p1 masterjobstatusid when orderby datein then p1 datein when orderby datedue then p1 datedue when orderby dateout then p1 dateout else null end end asc case when orderdirection desc then case when orderby jobnumber then p1 jobnumber when orderby ordernumber then p1 ordernumber when orderby custid then p1 custid when orderby status then p1 masterjobstatusid when orderby datein then p1 datein when orderby datedue then p1 datedue when orderby dateout then p1 dateout else null end end desc as rn p1 jobnumberp1 custidp1 dateinp1 dateduep1 dateoutp1 clientp1 masterjobstatusidp1 masterjobstatustimestampp1 owneridp1 stockcompletep1 ordernumber from vw jobs list p1 with nolock where customerid or custid customerid and userid or ownerid userid and shownotset and masterjobstatusid or showplaced and masterjobstatusid or showproofed and masterjobstatusid or showreproofed and masterjobstatusid or showapproved and masterjobstatusid or showontime and masterjobstatusid or showlate and masterjobstatusid or showproblem and masterjobstatusid or showcompleted and masterjobstatusid or showdispatched and masterjobstatusid or showunapproved and masterjobstatusid or showclosed and masterjobstatusid or showreturned and masterjobstatusid or shownostock and stockcomplete and search like filterexpression selectedkeys as select top pagesizesk rnsk jobnumbersk custidsk dateinsk dateduesk dateoutsk ordernumbersk masterjobstatusid from keys sk where sk rn pagenumber pagesize select sk rnj jobnumberj owneridj descriptionj clientsk custidsk ordernumber castdateaddd castisnullsk datein0 as datetime as nvarchar as datein castdateaddd castisnullsk datedue0 as datetime as nvarchar as dateduecastdateaddd castisnullsk dateout0 as datetime as nvarchar as dateout del methodticket invoiceemailed invoiceprinted invoiceexported invoicecomplete jobstatusj masterjobstatusidj masterjobstatustimestampjs masterjobstatusstockcomplete from selectedkeys sk join vw jobs list with nolock on jobnumber sk jobnumber join tbl system masterjobstatus js with nolock on masterjobstatusid js masterjobstatusid end
4163 in the system work on there are lot of stored procedures and sql scripts that make use of temporary tables after using these tables its good practice to drop them many of my colleagues almost all of whom are much more experienced than am typically do this truncate table mytemp drop table mytemp typically use single drop table in my scripts is there any good reason for doing truncate immediately before drop
4177 which websites books etc can you recommend me to improve my basic knowledge in microsoft business intelligence ssas ssis and ssrs please remember that im newbie in technical bi tool if possible it would be great to retrieve any material for task work that would be similiar as doing lab task at university fullmetalboy
4210 for example say have table businessbusinessid lattitude longitude all are indexed of course also there are million records say want to find businesses closest to for example how would do so if do select from business where some formula to compute distance here for example or if do select from business top in theory the computer will have to compute distance for all biz while in practice only those with lattitude and longitude within certain range that should be computed so how can do what want in php or sql for example am grateful with the answer so far am using mysql and they do not have anything more efficient than the obvious solution mysql spatial do not have compute distance function either
4214 so in short what should be the data type of latitude and longitude what sql command should call to get the first nearest restaurants for example detail have 100k biz record each with lattitude and longitude see that mysql actually support data type called point should use that instead does mysql support kdtree storage system http en wikipedia org wiki file kdtree animation gif is it best to use point data type rather than regular float data type to store latitutude and longitude eventually want to find things like the first restaurants closest to points for example and my databases contains lot of biz and points obviously computing the distance one by one for every records and for every points would be on and hence sucks notice that am aware of simpler solution described in how do application like yelp retrieve distance information from data base efficiently and will implement that my self too for start thats good answer however think there is one creme of the crop answer that should outperform that right in fact storing location based on latitude and longitude and finding stuffs nearest to it is very common problem expect mysql to have special design pattern for that does it have that where can learn more about it thanks
4215 have an object table which is populated from an integrated service which can change if needed from another database at certain points we need to manually add posts in another table objectobjectgroup objectid objectgroupid which is needed if object objecttype have certain integer value since the integration service doesnt handle that kind of update im thinking of adding trigger to the object table which in pseudo code would be the following if object objecttype begin if object objectnumber like string pattern begin insert into objectobjectgroup values end end is this setup wise or is there better way in terms of performance
4218 tl dr if identify an sql server as having build and find out that implies hotfix does it also imply that all hotfixes listed for versions below are installed or that only the hotfix for build is installed found the following list of microsoft sql server version numbers and their meaning in terms of service packs hotfixes product versions etc sql server version database sqlsecurity com my question is best illustrated with an example an excerpt from that list rtm q915793 interested in this rtm q910416 rtm q932557 in the above list if find out that the sql server am connected to is version does this imply that only hot fix q915793 is installed or that all of hot fixes q915793 q910416 and q932557 are installed ie does the build imply that all previous hotfixes are installed follow up bonus question is this fixed pattern for all the versions or are there exceptions to the rule
4252 our application fires an insert query to the mysql database to add records want to know whether or not the records get auto committed if run the rollback command when does the database perform rollback is rollback possible after commit
4255 have pile of sql databases that have been around for very long time and for varying reasons experienced transaction log autogrowth regular backups now keep the transaction log usage under control but the files are still large and fragmented id like to reclaim some of that space and boost performance by reducing the number of vlfs however id also prefer not to disrupt our scheduled backups how does dbcc shrinkfile affect the transaction log backup chain or does it does this change if add truncateonly to the command note that this is one time fix im well aware that regularly shrinking logfiles is bad thing had these databases been configured with rational initial sizes and autogrowth settings and had backups run perfectly every day of their history this would not be an issue
4274 ive seen lot of people use the coalesce function in place of isnull from internet searches ive found that coalesce is ansi standard so there is an advantage that we know what to expect when using it however isnull seems easier to read since it seems more clear what its doing also realize that isnull is kind of tricky since it acts differently on different database servers and in different languages all of that in my mind boils down to style and standards given that style is subjective is there any reason to use coalesce over isnull or vice versa specifically is there performance advantage of one over the other
4283 when should rebuild the indexes in my relational database sql server is there case for rebuilding indexes on regular basis
4286 im in the middle of database server migration and cant figure after googling and searching here how can list the database privileges or all the privileges across the server on postgresql using the psql command line tool im on ubuntu and my postgresql version is
4291 recently realized that we need to use special syntax is null to compare literal to null why does null not work here
4340 http en wikipedia org wiki database engine mentions about database engines aka storage engines what all storage engines can be used with the oracle database
4347 have two sql agent jobs which are scheduled to run at different intervals the first job runs full backup once day the second job runs transaction log backups every fifteen minutes as the database has grown the full backup is taking longer than originally planned even with compression and ive noticed from my logs that the transaction log backups are now running at the same time should change the schedule of the transaction log backup so that doesnt run when the full backup is running does it matter
4363 in my software at runtime need to check if the connected user has the privileges on some tables across different schemas from what found doing research came across these views all tab privs recd all tab privs should use one of these views or is there another better way to do it thanks
4373 does sentryone plan explorer work as advertised and is it legitimate are there any gotchas or something to be concerned about it looks like it shows the hot path in color commpared to ssmss nightmare of view for the estimated execution plan my concerns is does it modify any data maliciously or otherwise edit had just heard of it and never heard of the company before
4441 even microsoft discourages the use of sql server authentication mode but our applications require it ive read that its best practice to not let users use the sa login directly instead using windows authentication and allowing those accounts or account groups sysadmin privileges isnt that essentially the same thing what are the advantages disadvantages how does the best practice increase the security of my sql server instances does this only apply to production instances or to our internal development instances too
4475 the suggestion in bol is fairly vague back up master as often as necessary to protect the data sufficiently for your business needs we recommend regular backup schedule which you can supplement with an additional backup after substantial update if you venture further you will find these details the types of operations that cause master to be updated and that require backup to take place include the following creating or deleting user database if user database grows automatically to accommodate new data master is not affected adding or removing files and filegroups adding logins or other operations that are related to login security database security operations such as adding user to database do not affect master changing server wide or database configuration options creating or removing logical backup devices configuring the server for distributed queries and remote procedure calls rpcs such as adding linked servers or remote logins so if all our logins are added through windows groups and we dont make any other changes to the database does that mean that one time backup of the master is sufficient if not what is the standard backup interval for the master database
4479 googling around there seems to be mixed reports whether the size of varchar2 column in oracle impacts performance or not would like to give the question of varchar size little twist and hope to get some insight in this given multiline free text fields not short stuff like names that you want to store in an oracle database is there any point wrt performance or otherwise in not maxing out the varchar capacity varchar24000 on oracle but choosing smaller value such as or because that will likely be sufficient in of the cases anyway
4494 it seems odd to me that mysql would not handle this internally deadlock found when trying to get lock try restarting transaction does oracle db work around this issue after all oracle owns innodb
4506 whats the best way to add columns to large production tables on sql server r2 according to microsofts books online the changes specified in alter table are implemented immediately if the changes require modifications of the rows in the table alter table updates the rows alter table acquires schema modify lock on the table to make sure that no other connections reference even the metadata for the table during the change except online index operations that require very short sch lock at the end http msdn microsoft com en us library ms190273 aspx on large table with millions of rows this can take while is taking an outage the only option whats the best way to handle this kind of situation
4511 would think that this would be fairly simply question but ive actually had difficult time finding an answer for this the question can you move rows of data within partitioned table from one partition to another by simply updating the partition column so that it crosses the partition boundary for example if have table that has partition key create table sampletable sampleid int primary key sampleresults varchar100 not null with the partition function that maps to the primary key create partition function mypartitionfunc int as range left for values can move row from the first partition to the third partition by changing the sampleid from to say note im tagging this as both sql server and since they both support partitioning do they handle it differently
4521 may be asking the wrong question in the title here are the facts my customer service folk have been complaining about slow response times when doing customer lookups on the administration interface of our django based site were using postgres started logging slow queries and discovered this culprit select count from auth user where upper auth user email text like uppere deyk this query is taking upwards of seconds to run heres the query plan provided by explain query plan aggregate cost rows width seq scan on auth user cost rows width filter upperemail text deyk text because this is query generated by the django orm from django queryset generated by the django admin application dont have any control over the query itself an index seems like the logical solution tried creating an index to speed this up but it hasnt made difference create index auth user email upper on auth user using btree upperemail text what am doing wrong how can speed up this query
4540 is it good practice or would it have any adverse effects to use set of columns to identify row as being unique one being foriegn key the other three being float data types im attempting to build table that with keys linked would describe unique entry in the table im curious if this is good plan of attack or if there is better way for visual purposes picture the following table we have inventory items that are organized like the following table is symbolic of the primary key the lines are relationships sheet class sheet type sheet size sheet class sheet type sheet size sheet class sheet type length width thickness the data may present itself in the following way but for brevity ive excluded bringing over the linked columns sheet class sheet type sheet size tables sheet class sheet type length width thickness column values aluminum h32 t6 steel crs as it stands and ive shown in my schema above use simple auto increment integer primary key for entries in the sheet size table however id like to know if its better to use combination of the sheet type length width thickness columns instead given each entry in sheet size should share all these unique qualities and that an auto incrementing field wouldnt demonstrate this well enough is this the best route to take if im not explaining the situation well enough please let me know im finding myself needing to break out these portions class vs type vs actual stock sizes of an inventoried material for other logic purposes but im up for any other kind of feedback any guidance would be appreciated update after the answers posted ive decided to do combination of marks answer zeros answer decided its good idea to place unique constraint on the length width and thickness columns but also like the idea of breaking out material sizes in to unique rows and linking them with relationship unfortunately can not accept both answers so am going to accept zeros for taking what feel to be more critical look at the problem and offering schema adjustment thank you everyone for your answers
4557 ive heard all kinds of horror stories around gremlins living in database triggers and worse systems being brought down by the addition of trigger that caused chain of cascading ones im considering implementing strict policy about the use of database triggers for separation of concerns the initial thought is to say database triggers shall be used only for the purpose of capturing and maintaining the audit trail does anyone have similar policies are there any strong arguments against such policy
4576 am developing product that as part of its operation must track large number of files directories the idea is to store stat information in database then on boot create watches for each file files that change will be queued in the database for group sync to remote database they will be synced in order of priority number between information about database entries of stat info entire database read at boot only the file path is necessary queued files will have priority field nothing else needs be searched by insertions can be slow ive found couple databases that think will work but im not sure which would be the best redis store file path as key stat data as value queue would be list mongodb more query options than redis but still fast im thinking nosql database would be the best solution here as there isnt too much relational logic going on and the total data size isnt too large something like mb closer to mb did look at sqlite because it seems to be simple enough to embed in an installable application since this is distributed application for end users and not high load server the database doesnt have to support many simultaneous users the main priority here is to find database whose model makes the most sense so the question which database would be most applicable for this situation also are there any other databases that would make more sense for an application like this
4603 have gone through the definition on http en wikipedia org wiki database engine several times database engine or storage engine is the underlying software component that database management system dbms uses to create read update and delete crud data from database what do not understand is what is left to do isnt crud all that the databases do if the database engine performs these functions what does the rest of the database do
4610 know that we tend to avoid cursors and loops within sql server at every cost but what are some of the situations where you absolutely need procedural queries and set based queries just will not give you the results understand the difference between the two just have never come to situation where need to use cursor im wondering if there are such situations
4622 think we are all familiar with database normalization my question is what are some guidelines that you use when you want to denormalize the tables
4630 inspired by this post https twitter com sqlchicken status heaps are they considered an index structure or are they strictly table structure without index
4654 background would like to provide the subset of my database required to reproduce select query my goal is to make my computational workflow reproducible as in reproducible research question is there way that can incorporate this select statement into script that dumps the queried data into new database such that the database could be installed on new mysql server and the statement would work with the new database the new database should not contain records in addition to those that have been used in the query update for clarification am not interested in csv dump of query results what need to be able to do is to dump the database subset so that it can be installed on another machine and then the query itself can be reproducible and modifiable with respect to the same dataset example for example my analysis might query subset of data that requires records from multiple in this example tables select table1 id table1 level table2 name table2 level from table1 join table2 on table1 id table2 table1 id join table3 on table3 id table2 table3 id where table3 name in fee fi fo fum
4679 lost the sa password on machine and when log in to the machine directly using an account in the admin group sql server management studio will not allow me to log in using windows authentication my plan was to simply log into the server connect via windows authentication and reset sa to use new password since cant connect via windows authentication this wont work how else can reset the sa password
4693 just need to confirm that understand something correctly recently viewed an so question in which user posted an answer in linq like from in db table where column addminutes1 datetime now select to those unfamiliar with linq would expect the output of that statement not tested it in fairness to be select from table where dateaddmin column getdate posted an reply to this saying that the datetime manipulation should be on the variable in this case getdate so in fact the statement should reflect something like select from table where column dateaddmin getdate in my reply the bits im now unsure of assume the following indexes will not be used because of the manipulation of the column the query plans will be different partly because of the above not tested assuming so because of the above the 1st query will actually perform worse than the 2nd my question have missed anything out in my reasoning am correct lastly does any body have any good articles on sargability
4696 im using dapper to execute the following query against sql server r2 express instance from asp net mvc net application insert into customers type name address contactname contactnumber contactemail supplier values type name address contactname contactnumber contactemail supplier select identity the call to connection query int sql is throwing an invalid cast exception ive debugged it and its at the point where dapper calls getvalue on the returned sqldatareader the return type of getvalue is object inspecting it in the debugger shows its boxed decimal if change the select to select cast identity as int the return of getvalue is boxed int and the exception isnt thrown the id column is definitely of type int why would select identity return decimal some additional information the database is brand new the customers table is the only object ive added to it there are no other user tables views triggers or stored procedures in the database there are rows in the database there ids are the column isnt beyond the limits of an int my table definition is create table dbo customers id int identity11 not null type int not null name nvarchar not null address nvarchar not null contactname nvarchar not null contactnumber nvarchar not null contactemail nvarchar not null supplier nvarchar not null constraint pk customers primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary
4706 have read much on both sides of this debate is there signficant performance gain to be had by using only stored procedures over raw queries am specifically interested in sql server but would be interested in any and all databases
4717 im using matlab to feed the database but after time im getting too many connections error always using opendb and closedb in pair is it to fast the feeding do have to wait after inserted row connecting to host localhost user root password uptime threads questions slow queries opens flush tables open tables queries per second avg current database is forwind connection closed current status no connections open add row connecting to host localhost user root password uptime threads questions slow queries opens flush tables open tables queries per second avg current database is forwind connection closed current status no connections open add row
4721 consider the situation where have all my schema names in one table and all my table names in another table is it possible to do something like in the following pseudo code select value from select schema name from schemas select table name from tables or do have to break up the query into three selects
4750 my question consists of three parts when can be sure that my database design is perfect is returning to the database design to change some issues adding new column deleting column changing data type add new table etc considered bad practice or is it normal are there any websites or books just for training on erd and normalization want lot of samples practices and case studies with recommended answers to strengthen my skill in database design and avoid the poor database designs ive made note dont need books to explain the concepts what need is practices examples and case studies with recommended answers
4760 tried to find some information about blocking in sql server but could not find concise explanation for what it is and how it happens could you please enlighten me
4777 was given the task to migrate postgresql database to another server to do this im using the pgadmin on ubuntu by the way and using the backup and restore using the custom compress format backup and utf8 encoding the original database is in utf8 like so database favela drop database favela create database favela with owner favela encoding utf8 tablespace favela connection limit im creating this database exactly like this on the destination server but when restore the database from the backup file using the restore option it gives me some of these errors pg restore restoring data for table arena pg restore archiver db error while processing toc pg restore archiver db error from toc entry table data arena favela pg restore archiver db copy failed error invalid byte sequence for encoding utf8 0xe3a709 hint this error can also happen if the byte sequence does not match the encoding expected by the server which is controlled by client encoding context copy arena line when check which record triggered this error in fact some vartext fields have diacritical characters like used in portuguese for example ca and when manually remove them from the text in the records the error passes to the next record that has them since when copy has an error it stops inserting data on this table and dont want to replace them manually one by one to accomplish this but its kinda of strange because with utf8 there shouldnt be this kind of problems right dont know how they got there in the first place im only migrating the database and supose that somehow the database was like in latin1 and then was improperly changed to utf8 is there any way to check if table database has invalid utf8 sequences or any way to enforce reconvert these characters into uft8 so dont run into any problems when execute the restore thanks in advance
4782 have two datetime columns in sql server that need to query without the time portion of the datetime currently my query looks something resembling this just an example with dates as select date startdate union all select date dateaddday date from dates where date enddate select distinct id from table cross apply dates where date between convertdatetime convertvarchar startdate and convertdatetime convertvarchar enddate now this results in full clustered index scan surprise surprise am trying to think of ways of making this faster the actual query takes mins have thought of doing the following but havent tried any yet as ran out of time earlier use computed column with just the date part index said computed column not sure if this is even possible use an indexed view again not sure if this is possible will work easiest way would be to update the column and remove all time information but cannot do this any ideas update thanks for all of the answers so far think the point of the question was missed because was slightly unclear on what wanted my bad was only aiming to optimise the date conversion part of the query as the amount of data im dealing with is tiny in reality after the cross apply with year long date range sorry for the confusion on this for those optimising the rest of the query for me can see what people are saying by using but consider the following the parameters that are passed in is date range eg 1st to the end of this month the start date in the table can appear before or during the parameters date range eg only the end date is in the date range the end date in the table can appear during or after the parameters date range eg only the start date is in the date range lastly the start and end date in the table is in the parameters date range personally given the above could never get solution to work the only way could get it to work properly and not miss anything is by using cte and saying where date between startdate and enddate hope this clears things up thanks again
4794 want to enforce that only one record in table is considered the default value for other queries or views that might access that table basically want to guarantee that this query will always return exactly one row select id zip from postalcodes where isdefault true how would do that in sql
4810 why is my query non deterministic have query that do the following select sumfloat1 float2 coalescefloat31 from table when run this query get deterministic result but it seems not be the case when the query is run many times the last integer in the result differ between different runs all columns are floatnull should float truncation be the same for all runs
4814 as you can see from another question of mine generating test data is my theme right now at this point im still generating my test data by hand however this process always generates small amounts of data usually five ten rows since it is manual process are there any tools to automate this process particularly id like to be able to generate 1mil rows
4839 im an accidental dba and still learning when see from dmv or activity monitor that some process is blocking another process what should be done should simply kill those processes or is there way to release or maybe preemptively preempt this blocking
4859 sql server and oracle both have dense rank functions is there way to do something similar in mongodb without having to resort to mapreduce in other words suppose you have sql select clause like this select dense rank overorder by somefield desc somerank what is the best way to do the same thing in mongodb note this is repost of the mongodb question over here im hoping to get more feedback from dbas
4884 dont know oracle at all but have heard oracle dbas saying that working with oracle as dba is harder and more difficult and more demanding than working with sql server does this statement have basis
4906 when trying to restore backup to sql server express database got the following error restore failed for server sqlexpress microsoft sqlserver smoextended system data sqlclient sqlerror the database was backed up on server running version that version is incompatible with this server which is running version either restore the database on server that supports the backup or use backup that is compatible with this server microsoft sqlserver smo is there way to get backup which is compatible with the older in my case version from the newer in my case version of sql server express
4936 when we boot sql server san drives might not be availiable yet is there way to delay sql server start by minute right now we have to restart sql server after we boot the server
4965 does db ddladmin fixed database role include db datareader and db datawriter rights so that you dont have to grant those rights explicitly
4968 in general always use ints know that in theory this is not the best practice though since you should use the smallest data type that will be guaranteed to store the data for example its better to use tinyint when you know that the only data you will store is or null with very small chance of expanding that to or later however the only reason know for doing this is for storage purposes using byte on row instead of bytes what are the impacts of using tinyint or smallint or even bigint over just int other than saving space on your hard drive
4984 is it possible to clear all tables with one request have tried the following with no result delete abcdefghijklmno from jos bet details as jos bet 1x2 as jos bet 1x2 best as jos bet 1x2 prev as jos bet 1x3 as jos bet 1x3 best as jos bet 1x3 prev jos bet hcp as jos bet hcp best jos bet hcp prev jos bet ou kjos bet ou best jos bet ou prev jos bet debug jos bet deleted
5014 have read here that some extra data will be stored per row so we might see performance degradation but what other risks are there eg will this affect recovery of the database is there anything else we need to do to take advantage of this plan to execute these commands alter database databasename set read committed snapshot on alter database databasename set allow snapshot isolation on believe this will give us something closer to oracle where if one transaction is updating other transactions can still read the old data is this correct am looking into this because am sick of locking problems in sql server am hoping this might reduce the the occasional deadlocks our users see help overall performance of our application and encourage our developers to do more than one operation per transaction without fear
5022 have collegue who want to get an access to one sql server instance ill have to give him rights to this instance he should have rights to do is add and modify server logins add and modify maintenanace plans create backups from the databases schedule agent jobs dont want to give him sysadmin rights what rights should be given
5038 after reading slow sql query not sure how to optimize it got me thinking about the general performance of queries surely we need the results of the first table when other tables are joined to be as small as possible before joining inner joins for this question in order to make our queries that tiny bit faster example should this select from select from table1 where col val inner join table2 on col col2 be better faster than select from table1 inner join table2 on col col2 where table1 col val my theory is as follows this might not be the correct implementation am trying to remember from sql server internals book read msft press the query processor first gets the left table table1 joins the second table table2 and forms cartesian product before filtering out the necessary rows if applicable then performs the where order by group by having clauses with the seelct statement last so if in statement above the table is smaller the sql engine has less work to do when forming the cartesian products then when you reach the where statement you have reduced result set from which to filter in memory could be so far off the mark its unreal like said its theory your thoughts note ive only just thought of this question and havent had chance to run any tests my self yet note tagged as sql server as dont know anything about the implementation of mysql etc please feel free to answer comment anyway
5061 some research into checking when tables records were last updated modified or deleted has lead me to the pseudo column known as ora rowscn first do this select maxora rowscn from tablename take note of the number then do an insert update and delete check that max value before and after each it appears to increment for each type of change if youre wondering why am doing this we cache list of entities in our windows service this service runs on two load balanced servers so theres separate instance of each running when an update occurs on server server needs to know about it what want to do is cache maxora rowscn into variable every time our application goes to insert update or delete record it will get new max from the database if the value is different then it obviously knows it needs to go get new list from the database so my actual question is this are there any other snags should be aware of that might result in an insert update or deletion of record not incrementing this value edit can someone add ora rowscn as tag
5071 our database architecture allows multiple customers to exist in the same database yet we split them into mulitple database for administrative reasons patching backup etc question what would the performance implications be if we consolidated the customers into single db question we current have multiple customer in each db but we may have in each and say database so if we consolidated the dbs wed just have one db with customers would that make much difference to performance
5107 id like to know if there is way to send notification on deadlock if so what queries would be required understand that sql server takes care of deadlocks simply would like information on the queries involved found the following to determine long running queries select creation time last execution time total physical reads total logical reads total logical writes execution count total worker time total elapsed time total elapsed time execution count avg elapsed time substringst text qs statement start offset case statement end offset when then datalengthst text else qs statement end offset end qs statement start offset as statement text from sys dm exec query stats as qs cross apply sys dm exec sql textqs sql handle st where total elapsed time min order by total elapsed time execution count desc id like to know if the above is the right way to go or is there better way to determine if any query takes longer than specific interval say min as shown thanks
5131 have query in mysql as follows select distinct tablea cola tablea colb tableb cola from tablea left join tableb on tablea colc tableb cola where whereconditions order by tablea cola desc tableb cola asc limit now executing the query takes approximately seconds due to the size of the respective tables however this query is lot faster seconds select tablea cola tablea colb tableb cola from tablea left join tableb on tablea colc tableb cola where whereconditions order by tablea cola desc tableb cola asc limit im aware that this may result in non distinct rows but im not sure if this situation has been taken care of by the php code on the front end so actually want to compare the data of the two result sets both of which result in approximately rows if not limited can anyone think of way to do this in sql instead of doing it via php script ive compared the first rows by eye without any issues but as we all know it only takes one exception at the 101st row to mess everything up so need query that can execute few times every so often to see if there are rows that are in one result set that are not in the other
5146 have this table select from items id item position usb cable sd card mouse keyboard monitor sorting this table gives this result select from items order by position id item position keyboard usb cable mouse monitor sd card now want to update the table and save the order in the position column select from items id item position keyboard usb cable mouse monitor sd card can this be done with single query or do have to manually loop over all rows and do manual update in case the order is not fully defined for usb cable and keyboard above just arbitrarily decided the order
5160 have table and am always getting the data by ordering column in my case order by datenum however the selection is so slow because of the ordering is it possible to order the table after datenum this column is defined as unique if defined as an index would the selection will be faster
5166 setting cmptlevel to on sql server inhibits the use of pivot and apply but doesnt affect the use of analytical functions is there any rational behind this
5211 will increasing column nvarchar width necessarily drop the table in other words can the width be changed in production environment with active users figured that if the size is increasing as opposed to decreasing this wouldnt be problem
5222 remember reading this one article about database design and also remember it said you should have field properties of not null dont remember why this was the case though all can seem to think of is that as an application developer you wouldnt have to test for null and possible nonexistent data value for instance an empty string for strings but what do you do in the case of dates datetime and time sql server youd have to use some historic or bottomed out date any ideas on this
5233 is it true that rdbms systems are optimized for commit operations how much slower faster are rollback operations and why
5236 im working with windows application that uses local postgres database and stores some information in temporary table id like to have look at the temporary table but pgadmin and dbvis tell me error cannot access temporary tables of other sessions when trying to query the data tried changing the permissions of the schema and table but this didnt seem to help even though im accessing the database with the same user as the program itself at least in dbvis is there setting can change in my database that allows me to have root access to all session in my database
5278 have sql server table that contains types of results float nvarchar30 or datetime separate columns want to ensure that for any given row only one column has result and the other columns are null what is the simplest check constraint to achieve this the context for this is trying to retrofit the ability to capture non numeric results into an existing system adding two new columns to the table with constraint to prevent more than one result per row was the most economical approach not necessarily the correct one update sorry data type snafu sadly wasnt intending the result types indicated to be interpreted as sql server datatypes just generic terms fixed now
5302 im trying to set up database mirroring in sql server 2008r2 ive taken full backup and transactional backup from my principal database and ive restored both with norecovery however my mirror database is now stuck in recovering mode and when hit start mirroring on my principal it says it cant connect what am doing wrong edit should probably state that my database is rather large the mdf file is about 8gb so that could be why edit2 ive also tried doing this with both firewalls turned off so know its not firewall issue edit3 ive run the sql that mark suggested the principal results are here http piersonthe net principal xls and mirror are here http piersonthe net mirror xls its worth noting that got the following error when ran the query on the mirror msg level state line database rhscmssites cannot be opened it is in the middle of restore
5332 while testing some migration scripts with copy of production data scripts run fine with development data found curious situation constraint has changed so im issuing drop add commands alter table dup calle drop constraint dup calle uk1 alter table dup calle add constraint dup calle uk1 unique control id calle ayto dupl enable the drop command worked fine but the add one failed now im into vicious circle cannot drop the constraint because it doesnt exist initial drop worked as expected ora cannot drop constraint nonexistent constraint and cannot create it because the name already exists ora name is already used by an existing object type dup calle uk1 into sql developers search box and there it is owner table name tablescape everything matches it isnt different object with the same name it is my original constraint the table appears in the constraint details but the constraint does not appear in the tables details my questions whats the explanation for this how can ensure it wont happen when make the real upgrade in live server server is 10g xe dont have enough reputation to create the tag
5333 lets examine these two statements if condition or condition if condition and condition if condition is true will condition be checked if condition is false will condition be checked what about conditions on where does the sql server engine optimize all conditions in where clause should programmers place conditions in the right order to be sure that the sql server optimizer resolves it in the right manner added thank to jack for link surprise from sql code if or select true as result else select false as result if and select true as result else select false as result there is not raise divide by zero exception in this case conclusion if vb has short circuiting why cant sql server have it to truly answer this lets take look at how both work with conditions vb all have short circuiting defined in the language specifications to speed up code execution why bother evaluating or conditions when the first one is already true or and conditions when the first one is already false we as developers have to be aware that sql server works differently it is cost based system to get the optimal execution plan for our query the query processor has to evaluate every where condition and assign it cost these costs are then evaluated as whole to form threshold that must be lower than the defined threshold sql server has for good plan if the cost is lower than the defined threshold the plan is used if not the whole process is repeated again with different mix of condition costs cost here is either scan or seek or merge join or hash join etc because of this the short circuiting as is available in vb simply isnt possible you might think that forcing use of index on column counts as short circuiting but it doesnt it only forces the use of that index and with that shortens the list of possible execution plans the system is still cost based as developer you must be aware that sql server does not do short circuiting like it is done in other programming languages and theres nothing you can do to force it to
5346 many of my databases have fields defined as varchars this hasnt been much of problem since live and work in america where the only language that exists is american ahem after working with databases for about years ive found eventually run into problems with the limited nature of the varchar field and have to modify my fields to store data as nvarchars after having to make another update to table converting varchar field to an nvarchar just had the thought why are we still doing it this way ive long since made the mental decision to define all of new my text fields to nvarchar instead of varchar which is what learned to do from my text books when was in school years ago its and there was new release of sql server last year why do we continue to support varchar datatype when we can should instead be using nvarchar know that it is often argued that nvarchars are twice as large as varchars so storage space usage could be one arguement for maitaining varcars however todays users could define their nvarchars to store the data as utf instead of the default utf if they want to save on storage space this would allow for bit encoding if that is primarily desirable while giving assurance that the rare byte character that gets inserted into their db wouldt break anything am missing something is there good reason why this hasnt changed over the past years
5365 have database which tried to defragment all the tables at once by running this sql select alter index all on name reorganize char10 alter index all on name rebuild from sys tables and then copying and pasting the output to new query window and running that got no errors but still have fragmentation tried running both commands separately too and still have fragmentation note have been made aware that reorganize is unnecessary by aaron and im aware could use dynamic sql to automate this ran this to determine still have fragmentation select from sys dm db index physical stats db id null null null null where avg fragmentation in percent and got database id object id index id partition number index type desc alloc unit type desc index depth index level avg fragmentation in percent fragment count avg fragment size in pages page count avg page space used in percent record count ghost record count version ghost record count min record size in bytes max record size in bytes avg record size in bytes forwarded record count compressed page count clustered index in row data null null null null null null null null null clustered index in row data null null null null null null null null null clustered index in row data null null null null null null null null null clustered index in row data null null null null null null null null null clustered index in row data null null null null null null null null null nonclustered index in row data null null null null null null null null null clustered index in row data null null null null null null null null null know am missing something real basic but dont know what
5385 as software developer and an aspiring dba try to encorporate best practices when design my sql server databases of the time my software sits on top of sql server make the best possible design prior to and during development but just like any other software developer there is added functionality bugs and just change of requirements that demands altered created database objects my question is should query tuning be proactive or reactive in other words few weeks after some heavy code database modification should just set aside day to check out query performance and tune based off of that even if it seems to be running okay or should just be aware that less than average performance should be database check and going back to the proverbial chalkboard query tuning can take up lot of time and depending on the initial database design it could be minimal benefit im curious as to the accepted modus operandi
5390 it looks like database sharding is great if have huge collections what if have lots of fairly sized collections lets say that for collection of documents not very big comments sharding is effective is it also effective for collections with documents each think this question is still valid for table oriented databases if you replace collections with tables and documents with rows if possible id like to know the theoretical answer as well as the answer in the specific mongodb scenario if different from the theoretical answer
5417 id like you all to give me hints and advise on the following scenario we are considering using microsofts sql server r2 express for business web application the web application wont have more than cruds and wont have high data traffic all of them are simple web forms about data scalability the web application is basically focused on employee data like personal information vacancy control and the daily frequency control from what ive seen ive estimated maximum of simultaneous users generating data traffic and also our employee table should hold maximum of records as to physical limitations think were fine since dont think maximum of employee should generate gb for the next say ten years but since ive read that the express edition only manages gb of ram and uses only cpu thought itd be best to ask do you think it can manage to keep up good response and performance in this case so please advise on using sql server r2 express or some other database manager just keep in mind that it has to be free aditional info windows server gb of ram tb hd
5422 hello and thanks for taking time to read this question am using mysql and want to sort results using order by to one specific column but the results must be ordered according an specific criteria to this column for example to the following table want to order by group showing first the group items and in the end group items names group susanita miguelito mafalda manolito libertad felipe guille thanks in advance
5432 understand mutating table errors are caused by design flaw or problematic query an old query was recently put into production which throws mutating table error our dba solved the problem but we do not know how what exactly causes mutating table errors and how would our dba have fixed the problem
5455 have table with rows and gb of table space so about bytes per row the table has three columns varchar100 datetime20 and smallint the average length of the text in the varchar field is about so the raw data should be around bytes per row for the varchar for the datetime2 and for the bit integer note that the space above is data only not indices im using the value reported under properties storage general data space of course there must be some overhead but bytes per row seems like lot especially for large table why might this be has anyone else seen similar multipliers what factors can influence the amount of extra space required for comparison tried creating table with two int fields and rows the data space required was mb bytes per row compared to bytes of raw data another test table with an int and varchar100 populated with the same text as the real table uses bytes per row rows where would expect plus little so the production table has considerably more overhead is this because its larger id expect index sizes to be roughly logn but dont see why the space required for actual data to be non linear thanks in advance for any pointers edit all of the fields listed are not null the real table has clustered pk on the varchar field and the datetime2 field in that order for the two tests the first int was the clustered pk if it matters the table is record of ping results the fields are url ping date time and latency in milliseconds data is constantly appended and never updated but data is deleted periodically to cut it down to just few records per hour per url edit very interesting answer here suggests that for an index with much reading and writing rebuilding may not be beneficial in my case the space consumed is concern but if write performance is more important one may be better off with flabby indices
5460 how long would take to upgrade an oracle db which has 1t data from 10g to 11g usually roughly need to estimate the down time for it since it is prod db thanks much
5468 is there something like directive which can use in script to force ssms to enable disable sqlcmd mode
5487 have read somewhere long time ago the book states that we should not allow to having nested view in sql server am not sure the reason why we cant do that or might remember incorrect statement students select studentid first name last name schoolid from students create view vw eligible student as select from students where enroll this year teachers select teacherid first name last name schoolid from teachers create view vw eligible teacher as select from teachers where hascert and enroll this year schools create view vw eligible school as select top percent schoolid school name from schools sh join vw eligible student on schoolid sh schoolid join vw eligible teacher on schoolid schoolid at my workplace have investigated one of our in house database application checked through the objects found out that there are two or three layers of the view stack each other so that was remind me about what read in the past can any one help explaining it if it is not ok to do so want to know that it is limited to just sql server or it is for database design in general additional info updated an example from my company change bit to be more general without too many technical too many columns in this example mostly the nested view we used is based on abstract or aggregated view for example we have large student table with hundred of columns say eligible student view is based on students who enrolls this year and student eligible view could be use other places such as in stored procedure
5525 running microsoft sql server profiler every now and then it suggests me with bunch of new indexes and statistics to create estimated improvement from my understanding every added index can make an sql select query faster but also an update or insert query slower since the indexes have to be adjusted what wonder is when do have too many indexes statistics maybe there is no clear answer on this but some rule of thumb
5588 were using database setup from vendors application that has horrifically hard to read database table names and no documentation on what is stored where can see why one might want to obfuscate their table structure in proprietary app but one of the selling points of this application enterprise resource planning was its customizability table names are like aptrx accounts payable transactions and apmaster all curiously this is the vendors table its an extremely complex database so was wondering if there was any logic to the convention or if it was simply being obfuscated intentionally or otherwise to the best of my knowledge the length of the table name wont affect performance noticeably correct the database is very complex hundreds of tables so sorting makes sense but cant imagine why accountspayabletransactions isnt preferable to aptrx
5602 have the following indexed view defined in sql server you can download working schema from gist for testing purposes create view dbo balances with schemabinding as select user id currency id sumtransaction amount as balance amount count big as transaction count from dbo transactions group by user id currency id go create unique clustered index uq balances user id currency id on dbo balances user id currency id go user id currency id and transaction amount are all defined as not null columns in dbo transactions however when look at the view definition in management studios object explorer it marks both balance amount and transaction count as null able columns in the view ive taken look at several discussions this one being the most relevant of them that suggest some shuffling of functions may help sql server recognize that view column is always not null no such shuffling is possible in my case though since expressions on aggregate functions an isnull over the sum are not allowed in indexed views is there any way can help sql server recognize that balance amount and transaction countare not null able if not should have any concerns about these columns being mistakenly identified as null able the two concerns could think of are any application objects mapped to the balances view are getting an incorrect definition of balance in very limited cases certain optimizations are not available to the query optimizer since it does not have guarantee from the view that these two columns are not null is either of these concerns big deal are there any other concerns should keep in mind
5634 looking at the name data manipulation language dml would assume that all contained statements are actually for manipulating data as far as know the select statement can only be used to query data not change it not taking insert select into account here so is select part of dml and if so why
5666 ive been presented with some dedicated mysql servers that never use more than single core im more developer than dba for mysql so need some help setup the servers are quite hefty with an olap datawarehouse dw type load primary 96gb ram cores single raid array test 32gb ram with cores the biggest db is gb the total is around 1tb and mostly innodb tables solaris intel mysql note the biggest db is the replicated one from the oltp dr server and the dw is loaded from this it isnt full dw just last months to weeks so it is smaller than the oltp db observations on test server separate connections each has concurrent and different alter table drop key add index the tables have and million rows cpu usage goes up to one core is maxed out and no higher the alters take minutes single on the smallest takes questions what setting or patch is required to allow more than one core to be used that is why doesnt mysql use all cores available like other rdbms is it consequence of replication other notes understand the difference between an rdbms thread and an os thread im not asking about any form of parallelism some of the system variables for innodb and threads are sub optimallooking for quick win short term im unable to change the disk layout os can be tweaked if needed single alter table on the smallest table takes minutes shocking imo edit innodb thread concurrency is set to on both yes its wrong but wont make mysql use multiple cores innodb buffer pool size is 80gb on primary 10gb on test another instance is shut down this is ok for now innodb file per table on edit innodb flush log at trx commit innodb use sys malloc on innodb flush method should be direct but show variables doesnt show this innodb doublewrite off file system zfs and my sysadmin found this http blogs oracle com realneel entry mysql innodb zfs best practices to test innodb flush method isnt showing as direct when it should be will follow rolandomysqldbas settings let me know if ive missed anything important cheers update changed innodb flush method thread settings in rolandomysqldbas answer result core used for the tests positive result
5744 when run the explain analyze command on given query im having difficult time interpreting the outputted time value for example actual time do the internal decimals represent repeating characters sorry this may be noobish question but want to make sure im interpreting the results correctly groupaggregate cost rows width actual time rows loops
5761 working on tuning our database and queries for one of our products decided to pop open profiler in the qa environment and see what it shows saw for every insert called call was made to sp executesql with the text of the insert statement this seems to be big hit is there way to turn this off environment sql server r2 standard windows server r2 enterprise gb ram 2x4 xeon
5774 whenever manually insert row into table in sql server management studio the database is sql server my new row appears at the top of the list rather than the bottom im using identity columns and this results in things like id row first row second row third row when rows are fetched and not explicitly ordered this results in different appearance when the rows are fetched for the web app and changes what top query returns know can order by them but why is this happening most of my data is inserted through web application all inserts from this application result in first in first out ordering latest insert is at the bottom so the ids are all in row is there some setting in the server or management studio that causes this improper ordering
5780 what are the benefits of table level locking which is used by the myisam storage engine row level locking has lots of benefits like concurrent updates and reads that do not lock the table edit its widely considered that table level locking prevents from deadlocks but how prevention of deadlocks at the cost of concurrency is worthwhile
5815 have table create table mytable id serial name varchar10 primary key now want to add names to this table but only if they not exist in the table already and in both cases return the id how can do this with postgresql have seen few scripts for this but is there no single sql statement to do it can insert and return id with insert into mytable name values jonas returning id it works the first time and returns id but it fails if jonas already exist in the table but want to return the id even if the insert fails is this possible to do with postgresql
5834 must admit the question is quite broad so ill try narrowing it down bit in our company we are developers and have some sql server based installations running at our customers sites database sizes up to 100gb up to concurrent users intranet applications nobody of us has real good experience in running maintaining administrating whatever databases the customers not even that much its working ok till now but cant tell for sure if its because we doing everything right or if we just didnt hit areas situations we arent proficient in so im looking for the essential things you need to know when running database from dbas point of view you know the hard facts and know what matters most in your day for day job in which subjects should gather deeper knowledge what should ive heard of and what can care of not until face it for the first time im aware of the question software engineers and dbas but its not quite what was looking for there are also lots of books around but id like to hear it from those with practical experience
5859 am trying to delete all the duplicates but keeping single record only shorter id following query deletes duplicates but take lot of iterations to delete all copies and keeping original ones delete from emailtable where id in select from select id from emailtable group by email having countemail as its mysql edit ddl create table emailtable id mediumint9 not null auto increment email varchar200 not null default primary key id engine myisam auto increment default charset latin1 edit it worked like charm lead by dtest delete from emailtable where not exists select from select minid minid from emailtable group by email having count as where minid id
5887 have an oltp database hosted on sql azure instance want to pull copy of the database down from the cloud so can run some heavy extracts and olap style queries against it without impacting the source database how do pull copy of the database down to local sql server instance
5903 have database in ms sql2005 that can not backup whenever try to create backup of the selected database through sql mgm studio get this error the backup of full text catalog in not permitted because is not online this database was crated by detaching another database copying the data and log files and attaching the copies as new database how can make my full text catalog be online again also tried to set use full text search to false but without success here is my error
5926 importance of ram is an established fact but far less material is available about the importance of cores and multithreading when it comes to the usage of cpu by mysql am talking about the difference of running mysql on 4cores vs 6cores vs 8cores and so on do different storage engines use cpu differently
5989 remember learning to do this in dbms course for master of information services students to save yourself some typing you can type select t1 id t2 stuff from sometable t1 inner join othertable t2 on t1 id t2 id but why is this acceptable in stored procedures and such it seems like all it does is harm the readability of the statement while saving an extremely minor amount of time is there any functional or logical reason to do this it seems to add ambiguity rather than remove it the only acceptable reason can see for using this format is if you were adding semantically meaningful alias for example from sometable idstable when the table name isnt descriptive enough is table aliasing bad practice or is this just misuse of helpful system
5995 im taking class in databases theyre using oracle 10g in class but im having some trouble installing the database here in windows auth problems so thought id attempt 11g is 11g backwards compatible as in just for learning will be able to not use the added 11g features
6031 we need to do restore and cannot because other users are connected we thought we had disconnected every process but apparently not how can we from management studio kick off everyone else so we can do this backup
6035 generally bottleneck of rdbms am mysql user performance is disk access ssd provides great performance compared with conventional spindle drives question is it possible to improve performance by attaching multiple drives of reduced space because this way more heads will be available to read data like replacing 2tb 2k rpm drive with 500gb 2k rpm drives
6051 suppose you have the following table and data create table int int index kk engine memory insert into values when issuing select from where with no order by clause how does mysql sort the records by default
6108 understand one benefit of surrogate artificial keys in general they do not change and that can be very convenient this is true whether they are single or multiple field as long as they are artificial however it sometimes seems to be matter of policy to have an auto incrementing integer field as the primary key of each table is this always the best idea to have such single field key and why or why not to be clear this question is not about artificial vs natural but about whether all artificial keys should be single field
6115 have couple of questions regarding working of indexes in postgresql have friends table with the following index friends user id1 user id2 user id1 and user id2 are foreign keys to user table are these equivalent if not then why indexuser id1user id2 and indexuser id2user id1 if create primary keyuser id1user id2 does it automatically create indexes for it and if the indexes in the first question are not equivalent then which index is created on above primary key command
6117 currently have scheduled task that fires off each night at am that calls sqlcmd exe and passes it sql script to run for the backup shown below were pretty small company with growing needs due to major growth on the business side losing days of data at this point would cost tens of thousands of dollars vs couple hundred this time last year until can migrate this db platform to different solution where data mirroring occurs with major redundancy like sql azure what is the best thing can do to get more frequent backups does this script below force the db to be offline can run this script with users interacting with the db use companycrm go backup database companycrm to disk crmbackups companycrmcrm bak with format medianame companycrm backup name full backup of companycrm go update wow obviously much more dedicated dba community over here than on so thanks for the feedback so far only thing missing is the hows have shown the sql command above that im using to do daily backups but the incremental log backup examples are mia this is not large db it currently runs on sqlexpress when say ha or sql azure im specifically referring to the architecture in place that we do not have as small business this instance is currently running on our only server if that server crashes our time to recover becomes sticking point this is why sql azure becomes attractive
6122 need exclusive access to database is it possible using an sql command to detach all other users from postgres database or maybe closing all other connections and then gaining exclusive access this is for unit testing and tests are only run manually so there is no danger involved only old dead connections will be affected there are no other users connecting to these unittest databases the old dead connections come from developing this happens all the time when test that is being written or fails does not exit clean if someone also needs to keep locked out other users for while after disconnecting them in production scenario see scott marlowes answer below https dba stackexchange com see also this similar question on dba how to drop all connections to specific database without stopping the server
6145 this problem is really proving to be tricky one and quite annoying in sql server management studio up until few days ago my intellisense was working great then all of sudden it stopped the icon as it as enabled on the toolbar menu and under tools options text editor sql intellisense it says it is enabled there have tried refeshing the intellisense cache with ctrl shft but that doesnt work either any ideas what happened to my intellisense and what need to do to get it back
6150 because of the following warning in mysqld log warning unsafe statement written to the binary log using statement format since binlog format statement the statement is unsafe because it uses limit clause this is unsafe because the set of rows included cannot be predicted want to switch the replication format to mixed but according to the mysql document switching the replication format at runtime is not recommended when any temporary tables exist because temporary tables are logged only when using statement based replication whereas with row based replication they are not logged so the question is how can identify if there is any temporary tables exist to switch the binary log format safely
6158 posted this on stackoverflow and it was suggested this query was better suited here im trying to encourage the use and monitoring of autovacuum in some postgresql databases one objection hit often is that people dont trust autovacuum or there are bugs in autovacuum in which mean that its ignored in preference to scheduling vacuuming mostly our tables are small and this approach appears to work however with our larger also heavily updated tables this really doesnt work dead tuple counts increase exceed max fsm pages and the tables dont get cleaned up etc etc im just wondering if anyone has reference for autovacuum in being buggy or not working my own experience has shown that autovac works fine and where necessary adding entries to the pg autovacuum table does the trick id like to understand the problem with autovacuum if one exists
6191 have some tables that are partitioned and have several indexes on replicated slave after copying the snap shot verified safe to new slave and upgrading mysqld from to and restarting replication im getting innodb crashes with the error message invalid pointer these errors have happened across servers with different hardware and after running alter table coalesce partion the problem goes away for that table my question is larger in scope though and that is how do you identify innodb table corruption or rephrased how do you assess innodb table health is check table the only tool available to identify problems pre crash not sure if it matters but the crashes occurred running version log socket opt mysql sock port percona server gpl release rel21 revision
6197 have table with roughly rows in it and need to modify column definition to allow nulls have change script that will perform the change but would like to be able to re run the script so that the change will only occur if the column definition hasnt already been changed how do test column definition to identify if the column is null or not null
6246 am software developer and am helping my team hire mysql dba the core challenges that we are facing are slower queries and performance due to hibernate database management backups tuning patches security scalability due to increase in data from new data sources and accumulation of older data we plan to start data mining and data warehousing in the future not sure how but that is the direction we usually have programming cases where we ask developers to build something for an interview but its bit hard to do dba interview in the same fashion can you give suggestions on how the interview should be conducted
6252 pretty much all of our databases on certain servers do not require the full recovery model we dont do transaction log backups and the default should always be to create databases and specify the simple recovery model quite often and for certain practical reasons many databases are created using ssms however mistakes can be made and the operator can forget to specify the simple recovery model this leads to suprise few days later when the box is struggling with disk space due to three or four 60gb log files that have never been truncated can make the simple recovery model the default setting for new databases by configuring the recovery model on the model database however is this recommended if do this could it come back and bite me in any way in the future
6297 we have bak file from client that we have transferred to our developer offices for problem investigation the backup is currently 25gb and the restored database is about the same size however it needs 100gb to be restored believe this is because there database is set up to have 75gb transaction log size after restoring the database we can shrink the log file but is there way to do this in the restore
6310 the case is simple you have mysql database where you have only an sql query interface and you want to know the database structure with queries you can list tables with show tables command but how do you see the individual column names select statement shows empty set if no data is present and can not be thus used
6327 have sql server instances installed on the same server they have same table structures with same data have stored procedure deployed on both of them the stored procedure performs differently on them it takes seconds on one instance and seconds on the other to execute the procedure think the possible reasons for this are index fragmentation outdated statistics defragmented the indexes and updated the statistics but still no luck any ideas on this issue thanks
6368 have table like this id val kind want to make select that will return just the first row for each val ordering by kind sample output id val kind how can build this query
6383 have database job that runs each night to create warehouse table using my new database server and san have gotten the process down from hours to minutes have optimized it to run in the shortest amount of time through experimentation with index nolock force order loop join maxdop im happy with the improvement but hate not knowing why query takes as long as it does im perfectly ok with bottlenecks as long as know where they are when this query is running there are significant periods of time where the obvious resources are all underutilized what is sql server doing at these times
6387 got the following error message regarding sql query im running in program sql server sql heterogeneous queries require the ansi nulls and ansi warnings options to be set for the connection this ensures consistent query semantics enable these options and then reissue your query severity fixing it is easy set ansi nulls and ansi warnings on but wanted to know what heterogeneous query is google search brings up dozens of results telling me to set ansi nulls and ansi warnings nothing explaining what the term means the query is update srv db dbo table set column select column from srv1 db dbo table im thinking this is due to connecting to multiple database engines in one query as ive never gotten this error otherwise does heterogeneous just refer to querying two different database engines in this context
6395 now read the document about transaction id wraparound but there are something that really dont understand the document is the following url http www postgresql org docs static routine vacuuming html vacuum for wraparound preventing transaction id wraparound failures postgresqls mvcc transaction semantics depend on being able to compare transaction id xid numbers row version with an insertion xid greater than the current transactions xid is in the future and should not be visible to the current transaction but since transaction ids have limited size bits cluster that runs for long time more than billion transactions would suffer transaction id wraparound the xid counter wraps around to zero and all of sudden transactions that were in the past appear to be in the future which means their output become invisible in short catastrophic data loss actually the data is still there but thats cold comfort if you cannot get at it to avoid this it is necessary to vacuum every table in every database at least once every two billion transactions dont understand the statements would suffer transaction id wraparound the xid counter wraps around to zero and all of sudden transactions that were in the past appear to be in the future which means their output become invisible can someone explain this why after the database suffers transaction id wraparound would transactions that were in the past appear to be in the future in short want to know if the postgresql will in the data loss situation after transaction id wraparound by autovacuum for my personal views we can get the current transaction id by using txid current function whoes output is bit and will not be cycled so think the insertion transaction id of tuples which knows as xmin will nerver greater than the xid which get by txid current function except that you will use pg resetxlog reset reset transaction id after shuting down postgresql server am right thanks
6417 have few tables in my database that should not be cached how do tell sql server not to cache tables pages or how do flush single table from the cache flushing all the cache is not an option im using sql server and sql server r2
6468 have several tables where records can be uniquely identified with several broad business fields in the past ive used these fields as pk with these benefits in mind simplicity there are no extraneous fields and just one index clustering allows for fast merge joins and range based filters however ive heard case made for creating synthetic identity int pk and instead enforcing the business key with separate unique constraint the advantage is that the narrow pk makes for much smaller secondary indices if table has no indices other than the pk dont see any reason to favor the second approach though in large table its probably best to assume that indices may be necessary in the future and therefore favor the narrow synthetic pk am missing any considerations incidentally im not arguing against using synthetic keys in data warehouses im just interested in when to use single broad pk and when to use narrow pk plus broad uk
6474 have two tables 1st table called kimlik id ad ahmet mehmet ali 2nd table called siparis id kimlikid tarih miktar want to list via sql query to persons who doesnt give an order on result ad tarih ali
6504 have trigger on insert but if the trigger fails the insert fails too is there way to let the insert proceed even if the trigger fails edit use trigger to send email when new record is entered want the record to be saved regardless if the email was sent or not how would do that from sp
6507 is there way to execute multiple operations using the with statement something like with as select from tbl begin open outcursor for select from select count into outcount from end want to select some data and the count of it
6512 when attempting to run my maintenance plan receive the following error executing the query failed with the following error the index partition on table cannot be reorganized because page level locking is disabled we currently have row level locking enabled on this index can enable page level locking but am unsure what the repercussions are my question is what is the difference between the two locking schemes and what are their real world in production consequences
6526 have created database on mysql have query and when run this query on this mysql version get run time when import this database to another mysql server with same hardware and run the same query get over 120s and sometimes mysql hangs what is the difference between and or have tested and versions is it possible query takes longer in newer version something like mysql structure change sorry but cant put this query here but the query is like select fl passenger ticket fl aganc name as agancname fl pnr remark as remark fl pnr reservetime as reservetime fl pnr cancelpnr fl flight date fromcity as fromcity fl flight date tocity as tocity fl flight date flightdate as flightdate fl flightdate capacity adultper as adultper fl flightdate capacity childper as childper fl flightdate capacity infantper as infantper fl flightdate capacity cancel as cancelsegment fl flightdate capacity tax1adultpric fl flightdate capacity tax1childpric fl flightdate capacity tax1infantpric fl flightdate capacity tax2adultpric fl flightdate capacity tax2childpric fl flightdate capacity tax2infantpric fl flightdate capacity tax3adultpric fl flightdate capacity tax4adultpric fl flightdate capacity tax5adultpric as taxxtadultpric fl flightdate capacity tax3childpric fl flightdate capacity tax4childpric fl flightdate capacity tax5childpric as taxxtchildpric fl flightdate capacity tax3infantpric fl flightdate capacity tax4infantpric fl flightdate capacity tax5infantpric as taxxtinfantpric from fl passenger ticket inner join fl pnr on fl passenger ticket pnrid fl pnr pnrid inner join fl aganc on fl pnr agancid fl aganc agancid left join fl flightdate capacity on fl pnr pnrid fl flightdate capacity pnrid left join fl flight date on fl flightdate capacity flightdateid fl flight date flightdateid where fl passenger ticket ticketnumber and fl passenger ticket pnrid and fl pnr agancid and fl flightdate capacity aganccharterid and fl flightdate capacity cancel in and fl pnr reservetime and fl pnr reservetime order by fl passenger ticket rowid fl pnr reservetime have joins the table is innodb there are records the result is rows and columns explain result is show variables like innodb result
6539 mysql cannot use composite index in lookup in which the where condition doesnt include the columns forming left most prefix mysql cannot use the index to perform lookups if the columns do not form leftmost prefix of the index quote from this answer on postgresql caught my attention this is somewhat different in oracle which can sometimes also use columns that are not at the beginning of the index definition under what circumstances can oracle at least in 11g do lookup without the left most prefix columns existing in the query
6577 we recently set up new virtual machine running windows server and slapped copy of sql server on it the server had running and in use without hitch for about two weeks the only real maintenance weve done on it is to move the partition the database files resided done nearly week ago and we ran the database engine tuning advisor to look at some suggestions for new indices yesterday around 12pm just after 5pm while the software was being used as usual someone notified me that the program no longer seemed to work properly checked the database and not only was data specific to his use of the program was gone but every table of fifty tables or so seemed to be wiped of all records after talking to the application developers in extensive length it became clear that this wasnt remotely possible capability of their software in the scramble to get back from our feet we cloned backup of the vm over the running copy between restarting and its been running five for about day ever since consequently weve no logs of the events leading up to the problem itself have you seen anything like this happen before all records in all tables of database just gone does anyone have reasonable theory as to how this could occur and what if anything would be reasonable measure of prevention
6607 am having difficulty to grab the idea of pros and cons of table partitioning am about to start work on project which would have tables and one of them will be the main data table which will hold million records as it will be properly indexed table so am thinking of limiting the table records to million this way would have to create tables but am not quite sure about how it will improve the performance because they will be sitting on same machine 32gb ram am using mysql and tables would be myisam and big table would have index on id field and there are not further complexities like full text search etc please also shed light on table partitioning vs database partitioning
6647 according to the create index documentation up to columns can be combined into single composite index key weve got table with columns that need to form unique combination this table is not performance sensitive we rarely update values insert records we just need to ensure that we avoid duplicating our records and thought we could impose simple uniqueness constraint any ideas im open to avoiding the unique index constraint entirely if there is better way
6673 while the future of mysql is unknown havent had time to keep up with the known effects that oracle has had on mysql since it was bought from sun have there been changes of note either by oracle or the mysql community since the deal became public
6691 have sql server sandbox installation on my local computer have set the memory sql server instance can use to mb when running an intensive operation memory usage rises up to mb when operation is over sql server is still holding the memory how to free this memory reservation
6697 have general question about sql server tables design we currently have table that is over 600gb and grows at about 3gb day this table has the appropriate indecies but is becoming major hangup when running queries and just because of its size the question is should split the table into multiple tables by year and month this would fit how other departments split their large data sets up or should we leverage the partitioning that is built into sql server it appears that using the partitioning would require less code changes from what read when partitioning you still just query one table and the server handles how to get the data if we went the multiple table route we would have to handle pulling data from multiple tables
6698 have long running transaction called say t1 that performs some deletes updates and inserts on table in sql server r2 at the same time another process periodically runs select statements from this table under default isolation settings read committed think t1 blocks any select statements from running until the transaction commits or is rolled back what id like to see is for the select statements to function on consistent data even while the transaction is underway believe snapshot isolation can help but am not sure if im going in the right direction would this be the best isolation level for this application secondly dont have any control over the process that is calling the select statements but do have control over the net application that calls t1 would any isolation level changes be required on both the select statements and t1 or would it be sufficient to mark just t1 as having different isolation level
6713 what is the easiest and most efficient way to design database from my perspective there are couple of options for an applications data store design design the database as best as you can initially before writing any application code this gives you the advantage of having base data structure to work off of the disadvantage of this in my opinion is that you will have lot of changes as application specifics that affect the what where how of data changes throughout the application development cycle design the database as the application comes to fruition when you need some database objects as you write the application you develop the database parallel chronologically to the application the advantages would be less changes to the database structure as see it the disadvantage would be the division of time and development effort between application code and database development in your experience what do you find to be the most productive and efficient method
6731 we have some db scripts that need to be migrated from sql server r2 back to often scripts created in sql server r2 wont run on sql server installation which some customers still use so my question from your experience knowledge is this really necessary or does setting the compatibility level back to on sql server r2 fix the issue of unnoticed breaking scripts in sql server msdn says compatibility level provides only partial backward compatibility with earlier versions of sql server so im unsure here thanks would like concrete list of features will pass compatibility level but break on sql server or link if the list is short list we could convert that to an internal dont do that list and save lot of work
6736 select convertvarchar10 totalseconds convertvarchar10 totalseconds convertvarchar10 totalseconds as seconds from select datediff second dateouttime as totalseconds from attendance attn card register this one
6792 my story is that am working now almost years as an administrator deal with sql server and other it stuff most of my knowledge is self taught by reading books dealings with problems and asking and reading sites like this now want to circularize and certificate that by going to learning and try to pass microsoft official exams and programs where should start and what kind of curses and exams need to look for am pretty confused whit so many choices the way can go at one local microsoft certified partner they offered me courses for label of microsoft certified it professional server administrator equivalent mcsa also they are offering mscse as enterprise administrator do need both of this one so can continue learning for sql server administrator do need this ones at all what is the correct name in ms terminology for sql server administrator does there exist sql server developer calling in ms world or is this only extension for already developers of common ms languages such as
6834 was triying to create view by the simplest way like this use soccerdb go create view exampledbaseii as select id castname as varchar as namecastcity as varchar as city from team go how can do so that view keeps its link to the table so if change the table the view also changes without creating it again or creating new one is that possible im working with sql server r2 thanxs
6840 have key column for internal use which is just increasing integer but would like to have second unique column which is uuid but dont know how to have function called for the default value so that sql server is creating the uuid and not java is there any documentation that one could suggest to me for this
6859 have mysql db on server s1 mysql version 3ubuntu12 log have created master slave for this db on server s2 mysql version 1ubuntu4 log the db on s1 was using one data file ibdata after dumping the db to s2 set innodb file per table this made every table to have its own ibd file now everything went fine and smoothly after restarting mysql on s2 faced problem with getting this error error unknown table engine innodb on query default database mydb and when try to show engines get the following show engines engine support comment transactions xa savepoints myisam default default engine as of mysql with great performance no no no mrg myisam yes collection of identical myisam tables no no no blackhole yes dev null storage engine anything you write to it disappears no no no csv yes csv storage engine no no no memory yes hash based stored in memory useful for temporary tables no no no federated no federated mysql storage engine null null null archive yes archive storage engine no no no innodb is not listed in error log can see this innodb database physically writes the file full wait innodb cannot initialize created log files because innodb data files are corrupt or new data files were innodb created when the database was started previous innodb time but the database was not shut down innodb normally after that error plugin innodb init function returned error error plugin innodb registration as storage engine failed warning neither relay log nor relay log index were used so replication may break when this mysql server acts as slave and has his hostname changed please use relay log s2 relay bin to avoid this problem have tried to delete ib logfiles but this didnt work as well if delete ib logfiles and ibdata file innodb will return back normally but cant access my innodb tables ie after deleting ibdata1 and restarting mysql desc article error 42s02 table mydb article doesnt exist my innodb configuration in my cnf is as follows innodb file per table innodb flush method direct innodb log file size 1g innodb buffer pool size 4g innodb data file path ibdata1 10m autoextend innodb buffer pool size 384m innodb log file size 5m innodb lock wait timeout although the table is there has anybody faced such issue before any idea is highly appreciated thanks
6880 last week found my after insert or update trigger wasnt working after disabled and enabled it it started working again do not yet know why it stopped working is there any way to deal with this because this trigger is recording the value of daily jobs and is used for report purposes if this trigger goes dead in few days without my notice or error will be in hot water am using oracle 10g access the db by using sqldeveloper my trigger create or replace trigger master instance step trg after insert or update of sysidstep idinstance idparent step id on master wf instance step referencing old as old new as new for each row when new sysid declare stepsysid number crcode varchar50 crdate date step id number begin step id new step id select ss sysid into stepsysid from template wf step ws inner join template step stage ss on ss sysid ws stage id where ws sysid step id if stepsysid then insert into master fact cr progress values0 new instance idstepsysid new create dt end if dbms output enable10000 dbms output put linestart print end
6883 my question is regarding use of indexes should start indexing right from the start or when performance problem arises we can also create temporary index while executing query what are the pros and cons of such techniques
6900 im updating my identity overflow check script to account for decimal and numeric identity columns as part of the check compute the size of the data types range for every identity column use that to calculate what percentage of that range has been exhausted for decimal and numeric the size of that range is where is the precision created bunch of test tables with decimal and numeric identity columns and attempted to calculate their ranges as follows select power10 precision from sys columns where is identity and type is decimal or numeric this threw the following error msg level state line arithmetic overflow error converting float to data type numeric narrowed it down to the identity columns of type decimal38 with the maximum precision so then tried the power calculation directly on that value all of the following queries select power10 select convertfloat power10 select castpower10 as float also resulted in the same error why does sql server try to convert the output of power which is of type float to numeric especially when float has higher precedence how can dynamically calculate the range of decimal or numeric column for all possible precisions including of course
6912 have few million rows in my database already didnt know about the postgresql uuid data type when designed my schema one of the tables has 16m rows about 5m to records per shard growing at about 500k records per day still have the luxury of taking the production system down for few hours if required wont have this luxury in one or two weeks my question is will it be worthwhile to do so im wondering about join performance disk space use full gzipd dump is gib things of that nature table schema is twitter interactions table public twitter interactions column type modifiers interaction id character36 not null status text character varying1024 not null screen name character varying40 not null twitter user id bigint replying to screen name character varying40 source character varying240 not null tweet id bigint not null created at timestamp without time zone not null indexes twitter interactions pkey primary key btree interaction id twitter interactions tweet id key unique btree tweet id index twitter interactions on created at btree created at index twitter interactions on screen name btree screen name triggers insert twitter interactions trigger before insert on twitter interactions for each row execute procedure twitter interactions insert trigger number of child tables use to list them
6961 im more of network windows admin and ive been tasked with overseeing sql server upgrade project need to meet with the dbas and discuss their needs wants regarding the upgrade dont want to go in totally blind so thought would ask you guys first we are moving from sql server to sql server r2 and likely moving to windows server r2 where possible as dba what would be your concerns with such an upgrade anything youd like to see happen at the same time
6962 lets say in some random table you have column named status its real world values would be either enabled or disabled is it better for this columns data type to be an int bool or zero or to use enum with the values being enabled and disabled what are the advantages or disadvantages lets say instead of just two valid statuss you have or or even more do the advantages and disadvantages sway to one side or the other as the number of required values increases
6975 there is big table million records have to move of old records to other databasetable solution
6996 how do truncate the transaction log in sql server database what are possible best ways tried this from blog as follows from the setting database to simple recovery shrinking the file and once again setting in full recovery you are in fact losing your valuable log data and will be not able to restore point in time not only that you will also not be able to use subsequent log files shrinking database file or database adds fragmentation there are lot of things you can do first start taking proper log backup using the following command instead of truncating them and losing them frequently backup log testdb to disk nc backup testdb bak go remove the code of shrinking the file if you are taking proper log backups your log file usually again usually special cases are excluded do not grow very big
7023 create symmetric key securesymmetrickey with algorithm desx encryption by password nstrongpassword im trying to figure out about the sql server encryption once ive executed the code above is there any way to find out later what is the password value for the securesymmetrickey if now im doing this with certificates im the admin created create master key encryption by password db master key password go only know the password later create certificate mycertificate with subject my certificate subject create symmetric key mysymetrickey with algorithm triple des encryption by certificate mycertificate until now its all ok now when hacker comes to the computer all he have to do is open symmetric key mysymetrickey decryption by certificate mycertificate and then select convert nvarcharmax decryptbykeynamepass from tbl1 so where is the protection in certificates no one asked him for password like in password encryption as in my first question he only needed to know the certificate name open symmetric key mysymetrickey decryption by certificate mycertificate and its not problem to find out what is the certificate name so where is the protedtion in decypher data from the hacker when im using create symmetric key securesymmetrickey with algorithm desx encryption by password nstrongpassword declare str nvarchar100 set str lala open symmetric key securesymmetrickey decryption by password nstrongpassword whom am trying to protect the data from the data being sent from client to server the data is being sent by plain text cant activate sql commands before sending the data or people who has access to the sql server
7031 am searching for comparison table between mysql and sql server express comparisons should include usage limit tools management dev backup monitoring simultaneous connection its for my company need to know what should install its for migrating from access db if you have opinion suggestions thank you
7036 receive statistical data every seconds that want to store in my database so that can analyze later for example every seconds could receive the number of oranges sold at store in the last seconds later want to retrieve this data from the database and use it to generate charts showing information like the number of oranges sold for store over the last hours last weeks last months and last years if just dump everything into one table it seems like it would grow very quickly especially if you have lots of data sources stores my thought was that the data could be averaged so that it was less granular over time that is keep detailed records over the last couple of hours entries in the db for every seconds then perhaps averages of minute time spans for the last few weeks then keep averages of each day for the last few months etc this way you have large number of recent records good number of relatively older records and few old records however all the data is still there its just summed up and averaged into one entry over days or months instead of seconds does this approach make sense is there better approach how would organize this into table would it be multiple tables is sql probably mysql good fit or would something work better any thoughts on this would be greatly appreciated
7038 when taking backups from databases is it good practice to use logical backup device for backup location what is the benefit of using backup devices
7044 was wondering if there are any good solutions for recording data in nosql database and then converting them over to an rdbms for example if you wanted to capture some data quickly like session logs but then you want to be able to create reports on them later my favorite database is postgres so if your answer is relevant in postgres that would be great
7060 we have dedicated sql server r2 machine that is experiencing some strange memory issues the machine itself has plenty of resources including two quad core processors 16gb of ram and 64bit windows server r2 enterprise it is dell poweredge the strange problem is that the system is reporting of memory in use but sqlservr exe is only reporting 155mb in use the reason that suspect sql server is the issue is because if restart the sqlservr exe process the memory consumption returns to normal for period of time does anyone have any ideas on how can start to track this issue down thanks jason
7077 am creating database for records that extend prior to ad but mysql date and datetime fields only support dates starting at is there way that would be more convenient than either using bigint type to count seconds before after using unix timestamp or switching to database software that supports larger date ranges
7080 am new to databases but hope you can help me to understand something take part of this query select mindate as date assume that some of the dates are null what effect would it have would the query break
7087 when worked on oracle many years before used to execute commit command manually after each bulk insert in sql server auto commit is on by default which has advantages as well as hazards want to know whether the newer versions of oracle still has auto commit off by default also want to know how to put off auto commit in sql server
7093 am using sql server express edition noticed that recovery model is set to simple by default changed this option to full but want to know which other options to enable to guarantee full recovery if there is no backup is there any way to take scheduled backups of sql server express databases
7147 note this question has been updated to reflect that we are currently using mysql having done so would like to see how much easier it would be if we switched to cte supporting database have self referencing table with primary key id and foreign key parent id field type null key default extra id int11 no pri null auto increment parent id int11 yes null name varchar255 yes null notes text yes null given name how can query the top level parent given name how can query all of the ids associated with record of name foo context am not dba but am planning to ask dba to implement this type of hierarchical structure and would like to test some queries the motivation for doing so is described by kattge et al here is an example of the relationships among ids in the table create new database called testdb set old unique checks unique checks unique checks set old foreign key checks foreign key checks foreign key checks set old sql mode sql mode sql mode traditional create schema if not exists testdb default character set latin1 collate latin1 swedish ci use testdb table testdb observations create table if not exists testdb observations id int not null parent id int null name varchar45 null primary key id engine innodb set sql mode old sql mode set foreign key checks old foreign key checks set unique checks old unique checks add example data set insert into observations values 3null 5null
7156 have recently learned about 1nf 2nf and 3nf understand the definitions and the differences also had learned earlier about removing relationships from conceptual model by using bridging entities but at that time was not aware of normal forms the cool thing that see now about normalization is that if you start with one big messy relation the normalization steps automatically take care of the for you so you dont have to consciously think ok am bridging away my however decided to ask myself hypothetically which level of normalization specifically is responsible for the removal of relationships know it is not 1nf because could easily come up with examples with are both 1nf and however in all the simple examples contrived bringing them to 2nf made the go away but am not sure this is definitive since seem to be not very creative in coming up with exhaustive examples so pose this question is there 2nf relation which exists and is still which needs to go through 3nf to have the removed or does 2nf consistently disallow by itself thanks update let me try to explain myself better consider this simple example table book author isbn title authorid authorname book1 a01 king book1 a02 tolkien book2 a01 king book3 a02 tolkien this is in 1nf and the pk is isbn authorid to go to 2nf we remove the partial dependencies isbn title and authorid authorname and end up with book author isbn authorid book isbn title author authorid authorname now we have two real entity tables book and author plus the artificial bridge entity book author and we got there just by going to 2nf however if had started in different manner and made an er with book and author and relationship between them id have had to create the artificial book author table myself which happened above automatically at 2nf my question does it always happen at 2nf or do you sometimes need to get to 3nf to create the bridge however now that spell my own question out more think see huge error in it was asking at which xnf does the bridge get created but the fact of the matter is that my 1nf example above is nothing more than massive bridge going to 2nf doesnt create the bridge its more like it creates the land on either side of the bridge by pulling the real entities out of the bogus huge table so going to 2nf and higher seems to be less about bridging and more about removing data redundancies which is of course how it was presented to me in the first place
7186 what is sql server parallel data warehouse solution what is the benefit of pdw compared to normal dw architecture based on sql server and ssis ssas and ssrs
7205 thought this was solved with the link below the work around works but the patch doesnt working with microsoft support to resolve http support microsoft com kb ok so have an issue that wanted to throw out to stackoverflow to see if someone has an idea note this is with sql server r2 issue deleting records from table with records takes minutes when trigger is enabled and only seconds when the trigger is disabled table setup two tables we will call main and secondary secondary contains records of items want to delete so when perform the delete join on to the secondary table process runs prior to the delete statement to populate the secondary table with records to be deleted delete statement delete from main where id in select secondary valueint1 from secondary where secondary guid 9ffd2c8dd3864ea7b78da22b2ed572d7 this table has lot of columns and about different nc indexes tried bunch of different things before determined the trigger was the issue turn on page locking we have turned off by default gathered stats manually disabled auto gathering of statistics verified index health and fragmentation dropped the clustered index from the table examined the execution plan nothing showing as missing indexes and the cost was percent towards the actual delete with about percent for the join merge of the records triggers the table has triggers one each for insert update and delete operations modified the code for the delete trigger to just return then to select one to see how many times it is fired it only fires one time during the entire operation as expected alter trigger dbo tr main rd on dbo main after delete as select return to recap with trigger on statement takes minutes to complete with trigger off statement takes seconds to complete anyone have any ideas as to why also note not looking to change this architecture add remove indexes etc as solution this table is the center piece for some major data operations and we had to tweak and tune it indexes page locking etc to allow for major concurrency operations to work without deadlocks here is the execution plan xml names were changed to protect the innocent xml version encoding utf showplanxml xmlns xsi http www w3 org xmlschema instance xmlns xsd http www w3 org xmlschema version build xmlns http schemas microsoft com sqlserver showplan batchsequence batch statements stmtsimple statementcompid statementestrows statementid statementoptmlevel full statementoptmearlyabortreason goodenoughplanfound statementsubtreecost statementtext delete from main where id in select secondary valueint1 from secondary where secondary settmguid 9ddd2c8dd3864ea7b78da22b2ed572d7 statementtype delete queryhash 0xaea68d887c4092a1 queryplanhash 0x78164f2eef16b857 statementsetoptions ansi nulls true ansi padding true ansi warnings true arithabort false concat null yields null true numeric roundabort false quoted identifier true queryplan cachedplansize compiletime compilecpu compilememory relop avgrowsize estimatecpu estimateio estimaterebinds estimaterewinds estimaterows logicalop delete nodeid parallel false physicalop clustered index delete estimatedtotalsubtreecost outputlist update withunorderedprefetch true dmlrequestsort false object database mydatabase schema dbo table main index ix main indexkind clustered object database mydatabase schema dbo table main index pk main id indexkind nonclustered object database mydatabase schema dbo table main index uk main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered object database mydatabase schema dbo table main index uk main indexkind nonclustered object database mydatabase schema dbo table main index ix main indexkind nonclustered relop avgrowsize estimatecpu 85624e estimateio estimaterebinds estimaterewinds estimaterows logicalop top nodeid parallel false physicalop top estimatedtotalsubtreecost outputlist columnreference column uniq1002 columnreference database mydatabase schema dbo table main column relationshipid outputlist top rowcount true ispercent false withties false topexpression scalaroperator scalarstring const constvalue scalaroperator topexpression relop avgrowsize estimatecpu estimateio estimaterebinds estimaterewinds estimaterows logicalop left semi join nodeid parallel false physicalop merge join estimatedtotalsubtreecost outputlist columnreference column uniq1002 columnreference database mydatabase schema dbo table main column relationshipid outputlist merge manytomany false innersidejoincolumns columnreference database mydatabase schema dbo table secondary column valueint1 innersidejoincolumns outersidejoincolumns columnreference database mydatabase schema dbo table main column id outersidejoincolumns residual scalaroperator scalarstring mydatabase dbo main id mydatabase dbo secondary valueint1 compare compareop eq scalaroperator identifier columnreference database mydatabase schema dbo table main column id identifier scalaroperator scalaroperator identifier columnreference database mydatabase schema dbo table secondary column valueint1 identifier scalaroperator compare scalaroperator residual relop avgrowsize estimatecpu estimateio estimaterebinds estimaterewinds estimaterows logicalop index scan nodeid parallel false physicalop index scan estimatedtotalsubtreecost tablecardinality outputlist columnreference column uniq1002 columnreference database mydatabase schema dbo table main column id columnreference database mydatabase schema dbo table main column relationshipid outputlist indexscan ordered true scandirection forward forcedindex false forceseek false noexpandhint false definedvalues definedvalue columnreference column uniq1002 definedvalue definedvalue columnreference database mydatabase schema dbo table main column id definedvalue definedvalue columnreference database mydatabase schema dbo table main column relationshipid definedvalue definedvalues object database mydatabase schema dbo table main index pk main id indexkind nonclustered indexscan relop relop avgrowsize estimatecpu estimateio estimaterebinds estimaterewinds estimaterows logicalop index seek nodeid parallel false physicalop index seek estimatedtotalsubtreecost tablecardinality outputlist columnreference database mydatabase schema dbo table secondary column valueint1 outputlist indexscan ordered true scandirection forward forcedindex false forceseek false noexpandhint false definedvalues definedvalue columnreference database mydatabase schema dbo table secondary column valueint1 definedvalue definedvalues object database mydatabase schema dbo table secondary index ix secondary indexkind nonclustered seekpredicates seekpredicatenew seekkeys prefix scantype eq rangecolumns columnreference database mydatabase schema dbo table secondary column settmguid rangecolumns rangeexpressions scalaroperator scalarstring 9ddd2c8dd3864ea7b78da22b2ed572d7 const constvalue 9ddd2c8dd3864ea7b78da22b2ed572d7 scalaroperator rangeexpressions prefix seekkeys seekpredicatenew seekpredicates indexscan relop merge relop top relop update relop queryplan stmtsimple statements batch batchsequence showplanxml
7233 this is purely academic question in so much that it isnt causing problem and im just interested to hear any explanations for the behaviour take standard issue itzik ben gan cross join cte tally table use master go set ansi nulls on go set quoted identifier on go create function dbo tallytable int returns table with schemabinding as return with e1n as select union all select union all select union all select union all select union all select union all select union all select union all select union all select or rows e2n as select from e1 e1 or rows e4n as select from e2 e2 or rows e8n as select from e4 e4 or rows select top row number over order by select null as from e8 go issue query that will create million row number table select countn from dbo tallytable1000000 tt take look at the parallel execution plan for this query note the actual row count prior to the gather streams operator is after the gather streams operator the row count is the expected stranger still the value is not consistent and will vary from run to run the result of the count is always correct issue the query again forcing non parallel plan select countn from dbo tallytable1000000 tt option maxdop this time all operators show the correct actual row counts ive tried this on 2005sp3 and 2008r2 so far same results on both any thoughts as to what might cause this
7239 want to create view using with clauses but really cant find any references on correct syntax want smth like this with temptbl as select create view someview select from temptbl and what is the correct syntax for using several with clauses nothing useful on msdn
7249 im trying to create table in management studio and after reading about the new from sql on newsequentialid function thought id give it go this is what im doing but its not letting me the error message get is formtemplate forms table error validating the default for column formtemplateid am missing trick here im definitely running sql server r2
7301 in ms sql server am writing one query with conditional sort and my problem is that do not know how can sort conditional using two columns if wrote code like this it is working normaly select from table order by case pkr when kol then kol when nci then nci end do not know how to make conditional ordering for two or more columns select from table order by case pkr when kol nci then kolnci when kol mpci then kolmpci end there is an idea to make dynamic tsql and use sp executesql but am still looking for better idea
7339 for one report am making query where users suppose have to choice top values based on percents or fix amount of rows have two ideas calling two different sub stored procedure based on passed param if param percent begin exec sp data top by percent end if param perrow begin exec sp data top by perrow end other idea is to make dynamic tsql query something like this declare command ncharmax select command select top10 case param when percent then percent else end from table order by exec sp executesql command is there third solution for something like this what is better approach first one avoiding dynamic tsql but is harder to maintain code in two places am using mssql2005 as databse
7350 have sql statement that inserts rows into table with clustered index on the column tracking number insert into tabl name tracking number colb colc select tracking number col col from staging table my question is does it help to use an order by clause in the select statement for the clustered index column or would any gain acheived be negated by the extra sort required for the order by clause
7359 weve had some data loss and dont have reliable backup it seems we do have huge transaction logs and there was some thought that we might be able to use these to get back to certain point is it possible to help rebuild database using transaction logs sql server r2 btw posted from stackoverflow
7390 am about to design database which is going to run with postgresql am used to the magnificent tool called mysql workbench for mysql database it is useful and it looks good which kind of expect from database designing software if am about to learn new database designing tool want it to be the most popular one therefore my question is what are the most popular tools for designing the database in postgresql
7429 im designing an application and considering some options regarding the database system since im not familiar with microsoft sql server would like to know if its possible to have servers sharing the same files as shown below the idea is that the data is in raid system so it would grant us some safety this way we would save effort in synchronizing both databases and save some money on storage we only need one database at time the secondary is just for use in case of failure of the first is this possible im open to different approaches the main problem is the database redundancy our application must guarantee that at the moment im using windows and sql server 2008r2
7468 the title pretty much sums it up we are running out of space on our backup disks and need to remove old backups out of the set cant seem to find any information on this
7504 have written an ssis package to load test data into an empty database some of the tables are very large million rows once the ssis package has completed are there any commands should run as an apprentice dba to maximize the performance of the database for instance executed exec sp updatestats but it reported that no indexes required updating is there list of things to do once large amounts of data has been loaded or does sql server just take care of all that for you
7515 at my office we have query that is pretty ugly but runs pretty well in production and in the development environment 20sec and 4sec respectively however in our testing environment it takes over 4hrs sql2005 latest patches is running in production and development sql2008r2 is running in testing took look at the query plan and it shows that sql2008r2 is using tempdb by way of table spool lazy spool to store the returned rows from the linked server the next step is showing nested loops left anti semi join as eating up of the query the line between the two operators is at 5398mb the query plan for the sql shows no use of tempdb and no use of left anti semi join below is the sanitized code and the execution plans the plan in on top the 2008r2 on bottom what is causing the drastic slow down and change was expecting to see different execution plan so that doesnt bother me the dramatic slow down in query time is what troubles me do have to look at the underlying hardware since the 2008r2 version is using tempdb have to take look at how to optimize usage of that is there better way to write the query thanks for the help insert into table1 grouplock igroupid dlockeddate select table1 igroupid getdate from table1 where not exists select from linkedserver database table2 alias2 where alias2 firstname alias2 lastname dbo fnremovenonlettertable1 fullname and not dbo fnremovenonlettertable1 fullname is null and not alias2 firstname is null and not alias2 lastname is null or alias2 familyname dbo fnremovenonlettertable1 familyname and alias2 child1name dbo fnremovenonlettertable1 child1name and not dbo fnremovenonlettertable1 familyname is null and not dbo fnremovenonlettertable1 child1name is null and not alias2 familyname is null and not alias2 child1name is null or alias2 stepfamilyname dbo fnremovenonlettertable1 stepfamilyname and alias2 stepfamilynamechild1 dbo fnremovenonlettertable1 stepfamilynamechild2 and not alias2 stepfamilyname is null and not alias2 stepfamilynamechild1 is null and not dbo fnremovenonlettertable1 stepfamilyname is null and not dbo fnremovenonlettertable1 stepfamilynamechild2 is null and not exists select from table3 inner join table4 on table4 firstnametype table3 firstnametype inner join table5 on table5 lastnametype table3 lastnametype where table3 igroupid table1 igroupid and table3 bisclosed and table4 snametypeconstant new lastname and table5 sfirstnameconstant new firstname edit executed the query from different sql2005 instance pretty much the same execution plan as the good one still not sure how the two versions are running better to the 2008r2 linked server than the 2008r2 instances to the 2008r2 instances while dont deny that the code could use some work if it was the code being the problem wouldnt see the sameish exec plans across all of my trials regardless of sql version edit have applied sp1 and cu3 to both of the 2008r2 instances still no dice have specifically set the collocation in the linked server no dice have specifically set permissions of my user acct to be sysadmin on both instances no dice have also remembered my sql server internals and troubleshooting well see if can track this down some how thanks everyone for the help and the tips edit have done various permission changes to the linked server ive used sql logins domain logins have impersonated users have used the be made using this security context option have created users on both sides of the linked server that have sysadmin rights on the server am out of ideas would still like to know why sql2005 is executing the query so dramatically different from sql2008r2 if it was the query that was bad would be seeing the hrs run time on both sql2005 and sql2008r2
7573 am working on project and am unsure if there is difference between the way the find cursor works and the way the findone cursor works is findone just wrapper for find limit1 was looking around for it and maybe someone knows if mongodb has special method for it or not am working with the php api for mongodb if that makes difference
7600 do in memory olap engines have advantages over the traditional olap engines backed by enough ram to contain the entire cubes for example if use molap engine ssas and gb tb of ram where the entire cube or even star schema is ram resident what is the difference compared to something like tm1 sap hana
7603 im searching for way to create query to do the following lets consider tables products list of products tags list of tags tag ties table used to associate tag to product lets consider this structure for each table products id int autoincrement name varchar name of the product tags id int autoincrement label varchar label of the tag tag ties id int autoincrement tag id int reference to tag id ref id int reference to product id what want obtain all the products who are tagged with tags id and for example this query does not work as it returns the products having at least one of the tags select name as name id as id from products inner join tag ties ties on id ties ref id where ties ref id id and ties tag id in group by id order by name asc
7622 am going to be storing application level events user added thing user updated that etc for both public and private consumption below is an oversimplified model of the tables and their associations users id pk event type id pk description events id pk user id fk event type id fk created timestamp the events will be reported in timeline format newest events at the top for each user id also like to add custom uri parameters and the events context user updated my visa card am overlooking any potential pitfalls in this design
7647 have view which selects information from information schema columns which seems to give me both columns and views is there an easy way to tell which rows are columns and which are views should do comparison with information schema views need both columns and views just want to be able to tell which is which
7652 im programmer not dba be gentle overview innodb mysql mod perl script persistent connections script called every seconds by thousands of users problem high disk io presumably caused by updates slows everything down creating huge bottleneck queries update single table set refreshtime to current timestamp with two same table checks in the where clause select count four table join with indexes and bunch of ands in the where clause still pretty simple select ab four table join same four tables and bunch of ands in the where clause also pretty simple query cache is on solutions im not dba but suspect that its possible to have table in ram that periodically every seconds updates onto disk and in the event of catastrophic failure will automatically populate the ram table from the disk table upon restart but have no idea if its actually possible if its the best solution or what other options there are out there any thoughts or suggestions again im programmer so if someone either knows someone who does this for fee or can point me to very specific resources id be very appreciative create table openinvitations id int99 not null auto increment createtime timestamp null default null repaccepttime timestamp null default null rep id varchar64 not null default reprefreshtime timestamp null default null customer macaddr varchar14 not null default customerrefreshtime timestamp null default null stage char1 not null default parent varchar25 default null reason varchar64 default null primary key rep idcustomer macaddr unique key id id key customer macaddr customer macaddr constraint openinvitations ibfk foreign key rep id references rep id constraint openinvitations ibfk foreign key customer macaddr references customer macaddr engine innodb auto increment default charset latin1 id select type table type possible keys key key len ref rows extra simple oi ref primarycustomer macaddr customer macaddr const using where using index simple eq ref primaryfk rep primary xxx oi rep id using where simple eq ref primaryfk subscriber primary xxx subscriber id using where simple eq ref primary primary xxx charge id using where id select type table type possible keys key key len ref rows extra simple oi ref primarycustomer macaddr customer macaddr const using where simple eq ref primaryfk rep primary xxx oi rep id using where simple eq ref primaryfk subscriber primary xxx subscriber id using where simple eq ref primary primary xxx charge id using where id select type table type possible keys key key len ref rows extra simple openinvitations all customer macaddr null null null using where after fixing query id select type table type possible keys key key len ref rows extra simple openinvitations ref customer macaddr customer macaddr const using where
7660 checking my sql server log see several entries like this date source logon message login failed for user sa reason password did not match for the login provided client date source logon message error severity state date source logon message login failed for user sa reason password did not match for the login provided client date source logon message error severity state and so on is this possible attack on my sql server from the chinese looked up the ip address at ip lookup net which stated it was chinese and what to do block the ip adress in the firewall delete the user sa and how do protect my web server the best thanks in advance
7689 from what understand about index fragmentation this should not be possible the cases have found in my databases are non clustered example alter table dbo claimlineinstitutional add constraint pk claimlineinsitutional primary key nonclustered claimlineinstitutionalid asc with pad index on statistics norecompute off sort in tempdb off ignore dup key off online off allow row locks on allow page locks on fillfactor on primary update am querying dm db index physical stats avg fragmentation in percent so believe it is physical fragmentation am seeing
7741 while creating test database for another question asked earlier remembered about primary key being able to be declared nonclustered when would you use nonclustered primary key as opposed to clustered primary key thanks in advance
7742 like said in comment below im no dba and im still in learning process heres my scenario have an oracle server running couple schemas let call it source what would like to do is send copy of every objects to another oracle server lets call it target where theres already some objects that need to preserve ideally dont want to shutdown the source server also need to keep the already existing schemas of the target server so the new objects from source need to co exist on the target server with the already existing objects maybe by using instances that could be accomplish dont know im just suggesting what could be the solutions to achieve that
7753 ive run into problem on our production sql server where temp table objects take long time to drop obvious when small and synchronous drop is used cant reproduce this on other sql servers similarly specd with same numbers of spindles serving tempdb data files split in same number of files per physical core on sql enterprise sp2 update one more factor which is looking the most likely this server has databases on it another server with also runs these drops slow thats the only other server ive got with lot of dbs on it second update restart of the problem servers will allow the create and drop statements to execute instantaneously performance of the test degrades over the next few hours while application is running until it hits what appears to be plateau ive got job running in the background that is testing this every mins ill see what results are after few days and see if execution times are the same think they will be third update while none of the executing statements have shown latch waits on cpu resources using sp whoisactive see that during delta interval seconds run running query the cpu delta reports roughly milliseconds and when watch perf during execution there appears to be one core worth of cpu spike during the execution time these are on cpu boxes so it can be bit tough to see via perfmon when other traffic is occurring but it appears to be spiking cpu worth during execution of drop statements create and destruction of tiny temp tables with unique names one column no rows takes less than 20ms on most servers test it on on one server it takes seconds the vast majority of the time is spent on the drop statements during execution there are no explicit waits and no blocking being reported and perfmon doesnt show any load on the subsystem for either data or log files ive looked at peak and low usage times when high number of tables marked for destruction and low the operation takes seconds or so to handle the drop statements no matter what the issue is causing perceivable by clients slowdown to responsiveness sample code created like objects to get the second timing it appears to be about 300ms per drop print convertvarchar30getdate113 create table objects1 id uniqueidentifier not null create table objects12 id uniqueidentifier not null drop table objects1 drop table objects12 print convertvarchar30getdate113 timing consistently to seconds to execute nov begin temp table creation nov end temp table creation nov begin temp table drop nov complete temp table drop could you also run use tempdb dbcc loginfo and record the number of rows returned please add all output from the scripts to your original question noticed originally that had about vlogs so shrank and regrew to see if fragmentation was problem made no difference current dbcc loginfo fileidfilesizestartoffsetfseqnostatusparitycreatelsn io stats output database namephysical nameio stall read msnum of readsavg read stall msio stall write msnum of writesavg write stall msio stallstotal ioavg io stall ms msdbh microsoft sql server mssql mssql data msdbdata mdf4769156581732958 tempdbh microsoft sql server mssql mssql data templog ldf54457301771 modelh microsoft sql server mssql mssql data modellog ldf5471224 tempdbp tempdb data3 mdf260606611120432 tempdbp tempdb data2 mdf248479311118082 tempdbp tempdb data5 mdf251446911120862 tempdbp tempdb data7 mdf254207011125512 tempdbp tempdb data6 mdf251776711122372 tempdbp tempdb data0 mdf247681111135702 tempdbp tempdb data4 mdf246217911116492 tempdbp tempdb data1 mdf245631711118592 modelh microsoft sql server mssql mssql data model mdf51947986 masterh microsoft sql server mssql mssql data master mdf4064073265 msdbh microsoft sql server mssql mssql data msdblog ldf80159508 masterh microsoft sql server mssql mssql data mastlog ldf6401414 waitstats output wait typewait time spctrunning pct pageiolatch ex0 tokenandpermuserstore size is 2952kb select sumsingle pages kb multi pages kb as securitytokencachesizekb from sys dm os memory clerks where name tokenandpermuserstore
7754 what is the maximum number of cores used by single mysql server in production setting
7773 am going to rebuild one ibm server from scratch this server is dedicated to sql server instance running on windows r2 am going to make new raid configuration have scsi gb drives inside the machine and an ibm serverraid 8k controler what would be good way to set the raid levels should have two three or one field on my controler am considering to make one of following solutions use all the disk and make raid pool use disks for raid 1e pool and use it to store the database data and os and use the other disks in raid pool and use that to store the database logs some other combination is larger stripe unit size better this server will be subscriber to replicated database its primary task is going to be reporting and data retrieval with only the replication agent making writes the size of the database is around gb
7785 im trying to run mysqldump to create database snapshot and im finding it will randomly stop midway without reporting any error my database is relatively small about 100mb and is using innodb im running it like mysqldump force single transaction quick user myuser password mypass mydatabasehost mydb tmp snapshot sql checking the exit code reports my version is mysqldump ver distrib for redhat linux gnu i386 ive seen some similar posts and even an official bug report but neither solutions seem to apply how to get mysqldump to take complete database snapshot edit my database currently resides on amazons rds
7906 ve been running this script to try to find extraneous indexes select name as tablename name as indexname reserved page count as spaceinmb from sys dm db index usage stats inner join sys objects on object id object id inner join sys indexes on index id index id and object id object id inner join sys dm db partition stats on index id index id and object id object id where name tablename know that when last user seek scan lookup are all null that no users have used the index since last restart but wondering what system scans lookups seeks are because on certain table found that had no user activity but one had system activity days ago do anyone have any insight on what system scans seeks lookups might be these tables seem really over indexed and like to trim the fat
7924 we have situation were we can deploy instances of an applications in one mysql database using table prefixing or use different mysql databases for each instance of the application for setup central database app1 table1 app1 table2 app1 tablen appn table1 appn table2 appn tablen the end result being large db with many tables setup app1 db table1 table2 tablen appn db table1 table2 tablen the end result being many databases with some tables all things equal amount of data number of app instances etc what are the pros and cons of going with either approach what would be detrimental to database performance and maintenance the application is php based run over apache and were running mysql many thanks for your time and thoughts
7927 this is really weird problem when create new db from script im getting extra tables added these arent system type tables but regular tables that look like they may have come from past project there are of them each with columns either ints or nchar10 with no keys constraints triggers or indices have no idea where this is coming from or how can fix it right now just delete the tables but its bit annoying and id like to fix the issue here is script wrote that reproduces the error create database go use go create table yid int identity constraint pk yid primary keyyid go create table zid int identity constraint pk zid primary keyzid go after run this script my db is created but it has tables instead of running select from information schema tables just lists my extra tables as base table exactly the same as the ones defined all other columns are the same as well this is happening on sql server r2 microsoft sql server management studio
7956 this is the case that in the db im checking there is an archive table which keeps the user history and there is trigger or store procedure that after some time delete rows from this table in order to avoid the oversize of the same didnt design the db im just taking the maintenance of an application that use this db so dont know the name of these stored procedures or triggers what want to do is locate this stored procedure or trigger check the code and modify it to leave this user history longer on the table someone told me to check the sysobjects table where can actually see something with the same name of the table but this is the only information have been able to retrieve any advise thank you
7983 recently saw the question where statement sql construct have used often in constructing dynamic sql in an effort to write cleaner code from the perspective of the host language generally speaking does this addition to sql statment negatively affect query performance im not looking for an answer in regard to specific database system because have used it in db2 sql server ms access and mysql unless its impossible to answer without getting into specifics
7985 for security reasons need server side running debian squeeze logging of all queries that may have changed the content of mysql db and the user who issued it had to rule out the general query log because of performance issues it logs everything and thats too much io the binary log because it doesnt log the users name use tool like ngrep to catch the network traffic and filter for update delete etc because this will get me in mess with transactions and cant know if received query has really been executed couldnt find any settings that would have let me change the behavior of the mysql inherent logs so im looking for other solutions ive come up with two possibilities so far write the general query log to named pipe and attaching filter and writer to the other end of the pipe but im concerned about the performance of this transmitting the relevant logs separately to the server but that way id have to send the queries twice once for the db and again for logging it would be difficult to assure the logs are in sync with the db transactions locks etc and for security reasons it may not be wise to trust the client to really send the logs backgound the users access the db via java desktop application that opens an ssh tunnel to the mysql server im using eclipselink as persistence provider the application makes heavy use of transactions the server is running in shared environment do you have better idea on how to perform my logging
7994 have gb text file in the format of etc need to insert each line into table field such as create table dbo table field1 varchar not null field2 varchar not null what is the most efficient way of inserting this data is there premade program that can be used can it be done from the command line externally ive created script and select records create sql query around them and then insert this takes while is there better way better as in easier faster less overhead note ive tried 1m batch sizes but the query is long at those sizes so am stuck with 1k for now query used insert into table field1 field2 select union all select union all select etc
8011 ive got this innodb error in mysql mysqld was stopped cleanly but managed to lose ib logfile0 ib logfile1 afterward now after clean startup innodb has done its crash recovery went through the innodb force recovery business repaired hung myisam table and now replication is ready to go apart from this big numbers commified innodb error page log sequence number innodb is in the future current system log sequence number innodb your database may be corrupt or you may have copied the innodb innodb tablespace but not the innodb log files see innodb http dev mysql com doc refman en forcing recovery html innodb for more information this is on slave server the above error spews by the hundreds found this answer insert and delete gb worth of data so that the log sequence number becomes inflated big enough http forums mysql com read php225016350163 msg that magic number of 64gb comes from 4gb where that guys innodb log major number needed to increase from to mines going from to gb this will take days ill keep working on speeding up my script and running it in parallel to speed this up in the meantime im hoping someone else has better answer this is silly
8028 am making project where need to change around 36k records in one table daily im wondering what will perform better delete rows and insert new ones or update already existing rows for me it is easier to just delete all the rows and insert new ones but if this is going to fragment the table and indexes and impact performance then would prefer to make updates where possible and delete insert only when necessary this is going to be nightly service and am not looking to improve the speed of the process itself am more concerned about the performance of queries against this table in general where already have million records and how this nightly process will affect it should delete insert records or should update existing ones where possible for this nightly process
8033 im investigating column oriented databases and came across vertica my need is to feed the vertica database from code dont succeed in grabbing this information from vertica im told to use vsql and the copy command all want is issue insert statements to my vertica database can this be done for instance in postgresql you can do embedded sql by linking the postgres ecpg library to your binary have no idea if such thing exists for vertica and know of no other way any ideas
8111 have installed postgresql on my pc win have small java application connecting successfully to it with login sa and password the connection works however it is refused from pgadmin iii itself get error connecting to the server fe sendauth no password supplied how do connect to my database from pgadmin iii with an empty password edit this is just test not production code
8116 would like to monitor how many queries are utilizing the indexes is there any program that can show me live query performance and index utilization note am already aware of slow log file and its usage
8119 coming from mysql background where stored procedure performance older article and usability are questionable am evaluating postgresql for new product for my company one of the things would like to do is move some of the application logic into stored procedures so im here asking for dos and donts best practices on using functions in postgresql specifically regarding performance pitfalls
8128 we are just starting design for new data warehouse and were trying to design how our date and time dimensions will work we need to be able to support multiple timezones probably at least gmt ist pst and est we were initially thinking that we would have one wide combined date time dimension down to maybe minute granularity that way we have one key in our fact tables and all the different date time data for all supported timezones are in one dimension table date key gmt date gmt time ist date ist time etc kimball suggests to have separate day dimension from the time of day dimension to prevent the table from growing too large the data warehouse toolkit which sounds fine however that would mean we have two keys in our fact tables for each time zone we need to support one for the date and one for the time of day as im very inexperienced in this area im hoping someone out there knows the tradeoffs between the two approaches performance vs the management of all the different time zone keys maybe there are other approaches too ive seen some people talking about having separate row in the fact table per timezone but that seems like problem if you fact tables are millions of rows then you need to quadruple it to add time zones if we do the minute grain well have rows per year in our date time dimension table which doesnt sound too horrid for performance but we wont know for sure till we test some prototype queries the other concern with having separate time zone keys in the fact table is that the query has to join the dimension table to different column based on the desired timezone perhaps this is something that ssas takes care of for you im not sure thanks for any thoughts matt
8172 how can write sql to read an xml file into postgresql xml value postgresql has native xml data type with the xmlparse function to parse text string to that type it also has ways to read data from the filesystem the copy statement among others but dont see way to write native postgresql sql statements to read the content from filesystem entry and use that to populate an xml value how can do this
8182 am it graduate with good knowledge on sql and have done most of the sql programming on microsoft sql server in my academics but want to become oracle dba could you please suggest me structural way to to start learning and also please suggest me what background work is needed before digging into it thanks
8351 am running into deadlock scenario where the only participants in the deadlock appear to be single table and single stored procedure that deletes from that table drew that conclusion based on my analysis of the sql error log at the time of several of these deadlocks using the msdn article below as guideline to decipher the trace in the error log the table dextable and the stored procedure cleardextablerows are defined below there is another stored procedure insertdextablerow that inserts rows into dextable but that proc does not seem to be involved in the deadlock based on the entries in the sql error log the dextable has million rows in it and tends to grow steadily the respondent table is also large and tends to grow steadily it is accessed from high traffic volume website with pages that frequently call cleardextablerows and insertdextablerow in quick succession the deadlock has occurred between and times per day for the past days ive enabled sql trace for using dbcc traceon and just recently enabled flag theres good description of the output for these flags on detecting and ending deadlocks my questions are does it make sense that only this one stored procedure cleardextablerows is the cause of the deadlock if so can anyone offer good explanation of how this can happen and recommend way to fix it my suspicion is that the delete statements are causing contention on the pk for dextable which needs to be rebuilt frequently if not what additional trace should enable to dig deeper into the cause of the deadlock do want to learn here table definition create table dbo dextable exportid int not null respondentid int not null exported datetime not null constraint pk dextable primary key clustered exportid asc respondentid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go alter table dbo dextable with check add constraint fk dextable exports foreign key exportid references dbo exports exportid on delete cascade go alter table dbo dextable check constraint fk dextable exports go alter table dbo dextable with check add constraint fk dextable respondents foreign key respondentid references dbo respondents respondentid go alter table dbo dextable check constraint fk dextable respondents go alter table dbo dextable add default getdate for exported go cleardextablerows clear respondents export records to trigger re export create procedure dbo cleardextablerows respondentid int as delete dextable with rowlock where respondentid respondentid go insertdextablerow insert record noting export run for particular respondent create procedure dbo insertdextablerow exportid int respondentid int as if not exists select respondentid from dextable where exportid exportid and respondentid respondentid begin insert dextable exportid respondentid exported values exportid respondentid getdate end else begin update dextable set exported getdate where exportid exportid and respondentid respondentid end go and here are some of the log entries im not entirely sure whats helpful sql error log for one of the recent deadlocks most recent entries in the log are at the top and go further back in time as you read down 58spid18sunknownthis instance of sql server has been using process id of since pm local am utc this is an informational message only no user action is required 59spid20sunknownwaiter id process86a6478 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lockd32579f80 mode associatedobjectid 59spid20sunknownwaiter id process47eda8 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lockc48e52780 mode associatedobjectid 59spid20sunknownwaiter id processce50ce088 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id locka6ad4e580 mode associatedobjectid 59spid20sunknownwaiter id process8691198 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock95f600780 mode associatedobjectid 59spid20sunknownwaiter id process478da8 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock955c98200 mode associatedobjectid 59spid20sunknownwaiter id process700328 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock83fd3b200 mode associatedobjectid 59spid20sunknownwaiter id processffaef8 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock77b633580 mode associatedobjectid 59spid20sunknownwaiter id process86a6ef8 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lockdc536d580 mode associatedobjectid 59spid20sunknownwaiter event waitpipegetrow type consumer id processcd7b1b048 59spid20sunknownwaiter list 59spid20sunknownowner event waitnone type producer id process717198 59spid20sunknownowner event waitnone type producer id processffaef8 59spid20sunknownowner event waitnone type producer id process86a6478 59spid20sunknownowner event waitnone type producer id processdc28aeef8 59spid20sunknownowner event waitnone type producer id processce50ce088 59spid20sunknownowner event waitnone type producer id process47eda8 59spid20sunknownowner list 59spid20sunknownexchangeevent id port80314690 nodeid 59spid20sunknownwaiter id process717198 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock68f374980 mode associatedobjectid 59spid20sunknownwaiter id process716c58 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock60e8d8a80 mode associatedobjectid 59spid20sunknownwaiter id process47f198 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processdc28aeef8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lockb7c7f1c00 mode associatedobjectid 59spid20sunknownwaiter id processdc28aeef8 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processac352e9b8 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock7797b6500 mode associatedobjectid 59spid20sunknownwaiter event waitpipegetrow type consumer id processac352e9b8 59spid20sunknownwaiter list 59spid20sunknownowner event waitnone type producer id process6d5c18 59spid20sunknownowner event waitnone type producer id process716c58 59spid20sunknownowner event waitnone type producer id process478da8 59spid20sunknownowner event waitnone type producer id process8691198 59spid20sunknownowner event waitnone type producer id process86a6ef8 59spid20sunknownowner event waitnone type producer id process700328 59spid20sunknownowner event waitnone type producer id process47f198 59spid20sunknownowner list 59spid20sunknownexchangeevent id port80315870 nodeid 59spid20sunknownwaiter id process6d5c18 mode requesttype wait 59spid20sunknownwaiter list 59spid20sunknownowner id processcd7b1b048 mode 59spid20sunknownowner list 59spid20sunknownkeylock hobtid dbid objectname myserver database dbo dextable indexname pk dextable id lock46a62fe00 mode associatedobjectid 59spid20sunknownresource list 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id processdc28aeef8 taskpriority logused waitresource key d600a7d4a467 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xce4278410 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id processce50ce088 taskpriority logused waitresource key d1007416f809 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x946b46d30 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknownproc database id object id 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id processcd7b1b048 taskpriority logused waittime schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid loginname iis apppool myserver database isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknownproc database id object id 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id processac352e9b8 taskpriority logused waittime schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid loginname iis apppool myserver database isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process86a6ef8 taskpriority logused waitresource key ab0001e10f4e waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xdb9b53cc0 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process86a6478 taskpriority logused waitresource key 7500c3691103 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xdb9b53a80 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process8691198 taskpriority logused waitresource key d20082032104 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xabdc20870 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id processffaef8 taskpriority logused waitresource key f900d9903a2a waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xd9f26e080 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process717198 taskpriority logused waitresource key 4700497f7879 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x8006dcc0 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process716c58 taskpriority logused waitresource key 5a00f098709d waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x6c020d880 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process700328 taskpriority logused waitresource key 51003376bf57 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x92beba3d0 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process6d5c18 taskpriority logused waitresource key 150048fb6c35 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0xdbadb2560 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process47f198 taskpriority logused waitresource key 4700c2a10b35 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x6c2da4080 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process47eda8 taskpriority logused waitresource key 2a004ee465b9 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x6c2da4870 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknowninputbuf 59spid20sunknownwhere respondentid respondentid 59spid20sunknowndelete dextable with rowlock 59spid20sunknownframe procname myserver database dbo cleardextablerows line stmtstart sqlhandle 0x03000500f958193991f66b01a29e00000100000000000000 59spid20sunknownexecutionstack 59spid20sunknownprocess id process478da8 taskpriority logused waitresource key 1400c876e809 waittime ownerid transactionname delete lasttranstarted 16t20 xdes 0x857272d30 lockmode schedulerid kpid status suspended spid sbid ecid priority transcount lastbatchstarted 16t20 lastbatchcompleted 16t20 clientapp net sqlclient data provider hostname db13 hostpid isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 59spid20sunknownprocess list 59spid20sunknowndeadlock victim processdc28aeef8 59spid20sunknowndeadlock list 12spid83unknowndbcc traceon server process id spid this is an informational message only no user action is required
8374 would like your input on this have sql server 2008r2 ent ed 64bit with cores and 64gb ram there is one instance of sql server patched fully as of the max ram is set to 60000mb amount of free ram is according to task manager after few days online if change the max ram to below 53gb it will after few days to stabilize and have some free ram it is the sql process that does allocate the ram according to task manager how do come to terms with what the problem really is it goes without saying that did alot of testing already but havnt solved this to my liking yet and ohh we do not get the typical memory starvation lagging when the available ram is down to free update inspired by another related to ram on this page https dba stackexchange com used these two to see what the ram is being used for select top type as memory clerk type sumsingle pages kb as spa mem kb from sys dm os memory clerks group by type order by sumsingle pages kb desc option recompile select db namedatabase id as database name count as cached size mb from sys dm os buffer descriptors where database id system databases and database id resourcedb group by db namedatabase id order by cached size mb desc option recompile the amount used shown by these are first select kb second one mb that is total of about 52gb used by sql server so where did the rest of my ram go task manager show right now cached available free and sqlservr exe has memory private set max ram set to 60000mb as before
8379 im testing transactional replication between databases in one machinevirtual and in otheralso virtual none of my virtual machines is running an anti virus its all working as expected unless try to insert blob of considerable size 65mb in this case if insert blob of 3kb works just fine im having the following error the distribution agent failed to create temporary files in program files microsoft sql server com directory system returned errorcode this problem is documented here and already changed the settings of that folder so that is not read only anymore right click on the folder properties and changed the read only attribute im not sure if this is the right way to grant permissions to do it the reason say have no anti virus running is because also read that was one source for this problem as the antivirus might block the folder while inspecting it also tried to change distribution profile for oledb streaming that also didnt work what else can do to solve this problem
8456 want to create role named cp with some defined privileges then we will create some other roles which will be granted with cp role know oracle can do this job for examle grant resources to user name which means grant resources role to user do the follwing test in postgresql but it does not work any body know this create role cp and grant privilege postgres create role cp login nosuperuser nocreatedb nocreaterole noinherit encrypted password cp create role postgres grant connect on database skytf to cp grant postgres skytf skytf you are now connected to database skytf as user skytf skytf grant usage on schema skytf to cp grant skytf grant select on skytf test to cp grant create role cp and grant cp role privilege to cp skytf postgres postgres you are now connected to database postgres as user postgres postgres create role cp login nosuperuser nocreatedb nocreaterole noinherit encrypted password cp create role skytf grant cp to cp grant role test cp skytf skytf cp you are now connected to database skytf as user cp skytf select from skytf test limit error permission denied for schema skytf line select from skytf test limit
8461 is there an open source metadata management solution id like to create metadata repository that will hold the details of the metadata of database schemas tables and data items of hundreds of enterprise databases im especially interested in something that can automatically query the schema data of the databases to be able to track changes in the metadata related to tables changes to column data sizes tables and columns added etc
8492 ado net documentation shows the possibility of setting the transaction level for sql transaction to chaos it sounds unpleasant but if the feature is there presumably it has some legitimate use the set transaction isolation level command in bol ah see can use google and bol nothing seems to be named chaos and ado net does have modes that match up nicely to documented levels in addition to chaos what or who is this chaos level for and why does it have an unfriendly name refs the ado net enum
8496 when designing sql server data schema and the subsequent queries sprocs views etc does the notion of clustered index and order of data on disk make any sense to consider for db designs made explicitly to be deployed on ssd platforms http msdn microsoft com en us library aa933131v sql aspx clustered index determines the physical order of data in table on physical disk platform the design to consider them makes sense to me as physical scan of the data to retrieve sequential rows can be more performant than seek through the table on an ssd platform all data read access uses an identical seek there is no concept of physical order and data reads are not sequential in the sense that bits are stored on the same piece of silicon so in the process of designining an application database is the clustered index consideration relevant to this platform my initial thought is that it is not because the idea of ordered data doesnt apply to ssds storage and seek retreival optimization edit know the sql server will create one im just philosophizing about whether it makes sense to think about it during design optimization
8504 why is it that when we have null value in column and we order by the value ascending the nulls are sorted first select as test union all select union all select null union all select union all select order by test results in null keep thinking that null meant indeterminant or possible unknown if thats true wouldnt they sort last since the value could be greater than all other values or is this sorting option somewhere im on sql server 2008r2 but suspect this is true across all sql servers and probably across all rdbmss
8511 having asked this question on stackoverflow wondered where what have done is correct best practise basically every object that create is going into schema with the schema name reflecting usage for example have the schemas audit and admin amongst others this in turn leaves no objects in dbo is this ok is there anything else that need to do
8514 options that have occurred to me one big delete statement for example delete from where and or and same as above but chunk in smaller pieces 10k at time create stored procedure so all im sending are the the two query values
8533 this is rather short question when using microsoft sql server version and newer are there any security related reasons to prefer windows authentication over sql server authentication just to point it out im interested in security related concerns not in administrative or any other differences between the two update if any difference leads to or is security concern then im surely interested in
8580 have configured ssl on in postgresql conf and installed certificate etcetera does this ensure that all clients will always connect over ssl does ssl on it make it impossible to connect without ssl encryption are there other ways to ensure that all clients always connect over ssl tls kind regards kajmagnus
8599 is there way to configure sql server to limit the number of rows than an update statement can modify say wanted the limit to be rows and someone fired an update that would modify rows they would receive an error or some other preventive message
8618 am using ms sql server have table in database in which records are being inserted and deleted initially the table is empty java program probably inserts few records in it another java program quickly access the table and deletes the records all the records which were inserted and then manually run select query on the table so basically can see only the initial and final state of the table both of in which the table is empty need to know that is there way by which can see those records or at least check if something is being inserted deleted any help is appreciated
8627 read that if use isolationlevel readuncommitted the query should not issue any locks however when tested this saw the following lock resource type hobt request mode shared what is hobt lock something related to hbt heap or binary tree lock why would still get lock how do avoid shared locking when querying without turning on the isolation level snapshot option am testing this on sqlserver and the snapshot option is set to off the query only performs select can see that sch is required although sql server seems not to be showing it in my lock query how come it still issues shared lock according to set transaction isolation level transact sql transactions running at the read uncommitted level do not issue shared locks to prevent other transactions from modifying data read by the current transaction so am little confused
8680 am very new to database administration face lot of problems while setting up mysql master slave replication also face regular mysql replication troubleshooting issues can anybody helps to understand how should deal with all these
8700 know the common recommendation for the data volume in sql server is to use kb blocks stripes since the is typically done by entire extents cant find any good information regarding log file however ive been watching activity in process monitor for little while and it appears that the log file sizes range from bytes to just under kb im guessing this is dependent on the size of the transaction being logged and large ones get split up using multiple kb writes so assuming have the partition aligned to the raid stripes would it be safe to assume that kb blocks stripes will yield the best performance all other things being equal would expect that the smaller transactions the ones with byte writes arent coming heavily enough for the large block size penalty to have significant impact whereas the much larger transactions writing lots of kb blocks in rapid succession would be more important to tune for
8708 just learned that client company work for has decided to keep the auto update statistics options off for some of their sql servers and the dbas manually troubleshooting performance issues when they arise however this kind of does of not make sense to me why would you want to prevent the statistics from being updated
8747 well after learning dbms as subject got so many questions in mind normalization is one of them as learnt it there was lot more confusion and found that whatever we do in normalization process we can do it by general common sense also even while making projects also people are not used to follow it so is it really needed is it followed in the companies am asking this question because probably it might consume more time to normalize the database we can directly normalize it using just common sense therefore dont think there is any need of following the standard normalization procedure correct me if am wrong
8752 wondering how can implement sql to get results sorted by best match of like predicate have 100k articles in the database and when user call some items by part of name want to show the results ordered by the best match of asked query ill try to describe that by pseudo code select from articles where item nale like user input order by best match
8771 read in the oracle documentation about key preserved table in updating join views section however didnt find any simple to way understand it hope to receive some simple conceptual details other than the official oracle documentation
8774 have several views that are used to export data from sql server into csv files they are all executed via ssis packages our dba has decided to set the max degree of parallelism to and told me to use maxdop where think it makes sense have now seen many times that maxdop really helps to pull big data amounts especially when it results in table scan over all partitions questions how can use maxdop in view is there way around this restriction
8790 have used mysql and sql server all of my professional career the company will be working for uses postgresql can someone who was is in similar situation please give me some insight on what the biggest differences are and what types of client tools would use to connect to and manage the database thanks
8828 how can you show the sql that is currently executing on an oracle db extra information that would be useful would include user session id etc
8869 how can we restore mysql database with different name from mysqldump file dontt want to open dump file and edit it any other better methods
8885 im trying to understand an issue were having with sql server we are moderately transactional website and we have stored proc called sp getcurrenttransactions which accepts customerid and two dates now depending on the dates and the customer this query can return anything from zero to 1000s of rows the problem what weve experienced is that suddenly we will get number of errors typically execution timeout expired or similar for particular client while they try execute that stored proc so we examine the query run it in ssms and and find that it takes 30s so we recompile the stored proc and bang it runs now in 300ms ive spoken to our dba about this he has told me that the database created query plan when we created the stored proc he said that it was good plan for that set of parameters but if you throw certain set of parameters at it then the plan will not be the best plan for that data and so you will see it running slow the options presented to me are the move that problem query from stored proc and back into dynamic sql that has its execution plan created on every run this feels like step back to me and feel like there must be way around this is there any other way to deal with this issue any and all responses are appreciated
8917 when it comes to the dbo schema is it best practice to avoid using the dbo schema when creating database objects why should the dbo schema be avoided or should it which database user should own the dbo schema
8938 am using reporting database which for all practical purposes is read only database it is created data and dynamic reports are generated and viewed on this database was thinking of making this database read only some of things want to ask regarding read only databases are is only the data read only can we still create indexes or views on this database what are the performance benefits of using read only database select queries using shared locks etc is there any disadvantage of using read only database are there any best practices to follow while using read only databases
8978 what is the actual difference between row based and statement based replication am actually looking in terms of replications effect on the slave if am using row based replication then what is the effect on the slave and if am using statement based then what is the effect please also take the following parameters in consideration replicate ignore db and replicate do db thanks
8994 found this paper written by hugh darwen for avoiding nulls in my database link it describes how to implement databases in the 6th normal form so you can avoid nulls the logic is described in the language tutorial understand how to convert all this logic into sql server but at the end of the he shows how well this can be implemented in current database management systems and then see this part need to implement recomposition query can be done but likely to perform horribly might be preferable to store pers info as single table under the covers so that the tables resulting from decomposition can be implemented as mappings to that but current technology doesn give clean separation of physical storage from logical design perhaps something for the next generation of software engineers to grapple with it suggest storing pers info as single table under the covers but what does that really mean how would implement that in sql server
9011 im having trouble figuring out exactly how to place good boundaries for when and where to use lookup tables in database most sources ive looked at say that can never have too many but at some point it seems like the database would be broken down into so many pieces that while it may be efficient it is no longer manageable heres thrown together example of what im working with lets say have table called employees id lname fname gender position doe john male manager doe jane female sales smith john male sales pretend for moment that the data is more complex and contains hundreds of rows the most obvious thing see that could be moved to lookup table would be position could create table called positions and stick the foreign keys from the positions table into the employees table in the position column id position manager sales but how far can continue to break the information down into smaller lookup tables before it becomes unmanageable could create gender table and have correspond to male and correspond to female in separate lookup table could even put lnames and fnames into tables all john entries are replaced with foreign key of that points to the fname table that says an id of corresponds to john if you go down this rabbit hole too far like this though your employees table is then reduced to mess of foreign keys id lname fname gender position while this might or might not be more efficient for server to process this is certainly unreadable to normal person who may be trying to maintain it and makes it more difficult for an application developer trying to access it so my real question is how far is too far are there best practices for this sort of thing or good set of guidelines somewhere cant find any information online that really nails down good useable set of guidelines for this particular issue im having database design is old hat to me but good database design is very new so overly technical answers may be over my head any help would be appreciated
9015 im looking for portable sample database that can be created from script has simple er model has little data for database administration practicing purposes know that there is adventureworks northwind and pubs sample databases out there but im looking for something smaller any idea where can find this kind of database
9047 in my undergrad databases course my professor mentioned that some dbms software is so advanced that it can detect when long running query would benefit from an index the dbms can then create that index for the duration of the query to increase performance however dont think thats possible and some initial googling seems to agree with me does anyone know of dbms that actually implements this strategy
9109 am attempting to use sql case order by in stored procedure where am passed orderby parameter as tinyint orderby then date column should be asc orderby then date column should be desc my question is how can get the date column to sort desc when am passed for that parameter and have the string column sort asc in the same case order by statement this is what have now for the case order by order by case when orderby then convertnvarchar30 ccd certenddate tp lastname tp firstname end case when orderby then convertnvarchar30 ccd certenddate tp lastname tp firstname end desc this codes parses and returns result set without error but the 2nd case order by is all in desc sort order when would prefer to have ccd certenddate desc tp lastname asc tp firstname asc thanks in advance
9124 am using sql server and have been looking closely at the concept of key lookups http blog sqlauthority com sql server query optimization remove bookmark lookup remove rid lookup remove key lookup so if you have key lookup you can create an index with the include columns to cover the non index columns you have in the select statement for instance select id firstname from oneindex where city las vegas go this index will include key lookup create nonclustered index ix oneindex city on dbo oneindex city asc on primary go but this one will remove the key lookup create nonclustered index ix oneindex include on dbo oneindex city include firstnameid on primary go mean how much of an impact will this have on performance the key lookup has an operator cost of but what does that really mean how do you know that you need the second index there and at what point does it become the case that you are trying to add too many indexes and it is not worth it it seems to me that some queries can include index scans key lookups and still seem to perform very fast
9143 have been getting myself very confused could somebody kindly explain under what circumstances would want to use group by coalesce my guess is that would use it if wanted to conditionally group set of data by column if was not null and by column otherwise does that sound right
9196 am trying to understand how the indexes are managed for below common data types numeric integer decimals string varchar char datetime have few questions how the indexes are stored for different data types for example if have numeric data like string data like aaabbbcccaaaabc how the numericcolumn stringcolumn index will store this data on disk is there any different between the retrieval of numeric string indexes for example if try the select statements on numericindexed column on sringindexes columns how they are retrieved is the indexes are stored different by sql server oracle or they use the same logic regards
9197 im running postgres have postgres database on linux filesystem that is quite full ive run vacuum and vacuum full yet none of the space has been returned to the os when observe the size of the table in postgres it shows that the table is using considerably less space down to 80mb form 800mb is there something else that should be run
9249 when run mysqldump get an error mysqldump got error the user specified as definer root foobar does not exist when using lock tables this makes sense because foobar is legacy machine that no longer exists how do change the definer of all my tables to root localhost
9265 why do need to have primary key on my database for it to function correctly in every tutorial read you need to make the id key the primary key what does the primary key do differently than the regular cells
9268 it is possible to recalculate row size in sql server take this example declare counter int declare statement nvarcharmax set counter drop table kua2 create table kua2id int while counter begin set statement nalter table kua2 add id cast counter as nvarcharmax nvarcharmax exec statement set statement nalter table kua2 drop column id cast counter as nvarcharmax exec statement set counter counter end alter index all on kua2 rebuild dbcc cleantable 0kua20 alter table kua2 add id0 int alter table kua2 add id1 int alter table kua2 add id2 int alter table kua2 add id3 int when add id3 column got warning about bytes but the table has only int columns it still count the dropped nvarcharmax columns in row size the only thing that helps is recreate the table but for several reasons dont want to do that is there any way to tell the sql server to recalculate the row size somehow
9293 slightly unimportant but its piqued my curiousity ive just logged into an oracle 10g database for the first time using the oracle sql developer tools used generic user login which isnt my name nottstest2 from machine that is called something that isnt my name courgette my name is nowhere in the database and isnt associated with the login yet the server logs show connection from jon hopkins how does it know who am is it being pulled from my windows login in some way though im not using single sign in
9302 have very simple query that is showing in activity monitor and other statistics as having the most logical reads on my entire db server select maxresult date from tablex mm with nolock join tablex results mr with nolock on mr id mm id where days is not null and mm order id tablex has about million rows tablex results has about million rows what can do here to reduce the number of logical reads on this query im kind of confused on how such simplistic query can have such massive number of logical reads thanks index definition from comment tablex indexname pk type key1 key2 idx mp meds aa order id medpass date idx mp meds id and order id medpass date ix mp meds root order id da root order id medpass date pk mp med pk medpass meds id tablex resulsts
9306 how can dump specific table or set of tables without including the rest of the db tables
9352 postgresql has matrix of different high availability options which represent many different ways of building replication into an rdbms here is the postgresql high availability load balancing and replication feature matrix questions which of the approaches in the postgresql high availability matrix are supported by oracle does oracle do high availability with techniques that are not available with postgresql
9356 am student and am developing several projects as part of my academia while developing the database for one of the projects we came across situation where we thought about whether erd is necessary or not right now not every one of us are in agreement on developing the erd first and then developing database from it the majority of people prefers developing the database on the fly verbally according to the system requirementd on paper directly now am strict follower of database principles think the database should be developed from the erd only so just want to know the following is the industry following these principles am just wasting my time on developing an erd what are the benefits of developing an erd
9406 have log and logitem tables im writing query to grab some data from both there are thousands of logs and each log can have up to logitems the query in question is complicated so im skipping it if someone thinks its important can probably post it but when ran ssms estimated query plan it told me new non clustered index would improve performance up to existing index non clustered key colums logitem parentlogid datemodified name databasemodified query plan recommendation create nonclustered index logreportindex on dbo logitem parentlogid databasemodified just for fun created this new index and ran the query and much to my surprise it now takes second for my query to run when before it was seconds assumed that my existing index would cover this new query so my question is why did creating new index on the only columns used in my new query improve performance should have an index for each unique combination of columns used in my where clauses note dont think this is because the sql server is caching my results ran the query about times before created the index and it consistantly took seconds after the index it is now consistantly or less
9441 from this and this guess that there is no predefined named system exceptions for ora how can rewrite the following to catch only the error ora begin execute immediate create sequence test start with increment by exception when others then null end btw is there any syntax to catch errors by just providing the error codes
9494 in my legacy database lot of the time the schema needlessly allows null entries for particular columns to help find out which columns in particular need to do some querying of sql server that beyond my level of expertise my aim is to tighten up the schema bit and save myself from dealing with the null case in my code crude way option to help me solve my problem is to just get all records and select that contain at least one null entry then eye scan for columns that contain no null entries could of course just use where columna is null or columnb is null or columnc is null but this gets tedious for tables with lots of columns some tables in this database contain more than twenty columns also theres about tables in total so general solutions are best here are three options for answers that would satisfy me option get me all the records where at least one of the columns is null will eyescan for columns with no null entries option an even better answer would be some script that gets list of column names that contain at least one null entry then will see which nullable columns are not in this list manually option the best answer would get me list of column names that are marked as nullable in their schema even though no records exist with null entry in those columns thanks
9518 am running scripts php cli on linux each script are updating loop the data in the mysql database when typed mysqladmin proc in terminal can see lot of rows has been locked for seconds mostly are update queues how to improve the performance faster am using innodb engine top command top up days users load average tasks total running sleeping stopped zombie cpus us sy ni id wa hi si st mem 4138464k total 1908724k used 2229740k free 316224k buffers swap 2096440k total 16k used 2096424k free 592384k cached pid user pr ni virt res shr cpu mem time command mysql 903m 459m mysqld etc my cnf mysqld set variable max connections safe show database max user connections key buffer size 16m query cache size 350m tmp table size 200m max heap table size 200m thread cache size table cache thread concurrency innodb buffer pool size 400m innodb log file size 128m query cache limit 500m innodb flush log at trx commit server spec intel core quad q8300 ghz 4gb ram mysqladmin proc id user host db command time state info user localhost xxxxxxxxxxxxxx query updating update data set status error unknown error where number 0xxxxx user localhost xxxxxxxxxxxxxx query updating update data set status error invalid where number 0xxx user localhost xxxxxxxxxxxxxx query updating update data set status where 0xxxx user localhost xxxxxxxxxxxxxx query updating update data set status error unknown where number 0xx user localhost xxxxxxxxxxxxxx query updating update data set status error invalid where number 0xxxx user localhost xxxxxxxxxxxxxx query updating update data set status error unknown where number 0xxxx user localhost xxxxxxxxxxxxxx sleep null user localhost xxxxxxxxxxxxxx query updating update data set status error unknown where number 0xxx user localhost xxxxxxxxxxxxxx query updating update data set status where number 0xxxx explain id select type table type possible keys key key len ref rows extra simple data index merge processstatus processstatus null using intersectprocessstatus using where is mysql server using multiple cores edit more information data table create table data number varchar50 not null dob varchar50 not null other varchar50 not null status tinyint1 unsigned not null error varchar150 not null process varchar50 not null key process process key status status key number number engine innodb default charset latin1 number of rows in the data table indexed process status and number data table size mib according to phpmyadmin hard drive gb sata rpm no raid loop offset xx limit xx data select from data where status and process limit limit offset offset loop rows error errorcheck data number if error update data set status error error where number data number else update data set status where number data number end loop rows end loop limit select query to return only limited amount of rows and it will select the remaining rows during the next iteration using offset and limit read all the rows from the select query into an array of variables iterate your array of numbers update every row continue with the first step again mysql explain select from data where number 0xxxxxxxxxx id select type table type possible keys key key len ref rows extra simple data all number null null null using where iostat root host iostat linux el5pae xxxxxxx xxxxxxxxxxxx com avg cpu user nice system iowait steal idle device tps blk read blk wrtn blk read blk wrtn sda sda1 sda2 sda3 sda4 sda5 sda6 sda7
9587 how does one setup two identical servers for automatic failover in postgresql os centos postgresql compiled from source the postgres user account exists on both machines and has ssh passwordless key to connect to both machines my current setup master server configuration postgresql conf listen address wal level hot standby max wal senders checkpoint segments wal keep segments archive mode on archive command cp opt pgsql91 archive pg hba conf host replication all trust host replication all trust standby server postgresql conf and pg hba conf are identical to what is configured on the master server recovery conf standby mode on primary conninfo host trigger file opt pgsql91 data trigger txt thanks to hzroot now understand how to switch the server from standby to master using the following commands can synchronize the new slave with the new master and then get replication backup and running on the new master su postgres touch trigger txt in opt pgsql91 data recovery conf becomes recovery done psql select pg start backupbackup true rsync ssh opt pgsql91 data opt pgsql91 data exclude postmaster pid psql select pg stop backup on the new slave create the recovery conf cp recovery done to recovery conf vi recovery conf change ip address primary conninfo host start postgresql so my questions are now is this the correct way to switch roles has anyone automated this process if so what did you do if synchronous replication is enabled noticed the new master server wont commit any transactions because it is waiting for the slave to respond there is no slave however because the other server the old master is down is this correct or do need to temporarily disable synchronous replication while the new slave is down
9638 why does the following query return infinite rows would have expected the except clause to terminate the recursion with cte as select from values12345 as select from cte where in union all select from select from cte except select from select from came across this while trying to answer question on stack overflow
9662 have the following table create table test id smallint unsigned auto increment age tinyint not null primary keyid check age the problem is that the check constraint does not work on the age column for example when insert for the age field mysql accepts it
9689 want to do this insert into adminaccounts name select name from matrix but dont want to create duplicates ie ran this few weeks ago and need to update the data
9746 have an innodb table that want to alter the table has 80m rows and quit few indices want to change the name of one of the columns and add few more indices what is the fastest way to do it assuming could suffer even downtime the server is an unused slave is plain alter table the fastest solution at this time all care about is speed
9759 have table like the following create table my table id int8 not null id int8 not null id int8 not null id int8 null constraint pk my table primary key id constraint constrainte unique id id id and want id id id to be distinct in any situation so the following two inserts must result in an error insert into my table values null insert into my table values null but it doesnt behave as expected because according to the documentation two null values are not compared to each other so both inserts pass without error how can guarantee my unique constraint even if id can be null in this case actually the real question is can guarantee this kind of uniqueness in pure sql or do have to implement it on higher level java in my case
9762 lets say have table called dbo groupassignment groupid rank the pk is groupid rank normally the ranks within group are contiguous sequence of integers starting from but its possible for groupassignment to get removed leaving gap when new assignment is made for the group want to fill the first gap available so how could calculate this in sql server
9765 as an interest if would to transition from sql server dba to oracle what will be the major learning or unlearning that would have to do would assume the concepts are the same and the difference is merely the programming language but have not seen the other side of the door
9798 have the following table create table trans id serial primary key trans date date trans time time want to have the following view create or replace view daily trans as select trans date maxtrans time as first mintrans time as last calculate statusmintrans time maxtrans time as status group by trans date with columns that specify the ids of the max and min trans time how do do that
9829 this white paper compares the performance for individual select insert delete update and range select statements on table organized as clustered index vs that on table organized as heap with non clustered index on the same key columns as the ci table generally the clustered index option performed better in the tests as there is only one structure to maintain and because there is no need for bookmark lookups one potentially interesting case not covered by the paper would have been comparison between non clustered index on heap vs non clustered index on clustered index in that instance would have expected the heap might even perform better as once at the nci leaf level sql server has rid to follow directly rather than needing to traverse the clustered index is anyone aware of similar formal testing that has been carried out in this area and if so what were the results
9852 while developing on mysql really miss being able to fire up profiler find sqlyog is good enough replacement for query analyzer but have not found tool that works like sql profiler for the mysql folk who have not seen microsofts sql profiler here is screenshot at my previous job we had tool that trumped sql profiler and even gave us stack traces does anyone know of any tools like the ones mentioned that works with mysql fyi can get the altiris profiler to work with mysql but it will involve running windows furthermore its not really symantec sku so licensing is really tricky
9953 have to develop cms which will support two language english arabic this cms will be sort of article publishing site while designing analysis found that some articles are more than characters in length my table has some column as pageid int pagetitleenglish nvarchar200 pagetitlearabic nvarchar200 pagedescenglish nvarchar500 pagedescarabic nvarchar500 pagebodyenglish nvarcharmax pagebodyarabic nvarcharmax if keep pagebody as nvarchar4000 then limited to characters and if have to store arabic version then need bytes as arabic is unicode and take time more space then ascii so am only left with option of defining pagebody as nvarcharmax this will have it downside from performance point of view my actual question is if some data in pagebody column is less than characters will it ms sql store than data in inline column or separately in the database looked for this on google also but didnt find any relevant answer and how can improve performance in such scenario any suggestions for best practice for such design of multilingual cms are welcome need to support only two languages arabic english
9962 suppose have user table in my site in which there are around million users records in the table for speeding up my login process is it good approach to split my user table one for their information and one for their login if we can run query similar to the one below from one table select usernamepassword from users where username test and password is it necessary to split it and does this speed up my sites login process
10005 this question is about an issue somewhat more complicated than the one which has already been addressed in these old questions all of which are duplicates of one another suggestion for database structure for multilanguage jun whats the best database structure to keep multilingual data feb what are best practices for multi language database design may schema for multilanguage database nov the most popular database scheme for backing multilingual user interfaces seems to be having all the translated texts of all languages in one table with columns the text id the language code and the text itself the text id and the language code together make up the primary key thats all very fine but now consider complication suppose that the texts need to be searchable suppose for example that this is multi language shop this means that for every product category entered into the database the shop owner will enter the name of the product category in each and every one of the supported languages and then the shopper will be able to search for the product category by name in their own language there is problem collation different languages have different collation sequences and the collation sequence which works for one language does not work for another so if all texts of all languages are on single column what collation sequence are they going to have how are we going to query the database to find the text id of specific text while in web product search accuracy and performance might not be awfully important for the purposes of this discussion let us assume that they really matter most database administrators are familiar with the concept of collation in the sense of the collation of the database luckily thats just the default collation which is used if no other collation information is present but there exist other places too where collation can be specified the sql create index command supports collation specification though rumors have it that microsoft sql server does not support it does anyone know about that the sql select statement also supports collation but in this case the collation specification works as function causing an index scan instead of an index lookup something which might be impermissible if we want performance then again if thats the best we can have it might be better than nothing also hear that on microsoft sql server you can have non persisted computed columns on which you can specify collation and create filtered index though have never heard of this before and if it is microsoft sql server only feature then id rather refrain from using it no matter how cool and well thought out it is so in light of all that how do we structure our database and how do we perform our queries if the goal is an updatable and searchable multilingual database this question was inspired by discussion that took place here how will nvarcharmax store data in database will it be fast if some data is less then characters
10034 question for you dbas and such how do you go about learning the skills to become dba without having the on hands or on the job training my experience in db work has been messing with mysql via myphpadmin or something similar not doing ton of in depth work classes sql books
10057 basically am ms sql developerand relatively new to oracle even after great deal of research was unable to find any way of copying entire database from one server instance to another in ms sql this is usually done through backup database and restore database sql query we can even use external tools such as management studio to create scripts but how this can be done in oracle 11g express any ideas
10113 whats the most efficient way to retrieve date ranges with table structure like this create table somedatetable id int identity1 not null startdate datetime not null enddate datetime not null go say you want range for both startdate and enddate so in other words if startdate falls in between startdatebegin and startdateend and enddate falls in between enddatebegin and enddateend then do something know there are few ways to probably go about this but what is the most advised
10146 we were recently using this query to find tables in our database that didnt have clustered indexes and found that one of the results it reported back was the sys sysfiles1 table were running sql server and was under the impression that this table wasnt used anymore as some answers have pointed out there seems to be misconception that this table only exists for databases upgraded from sql server furthermore im not able to directly select anything out of the sys sysfiles1 table though am able to select directly out of the sys sysfiles view running the following to create fresh database against local installation of sql server version microsoft sql server r2 sp1 x64 though weve seen it on our production instance of sql server as well illustrates what mean create database sysfilestesting returns row select from sysfilestesting sys objects where name sysfiles1 throws invalid object name error select from sysfilestesting sys sysfiles1 why is sys objects reporting the existence of sysfiles1 table
10167 my supervisor is hesitant to use trigger in process because if there is network interruption at just the right time the process would not complete does sql server include the trigger in the calling procedures transaction if not what is the best way to implement this
10199 if is friend of then should store both values ab and ba or one is enough what are the advantages and disadvantages of both methods here is my observation if keep both then have to update both when receive request from friend if dont keep both then found it difficult when having to do multiple join with this table currently keep the relationship one way so what should do in this case any advice
10208 one of my postgresql servers hosts several databases which receive constant stream of data the data is not particularly structured it amounts to the current time and variety of observed data for that particular instant the data rate is fairly high it works out to about gigabyte day for one database about tenth of that for another one dont expect this rate to increase read performance is much lower priority and is currently acceptable in the logs have this message log checkpoints are occurring too frequently seconds apart hint consider increasing the configuration parameter checkpoint segments this value is currently set to which is courtesy of pgtune what are the settings should consider to improve write performance would prefer to keep as much safety as possible considering the volume of data coming in could accept losing some recent data in failure as long as the bulk of the data were intact edit im using postgresql for now but plan to upgrade to am not posting the hardware details because while acknowledge their importance ultimately will be needing to make this optimization on several machines with very diverse hardware if the hardware is essential to the answer please give me the general information so can apply the answer to machines with different hardware configurations
10215 given simple three table join query performance changes drastically when order by is included even with no rows returned actual problem scenario take seconds to return zero rows but is instant when order by not included why select from tinytable one narrow row join smalltable on id tinyid one narrow row join bigtable on smallguidid guidid million narrow rows where foreignid doesnt match order by createdutc try with and without this order by understand that could have an index on bigtable smallguidid but believe that would actually make it worse in this case heres script to create populate the tables for test curiously it seems to matter that smalltable has an nvarcharmax field it also seems to matter that im joining on the bigtable with guid which guess makes it want to use hash matching create table tinytable id int primary key identity1 foreignid int not null create table smalltable id int primary key identity1 guidid uniqueidentifier not null default newid tinyid int not null magic nvarcharmax not null default create table bigtable id int primary key identity1 createdutc datetime not null default getutcdate smallguidid uniqueidentifier not null insert tinytable foreignid values7 insert smalltable tinyid values1 make million rows declare int set insert bigtable smallguidid select guidid from smalltable while begin insert bigtable smallguidid select smallguidid from bigtable set end ive tested on sql and 2008r2 with same results
10277 should developers be given permission to query select read only production databases the previous place worked the development team had the db datareader role where work now the development team cant even connect to the production instance one of the test instances is copy of production restored from production backup once week so there arent any problems with developers actually seeing the data what good reasons are there for not allowing developers to query production except for simply not wanting them to have access to read sensitive data
10383 ive tired to explain to every new junior developer in our team why he should use primary keys and how to do that so decided to write small whitepaper which every new developer should read here is the draft of it disclaimer know and understand the difference between clustered index and primary key in the following question primary key means primary key clustered and without primary key means without pk and clustered index note that it is whitepaper for junior and not sql programmers all the things that they do reviewed before entering to the main development branch im not going to explain to them when there will be benefit of using clustered index and nonclustered primary key else will fall to premature optimizations hell the question is what else should add to the document may be what should change in it and what to explain more in detail here comes the draft primary keys have to constrain any table within database without pk table considered as heap and sql server has very limited uses of this type of data the only thing should say it is suitable buffer for fast bulk loading data from the outside of sql servers engine avoid using natural primary keys primarily because of theirs natural gauss distributions for example in phonebook table with primary key based on family and name will be many smiths and wilsons and much fewer zimmerbergs and this states that pages containing smiths and wilsons will be splitted more often than other pages and queried also more often which multiplies the performance impact that leads to performance degradation because of primary keys page fullness and most of searches will hit pks sparsed pages moreover even using ssn or id number which have hope flat distribution as pk does not solve the problem of pks page splitting because this numbers are not in any organized order secondary natural pks are often composite that creates composite foreign keys and wide indexes and as result hurts performance so avoid using composite pks better using simple surrogate pk and composite unique index than all in one composite pk because it leads to composite fk and wide indexes due to statement that every secondary index on table with pk has to include whole pk within avoid using surrogate primary keys other than integer or uniqueidentifier types during database design phase it is very important to identify entities and corresponding tables which may have in perspective zillions of rows or tables which keys have to be not only table wide unique but db or even world unique or which have to be joined with other tables by this key over several hop tables this tables better to have uuid pk others ordinary integer because sql server very well fine tuned to use integer pks these two types can guarantee both vector distribution identity or newsequentialid and monotonous sequence main database design rule twenty minutes spent to well thought out design will save days or even weeks during production databases maintenance
10396 say have table clustered on primarykey and in all cases want my results to be ordered by primarykey so additionally always order by primarykey in all queries does this order by affect performance in any way or is it ignored by the profiler as the rows are already in this order in my instance am using sql server database
10409 first of all know squat about oracle databases vendor who is looking at converting our data from an older piece of software with an oracle db back end wants dump of our database to see what they need to do to bring that data over to our new software which is ms sql server based know in ms sql server right click on the database and say backup and can create backup of the database to file and ftp that backup to the vendor how would do the equivalent in oracle it appears like we might have version 10g of oracle
10474 how can copy my public schema into the same database with full table structure data functions fk pk and etc my version of postgres is need to copy schema not database
10492 im currently experimenting bit with pl pgsql and want to know if there is more elegant way to do something like this select data into data from doc where doc id id and group cur group cur order by id desc limit exception when no data found then select data into data from doc where doc id id and global cur global cur order by id desc limit exception when no data found then return null
10496 am just using very few innodb tables less than 1mb but during mysql startup it said innodb initializing buffer pool size 0m does it mean even am using in such small size the server still use 128m ram
10535 if user never runs rebuild or reorganize on their database does sql server still somehow defragment the indexes msdn suggests that if an index is over fragmented it is recommended to run rebuild instead of reorganize would running reorganize multiple times do the same things as rebuild wonder about this because have client that has highly fragmented index they run reorganize against that index every weekend and over time it seems like their index becomes defragmented does this make sense
10587 is there way to query the owner of all jobs in sql server r2 discovered when maintenance plans are edited the owner gets changed so want to make sure they are all owned by sa
10655 am report developer who wants to make my queries as efficient as possible used to work with dba who told me believe because was always dealing with reports on production server to use nolock in every single query now work with dba who has banned nolock under any circumstance even when report of mine due to considerable lack of indexes on couple of tables is stopping replication and system updates in my opinion in this case nolock would be good thing since most of my sql training has come various dbas with very different opinions wanted to ask this to wide variety of dbas
10657 have created stored procedure in mysql using the following syntax drop procedure if exists sp set comment count delimiter create procedure sp set comment count in id int begin ac allcount declare ac int default select count as ac into ac from usergroups as ug left join usergroup comments as ugm on ugm gid ug id left join mediagallery as dm on ugm mid dm id where dm status not in and ug id id update usergroups set allcount ac where usergroups id id end delimiter fyi ive greatly simplified the stored procedure but do know it works without any issues what id like to be able to do is set up trigger from usergroup comments that works like this drop trigger if exists usergroups comments insert create trigger usergroups comments insert after insert on usergroups comment for each row begin call sp set comment countnew gid end but for some reason every time do mysql throws an error at me thats less than helpful stating that theres syntax error on line ive combed through the mysql documentation and found some information on restrictions of triggers but found it to be fairly convoluted http dev mysql com doc refman en stored program restrictions html any ideas would be helpful
10694 had to write simple query where go looking for peoples name that start with or select name from spelers where name like or name like order by was wondering if there is way to rewrite this to become more performant so can avoid or and or like
10716 something bad happened yesterday view that was created sometime back ago was modified by someone which eventually broke the reports unfortunately somebody knowingly or unknowingly did this modification in production database my question is there way script software freeware etc by which we can come to know who username did this modification so that can revoke the access to production database for that user if my question is unclear please comment
10729 have table that has fields with an auto increment primary key its hard to look at table that has this many columns so frequently use select statement to look at of the fields that mentally categorize as group and fields that categorize as group would face performance hit on reads writes if split this table into two tables one with fields and another with fields both would have the same auto increment primary key so that they could be joined on the primary key if need to look at at fields at once
10776 how does one scale sql server or at its basic understand there are two options scale up if cpu bound can clearly see going from cpu core to to or if ram usage rockets just adding more ram does sql server actually pick up the slack and scale up that way assuming no application level changes to minimize speculation lets assume im not doing something dumb like burning cpu cycles doing cross joins etc scale out its not very clear how scaling out would work mean if added another sql server right next to my first one how does the query know which server to run on is there some load balancer at the front and does it come with the sql server software does it entail application level changes for scaling out to work or do have to shard the data and have custom code that calls up the correct database server depending on the data sharding key would appreciate input from more experienced folks
10818 when comparing the execution time of two different queries its important to clear the cache to make sure that the execution of the first query does not alter the performance of the second in google search could find these commands dbcc freesystemcache dbcc freesessioncache dbcc freeproccache in fact my queries are taking more realistic time to complete after several executions than before however im not sure this is the recommended technique whats the best practice
10852 use the root account created the account but cant use the account to connect to mysql server when specify the host parameter can successfully connect without the parameter please see the transcript below hope someone can help me to explain it thanks mysql grant all on to identified by error you have an error in your sql syntax check the manual that corresponds to your mysql server version for the right syntax to use near at line mysql grant all on to identified by query ok rows affected sec mysql show grants for grants for grant all privileges on to identified by password 667f407de7c6ad07358fa38daed7828a72014b4e row in set sec mysql exit bye root localhost mysql localhost enter password error access denied for user localhost using password yes root localhost mysql enter password error access denied for user localhost using password yes root localhost mysql enter password error access denied for user localhost using password yes root localhost mysql welcome to the mysql monitor commands end with or your mysql connection id is server version mysql community server gpl copyright oracle and or its affiliates all rights reserved oracle is registered trademark of oracle corporation and or its affiliates other names may be trademarks of their respective owners type help or for help type to clear the current input statement mysql mysql status mysql ver distrib for linux x86 using readline connection id current database current user localhost ssl not in use current pager stdout using outfile using delimiter server version mysql community server gpl protocol version connection localhost via unix socket server characterset utf8 db characterset utf8 client characterset utf8 conn characterset utf8 unix socket var lib mysql mysql sock uptime days hours min sec threads questions slow queries opens flush tables open tables queries per second avg mysql edit yes mysql is listening on port root localhost nmap localhost starting nmap http www insecure org nmap at cst interesting ports on localhost localdomain not shown closed ports port state service tcp open ssh tcp open smtp tcp open rpcbind tcp open ipp tcp open unknown tcp open mysql nmap finished ip address host up scanned in seconds root localhost
10873 have two database servers connected via linked servers both are sql server 2008r2 databases and the linked server connection is made via regular sql server link using the current logins security context the linked servers are both in the same datacentre so the connection shouldnt be an issue use the following query to check which values of the column identifier are available remotely but not locally select identifier from linkedserver remotedb schema tablename except select distinct identifier from localdb schema tablename on both tables are non clustered indexes on the column identifier locally are around 6m rows remotely only yet when looking at the query plan of the execution time is devoted to executing remote query also when studying the complete query plan the number of estimated local rows is instead of which is the number of estimated rows when selecting only the query coming after except when executing this query it takes long time indeed it makes me wonder why is this is the estimation just way off or are remote queries on linked servers really that expensive
10901 im researching defragmenting databases and it seems the following sql statement is what im looking for alter index all on mytablename rebuild withonline on when pull the info from sys dm db index physical stats see the percents are very high and percent so that tells me want rebuild and not reorganize have few question before executing these rebuilds can it be ran while processes are using the database online on tells me yes but want to confirm it wont crash anything or is it better to run when its not in use read rebuild makes things run slower is that just while the indexes are being rebuilt or forever after how long will it take to rebuild all indexes or rather each one are there any side effects or other info that need to be aware of this is production live database edit and finally what is the best way to go about rebuilding it looping through all the objects with percentage of greater than or thank you
10931 for ages have been using mysql recently noticed that this version is no longer supported in repository so need to upgrade mysql assume that changing the version should be complicated process with changes in my cnf and may be with the changes in the code questions which version is the most stable and preferred which version should use
10950 usually our weekly full backups finish in about minutes with daily diff backups finishing in minutes since tuesday the dailies have taken almost hours to complete way more than should be required coincidentally this started happening right after we got new san disk config note that the server is running in production and we have no overall issues its running smoothly except for the io issue thats primarily manifested itself in the backup performance looking at dm exec requests during the backup the backup is constantly waiting on async io completion aha so we have disk contention however neither the mdf logs are stored on local disk nor backup drive have any activity iops we have plenty of memory disk queue length as well cpu hovers around no issue there either the san is dell md3220i the lun consisting of 6x10k sas drives the server is connected to the san through two physical paths each going through separate switch with redundant connections to the san total of four paths two of them being active at any time can verify that both connections are active through task manager splitting the load perfectly evenly both connections are running 1g full duplex we used to use jumbo frames but ive disabled them to rule out any issues here no change we have another server same os config r2 that is connected to other luns and it shows no issues it is however not running sql server but just sharing cifs on top of them however one of its luns preferred path is on the same san controller as the troublesome luns so ive ruled that out as well running couple of sqlio tests 10g test file seems to indicate that io is decent despite the issues sqlio kr t8 o8 s30 frandom b8 bn ls fparam txt ios sec mbs sec min latencyms avg latencyms max latencyms histogram ms sqlio kw t8 o8 s30 frandom b8 bn ls fparam txt ios sec mbs sec min latencyms avg latencyms max latencyms histogram ms sqlio kr t8 o8 s30 fsequential b64 bn ls fparam txt ios sec mbs sec min latencyms avg latencyms max latencyms histogram ms sqlio kw t8 o8 s30 fsequential b64 bn ls fparam txt ios sec mbs sec min latencyms avg latencyms max latencyms histogram ms realize that these arent exhaustive tests in any way but they do make me comfortable in knowing that it isnt complete rubbish note that the higher write performance is caused by the two active mpio paths whereas reading will only use one of them checking the application event log reveals events like these scattered around sql server has encountered occurrences of requests taking longer than seconds to complete on file xxx mdf in database xxx the os file handle is 0x0000000000003294 the offset of the latest long is 0x00000033da0000 theyre not constant but they do happen regularly couple per hour more during backups alongside that event the system event log will post these initiator sent task management command to reset the target the target name is given in the dump data target did not respond in time for scsi request the cdb is given in the dump data these also occur on the non problematic cifs server running on the same san controller and from my googling they seem to be non critical note that all servers use the same nics broadcom 5709cs with up to date drivers the servers themselves are dell r610s im not sure what to check for next any suggestions update running perfmon tried recording the avg disk sec read write perf counters while performing backup the backup starts out blazingly and then basically stops dead at crawling slowly towards but taking 20x the time it shouldve shows both san paths being utilized then dropping off backup initiated around notice all looking good and then theres series of peaks im not concerned with the writes only reads seem to hang note very little action on off though blazing performance at the very end note 12sec maximum though average is overall good update backing up to nul device to isolate read issues and simplify things ran the following backup database xxx to disk nul the results were exactly the same starts out with burst read and then stalls resuming operations now and then update io stalls ran the dm io virtual file stats query from jonathan kehayias and ted kruegers book page as recommended by shawn looking at the top files one data file each all results being data files it would seem like reads are worse than writes perhaps because writes go directly to the san cache whereas cold reads needs to hit disk just guess though update wait stats did three tests to gather some wait stats wait stats are queried using glenn berry paul randals script and just to confirm the backups are not being done to tape but to an iscsi lun results are similar if done to local disk with results similar to the nul backup cleared stats ran for minutes normal load cleared stats ran for minutes normal load normal backup running didnt complete cleared stats ran for minutes normal load nul backup running didnt complete update wtf broadcom based on mark storey smiths suggestions and kyle brandts previous experiences with broadcom nics decided to do some experimentation as weve got multiple active paths could relatively easily change the configuration of the nics one by one without causing any outages disabling toe and large send offload yielded near perfect run processed pages for database xxx file xxx on file processed pages for database xxx file xxx on file backup database successfully processed pages in seconds mb sec so which is the culprit toe or lso toe enabled lso disabled didnt finish the backup as it took forever just as the original problem toe disabled lso enabled looking good processed pages for database xxx file xxx on file processed pages for database xxx file xxx on file backup database successfully processed pages in seconds mb sec and as control disabled both toe and lso to confirm the issue was gone processed pages for database xxx file xxx on file processed pages for database xxx file xxx on file backup database successfully processed pages in seconds mb sec in conclusion it seems the enabled broadcom nics tcp offload engine caused the problems as soon as toe was disabled everything worked like charm guess wont be ordering any more broadcom nics going forward update down goes the cifs server today the identical and functioning cifs server started exhibiting io requests hanging this server wasnt running sql server just plain windows web server r2 serving shares over cifs as soon as disabled toe on it as well everything was back to running smooth just confirms wont ever be using toe on broadcom nics again if cant avoid the broadcom nics at all that is
10952 saw the above ansi warning message today when running colleagues script and dont know which of the many statements caused the warning to be shown in the past ive ignored it avoid nulls myself and so anything that would eliminate them is good thing in my book however today the word set literally shouted out at me and realised dont know what the meaning of the word is supposed to be in this context my first thought based on the fact it is upper case is that it is referring to the set keyword and means assignment as in update table set on delete set null set identity insert table on according to the sql server help the ansi warnings feature is based on iso ansi sql the spec for which makes just one use of the term set operation in subsection title hence in title case in the data assignment section however after quick googling of the error message see examples that are select queries with seemingly no assignment involved my second thought based on the wording of the sql server warning was that the mathematical meaning of set is implied however dont think that aggregation in sql is strictly speaking set operation even if the sql server team consider it to be set operation what is the purpose of putting the word set in capitals while googling noticed sql server error message table does not have the identity property cannot perform set operation the same words set operation in the same case here can only refer to the assignment of the identity insert property which brings me back to my first thought can anyone shed any light on the matter
10975 we currently use mixed mode for our sql server authentication ive tried to convince our dba to allow us to use windows authentication so we can use team foundation server however he absolutely refuses to allow us to have it accord to him we cannot have windows authentication since we are planning to become pci compliant eventually and pci requires mixed mode from what see online its the opposite the pci standard actually prefers windows authentication over mixed mode can someone give me some more information about this preferably url that states the correct information so can direct it to our department head
10990 often use ssms to test my slow stored procedures for missing indexes whenever see missing index impact xxx my kneejerk reaction is to just create the new index this results in faster query every time as far as can tell any reason why shouldnt continue to do this
10991 when testing stored procedures in ssms it sometimes says there is missing index on sometemptable 000000000000005b somefield etc etc when do add them like this to the sp create nonclustered index name of missing index sysname on dbo sometable somefield go never seem to see speed improvement so often do not add such indexes when should be adding such indexes
11005 is it possible to query the last successful backup date and perhaps the type of backup of each database in sql sever r2
11031 have added indexes to table which are used for searching result am showing results by asc or desc order so that column should have index or not have more indexes on that table how performance will affect by making or not making index to that column
11032 is there query that will do that found some queries that can do this for one table but wasnt able to modify it so can see tablename column type
11043 have large mysql db 150gb and only now ive noticed that the innodb file per table is set to off which causes the entire db to be hosted on one single file ibdata1 want to activate innodb file per table and have it retroactively split the db into several files whats the best way to do this
11046 is it possible to disable trigger momentarily but for just one table for example have table tablea with an on insert update and delete trigger also have table with the same triggers but they only affect certain columns in table now have an update query that uses both tables know the updates in table need to fire the triggers but the updates in table definitely do not need to fire the triggers so would like to disable those triggers until the updates are done is this possible im using mysql addendum here is trigger table essentially begin if old status and new status then if old geo lat is not null and old geo long is not null then delete from geo where datatype in and foreignid new id end if elseif old status and new status then if new geo lat is not null and new geo long is not null then insert into geo datatype foreignid long lat hostid morton status values ifnew grouptype new id new geo long new geo lat new hostid new status end if elseif new status then if old geo lat is not null and old geo long is not null and new geo lat is null or new geo long is null then delete from geo where datatype in and foreignid new id elseif old geo lat is null or old geo long is null and new geo lat is not null and new geo long is not null then insert into geo datatype foreignid longitude latitude hostid morton status values ifnew grouptype new id new geo long new geo lat new hostid new status elseif old geo lat new geo lat or old geo long new geo long or old status new status then update geo set lat new geo lat long new geo long status new status where datatype in and foreignid new id end if end if end here are the triggers on table essentially create trigger usergroups comments insert after insert on usergroups comment for each row begin call sp set comment countnew gid end here is the stored procedure thats fired from table delimiter create procedure sp set comment count in id int begin ac all count oldac old all count declare ac oldac int default select count as ac into ac from usergroups as ug left join usergroup comments as ugm on ugm gid ug id left join mediagallery as dm on ugm mid dm id where dm status not in and ug id id select allcount into oldac from usergroups where ug id id if oldac ac then update usergroups set allcount ac where usergroups id id end if end
11101 im mostly an application developer but find myself having to do all the up front database work for my current project btw its ms sql server as first decision im trying to figure out whether to divide my state using seperate databases or using seperate schemas in the same database ive done little reading on sql server schemas and it seems like natural way to seperate object domains which like but im not sure if there may be hidden costs to this pattern what are the more practical things should consider when selecting between these two approachs if avoid the dbo mytable in favor of myschema mytable will be creating other challenges or problems for my architecture as side note at some point this will be handed over to real dba to maintain support so im trying to make sure dont make their lives harder
11124 with sql server there are flagship editions enterprise edition business intelligence standard the full comparison between the three http www microsoft com sqlserver en us future editions sql2012 editions aspx the business intelligence edition implies that the purpose of it is for data warehousing and covers what seems to be key concerns for that self service business intelligence alerting power view powerpivot for sharepoint server advanced corporate bi tabular bi semantic model advanced analytics and reporting vertipaq in memory engine advanced data integration fuzzy grouping and lookup change data capture advanced data mining enterprise data management data quality services master data services however the enterprise edition is the only version that has data warehousing columnstore index compression partitioning what functionality does this entail that is seperated between the bi and enterprise editions
11233 have sql server r2 database in suspect mode tried to fix it running this query exec sp resetstatus yourdbname alter database yourdbname set emergency dbcc checkdb yourdbname alter database yourdbname set single user with rollback immediate dbcc checkdb yourdbname repair allow data loss alter database yourdbname set multi user but the repair output was this message warning you must recover this database prior to access msg level state line check terminated failure was detected while collecting facts possibly tempdb out of space or system table is inconsistent check previous errors warning the log for database servedb has been rebuilt transactional consistency has been lost the restore chain was broken and the server no longer has context on the previous log files so you will need to know what they were you should run dbcc checkdb to validate physical consistency the database has been put in dbo only mode when you are ready to make the database available for use you will need to reset database options and delete any extra log files msg level state line check terminated failure was detected while collecting facts possibly tempdb out of space or system table is inconsistent check previous errors how to repair the database fully this repair which made works only for some days then database again goes to suspect mode
11292 need to read some data from database given by its backup file so issue the restore database command which creates two files the database mdf file and the log ldf file is it possible to restore the database without creating the ldf file
11341 am defining schema for new set of resources using sql server in this case each record row will need to store xml fragments from time to time although not frequently ill need to query the xml to find element and attribute values if left to my own devises would tend to use the xml data type although ve been led to believe this is wrought with issues so that leads me to my questions given this scenario what factors should be considering when trying to decide between storing xml in an xml column vs varcharmax column if it helps here are some additional details no decision has been made regarding the use of schema for these fragments xsd sizes of the fragments will range from small to very large all xml will be well formed over the course of day there will be up to fragments collected with online query support needed for months queries against the xml will happen throughout the day but should remain light with few concurrent queries of this type
11396 have two purposes for this to have an offsite backup in case of region wide problem with amazon web services to copy production data from production billing account to beta billing account currently it does not appear that amazon supports either of these two use cases out of the box have seen mysqldump and xtrabackup see form post mentioned have also seen more complicated process documented here new rds server in the source billing account prod is spun up off recent backup new ec2 instance is spun up that has access to the rds server in step mysqldump is used to make backup of this database backup is copied to an offsite location s3 in separate account and or region fresh rds server is spun up database dump is imported tips and suggestions are appreciated
11405 given any version of oracle how do find my current scn what is the maximum possible scn
11430 have non clustered index with million leaf level rows currently it does not have any included columns would like to add one included column while the site is online will this significantly affect performance if do it through the ssms gui
11444 im setting up test environment for development and need to export some static data from tables in production ideally the exported product would take the form of script with all the required insert statements in doing so can reset the development environment quickly after each test can anyone tell how to accomplish this using sql server sql management studio and or visual studio note none of the tables have any foreign keys have full permissions in the source database and the number of rows per table is no more than few dozen
11475 given this declare type array is table of varchar22000 index by binary integer my array array count integer would like to do begin some code filling the my array array obviously count elements does not exists this is what im looking for count count elementsmy array dbms output put linemy array containts count elements end is there something better than creating procedure doing basic loop incrementing counter maybe pl sql native function already does this count elements
11527 the database our developers are working on is too large have lot of database objects we have to control db objects changes change management our company cannot have person who would be responsible for db changes only so we need source safe for database objects something like version control for standard code but more related to database that can synchronize database and scripts what is the best one reliable cheap functional choose the two ones
11531 im looking for table listing oracle and standard character sets for example oracle standard weiso98859p1 iso al32utf8 utf etc etc does this kind of resource exist couldnt find any
11631 my colleagues and are having discussion about the normalization of descriptive traits in the database such as status or type lets call the central table of the discussion order in my regular design approach would define another table orderstatus to describe the status of an order and then create foreign key with relationship orderstatusid on the order table this would give me referential integrity id be able to join the status at all times and my possible values are always present in the orderstatus table my colleague doesnt like this degree of normalization so hell instead define varchar field orderstatus on the order table this field would contain the values directly the possible values of status are defined in his application more specifically in an enum of orderstatuses and as such are not available to me unless have access to the source code of said application im used to having the entire context of the database exist in the database as relationships and tables and having to write where orderstatus sold as opposed to where orderstatusid bugs me what do think im looking for pros and cons against both approaches but im primarily concerned about performance and readability maintainability
11638 ive found that big nasty data extraction query that runs daily needs updated stats to avoid making horrible query plans based on incorrect rowcount estimates lets not worry about whether or not my stats should be updating automatically my question as noted in the title is should be concerned about incorrect query plans sticking around if query plan happens to be prepared before given set of statistics is updated at time when the desisions made by the optimizer turned out to be wrong or to stats updates automatically cause dependent query plans to be flushed if the plans stick around is there way to figure out which plans depend on given indexs statistics know could go digging in the dmv docs just hoping someone already has the answer
11657 have been asked to create something which tracks the daily cost to collect on accounts and am trying to figure out database table schema that would support this heres what know company has over million accounts of these they currently work an average of per month that changes with staffing levels which are currently low they have different cost types theyd like to track and they have warned that they might add more in the future they want the costs to be tracked daily costs are not split across the entire inventory they are either split across the of accounts that are worked per month or users can enter account identifiers to apply cost to group of accounts or they could simply specify which accounts to apply the cost to my first thought was normalized database accountid date costtypeid amount my issue with this is do the math this table is going to get huge quickly assuming all cost types get applied to all worked accounts for the current month thats 200k days in month which is somewhere around million records per month or close to billion records per year my second thought was to denormalize it bit accountid date totalcost costtype1 costtype2 costtype3 costtype4 costtype5 costtype6 costtype7 costtype8 costtype9 costtype10 costtype11 costtype12 costtype13 this method is more denormalized and can create up to million records per month 200k days in month or about 72million per year its lot less than the first method however if the company decides on new cost type in the future another database column will need to be added of the two methods which do you prefer why is there another alternative that you can think of which would handle this better am most interested in reporting performance both summerized and detailed reports the job that will spread the costs out over accounts will be run nightly when no one is around secondary concern is database size the existing database is already almost 300gb and believe the space on disk is around 500gb the database is sql server
11664 have table that holds from very basic info just title and few date fields theres one field called comments which is varchar4000 most of the time we leave it blank but some times will enter large amount of data here is this really bad design or is this just slightly inefficient would assume the creating separate table for this column would be better note this is sql server
11670 was working on new project which has the requirement to use databases arguing performance stability optimization is easier implemented while dont agree im having trouble collecting good arguments to use single database while the databases splits the tables in logical domains one argument have so far is data integrity cant use foreign keys between databases what are good pros cons for using single or multiple databases summary so far arguments losing data itegrity cant use foreign key over databases losing restore integrity gaining complexity db user roles small odds server database will go down solutions use schemas to seperate domains poc use dummy data to proof point in dbs execution plans
11697 which is faster select from inner join on record id forignkey notindexed notunique or select from inner join on forignkey notindexed notunique record id
11701 is there measure of time frame as to which the execution plan of inline queries will cached
11719 we have sensitive information names of people and were looking to put into place script to scrub of the names has anybody come up with good algorithm to do this something other than just making all of the last names test or something thanks
11758 ive been trying to figure this out for while lets say you have person for example an attribute of person is his or her social security number right but person also has social security number so in an er diagram you could draw square box for person and square box for ssid and you could connect them by diamond has alternatively you could draw circle ssid and you could connect it to square person box this isnt just true of ssid though its possible to draw either or with concrete things like car pet or phone number or with conceptual things like phone number friendship or mood so where do draw the lines is the object of design just to get everything down on paper and looking nice or should be using guide
11806 new centos installation was running an import of large db 2gb sql file and had problem the ssh client seemed to lose the connection and the import seemed to freeze used another window to login to mysql and the import appeared to be dead stuck on particular 3m row table so tried drop database huge db minutes later nothing in another window did etc init mysqld restart the drop db window messaged server shutdown then actually restarted the physical server logged back into mysql checked and the db was still there ran drop database huge db again and again im waiting already about minutes once again its fresh installation the huge db is the only db other than system dbs swear ive dropped dbs this large before and quickly but maybe im wrong ive successfully dropped the database it took something like minutes also note that think was mistaken when thought the mysqldump import was dead the terminal connection was lost but think the process was still running most likely killed the import mid table the 3m row table and probably of the way through the whole db it was misleading that top showed mysql using only of memory when it seemed like it should be using more dropping the db ended up taking min so again might not have had to restart the server and possibly could have just waited for the drop to finish but dont know how mysql would react to getting drop query for the same db that its importing via mysqldump still the question remains why does it take 30min to drop 2gb database when all it should have to do is delete all the db files and remove all references to the db from information schema whats the big deal
11893 need to remove database from postgresql db cluster how can do it even if there are active connections need sort of force flag that will drop all connections and then the db how can implement it im using dropdb currently but other tools are possible
11906 do you know what the best practices would be regarding ssas and having real time reports am not sure if we should be pulling data off our live database or using ssis to move data periodically to data warehouse database before re processing the cube any advice on what the best approach is would be greatly appreciated
11912 lets say have myisam table with data length of 8gb and an index length of 2gb so total data size of eleven gig how much memory would this require were to convert it to memory table gig or more
11925 table structure create table dbo order details2 orderid int not null productid int not null unitprice money not null default quantity smallint not null default discount real not null default constraint pk order details2 primary key clustered orderid asc productid asc with pad index off ignore dup key off on primary on primary table values orderid productid uniteprice quantity discount required output orderid productid uniteprice quantity discount my table has two primary keys want to get the unique record from this table the required output contain column orderid and max productid row just one row only need helping hand to solve this issue if have any query please ask
11956 so we have customer site that is complaining about some seriously slow performance took one look and its obvious that the problem is because somebody else grrrr designed table holding some million plus records without clustered index now want to create clustered index on that table but in my test environment my create index command has been running for an hour and its still not done the customer site is shop floor that works and cannot afford an hour of down time while create an index is there some less brute force method of creating the index that will either finish the job quickly or do it in some smart way that will not totally kill the servers performance while its busy we are using sql server enterprise edition
11962 im currently uploading one of wikipedias dump files it has 1gb compressed and 7gb uncompressed it has been importing it for hours already the size of the imported table in mysql is at the moment 5gbdata indexes will it continue to import until the size of the table will reach 7gb note the data directory of the database in on an external hdd considering this is only development so im not concerned by the real performance
12024 situation developer workstations that typically just use the client tools to connect to other non local sql servers however there are rare times that having the full version of sql server to do local development will be beneficial in an effort to not waste system resources running sql server all the time would like to know the best way to turn it off while still allowing usage of client tools management studio etc
12061 have set max connections to around in my cnf file max connections have upgraded to and now see the following line in the error log warning changed limits max open files max connections table cache why does mysql changed the max connections value to mysqladmin variables grep max connections max connections soft and hard open files restricted by os is ulimit sa grep open files open files ulimit ha grep open files open files the number of actually used max connections mysql show status like used connections variable name value max used connections
12080 so this is what have simple table classes to students class student math alice math bob math peter math anne music bob music chis music debbie theres classes every student attends classes bad analogy know for testing theres students in the db so there is rows but this db should handle several million students thats why im using mysql ndbcluster anyway query like this select student countclass as common classes from classes table where class in my subject list group by student order by common classes desc resulting in something like student commonclasses brad melissa chris bob takes about second with innodb engine on one server which is ok on ndbcluster with datanodes the same query takes up to seconds which is far too much dont know how the above statement is treated internally but guess that there is lot of communication between the nodes which makes it slow due to latency can someone tell me what happens in the cluster when perform this query how can make it faster note this is question that came up after posted this question https stackoverflow com questions how do compute ranking with mysql stored procedures for more informations have look there
12082 im migrating 15gb database from sql server to new server running sql server and along with that need to create all the new maintenance plans can take care of all the backup stuff but the table maintenance baffles me some does anyone have any input on how often should or how often you do would suffice too the following tasks check database integrity rebuild indexes reorganize indexes update statistics shrink database am missing anything again if you can share how often you do these tasks that would be great and or share any general information about your approach to table maintenance that would be helpful lastly does it matter what order run these tasks in when setting up job oh and im open to any links which might be of help
12150 have table in postgres database the table bloated to almost 25gb but after running vacuum full and cluster the table size was dramatically smaller well under 1gb few weeks later and its back up to 5gb and climbing this is not table that has frequent deletes so im at loss as to what is causing the bloat this only happens to single table have separate structurally identical database serving the same software the table in that database has not shown any bloat any ideas
12185 imagine stream of data that is bursty it could have events arrive very quickly followed by nothing for minute your expert advice how can write the insert code for sql server such that there is guarantee that sql caches everything immediately in its own ram without blocking my app for more than it takes to feed data into said ram to achieve this do you know of any patterns for setup of the sql server itself or patterns to set up the individual sql tables im writing to of course could do my own version which involves constructing my own queue in ram but dont want to reinvent the paleolithic stone axe so to speak
12250 how do you usually warm up your database run similar queries to this for every table in db select from ip log order by id select ip member id from ip log order by ip member id is that the best way or there is something cooler
12258 we run job tracking service as part of the company software reports are generated by sql server standard reporting services usually daily but often as and when needed before meetings etc as the jobs progress their status changes and the details of the reports change due to the legacy of the front end application and the database we do not store as many milestones as needed for managers to look back at historical reports managers quite often ask can we see where xyz was last week or things like let have look how project1 was performing last june but running the reports on the project1 database backup all show complete as it is now finished what is the best way to enable historical reporting at daily level with sql server standard ideally we like to keep months of different reports stored daily if possible
12271 ive read in forum that if your database capacity is more than 1g you should buy its license is this true how much will it cost
12337 we have the following setup multiple production database containing private data which is used by desktop software web database for public website which needs some data from the private databases an intermediary database which contains few views and stored procedures that pull data from the private databases currently the website logs in to web database and the web database connects to intermediary database to pull data or execute stored procedures on the production databases all the databases are on the same sql instance and the entire process uses the same user account the user account has full access to the web database and the intermediary database but can only access specific views and stored procedures of and private database is this really more secure than just making the public database connect directly to the private ones it seems like the intermediary database is only there to complicates things since the same login is used to access data in all the databases and it is already limited to just the views sps it needs in the private databases am hoping to remove it
12376 have an object called cot ntn pi was told this was synonym it doesnt appear in the all synonyms table it looks like view or table but cant find it in the all objects table can select from it but cant drop it as it doesnt exists and cant create new table with the same name as the name is already used by another object am going mad or doing something really stupid
12388 im having confusing problem after changing the computer name of remote server hosting local sql server instance basically remote server was moved from one site to another in order to facilitate this backed up and restored the old database to new database name clearing out the data so it could be used as fresh database for the client software also changed the computer name as we always do so to identify each server by its site number the database can be connected to by the client software just fine and can log in directly to sql server fine however one of my sql server agent jobs fails with an error in the event log sql server scheduled job nightly reset 0x4f76fdfff6dffe4ea0de4a70252ad3bd status failed invoked on message the job failed unable to determine if the owner site admin of job nightly reset has server access reason could not obtain information about windows nt group user site admin error code 0x534 sqlstate error now site is the old computer name which has been changed and the server has been reset connect manually using site the new site number and it shows me as being connected to the sql server with site admin however when look at the properties of the agent job it shows the owner as being site admin and when attempt to browse for users to change it site admin doesnt show up as an option only site admin if script out new job from this one and manually change the owner to site admin the new job is created with the owner site admin looking in sys servers or via sp helpserver only have one entry the current computer name however select servername returns the original development machine name two name changes ago in short cant run this important sql server agent job because it belongs to user that no longer exists and cant figure out how to change it or create it as the correct user
12437 im trying to use restore headeronly to get the date when the backup im about to restore was made the command restore headeronly from disk path to bak file works fine in query analyzer and gives resultset with something like columns the problem is actually accessing this from code can get this into temp table by declaring every single one of the ish columns inserting into it with exec and getting the value want from there the problem is that really want to avoid having to declare the entire resultset as temp table as it seems like very brittle solution if they ever add columns to it in future versions is there any way to just get single column out of this resultset without declaring all the columns
12453 have an net console app exe which connects to various sql databases using sql server authentication with appropriate credentials now there is change required in the exe to use windows authentication and use the network credentials to log in sql server all sql servers are on diffrent locations have the network credentials with me but am confused how to connect to remote sql server using these and retrive data from that please help
12474 are there any best practices for configuring maintenance plan in sql server currently im removing database backups and transaction logs greater than hours old then backing them up problem ive seen is that the transaction log is still very large should be including shrink database plan task
12512 have just added computed column to table and it is showing up as an int and id like to make it bit is this possible im having hard time finding any info on this below is the computed column isactive as case when datedecommissioned is null then else end persisted
12564 is there reason we can specify tablename with schema tablename instead of just schema tablename is it just to allow spaces in tablename
12569 im new here so be kind to me have the following scenario have many tables which for the sake of simplicity are represented in view in my mysql database my problem is that need value in this view representing if it is one kind of event or another simple boolean which tried to achieve with gu stoppinguniteventme ese monitoringelement as isstopingevent the result is represented as int and so is read by entity framework the problem is that really need boolean return value which tried to achieve with castgu stoppinguniteventme ese monitoringelement as boolean as isstopingevent this resulted in an error one that is not displayed to me in mysql workbench only receive that annoying you have an error in can you guys please help me out tried to solve it in my application but really preffer this solved in the database since it will be used by other software later
12580 imagine you have two different tables queries that are supposed to have return identical data you want to verify this whats an easy way to show any unmatched rows from each table just like the example below comparing every column assume there are columns in the tables many of which are nullable when there is no pk or there could be duplicates per pk joining on just pk columns isnt enough and it would be disaster to have to do full join with join conditions that properly handle nulls plus nasty where condition to exclude the matched rows usually it is when im writing new query against unscrubbed or not fully understood data that the problem is worst and the likelihood of pk being logically available is extremely low cook up two different ways to solve the problem and then compare their results the differences highlighting special cases in the data that was unaware of the result needs to look like this which col1 col2 col3 col30 tablea cat mismatch tableb cat mismatch tableb cat mismatch tablea cat no corresponding row tableb lizard null no corresponding row if col1 col2 do happen to be composite key and we order by them in our final result then we can easily see that and have one row different that should be the same and each has one row that is not in the other in the above example seeing the first row twice is not desirable heres ddl and dml to set up sample tables and data create table dbo tablea col1 varchar10 col2 int col3 int col4 varchar10 col5 varchar10 col6 varchar10 col7 varchar10 col8 varchar10 col9 varchar10 col10 varchar10 col11 varchar10 col12 varchar10 col13 varchar10 col14 varchar10 col15 varchar10 col16 varchar10 col17 varchar10 col18 varchar10 col19 varchar10 col20 varchar10 col21 varchar10 col22 varchar10 col23 varchar10 col24 varchar10 col25 varchar10 col26 varchar10 col27 varchar10 col28 varchar10 col29 varchar10 col30 varchar10 create table dbo tableb col1 varchar10 col2 int col3 int col4 varchar10 col5 varchar10 col6 varchar10 col7 varchar10 col8 varchar10 col9 varchar10 col10 varchar10 col11 varchar10 col12 varchar10 col13 varchar10 col14 varchar10 col15 varchar10 col16 varchar10 col17 varchar10 col18 varchar10 col19 varchar10 col20 varchar10 col21 varchar10 col22 varchar10 col23 varchar10 col24 varchar10 col25 varchar10 col26 varchar10 col27 varchar10 col28 varchar10 col29 varchar10 col30 varchar10 insert dbo tablea col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 col11 col12 col13 col14 col15 col16 col17 col18 col19 col20 col21 col22 col23 col24 col25 col26 col27 col28 col29 col30 values cat cat porcupine null tapir null null insert dbo tableb col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 col11 col12 col13 col14 col15 col16 col17 col18 col19 col20 col21 col22 col23 col24 col25 col26 col27 col28 col29 col30 values cat cat lizard null porcupine null tapir null null
12581 im using classic time based partitioning using triggers have found need for separate trigger which runs on the original table create table twitter interactions create or replace function insert twitter interactions create trigger insert twitter interactions trig before insert or update on twitter interactions for each row execute procedure insert twitter interactions create or replace function maintain data pointers create trigger maintain data pointers trig before insert or update on twitter interactions for each row execute procedure insert twitter interactions havent fully verified but suspect the partitioning logic runs before the maintain trigger and since the row doesnt end up in the parent table then the 2nd trigger never fires what happens if want to run an after insert or update as well since the row doesnt make it into the original table then im at loss to implement the after logic
12596 ms sql supports tvp useful feature for bulk uploading data to stored proceedure for processing rather than create user defined type is it possible to leverage an existing table definition for example is it possible to create stored proceedure with the following signature create procedure usp insertproductionlocation tvp locationtable readonly the documentation seems to suggest that this is not possible sample code sample code from http msdn microsoft com en us library bb510489 aspx use adventureworks2008r2 go create table type create type locationtabletype as table locationname varchar50 costrate int go create procedure to receive data for the table valued parameter create procedure usp insertproductionlocation tvp locationtabletype readonly as set nocount on insert into adventureworks2008r2 production location name costrate availability modifieddate select getdate from tvp go declare variable that references the type declare locationtvp as locationtabletype add data to the table variable insert into locationtvp locationname costrate select name from adventureworks2008r2 person stateprovince pass the table variable data to stored procedure exec usp insertproductionlocation locationtvp go the following is not part of the original source code create table locationtable locationname varchar50 costrate int go
12611 turned innodb flush log at trx commit and get very fast write speed but is it safe be used in production web site
12619 in oracles document the query optimizer under view merging found the following information the view merging optimization applies to views that contain only selections projections and joins that is mergeable views do not contain set operators aggregate functions distinct group by connect by and so on emphasis mine yet can only guess as what such projection actually refers to
12650 it is the case that identity insert can only be set to on in one database table at time but why since identity columns arent globally unique cant think of any dangerous situation that could be caused by inserting identities into more than one table at the same time at least not more dangerous than generally fudging with identity insert identity insert should rarely be used but what is the reason for the hard limit
12655 in sql server statistics are updated automatically when auto update statistics in true which is the default is there reason to update statistics manually and in what circumstances
12698 lets say you have the following code please ignore that its awful begin tran declare id int select id id from tablea update tablea set id id tablea must have only one row apparently commit tran id is returned to the client or used somewhere else to my eye this is not managing concurrency properly just because you have transaction doesnt mean someone else wont read the same value that you did before you get to your update statement now leaving the code as is realize this is better handled as single statement or even better using an autoincrement identity column what are sure ways to make it handle concurrency properly and prevent race conditions that allow two clients to get the same id value im pretty sure that adding with updlock holdlock to the select will do the trick the serializable transaction isolation level would seems to work as well since it denies anyone else to read what you did until the tran is over update this is false see martins answer is that true will they both work equally well is one preferred over the other imagine doing something more legitimate than an id update some calculation based on read that you need to update there could be many tables involved some of which youll write to and others you wont what is the best practice here having written this question think the lock hints are better because then you are only locking the tables you need but id appreciate anyones input and no dont know the best answer and really do want to get better understanding
12715 we would like to use two different tables one will hold an object when it is active and the other will hold the object once it becomesnon active the id is therefore unique per both tables combined how can we create constraint for the id such that the id appears only once for both tables we are using sql server
12739 many times need to write something like the following when dealing with sql server create table table name column1 int column2 varchar200 insert into table name execute some stored procedure but create table which has the exact syntax as the result of stored procedure is tedious task for example the result of sp helppublication has columns want to know whether there is any easy way to do this thanks
12769 would this ever make sense to have application enforce the database integrity instead of having foreign keys check constraints etc how much of performance improvement one can expect for not enforcing database integrity through internal database tools
12779 we have found handful of rows in our db that violate an active constraint how is this possible the constraint is active as we cant just manually add row that bypasses this constraint however when we run checkconstraintsfiles we find that it has been bypassed on handful of occasions during our test runs the rows in question were all created within half second of one another suggesting some kind of race condition heres the constraint being applied to the table the rule is meant to ensure name uniqueness in given parent folder alter table files add constraint uniquenameinparentfolder check checkuniquenameinfolderparentfoldersid name this constraint calls function that looks like this first check for the new name in the folders table if select count from folders where parentfoldersid foldersid and name name begin then check for it in the files table if select count from files where parentfoldersid foldersid and name name return end return individual rows are added inside transactions so im having hard time understanding how duplicate rows are sneaking past this constraint
12809 this question is about sql server index performance with varchar2000 as an include in covering index trying to improve performance in an slow and unstable database application in some cases data is accessed through large varchar strings with the queries including multple string operations like substring space and datalength here is simplified example of access update fattable set col3 substringcol3110 substringcol312datalengthcol3 from fattable where substringcol3101 and col2 the schema looks like this create table dbo fattable id bigint identity11 not null col1 nchar not null col2 int not null col3 varchar not null the following index has been defined with covering field on the large text column create nonclustered index indexcol2col3 on dbo fattable col2 asc include col3 from what ve read it is bad to put large data fields in an index ve been reading several articles including http msdn microsoft com en us library ms190806 aspx which discuss the impact of paging and disk size on index performance this being said the query plan definitely uses the covering index don have enough information to determine how much this is actually costing me in terms of system load do know that overall the system is performing poorly and am concerned that this is one of the issues questions is putting this varchar2000 column in the index include ever good idea since the include fields are stored in leaf nodes do they have much impact index performance update thanks for the excellent replies this is an unfair question in some ways as you guys say there is no absolute right answer without actual statistics and profiling like so many performance issue guess the answer is it depends
12810 as demonstrated by recent question of mine locking and concurrency are hard can you suggest any good resources for intermediate to advanced sql professionals to do thorough study on these that would properly learned enable better navigation of all the inherent pitfalls in this area im thinking of all kinds of resources tutorials blogs manual pages pass sessions or anything
12913 so let me preface by saying do not have total control over my db design so lot of the aspects of the current system cannot be changed for the purposes of this scenario comments about how we should rethink aspects of the design are likely correct but unhelpful have very large table approx fields wide and about 600m rows that drives large number of processes this is in data warehouse situation so we dont have any updates inserts outside the scheduled load process so it is heavily indexed decision has been made to try partitioning this table and have some concerns about indexing partitioned table dont have any experience with partitioning so any input or links are appreciated couldnt locate specifically what am after on bol or msdn currently we cluster on field that well call incidentkey which is varchar50 and not unique we could have between records with the same ik no comments please we do often get new data on old incidentkey records so its not sequential either understand need to include my partition field incidentdate in my clustered index key for the partition to work correctly im thinking it would be incidentkey incidentdate the question is how will the mechanics of clustered index work on part key in partitioned table if record in new partition should be before record in an old partition in the clustered index for example have records incidentkey date abc123 abc123 abc123 xyz999 xyz999 if get new record for abc123 it will need to be in the clustered index before xyz999 how does this work im assuming fragmentation and pointers but cant find any info on the physical storage and configuration of non partitioned clustered indexes on partitioned tables with dual part keys
12941 im using sql coalesce function where the first argument will not be null on about of the times it is ran if the first argument is null the second argument is quite lengthy process select coalescec firstname select top firstname from tablea join tableb on if for example firstname john would sql server still run the sub query know with the vb net iif function if the second argument is true the code still reads the third argument even though it wont be used
12977 is raid suitable for mysql installation let me explain my application further my application is socket programming which will connect with gps device to receive gps string and there after do further processing the socket programming will be in another server and db in another server so during the further processing is where it will query from the db so here guess there will lots of rite minimally during the further processing there will be minimum of select and the insert will be minimum but at times it can be even minimum or more and also number of updates hope am clearer now
12991 where can find ready to use database models dont need database with data in it but only schemas uml diagrams perhaps something like the data models at this link but much more complex and real world
13075 want to connect to an oracle database located on another host using sqlplus this page suggested adding an item on my tnsnames to conenct to that database local sid description address protocol tcphost hostname networkport connect data sid remote sid and then use that in sqlplus sqlplus user pass local sid however in my circumstances modifying the local tnsnames is not possible is it possible to connect to remote database just by using sqlplus argument without having to change tnsnames something like sqlplus user pass remote sid hostname network know this one is not valid
13083 was testing some stuff and added grant usage on statistics to cptnotsoawesome localhost identified by password so now when do show grants for cptnotsoawesome localhost can see that one of them is grants for cptnotsoawesome localhost grant usage on to cptnotsoawesome localhost identified by password somepew pewstring now want to remove it as think its security hazard so do the revoke usage on from cptnotsoawesome localhost identified by password flush privileges but it still shows that usage grant in the grant list grants for cptnotsoawesome localhost grant usage on to cptnotsoawesome localhost identified by password somepew pewstring any ideas why what am doing wrong
13093 ran into problem with our replication server essentially we have databases database1 and database2 master server has both slave has only database1 there is replicate do db database1 set in change master to configuration now what happened is we are using code igniter and one of the programers created database2 and started inserting info into it code igniter sets default database to database1 now the result is for every query he produced get an error on show slave status error table database2 tbl40 doesnt exist on query default database database1 query insert into database2 tbl40 date day values so essentially he fixed the problem afterwards but the replication doesnt work as there is around queries that will produce that error for replication server my question is is there some way to clear queries like that from the binlog or need to write script that will do set global sql slave skip counter for every query that produces and error
13112 what is the difference between common table expression cte and temp table and when should use one over the other cte with cte column1 column2 column3 as select column1 column2 column3 from sometable select from cte temp table select column1 column2 column3 into tmptable from sometable select from tmptable
13118 im seeing some behavior right now in sql server r2 and cant decide if the server is underperforming or not ive set up test to insert an integer into table as defined below times dbo test1 id int identity11 text varchar2000 timestamp datetime not null defaultgetdate both the application code below inserting the data and the sql server instance are on the same physical box the results the process is taking almost minutes to loop and insert the data not really sure how this can be possible for int using sqlconnection connection new sqlconnection server connection open using sqlcommand command new sqlcommand insert into dbo test1 text select connection command executenonquery connection close edit update to question as per aaronbertrands response how big is as specified its an integer in loop dont care about text that may or may not go in the text column what kind of drives its sata http www seagate com ww index jspvgnextoid 2ee06c02c732f110vgnvcm100000f5ee0a0arcrd is the database tiny yes this is possible issue is the database remote please read the question more closely everything is on the same physical box right in front of me same issues with two boxes across network though but im not trying to debug network issues right now just sql server insert issues good moneys on the autogrow issue right now edit update again is the machine vm no application connection method researching that since everythings on the same box im not sure how it connects triggers none resource governor not set up profiler the profiler doesnt show any delays whatsoever which is bugging the hell out of me blocking this is the only thing happening so there shouldnt be any blocking or waits perhaps excluding log waits on my box everythings on the same physical disk however big however on other boxes with the same problem this is not the case so will not pursue this as problem at this time
13167 weve been having number of issues with our indexes lately which our dba team has attributed to statistics not having been run recently this has made me wonder how can check if statistics have been recently updated via sql management studio apologize if this question isnt explaining this very well ive only been introduced to statistics until now and prior to this would look to indexes whenever ive had performance related issues edit im using the following but receiving syntax error use databasename exec sp autostats schema tablename the error im receiving is msg level state line incorrect syntax near why is this
13190 im using red gate sql compare to create release script based on differences between svn and database this results in script containing bunch of table and procedure changes and it works fine however one thing puzzles me its using transaction isolation level serializable know what it does to dml statements but im not sure what it means for ddl can someone enlighten me perhaps with an example
13243 its my understanding that sql server generates undo files when applying restore to database using the restore with standby from the msdn documentation emphasis mine the standby file is used to keep copy on write pre image for pages modified during the undo pass of restore with standby the standby file allows database to be brought up for read only access between transaction log restores and can be used with either warm standby server situations or special recovery situations in which it is useful to inspect the database between log restores after restore with standby operation the undo file is automatically deleted by the next restore operation if this standby file is manually deleted before the next restore operation then the entire database must be re restored while the database is in the standby state you should treat this standby file with the same care as any other database file unlike other database files this file is only kept open by the database engine during active restore operations the standby file name specifies standby file whose location is stored in the log of the database if an existing file is using the specified name the file is overwritten otherwise the database engine creates the file the size requirement of given standby file depends on the volume of undo actions resulting from uncommitted transactions during the restore operation what is the undo pass referred to at the beginning from what understand those are the operations written in the log file that havent been committed if this is correct why are those operations on the log file if they havent been committed in the first place and why does the restore with standby operation need to store them someplace to be able to bring the database up for read only access in other words why cant they just be thrown away
13349 lets assume that we have the following situation we have an oracle schema user user test with the create session privilege sys as sysdba creates couple tables on schema user test is it possible to forbid access for the user user test to tables on his schema
13455 all learned in school was sql which saves data to tables right now am working on project where data is stored in xml files additionally every xml contains reference to visual files jpeg the xml itself contains over one thousand coordinate points plus additional information on the data in my opinion it would make no sense to store this information in tables besides couldnt store jpeg files with sql either what would be appropriate solution or is there an error in reasoning on my side as you can see am pretty new to databases so any constructive suggestions links and advice is welcome
13468 ive read about different upsert implementations in postgresql but all of these solutions are relatively old or relatively exotic using writeable cte for example and im just not psql expert at all to find out immediately whether these solutions are old because they are well recommended or they are well almost all of them are just toy examples not appropriate to production use what is the most thread safe way to implement upsert in postgresql
13470 on my database structure in sql server have types of products which requires different information about the order so created one customers table and three different orders tables ordersforproductas ordersforproductbs ordersforproductcs all orders table has one to many relationship on customers table also have another table which is payments and will hold the payment details inside but have doubts here on how to structure it as have multiple product types and customer may have orders for multiple products at the same time need to relate those three order tables to payments table the other issue is that customer may have an order for only one type of product so the fk columns on payments table needs to be nullable my question is whether those nullable fk columns would be headache for me on the long run or not generally speaking would it be considered as bad practice to have nullable fk columns on table
13518 am running postgresql database server and psql terminal front end on ubuntu lucid lynx and would like to span single transaction over several sequential psql sessions when connect to my database with psql new connection is established and server backend process for this connection is created when disconnect the connection is released and the backend process terminates non xa transaction is bound to the scope of connection so obviously there is no straight forward way to span single transaction over several psql sessions what would like to achieve is that the following sequence of commands can be run within single transaction and therefore return the same transaction timestamp on each call of now tscho test sudo postgres psql no align tuples only select now tscho test sudo postgres psql no align tuples only select now database log cet log connection received host local cet log connection authorized user postgres database postgres cet log duration ms statement select now cet log disconnection session time user postgres database postgres host local cet log connection received host local cet log connection authorized user postgres database postgres cet log duration ms statement select now cet log disconnection session time user postgres database postgres host local clearly this is not what really want to do want to be able to execute several bash scripts that connect to the database and execute sql statements and scripts with psql within single transaction afaik the xa protocol would allow begin transaction and prepare transaction on different connections but postgresql does not support this my first shot to solve this problem was to setup the pgbouncer connection pool and configure it as simple proxy with exactly one connection to the target database session pooling mode my reasoning was that pgbouncer would establish this connection at start up and that can then connect disconnect to from the proxy with psql while the connection to the database keeps open tscho test sudo postgres psql pgproxy pgbouncer no align tuples only select now tscho test sudo postgres psql pgproxy pgbouncer no align tuples only select now this actually works out quite well as the database log shows cet log connection received host local cet log connection authorized user postgres database postgres cet log duration ms statement select cet log duration ms statement select now cet log duration ms statement discard all cet log duration ms statement select now cet log duration ms statement discard all but there is little problem with this approach as soon as begin transaction on the proxy connection and disconnect tscho test sudo postgres psql pgproxy pgbouncer no align tuples only start transaction start transaction the connection is released by pgbouncer cet log duration ms statement start transaction cet log disconnection session time user postgres database postgres host local of course this makes perfect sense for connection pool its job is to provide shared connections for several clients but to isolate the transactions of these clients but for my use case shared transaction is exactly what would need so my question is now is there way to configure pgbouncer or another connection pool to not release the connection upon disconnection after begin start transaction or is there another way to achieve what would like to do all further questions to this post comments and of course answers appreciated
13522 when creating stored procedure in sql server you are allowed to refer to tables which do not exist but if the table does exist then any column you refer to in the procedure must exist in that table deferred name resolution is it possible to instruct sql server to defer the name resolution of all tables referenced in procedure irrespective of whether they exist or not do want to keep the general syntax checking so even if it were possible hacking the stored procedure definition into system table isnt an option expect my asking to do this might seem little bit weird so heres some background auto generate table definitions and stored procedures from an application written in and its very difficult for me to change the code to order the changes as sql needs them my code guarantees that the schema is consistent within transaction but currently cant guarantee that the table columns are defined before define the stored procedure which references them below is canonical example of the sql created by the which illustrates the problem im trying to solve say this table already exists create table mytable nvarcharmax go my code creates something like this begin tran go the stored procedure gets generated first create procedure mysproc as begin select ab from mytable end then the table update alter table mytable add nvarcharmax commit tran it is possible for me to fix this in the code but im hoping for simple magic tweak can pull in the sql this will save lot of time for me
13523 we have database for product that is write heavy we just bought new server machine with ssd to help to our surprise the insertions were not faster than on our old machine with much slower storage during benchmarking we noticed that the io rate exhibited by the sql server process was very low for example ran the script found on this page except that added begin tran and commit around the loop at best could see the disk usage reach 7mb while cpu barely touched the server has 64gb installed and is using the total run time was minutes seconds for the first call down to around minute for subsequent calls the database is on simple recovery and was idle during the test dropped the table between each call why is such simple script so slow the hardware is barely being used at all both dedicated disk benchmarking tools and sqlio indicate that the ssd performs correctly with speeds upward of 500mb for both reading and writing understand that random writes are slower than sequential writes but would expect simple insert like this to table without clustered indexing to be much faster ultimately our scenario is much more complex but feel that need to understand simple case first in nutshell our application deletes old data then uses sqlbulkcopy to copy new data to staging tables performs some filtering and finally uses merge and or insert into depending on cases to copy the data to the final tables edit followed the procedure linked by martin smith and got the following result wait type wait count total wait ms resource wait ms signal wait ms network io logbuffer pagelatch up sos scheduler yield writelog pageiolatch up latch sh find it weird network io takes most of the time considering there are no result to display and no data to transfer anywhere other than to the sql files does the network io type includes all io edit created 20gb ram disk and mounted database from there the best time had on the ssd is 48s with the ram disk it went down to seconds network io is still the biggest wait the maximum write speed to the ram disk was about 250mb while its able to do multi gigabytes per second it still wasnt using much cpu so whats holding up sql
13600 am currently college student and am taking an introduction to oracle 10g cannot seem to make it to class all of the time and am curious to ask if there is any virtual oracle server or something along those lines that would allow me to have in nutshell virtual database in which can connect to via sql plus and enter commands that am learning am running windows home premium thank you so much
13650 does sql server execute queries in parallel in other words if run heavy query that takes seconds to execute and at the same time start another heavy query that takes seconds will the second query actually start after seconds or will they start both at the same time
13668 was poking around ssms and noticed the size of my int columns were bytes expected but was bit shocked to see my bit columns were whole byte did misunderstand what was looking at
13698 what is the difference between connection and session and how they are related
13703 have very simple mysql table where save highscores it looks like that id name score so far so good the question is how do get whats users rank for example have users name or id and want to get his rank where all rows are ordinal ordered descending for the score an example id name score ida boo lala bash assem in this very case assems rank would be because he got the 3rd highest score the query should return one row which contains only the required rank
13730 how should name my tables when creating new database singular client or plural clients
13742 have 3gb database that is constantly modified and need to make backups without stopping the server postgres my pg dump runs for minutes what if the data is modified during the process do get consistent backups dont want to find out when disaster strikes postgres documentation http www postgresql org docs static app pgdump html doesnt say anything about this
13744 am looking into various database types and dbmss for new project am wanting to start in the summer have built systems in mysql and postgresql now am wanting to expand my knowledge and experience in databases my project will be type of social network aggregate knowledge thing still havent developed term to describe it yet have been looking at cassandra use its own type of query language it seems to be good for feature rich content and delivering high performance query execution however am not too keen on it because it requires java environment to work on and would prefer to have nothing to do with oracle mongodb nosql type of dbms great scalability however you lose all the capabilities already available on the proven sql language like business information queries requirements of the system data text dates times xml small ints blob structure behavioir normalised 3nf non realtime relational scalable robust environment unix linux no java preferably run on was wondering if you could point me to any other database systems that should research into have also had look at object relational databases quite like the idea of them working with php objects pdos however their performance seems bit poor seeing as there will be dbas here any feedback on these systems that you have operated would be appreciated thanks
13757 can make sure my app is in consistent state can rollback all the uncompleted transactions if any just in case its ok can detach the database what do need the log file for after that im particularly talking about highly controlled environment so the real question is how can explicitly force everything to be alright in order to avoid possible data loss does anyone here have experience doing this kind of operation update the reason for this is that im not fan of immense log files the highly controlled environment is my pc running single app in single user mode im the developer of this app and have complete control over the code changes id prefer to delete rather than shrink so please do not suggest that simply shrink the file udate practice ive had this process in production for more than half year without any issue
13840 im trying to understand our software vendors decision to keep date and time in separate columns for example when the row was created or updated both time and date are datetime columns we are using sql server the database holds our erp systems data and believe the largest tables contain about million rows most of the tables are roughly between rows would personally by default choose single datetime for single timestamp this would allow easier time difference calculations and the date and time parts could be easily extracted from the timestamp it would also consume less space is separating date and time bad practice or is there something very brilliant in this design dont understand
13851 have basic knowledge of sql and sql server components my goal is to master my skills and learn everything about sql server to eventually become dba in the future would like to understand deep sql server internals how exactly everything works when and why could you please suggest me good place to start imho its just not possible by doing the programming work
13859 assuming have multiple relations in my database for example store employee and sale and want to connect pairs with simple binary relationship personally would create tables named employee store and employee sale with natural key composed of the foreign keys now my colleague insists on creating one table for multiple relationships for the above example there could be table called employeelinks employeelinks idlink int pk idemployee int fk null idstore int fk null idsale int fk null linktype int not null please help me with good reasons why this is not good idea have arguments of my own but would like to keep them private and hear your unbiased opinions edit initially the table above would have no primary key because the foreign keys allow null surrogate key is the only option
13882 background have network of approximately sensors each of which has about data points that we collect on minute intervals these data points are typically int values but some are strings and floats this data should be stored for days more if possible and still efficient database design when originally tasked with this project wrote app that wrote comma separated files for each sensor at the time there were not as many when someone wanted to look at trends we would open the csv in excel and graph it as needed things grew and we switched to mysql database created table for each sensor yes know lots of tables it has been working well but it has some limitations with so many tables it is obviously impossible to write query that will find data among all the sensors when looking for particular value for the next version switched to microsoft sql server express and put all sensor data into one large table this also works and lets us do queries to find values among all sensors that are of interest however ran into the 10gb limit for the express version and have decided to switch back to mysql rather than invest in sql server standard the question am happy with mysql performance and scalability but am uncertain if sticking to the all data in one table approach is best 10gb in single table seems to be asking for different design should mention that the need to query data for graphing is still there and im concerned that there will be performance issues for query that graphs for example temperature data for one sensor over the full days in other words the graph should be something that is quick to produce without waiting for sql to sort through piles of data just to isolate the sensor of interest should split this table up in some way to increase performance or is it not unusual to have such large table have indexes on the sensor id and timestamp columns which is pretty much the defining boundaries for any query get data for sensor from time to time ive read little bit about sharding and partitioning but dont feel those are appropriate in this case edit based on comments and answers so far some additional info may be helpful not indefinite storage currently do not store data past days daily run query that removes data older than days if it becomes important in the future will store more but for now it is sufficient this helps keep the size in check and performance higher engine type the original mysql implementation used myisam when creating the tables this time for the new implementation one data table instead of many theyve defaulted to innodb dont believe have requirement for one or the other normalization there are of course other tables besides the data collection table these support tables store things such as network information for the sensors login information for users etc there isnt much to normalize as far as know the reason the data table has so many columns is that there are that many variables from each sensor multiple temperatures light levels air pressure etc normalization to me means that there is no redundant data or repeating groups at least for 1nf for given sensor storing all values at particular time requires one row of data and there are no relationships involved there that see could break apart the table functionally making for example all temperature related values in one table and all air pressure related values in another while this might improve efficiency for someone making temperature only query still have to insert all of the data at once still the efficiency gain might be worthwhile for select operations obviously would be better off breaking apart the table vertically based on how often users request the data perhaps this is all should do suppose in asking my question am looking for confirmation that doing this would be worthwhile edit data usage ultimately much of the data is never looked at or needed because we typically focus only on items with problems but in attempting to find problems we use various tools to search the data and determine what items to zoom in on for example we noticed correlation between memory usage value customer specific proprietary software program and reboot crash one of the data points collect relates to this memory usage and was able to look at historical data to show that devices become unstable after particular memory usage is exceeded today for the subset of devices running this software check this value and issue reboot command if it is too high until this was discovered did not think collecting this data was of value for this reason ive maintained that the some data points be collected and stored even if the value is questionable but in normal day to day usage users typically examine perhaps dozen of these parameters if user becomes interested in particular geographic area he may using software generate graphs or spreadsheets of data for perhaps few dozen sensors its not uncommon to look at day graph with two or three plot lines showing such things as temperature air pressure and light levels doing this would run query similar to this select sensor id location data timestamp temp1 air1 light1 from data where data timestamp and sensor id in in the original mysql version where each sensor had its own table three separate queries would be issued but the results combined in software to create the graph because the data table contains so many rows million despite having indices on id and data timestamp the performance is notably worse than the multiple table scenario rows returned in seconds as opposed to less than one second with this example the ability to find which sensors meet certain criteria is practically zero in the multiple table schema and thus the reason for moving to single table this type of query can be done by multiple users in quick succession as they select different groups of data and compare the graphs from each result it can be quite frustrating to wait nearly seconds per graph or spreadsheet data is discarded after days it could be archived but it not currently requirement hopefully this information helps more adequately show how the data is used after collection and storage
13891 have set of scripts that need to be run in certain order would like to create master file that lists all of the other files and their correct order basically like an include file from or asp vbscript
13911 the tempdb of server sql server increases to 500gb several times every month is it possible to find out which sql statements caused this problem the problem is usually not caused by create table temp insert into temp or select into temp but complex joins the initial size of some of the tempdb files is also automatically set to much bigger values every time how to prevent it sometime the cached plans prevent resizing shrinking the files how to find which one hold the tempdb
13931 it is often repeated that the big data problem is that relational databases can not scale to process the massive volumes of data that are now being created but what are these scalability limitations that big data solutions like hadoop are not bound to why cant oracle rac or mysql sharding or mpp rdbms like teradata etc achieve these feats am interested in the technical limitations am aware that the financial costs of clustering rdbms can be prohibitive
14047 im creating database in which there will be around tables with every table containing tens of millions of rows and each table containing single important column and primary foreign key column in order to maximise query efficiency in the face of heavy updates and insertions and make heavy use of clustered indexes two of the tables will contain variable length textual data with one of them containing hundreds of millions of rows but the rest will contain only numeric data as really want to squeeze every last drop of performance out of the hardware have available about 64gb of ram very fast ssd and cores was thinking of allowing each table to have its own file so that no matter if im joining on or more tables each table will always be read using separate thread and the structure of each file will be closely aligned with the table contents which would hopefully minimise fragmentation and make it faster for sql server to add to the contents of any given table one caveat im stuck on sql server r2 web edition which means cant use automatic horizontal partitioning which rules that out as performance enhancement will using one file per table actually maximise performance or am overlooking built in sql server engine characteristics that would make doing so redundant second if using one file per table is advantageous why does create table only give me the option to allocate the table to file group and not to specific logical file this would require me to create separate file group for every file in my scenario which suggests to me that perhaps sql server isnt envisioning the advantages am assuming would come from doing what im proposing
14155 in the database structure of create table country name varchar40 not null primary key name engine innodb default charset utf8 create table city name varchar40 not null primary key name engine innodb default charset utf8 create table map country varchar40 not null city varchar100 not null primary key countrycity foreign key country references country name on delete cascade foreign key city references city name on delete restrict engine innodb default charset utf8 expect to delete parent from city by leaving the corresponding value in child intact by these three equal commands foreign key city references city name on delete no action foreign key city references city name on delete restrict foreign key city references city name but when using no action or restrict or omitting on delete mysql does not allow me to delete from parent column with this error error cannot delete or update parent row foreign key constraint fails test map constraint map ibfk foreign key city references cityname on delete restrict where am wrong isnt it the responsibility of the sqls no action to delete the parent and leave the child orphan
14172 am developing this application with an oracle database havent been documenting the data model but now an auditor wants to have an entity relationship diagram of my database there are many tables and creating the erd in visio manually is out of the question know toad has database export er diagram option but the result is not exactly presentable because of ugly layout and difficult to edit do you know an alternative way which can create presentable er diagram preferably it will be nice if the resulting diagram is editable and also if the tool is free
14246 have couple questions for those more familiar most of my instances have been running antelope despite having support for barracuda was looking to play around with some compresses innodb tables my understanding is this is only available under the barracuda format see innodb file format is dynamic so can just switch over with out bounce are there any implications of doing this should be aware of all can tell is that means new tables or subsequently altered will be created with that format is this all correct was hoping to have to not go through and convert all my tables is is kosher to have antelope and barracude tables coexisting in the same tablespace even if it works are there any gotchas to look out for from what ive read and gathered from my tests the answers are yes yes im not sure update ive been running some dynamic and some compressed tables in various instances since this post with out issue further neglected to read http dev mysql com doc refman en innodb file format identifying html at the time after you enable given innodb file format this change applies only to newly created tables rather than existing ones if you do create new table the tablespace containing the table is tagged with the earliest or simplest file format that is required for the tables features for example if you enable file format barracuda and create new table that is not compressed and does not use row format dynamic the new tablespace that contains the table is tagged as using file format antelope so tables will be created as antelope even if you allow barracuda the mixing is unavoidable unless you specify every table as row format dynamic or compressed table there is no indication you should do complete dump and reload when introducing your first barracuda table such as is recommended when upgrading major versions of mysql
14327 have installed oracle fdw successfully after create extension oracle fdw and foreign table it shows the following error is this server configuration problem env oracle postgresql create foreign server skytf create server oracle srv skytf foreign data wrapper oracle fdw skytf options dbserver manua create server skytf grant usage on foreign server oracle srv to skytf grant create mapping user skytf create user mapping for skytf skytf server oracle srv skytf options user read only password read only create user mapping create foreign table skytf create foreign table ft test skytf id integer skytf name character varying20 skytf server oracle srv skytf options schema ocp table test create foreign table skytf skytf skytf skytf select from ft test error error connecting to oracle ocienvcreate failed to create environment handle detail
14372 am optimizing some server application task that uses database querying complex calculations data insertions execution of this task spends about minutes have tested it more times and it predictable time to do it then executed script alter index all on dbo tablename rebuild for every table in database and what see now the time of execution my task is increased to minutes whats going on if these is not any external influences on this task was waiting for increasing of performance due rebuilding fragmented indexes but got degradation
14388 suppose we have table that has foreign key constraint to itself like such create table foo fooid bigint primary key parentfooid bigint foreign key parentfooid references foo fooid insert into foo fooid parentfooid values null update foo set parentfooid where fooid this table will have the following records fooid parentfooid there are cases where this kind of design could make sense the typical employee and boss employee relationship and in any case im in situation where have this in my schema this kind of design unfortunately allows for circularity in data records as shown in the example above my question then is is it possible to write constraint that checks this and is it feasible to write constraint that checks this if needed only to certain depth for part of this question it may be relevant to mention that expect only hundreds or perhaps in some cases thousands of records in my table normally not nested any deeper than about to levels ps ms sql server update march 14th there were several good answers ive now accepted the one that helped me understand mentioned possibility feasibility there are several other great answers though some with implementation suggestions as well so if you landed here with the same question have look at all answers
14402 need to keep track of deleted items for client synchronization needs in general is it better to add tombstone table and trigger that tracks when row was deleted from the server database basically adding new row to the tombstone table with the data from the deleted item or to keep the items in the original table and flag them as deleted typically with column of type bit to indicate that row is deleted and another column to track when the delete occurred
14418 is this normal to be exact it uses kb it occasionally shoots up to if query takes too long my total system memory is 3gb and am running winxp is there specific amount of ram recommended for sql servers the reason am asking this question could our database have problem if it uses this much ram we dont have dba here just programmer also does running multiple instances of sql servers affect performance does running multiple databases hit peformance we use only but there are databases running old ones am using sql server express it is not huge database just about records
14490 from msdn unlike derived table cte can be self referencing and can be referenced multiple times in the same query im using ctes quite lot but ive never thought deeply about the benefits of using them if reference cte multiple times in the same query is there any performance benefit if im doing self join will sql server scan the target tables twice
14511 see code from developers using implicit date conversion would like definitive answer to why they should not do this select from dba objects where created mar
14535 was requesting the conceptual schemas from government agencys information system for my research my request has been denied on the grounds of it being security risk dont really have extensive database experience so cant verify that claim is disclosing your schema really that big of security risk mean those are pretty abstract and divorced from the hardware and software implementations an explanation of how an attacker could exploit conceptual schemas would be appreciated thanks
14544 im confused about tablespaces in postgresql is it something like lvm mean when disk is getting full can we add another disk format and then create tablespace tblspace location media disk2 data is it enough or we should manually alter databases tables or indexes to take benefit of it
14565 query select distinct email from mybigtable where account id takes 1s query select count as total from mybigtable where account id and email in include all from above result takes 2s query select count as total from mybigtable where account id and email in select distinct email from mybigtable where account id takes minutes and its in the preparing state why does this take so much time table is innodb with 2mil rows on mysql
14570 we need to add priority column to table that gets hit about times second approx selects inserts and updates the column will be simple number1 the priority does not matter for the inserts or updates not part of the primary key which ill enforce separately we basically dont want to have to do an order by over range scan times second as the number executed will drop massively does an index organised table guarantee that priority will always come before priority when running the following query select from my table where rownum for slightly more context typical query would be select from my table where modto numberto chartstampss1 and done is null and country gbr and rownum the pk constraint for the iot would become priority rest of the pk with separate constraint on the pk solely for structure done is null in approximately of the table so this isnt very selective anyway main index used is think country done to numberto chartstampss we tested about combinations and this came up top by long way am completely unwilling to add any time at all to these queries 01s day added by select is minutes day wed much rather settle for good enough than perfection
14600 ive seen lot of material covering the business aspect of ssas but not really much about the important aspects of administration and management from the point of view of administering an instance of sql server analysis services what does working dba have to know about ssas to manage it correctly and efficiently
14661 have report that shows the count of events for the past hours grouped by the hour sounds easy enough but what am struggling with is how to include records that cover the gaps here is an example table event eventtime datetime eventtype int data looks like this need to create result set that has one record for every hour of the past hours regardless of there being events during that hour or not assuming the current time is the report would show roughly hour eventcount came up with solution that uses table that has one record for every hour of the day managed to get the results was looking for using union and some convoluted case logic in the where clause but was hoping somebody had more elegant solution
14730 my company is facing the decision whether to purchase sql server denali or sql server r2 for new database server am looking for objective reasons to choose one over the other our requirements standard edition for financial reasons and lack of need for enterprise features oltp workload this means we dont need the new windowing functions and column store indexes database size of gb no business intelligence features needed only the relational engine is required synchronous database mirroring currently the following reasons are known to me sql server denali newest version available sql server r2 proven technology cant seem to find lot of technical reasons to prefer one over the other basically it comes down to choosing proven technology that is running successfully vs the newest and greatest version available what are objective reasons to make the decision
14740 wrote script to reindex indexes in database here is one of them echo nreindex for unq vbvdata vehicle started at date log file psql username hostname dbname reindex index scm main unq vbvdata vehicle if eq then echo reindex for unq vbvdata vehicle finished at date log file else echo reindex for unq vbvdata vehicle failed log file exit fi the problem is can not run this script in standalone mode psql is prompting password every time it runs there is also two limitations can not create user on database with no password because reindex locks tables should use sleep num between each reindex is there any automatic solution
14757 where can learn more advanced sql programming to improve my ability in sql have reviewed the database adventureworks but need more
14774 in sql server why is tinyint stored with 9b in the row for some reason there seems to be an additional one byte at the end of the null bitmap mask use tempdb go create table tbl tinyint not null go insert into tbl values go dbcc ind tempdbtbl go dbcc traceon page dump will go the console go dbcc page tempdb11683 go results reversed the bytes due to dbcc pages showing the least significant byte first record size 9b taga 0x10 1b tagb 0x00 1b null bitmap offset 0x0005 2b our integer column 0x01 1b column count 0x0001 2b null bitmap 0x0000 2b what
14775 have core mysql server with gb of ram holding myisam tables total 4gb of data since the databases size is several times smaller than the available memory how can make full usage of my servers resources what settings should attempt configure in my cnf can force data indexes to stay in memory
14789 was researching something else when came across this thing was generating test tables with some data in it and running different queries to find out how different ways to write queries affects execution plan here is the script that used to generate random test data if exists select from sys objects where object id object idt and type in nu drop table go create table c1 int identity11 not null c2 int null go insert into select top from select t1 number t2 number newid from master spt values t1 cross join master spt values t2 where t1 type and t2 type order by go update set c2 null where c2 go create clustered index pk on c1 go create nonclustered index on c2 go now given this data invoked the following query select from where c2 or c2 is null to my great surprise the execution plan that was generated for this query was this sorry for the external link its too large to fit here can someone explain to me whats up with all these constant scans and compute scalars whats happening nested loopsinner join outer references expr1010 expr1011 expr1012 merge interval sorttop order by expr1013 desc expr1014 asc expr1010 asc expr1015 desc compute scalardefine expr1013 expr1012 and null expr1010 expr1014 expr1012 expr1015 expr1012 concatenation compute scalardefine expr1005 null expr1006 null expr1004 constant scan compute scalardefine expr1008 null expr1009 expr1007 constant scan index seekobject seek c2 expr1010 and c2 expr1011 ordered forward
14791 need to list columns from table in the table definition order select from syscolumns where id object idmytable order by colid by examining syscolumns tables two columns look relevant colid and colorder the msdn article on syscolumns says colid smallint column or parameter id colorder smallint identified for informational purposes only not supported future compatibility is not guaranteed tried to run select from syscolumns where colorder colid which yielded no rows and that makes me think that these columns has the same values most of the time it does look that the safest bet is to use colid however would be curious to know is there difference between these two columns and if there is what is this difference also the msdn article does not confirm that colid reflects the order of the table definition while this is reasonable to assume that this is the case could you please let me know if you are sure that its the case how you know that this is
14864 im trying to figure out an easy query can do to test if large table has list of entries that has at least one blank null empty value in any column need something like select from table as where anyt is null dont want to have to do select from table as where c1 null or c2 null or c3 null this would be huge query
14875 is it possible to set each database in mysql to use separate datadir im running userdir development sandbox server and would like to put the mysql data files for the databases for that user in their home user mysql directory linux ubuntu server mysql server version storage engine type innodb how would you do this
14989 if run the following code select policynumber maxdecpageid as decpageid risk from statriskdecpages where policynumber ar group by policynumber risk get the following results policynumber decpageid risk ar ar ar all really want to retrieve though is the policynumber and the maximum decpageid which in this case would be along with the risk numbers which should be and the query is also returning decpageid even though it is not the maximum decpageid for the policynumber because it has different risk the results would like returned are policynumber decpageid risk ar ar have figured out different queries can use to return my desired results but dont think they are the most efficient the queries came up with are select policynumber maxdecpageid as decpageid risk from statriskdecpages where policynumber ar and decpageid select maxdecpageid from statriskdecpages where policynumber ar this returns the desired results but dont want to have to specify the policy number more then once in the query is there way to call the policynumber into the sub query from the outer query the other query came up with was select t1 policynumbert2 decpageid t2 risk from select policynumber maxdecpageid as decpageid from statriskdecpages where policynumber ar group by policynumber as t1 left join statriskdecpages as t2 on t1 policynumber t2 policynumber and t1 decpageid t2 decpageid like this query because only have to specify the policynumber time and can also expand the query so can return the info for multiple policynumbers what need to know is if this is the most efficient way of writing the query it seems little redundant might be wrong but think there might be better more efficient way of writing the query any suggestions
14996 how do get list of all the partitioned tables in my database which system tables dmvs should be looking at
15063 have column with data type nvarcharmax and wold like to index this however since its too big its not possible so figured could create persisted computed column based on that column with the formula leftisnull fieldvalue however this column also gets data type of nvarcharmax so cant create an index for it is it possible to index it somehow without using full text index
15108 about once week have to resolve blocking chain on sql server database caused by long lived read lock from an access front end the lock is taken out whenever user opens certain form and is released once the user has finished scrolling through the form or closes it since many of our users open this form as reference these locks stay around for while any update to the table causes the blocking and suddenly nobody can select from this table since theyre all waiting on the first lock this is quite problem for us since lots of apps rely on this data understand that this locking behavior is part of how access works with linked tables ive been solving the problem from activity monitor by killing whichever select process is the head blocker whenever find out about it this is problem not only because it takes me time to do it manually but also because its reactive by the time hear about it its already been problem for lot of people id like to know if there is an automatic way to check for these long lasting blocking chains and either be emailed or have the problem resolved automatically the logic seems straightforward enough if any process matching this select query has been blocking for longer than minute notify me kill it but dont know how to implement this with sql server for what its worth think the proper solution is to fix or rewrite the app however due to departmental politics this is not an option for the next few months so im looking for stopgap
15186 what are audit tables how are they useful came across them reading this article
15199 im building an inventory database to store enterprise hardware information the devices the database keeps track of range from workstations laptops switches routers mobile phones etc im using device serial numbers as the primary key the problem im having is that the other attributes for these devices vary and dont want to have fields in the inventory table that are unrelated to other devices below is link to an erd of part of the database some fk relations are not shown im trying to set it up for example so device with workstation device type cant be put into the phones table this seems to require the use of lot of triggers to validate the device type or class and new tables anytime different device with different attributes will be tracked not to mention all of the one to one relationships which will make joins nightmare there are more one to one relationships not shown looked into setting up attribute tables that can be mapped to serial numbers but that would allow attributes that do not apply to device type to be assigned to device someone could assign phone number attribute to workstation if they wanted found an explanation on this site that gave the following structure this structure would work great if the attributes were all applicable to the items am storing for example if the database was storing only mobile phones the attributes could be things like touchscreen trackpad keyboard 4g 3g whatever in that case they all apply to phones my database would have attributes like hostname circuittype phonenumber which only apply to specific types of devices want to set it up so only the attributes that apply to given device type can be assigned to device of that type any suggestions on how to setup this database im not sure if this is proper use of one to one relationships or if there is better way to do this thank you in advance for taking the time to look into this here are some of the other threads read they gave me some good insight but dont think they really apply https stackoverflow com questions how to structure database for inventory of unlike items https stackoverflow com questions database structure for items with varying attributes https stackoverflow com questions product inventory with multiple attributes https stackoverflow com questions question about setting up inventory database https stackoverflow com questions how to best represent items with variable of attributes in database
15231 am working on back of the envelope calculation for 100tb reporting database setup am seeking out thoughts from the experts here proposed environment storage capacity 100tb tables sizes ranging from 1gb to 5tb mean size could lie between 100gb 200gb etl jobs may require join between tables of 10s of millions of rows with join keys ranging from bytes to bytes such joins should finish in under minutes live selects initially only interested in select speeds should support selects second updates second will be relatively much smaller number and can be ignored for this exercise need 24x7 availability independent db servers should be available to serve select calls with data replicated questions at present am looking at oracle how has your experience been with other commercial or opensource solutions for large databases what hardware os have you seen to work best am planning for linux on dell is network storage such as netapp must what issues do you foresee with using commercial off the shelf disks once the hardware and os are ready how much time would you set aside to setup configure db storage etc what team compositions worked best in the environments you have observed mean the various admins os admin oracle db admin required to manage and operate such setup how many of them might be needed to achieve 24x7 uptime any approximation range on db licenses network storage costs know dont have all the environment details am not looking for exact details an approximation is sufficient though some of the questions might be best answered by managers am interested in admins perspective appreciate your input
15241 question on non clustered index with included columns db ms sql server read blog optimized non clustered index maintenance which gives information on query plans when update statements is executed and clustered index and non clustered index are defined for table have question on non clustered index with included columns im referring same example provided by blogger create table pk int int int int int int create unique clustered index tpk on tpk create index tb on tb create index tcd on tcd create index te on te this is new non clustered index with included columns create index tf on te includea insert values0 update set if no index tf is defined then only update on clustered index will be performed and nonclustered index insert and delete operations will not be performed but what will happen when tf is defined
15250 want case sensitive search in sql query but by default mysql does not consider the case of the strings any idea on how to do case sensitive search in sql query
15276 in sql server have user in particular database and ive been asked to grant them access to all of the non system views of the database only believe this can be done by editing securables of type view and granting select on each one but there are many many views is there more efficient way to accomplish this
15312 ive used full outer joins before to get my desired results but maybe dont fully understand the concept because am not able to accomplish what should be simple join have tables which il call t1 and t2 with fields each t1 policy number premium t2 policy number loss what am trying to do is to get the sum of premium and sum of losses from both tables and also the policy number the code am using is select sumpremium prem sum sumloss loss sum t1 policynumber from t1 full outer join t2 on t1 policynumber t2 policynumber group by t1 policynumber the above code will return the correct sum totals but it will group all records where there isnt policy number match under null policy number would like my result to look like this policy number prem sum loss sum null null etc do not want result that shows null policy number as shown below since there is no such thing as null policy number this is just the total for when the policy number from both tables dont match policy number prem sum loss sum null null if select and group by t2 policy number instead of t1 policy number then get something like below as record policy number prem sum loss sum null null again dont mind seeing null under prem sum or under loss sum but dont want null under policy number would like my results to be something like policy number prem sum loss sum null null ect thought the full outer join would accomplish this but guess am missing something was thinking maybe could select and group by both t1 policy number and t2 policy number as sub query and then maybe do case in the outer query or something dont think it should be this complicated any ideas or advice
15326 created database schema in 11g express edition however there are some apex tables and some dummy data is it safe to delete them also would like to know what does apex means and why they have dollar sign thanks apex acl table apex ws files table apex ws history table apex ws links table apex ws notes table apex ws rows table apex ws tags table apex ws webpg sections table apex ws webpg section history table used sys to create new user the new user was created along with workspace and database schema please see the images above
15335 as explored the clustered tree index system of innodb think that the presence of many null or small columns has no significant effect on innodb performance does the presence of excess columns slow down mysql performance tried to practically test but no significant effect however think it should be compared under heavy load this is the reason that am curious to learn about technical reasoning on this matter
15342 have database called fdb have created this database on my pc with sql server express edition and added one table made backup of this database the sql server version on the server is copied this backup to the server and tried to restore it but get this message the media family on device program file microsoft sql server mssql mssql bacup fdb bak is incorrectly formatted sql server cannot process this media family restore headeronly is terminating abnormally microsoft sql server error
15350 using sql server and later want to add rowversion column to large table however when simply alter table tablename add rowversion rowversion not null then the table is unavailable for updates for too long what strategies can use to reduce this downtime ill consider anything the simpler the better of course but ill consider any strategy my thinking is that as last resort could maintain copy staging table maintained by triggers and then sp rename the staging table into the original table but im hoping for something simpler easier
15371 have schema with number of views need to check the execution plans to make sure the appropriate indexes are in place and being used how do do this id rather not have to copy and paste the output from show create view viewname into explain especially as some of the views are built on top of other views and this would be quite pain
15388 is it better to define foreign keys in the database or in the code part of an application
15403 consider we have large set of statistical data for record int columns is it better to keep the entire set in one table as they all belong to record or creating another table connected with one to one relationship the advantage of the former is to avoid join and have quick access to all statistical data for the corresponding record the advantage of the latter is to keep the column tidy the first column is read intensive and the second write intensive of course think it has no significant effect on the performance as use innodb with row level blocking in general want to know if it is practical useful to separate different sets of data for single record
15463 am trying to create the following table in mysql and is giving errors create table customer cus id int not null auto increment cus name varchar cus dob date cus addr varchar cus email varchar cus tel varchar cus pw varchar cus joindate datetime cus lastaccess date you have an error in your sql syntax check the manual that corresponds to your mysql server version for the right syntax to use near cus dob date cus addr varchar cus email varchar cus tel varchar cus pw at line
15506 ive got table that holds two types of notes collection notes and delivery notes they are identical data structures hence using the same table create table notes id int identity11 not null type int not null customerid int not null etc am migrating data from legacy system into this table and there is requirement that collection and delivery notes have their own sequential numbers had previously implemented two sequence tables as create table collectionnotesequence id int identity11 not null noteid int not null where the id column is the unique sequential id for collection notes and then the noteid foreign keys to notes id its getting towards time to do the final real data migration and this setup seems hard to work with is there way could bin the two sequence tables and add noteno field to the notes table such that noteno would be sequential depending on the note type is this composite key or something the new table might look like create table notes id int identity11 not null noteno int not null type int not null customerid int not null etc and the data would look like id noteno type customerid im using ms sql server
15530 if create foreign key constraint for table photos in phpmyadmin later see that the constraint is named photos ibfk and the next constraint is called photos ibfk etc from this have gathered that tablename ibfk constraintindex is the convention for db constraints in mysql is this correct what does ibfk stand for
15531 it was convenient that myisam used to store each table in corresponding file innodb has made advancements in many aspects but wonder why innodb stores all databases in one file ibdata1 by default understand that innodb will map the location of data in the file by individual index files for tables but do not understand why it mixes all data in one file and more importantly why mix the data of all databases on the server an interesting feature of myisam is that one can copy paste database folder to another machine and then use the database without dump
15572 what are the major differences between unique key and primary key in mysql
15583 in sql server am using rank over partition by col2 order by col3 desc to return data set with rank but have hundreds of records for each partition so will get values from rank but want only up to ranks in each partition example id name score subject joe math jim math tim math joe history jim history tim history joe geography tim geography jim geography want the result to be select subject name rank over partition by subject order by score desc from table subject name rank math joe math jim history jim history joe geography tim geography jim want only rank and in each category how do do this
15595 what is percona how does it differ from mysql when should we consider switching or upgrading from stock mysql to percona to add some specifics in our situation we almost exclusively use innodb which understand percona has done lot of optimizing for with extensive foreign key constraints and few stored procedures what we are finding at present is that mysql is poorly optimizing our queries and so any query that goes above joins we have to build explicitly with straight joins to improve performance
15632 suppose have table stuff with column city and no indices including that column populate the table then decide to create an index create index stuffoncityindex on dbo stuff city asc and the database confirms that create index succeeded will the newly created index immediately include all existing entries from the table or will it be slowly built in background in other words if think that creating an index will improve performance then will see that improvement immediately upon create index completion
15682 have number of clients with sql server and thats what have here on my server too use backup files to send databases back and forth between clients and at my office have read that when you create backup from sql server there is no way to restore it onto instance assumed that the compatibility level would take care of this problem but it doesnt therefore am at loss as to how to upgrade other than upgrade all my clients all at once which is impossible can think of no clean way to do this have the need to send database to client as well as receive database from client this is my first version upgrade on sql server so im new to this problem any ideas on how to proceed
15720 have postgres table with million rows ran the below update on it with stops as select id rank over order by offense timestamp defendant dl offense street number offense street name as stop from consistent master where citing jurisdiction update consistent master set arrest id stops stop from stops where master id stops id this query took hours to run am running this on physical core i7 q720 laptop processor plenty of ram nothing else running the vast majority of the time no hdd space constraints the table had recently been vacuumed analyzed and reindexed the whole time the query was running at least after the initial with completed cpu usage was usually low and the hdd was in use the hdd was being used so hard that any other app ran considerably more slowly than normal the laptops power setting were on high performance windows x64 heres the explain update on master cost rows width cte stops windowagg cost rows width sort cost rows width sort key consistent master offense timestamp consistent master defendant dl consistent master offense street number consistent master offense street name seq scan on master cost rows width filter citing jurisdiction hash join cost rows width hash cond stops id consistent master id cte scan on stops cost rows width hash cost rows width seq scan on master cost rows width citing jurisdiction only excludes few tens of thousands of rows even with that where clause im still operating on over million rows the hard drive is whole drive encrypted with truecrypt 1a that slows things down bit but not enough to cause query to take that many hours the with part only takes about minutes to run the arrest id field had no index for foreign key there are indexes and foreign keys on this table all other fields in the query are indexed the arrest id field had no constraints except not null the table has columns total arrest id is of type character varying20 realize rank produces numeric value but have to use character varying20 because have other rows where citing jurisdiction that use non numeric data for this field the arrest id field was blank for all rows with citing jurisdiction this is personal high end as of year ago laptop am the only user no other queries or operations were running locking seems unlikely there are no triggers anywhere in this table or anywhere else in the database other operations on this database never take an abornmal amount of time with proper indexing select queries are usually quite fast
15729 am using sqlite and need to store prices sqlites real data type says it uses floating point which is unacceptable storage for prices is there data type besides text that can use to store prices numerically so they sort correctly
15730 ive got an update trigger on table that watches for specific column changing from one specific value to any other value when this happens it updates some related data in another table via single update statement the first thing the trigger does is check to see if any updated rows had the value of this column changed from the value in question it simply joins inserted to deleted and compares the value in that column if nothing qualifies it bails out early so the update statement doesnt run if not exists select top custnmbr from inserted inner join deleted on custnmbr custnmbr where custclas misc and custclas misc return in this case custnmbr is the primary key of the underlying table if do large update on this table say rows this statement takes ages even if havent touched the custclas column can watch it stall on this statement for several minutes in profiler the execution plan is bizarre it shows an inserted scan with executions and million output rows that runs through filter on the custclas column it joins this via nested loop to deleted scan also filtered on custclas which executes only once and has output rows what idiotic thing am doing here to cause this note that the trigger absolutely must properly handle multi row updates edit also tried writing it like this in case exists was doing something unpleasant but its still just as terrible declare custnmbr varchar31 select top custnmbr custnmbr from inserted inner join deleted on custnmbr custnmbr where custclas misc and custclas misc if custnmbr is null return
15846 is there way to deny drop permissions for specific table from user or role
15859 we have payment table and agents get commission on payments commission is based on few different factors such as how long it took to get the payment so there is some calculations involved when figuring out the commission rate the agent gets but nothing obscenely complex for example it will probably never be more complex than this select payments amount case when datediffyear client received payments datepaid then rates rate1 when datediffyear client received payments datepaid then rates rate2 else rates rate3 end would it make sense to build 2nd table to hold this data instead of querying for it anytime its needed or should just stick with run time queries that pull the data whenever its requested and more importantly what are the factors to use when determining if query should be run anytime the data is needed or if the data should be stored in separate table of its own
15860 does anyone know if ms sql server text queries are optimised for upper or lower case strings read somewhere the it was uppercase but cant seem to find the reference to it again of course if this is entierly wrong and no optimisation takes place this would also be useful to know thanks for your help cm
15878 have table of producers and table of products both of which are of the form id int primary key name nvarchar producer can carry multiple products so was going to create table called producerdetails that would have producerid int foreign key to producers id productid int foreign key to products id then started to question myself so thought id ask the experts would it be better database design to have an additional id int primary key column in my producerdetails table or is that unnecessary im using sql server r2 if that make any difference at all edit the relationship between these tables would be many to many believe sorry didnt make that clear producer can carry multiple types of products and the same product could be produced by multiple different producers apologize if this question is overly simple referential integrity database design is not my strongsuit although im trying to improve that
15906 we have an application where just one of the table will be growing into million of lines but the rest will just below million so what is the advice should we go with innodb file per table or leave just as one ibd read some articles say do not go with it as you need more disk access when there joins to be performed we will have join between this table and others for reporting generation purposes
15994 thought about writing simple tail like utility to trace the progress of some figures within the database create or replace function tail return varchar2 tab pipelined as number begin loop exit when select count into from where pipe rowsysdate dbms lock sleep60 end loop return end tail and then id like to select from tabletail in sql plus in order to fetch the rows one by one set arraysize yet the records except the first one are fetched in pairs is there an explanation for this and how can get the records as soon as one is piped
16002 im doing the conceptual model for survey database the goal is store the answers given by users its going to be an android app have three entities user question and option question will have one or more options for example how many employees do you have options will have text and value the value selected by user user will select one or more of these options my conceptual design is dont know how to associate an answer with an user how can represent that relation do have another entity to represent option value this model will store questions and pre made answers offered answers and allows them to be re used in different surveys have to represent question like this one this question is related to this one survey database design first version are there errors
16127 need to manually retrieve few rows from sql server database backup normally create new database restore the database backup to the new database and run my queries the database is huge however so it takes forever to restore is there an easier way to get at the older data
16201 have been trying to figure out nice way of doing this for while now but have had hard time finding the right pieces to do this am guessing this must be possible to put it in simple terms here is what would like to accomplish php other front end socket locally hosted pooler pool of persistent tcp ip connections externally hosted mysqld does such tool way of doing things exist we would basically like to implement persistent mysql connections without using mysql pconnect respectfully ask that we not start to discuss about how persistent connections are not needed etc they are we are running out of time wait ports and are having other issues which would be solved if this type of system was implemented so yea to summarize we would to implement mysql connection pooler that is socket based on the local end and persists the connections that are made to lan externally hosted mysql server we do not use transactions or anything else that would be affected from the mysql connections being recycled we are running linux on the front end with master master percona cluster thanks
16208 there are many articles exaggerating imho of course the need for innodb file per table understand that with innodb file per table there should be better control over the individual tables like backup each table separately however the claim for better performance is questionable in my test there is no difference in performance of innodb file per table and ibdata1 for database of 60gb of course it was simple test with normal queries and the situation can be different for complicated queries in real life this is the reason that asked this question bit linux with ext4 can effectively handle large files with innodb file per table more disk operations are needed and this is significant in complicated joins and foreign key constraints tablespace is shared on single ibdata how dedicated tablespaces for separate tables can save disk space of course it is easier to free table space for each table with alter but it is still an expensive process with table lock question does innodb file per table has an effect on better performance of mysql if yes why
16211 am getting more and more interested by the nosql technology and can read several posts on se about how it works and the different products available however wonder if there are some canonical references books or articles which we can site in research paper for example and which we can read to have good overview of what the benefits disadvantages are how it works
16372 when creating tables from multiple joins for use in analysis when is it preferred to use views versus creating new table one reason that would prefer to use views is that the database schema has been developed by our administrator from within ruby and am not familiar with ruby can request that tables be created but requires an additional step and would like more flexibility when developing testing new joins started using views following the answer to related question on so when to use when to use sql the top voted answer begins do the data manipulations in sql until the data is in single table and then do the rest in have started using views but have run into few issues with views queries are much slower views do not get dumped from the production to backup database that use for analysis are views appropriate for this use if so should expect performance penalty is there way to speed up queries on views
16397 because of having some problems decided to re create all users except for root localhost this works fine but the newly created user has no right to do anything what want is to simply give all rights to root at some local ip as root localhost tried create user root grant all on to root the first command works the second one fails with the message error access denied for user root localhost dont get why root localhost cant do everything im sure didnt mess with its privileges from show grants for root localhost get grants for root localhost grant select insert update delete create drop reload shutdown process file references index alter show databases super create temporary tables replication slave replication client create user on to root localhost with grant option grant all privileges on to root localhost with grant option whatever this means am missing needed privilege can it be fixed im working with mysql ver distrib for debian linux gnu x86
16433 what does nvl stand for im talking about the oracle and informix perhaps some others too function used to filter out non null values from query results similar to coalesce in other databases
16436 alright feel like this is terrible idea but need some help understanding why this is bad im still working on implementing disaster recovery business continuity solution for our datacenter were running mssql enterprise and we plan on running passive instance in the cloud currently im suggesting log shipping but ive been asked to explore using rsync or something similar to push mdf ldf file deltas to the cloud instead of using the internal tools the goal would be to reduce our footprint and not run the cloud sql instance most of the time due to licensing issues our passive dr license is already in use in our current datacenter but if that center goes down the license becomes available ive found solution that uses vss to create and push deltas even if the files are locked but im wondering what sorts of issues could show up can get some insight we would be pushing deltas every minutes the database in question is roughly 2gb with maybe mb of logs generated in minute window
16461 have table that stores unix timestamp to query this as date im attempting to convert this timestamp to datetime type in view unfortunately can only seem to get the date portion out this link describes how to do the conversion but requires changing the nls date format to include the time portion the default currently only shows the date portion need the date and time unfortunately that solution only works at session level as developer id rather not have to go running off to our managed service provider to ask them to change the value at database level especially since it may impact other applications is there way can convert the timestamp in sql to datetime without modifying the system
16484 need to change about procedures and packages in the database due to migration that will accomplish this weekend we will do migration from one server to exadata however the database has been developed in very sloppy way the bank carries out number of text files written directly to disk but nobody uses directories in exadata the path to writing the files will be different due to the use of dbfs for that must change all calls via utl file let me give an example currently the code is this file utl file fopen file folder documents filename what want to do create directory create or replace directory directory name as file folder documents change the procedures for file utl file fopen directory name filename during migration only change the directory create or replace directory directory name as dbfs documents the real question is there way make search and replace changing all procedures in the database at once mean theres way to change all file folder documents to directory name
16493 besides using sql server profiler is there any way to track which stored procedures are being used or at least when they were last executed
16554 have following question what normal form does surrogate key violate my thought was the 3rd normal form but im not quite sure its just an assumption am making could someone explain that to me
16587 have an application that intermittently has break in the connection between it and the sql server instance it requires to function what is good way to go about troubleshooting this type of problem is there some kind of log where sql server logs something if it drops connection on purpose understand that just about anything in between the application and the database could be causing the connection to be lost dont think its network connectivity issue between the client and the server because this application is being served via citrix xenapp and all the other instances running on that same host are not having trouble at or around that point in time when saw the problem if it matters this is sql server enterprise edition my application is visual foxpro based application being served to client machines via citrix xenapp this problem is rare and intermittent times day over hundreds of clients also sql server and citrix are running on virtualized server infrastructure tl dr how do troubleshoot intermittent database connectivity loss update this is the message in the application error log when the problem happens connectivity error microsoft odbc sql server driver communication link failure the only time ive seen that message before is when sql server was completely overloaded and when the actual network connection itself was having problems disconnected dont think either of those two things are the case in this instance
16612 have the following table and index definitions create table munkalap munkalap id serial primary key create table munkalap lepes munkalap lepes id serial primary key munkalap id integer references munkalap munkalap id create index idx munkalap lepes munkalap id on munkalap lepes munkalap id why are none of the indexes on munkalap id used in the following query explain analyze select ml from munkalap join munkalap lepes ml using munkalap id query plan hash join cost rows width actual time rows loops hash cond ml munkalap id munkalap id seq scan on munkalap lepes ml cost rows width actual time rows loops hash cost rows width actual time rows loops buckets batches memory usage 115kb seq scan on munkalap cost rows width actual time rows loops total runtime ms its the same even if add filter explain analyze select ml from munkalap join munkalap lepes ml using munkalap id where not lezarva query plan hash join cost rows width actual time rows loops hash cond ml munkalap id munkalap id seq scan on munkalap lepes ml cost rows width actual time rows loops hash cost rows width actual time rows loops buckets batches memory usage 4kb seq scan on munkalap cost rows width actual time rows loops filter not lezarva total runtime ms
16616 was taught not to use the name id for the identity column of my tables but lately ive just been using it anyways because its simple short and very descriptive about what the data actually is ive seen people suggest prefixing id with the table name but this just seems to make more work for the person writing the sql queries or the programmer if youre using an orm like entity framework particularly on longer table names such as customerproductid or agencygroupassignementid one third party vendor we hired to create something for us actually named all their identity columns ident just to avoid using id at first thought they did that because id was keyword but when looked into it found that id isnt keyword in sql server which is what we are using so why do people recommend not using the name id for an identity column edit to clarify am not asking which naming convention to use or for arguments to use one naming convention over the other just want to know why its recommended to not use id for the identity column name im single programmer not dba and to me the database is just place to store my data since usually build small apps and typically use an orm for data access common field name for the identity field is much easier to work with want to know what am missing out on by doing this and if there are any really good reasons for me not to do this
16725 have been taught in my mssql classes this is how to join two tables select from firsttable join secondtable on id id now in my professional life came across join queries like this select from firsttable secondtable where id id know the second option was once norm but perhaps now abandoned find that in complex queries where join tables have number of sub queries the second form is lot easier to understand and is short and pretty questions which one should use is there an advantage of one over the other
16763 question have script with around thousand insert from select statements when try and run it get an error message stating that have run out of memory how can get this script to run context added some new data fields to make an app play nice with another app the client uses got spreadsheet of data from the client full of data that mapped current data items to values for these new fields converted spreadsheet to insert statements if only run some of the statements it works but the entire script does not no there are no typos if there is different way should be loading this data feel free to chastise me and let me know
16809 ive read an article that mentioned we can achieve inserts per second by using the load data in file statement that reads from csv files and inserts the data into database why should it differ from normal inserts edit reduced the round trip by calling just one insert statement insert into tblname values null2some text here0null2some text here1 null2some text here2null2some text here3 null2some text here3000 what about this
16846 am setting up mysql database for accounting software one of the fields is to save the currency used for each transaction what field type would you recommend details no more than different currencies will be used all currencies have three letter iso names the currency column will sometimes be used in an case statement and sometimes in where statement the table will be quite large so want to make the optimal choice storage size is not an issue speed and to lesser extend ease of use is am considering using char field and saving the currencies as usd cad etc using an enum field and defining the currencies as options using tinyint field and relating this to another database that holds the iso code for each currency using tinyint field and instead of relating database and having to do joins just save the list of very static and non changing currencies in php array saves me some joining but still allows me to use tinyint anybody any suggestions as to what would be best
16875 have previously saved copy of var lib mysql ddms directory ddms is the schema name now installed new mysql on freshly installed ubuntu lts by running apt get install mysql server believe version was installed after copy the ddms directory under var lib mysql some of its tables work fine these are the tables with an associated set of three files frm file myd file and myi file however there are two tables with different set of files frm file and ibd file these two tables didnt show up in the table list in phpmyadmin when look at the error log it says error cannot find or open table ddms dictionary item from the internal data dictionary of innodb though the frm file for the table exists maybe you have deleted and recreated innodb data files but have forgotten to delete the corresponding frm files of innodb tables or you have moved frm files to another database or the table contains indexes that this version of the engine doesnt support please help with restoring these two tables thanks
16884 im an oracle dba that also has sybase experience what are the major architectural and conceptual differences between the two rdbms platforms an answer similar to the sql server oracle question here would be of most use
16895 am writing stored procedure that takes database name as an argument and returns table of that databases indexes and their fragmentation level this stored procedure will live in our dba database the db that contains tables the dbas use for monitoring and optimizing things the systems in question are all sql server r2 if that makes difference have the basic query worked out but am stuck on trying to provide the indexes actual names to the best of my knowledge that information is contained in each individuals sys indexes view my specific problem is trying to reference that view programmatically from another databases stored procedure to illustrate this is the portion of the query at issue from sys dm db index physical stats db idnullnullnullnull inner join sys indexes on object id object id and index id index id and index id the query works fine when executed from the database identified by db id because it is using the proper sys indexes view if try to call this from the dba database however it all comes up null as the sys indexes view is for the wrong database in more general terms need to be able to do something like this declare db name nvarchar255 my database select from db name sys indexes or use db name have tried switching databases or referencing other databases using combinations of string concatenation and object name object id db id functions and nothing seems to work id appreciate any ideas the community might have but suspect will have to retool this stored procedure to reside in each individual database thanks in advance for any suggestions
16956 can anyone point me to good sql sniffer where can watch the sql commands that run against my database in real time if its free that would be great
16969 how cpu intensive is opening and closing of db connection for web app in mysql when the db software is on localhost when the db software is on another machine
16993 cant believe they make it this hard am at loss about how to view the data in my database is there an easy way to see what data is in my tables with pgadmin iii alternatively is there program that could use that does not suck
16999 not sure whether this is more or less appropriate place to ask this question originally posed at stack overflow in sql server have view over tables and that looks roughly like create view as select from union all select from reading from causes query to take intent shared locks on the base tables but also takes an intent shared lock on the view object itself it is clear why we need the is locks on the tables and we can see that the is lock on the view prevents concurrent modification to the tables underlying the view thats fine the query plan contains no mention of the view its completely compiled out and the resulting plan in this case is simple concatenation of rows from the two base tables indeed the only mention of the view in the query plan xml is in the statement text if you add second view over the tables reading from does not cause any lock to be taken on this rules out that the engine just takes an is lock on all views over and how does the database engine know to take lock on the view is the statement text parsed again is there some other channel of information between the query planner and underlying execution to pass this information if so what if the latter the details of the mechanism by which the storage engine knows to lock the view can fairly be considered internal however the fact that it does this is user visible and would expect it to be documented somewhere
17030 im trying to add column to database the query has been running for 25mins and its locking web access to the table and breaking our website alter table mytable add mynewcolumn varcharmax not null default the table contains binary data in different column and is quite large will cancelling by using the red cancel executing query button cause additional problems im just trying to figure out if should attempt to cancel the query at this point and what will happen since its been running for so long
17057 have one table that is taking up close to of hd space on our server have decided to drop few columns to free up space but need to return the space to the os the problem though is that im not sure what will happen if run vacuum full and there is not enough free space to make copy of the table understand that vacuum full should not be used but figured it was the best option in this scenario any ideas would be appreciated im using postgresql
17069 in sql server is it possible to have primary key on set of columns without either clustered or nonclustered indexes on the same set of columns am aware of the fact that primary key and clustered index key are separate concepts and that we can create primary key without clustered index on it see below alter table dbo sample add constraint pk sample seqguid col1 primary key nonclustered seqguid col1 but my question is to see if it is possible to create primary key on table without clustered or nonclustered index on it
17092 this is purely theoretical question lets say have an application deployed on multiple servers load balancer multiple scalable applications servers single database server for the moment on the two first parts do know what to look for but what about the database server what kind of hardware should look for is cpu frequency relevant for database server are multiple core cpus relevant is ram more important than cpu ps supposing the chosen database is mysql or postgresql
17103 is it possible to have sql server r2 standard automatically execute stored procedure in any given database that is restored or attachced to the instance ive got close to solution by creating server level trigger that executes stored procedure in given database after the ddl event create database or alter database is fired unfortunately this does not work for databack backup restores to elaborate we have clean up stored procedure that exists in every database that we restore and im looking for way to have this get executed automatically whenever backup is restored to the instance googling has pointed me to configuring either audits or policies in sql server to get this functionality but these these features are quite overwhelming at first glance so cant tell if audits or polices are the avenue to start investigating
17197 im working on an accounting system and for each transaction need to save if this is either debit or credit can think of two ways mysql database method amount decimal type enum debit credit method debit decimal credit in the first setup save the type of transaction but in the second way rather save the amounts in the debit or credit column pros of this method are that can more easily sum both debit and credit totals than in method but am wondering if there is common way to do this
17217 have child table that is something like this cust date table customer id some date balance would like to be able to get result set like this one record for each client with the latest date customer id some date balance know that can do this for each individual customer id with the following sql sql server syntax select top some date customer id balance from cust date table where customer id order by some date desc customer id some date balance but im not sure how to get all three of the records want im not sure if this is situation that calls for sub query or something else please note that the max date can be different for any given customer id in this example customer 3s maximum date is whereas the other records have max date of have tried select customer id max some date as latest date balance from cust date table group by customer id balance the problem is this doesnt return just the one row for each customer it returns multiple rows
17265 while connected to our production server sql server very powerful machine this select statement takes seconds spitting back all fields mb of data in total select top from person withnolock from any other box on the same network connecting using sql authentication or windows authentication the same query takes minute seconds am testing with this very simple statement to illustrate that its not an indexing problem or query related problem we have performance issues with all queries at the moment the rows come in chunks and not all at once get my first rows instantly and then wait for over minute for the batches of rows to come in here are the client statistics of the query when it is ran from the remote box query profile statistics number of insert delete and update statements rows affected by insert delete or update statements number of select statements rows returned by select statements number of transactions network statistics number of server roundtrips tds packets sent from client tds packets received from server bytes sent from client bytes received from server time statistics client processing time ms seconds total execution time ms wait time on server replies we can see that the client processing time is equal to the total execution time does anyone know what steps can take to diagnose why the transfer of the actual data is taking long time is there an sql configuration parameter that restricts or limits data transfer speed between machines
17267 need an aggregate function that mysql doesnt provide would like it to be in mysqls flavor of sql that is not in how do do this what im stuck on is creating an aggregate function the docs dont seem to mention how this is done examples of desired usage of product function mysql select productcol as from table row in set sec mysql select col productcol as from table group by col col rows in set sec
17277 know shrink is the devil it reverses page order and is responsible for skin cancer data fragmentation and global warming the list goes on that being said say have gb database and delete gb of data not on one table but general pruning of old data on database wide level covering of the tables does this constitute an appropriate use case for shrinking the database if not what are the appropriate steps to take to clean house after removing such high percentage of data from database can think of two rebuild indexes and update stats what else
17302 is anybody using hierarchyid in real production with tables of reasonable size more than few thousand rows is it reliable performant so far have not found anyone not affiliated with the vendor recommend it and paul nielsen advises against it here what is your experience with using hierarchyid in actual production systems which criteria have you used when you were choosing hierarchyid over its alternatives
17339 have copied database was working on so could change some key elements of its design for my new version want to delete totally some tables from the database however cannot due to some foreign key constraints how can view the foreign key constraints that exist on table how can delete the foreign keys and the table am viewing this doc for 2008r2 but am not understanding it sys foreign keys
17367 am importing gb foobar sql to restore table in local database mysql localhost root my data foobar sql mysql version usr local mysql bin mysql ver distrib for apple darwin9 i386 using readline how can monitor its progress
17398 have powerful machine with gb ram created one oracle instance with gb as sga target am not able to create another oracle instance with sga target 10g even when keep the first database down if set sga target 10g it gives below error on startup ora system defined limits for shared memory was misconfigured while free shows that there is enough memeory available though total used free shared buffers cached mem buffers cache swap do need to increase swap space any pointer in this regard is highly appreciated also for gb memory for creating many instances what would be best value for swap space is there some way to caluculate this my objective is to have at least two instances each with sga target 20g and will keep only one instance up at time if am missing any concept here output of ipcs im is as below shared memory limits max number of segments max seg size kbytes max total shared memory kbytes min seg size bytes
17431 how can myisam be faster than innodb if myisam needs to do disk reads for the data innodb uses the buffer pool for indexes and data and myisam just for the index
17439 have project that could benefit from using database but have no experience with databases dont have access to server and have relatively little experience working with things living server side if im going to have to tackle learning curve id prefer to learn something with broad applicability such as sql but would settle for learning something like access if it is sufficiently powerful for the task im currently trying to tackle of course id also rather not drop on access if it can be helped since im just tinkering ive downloaded libreoffice base as well as something called sqlitebrowser but wanted to check first before invest time learning those particular applications and their flavors of sql whether those tools will be sufficient for what want to do want to be able to import data from csv or from excel run queries that equate to select where this is that and this contains that and any of these contain that write new field which indicates those results which match given query again im willing to learn but it would be nice not to have to learn bunch of intermediate stuff about it before can focus on learning databases and if necessary the particulars of given application
17529 have mysql servers say abc what want to do is that want to make all of them master as well as slave if there is update on any of the mysql servers it should be replicated to all of the servers have studied about the circular replication and found it can be implemented with it can anybody please give me all the steps to accomplish the replication as stated between three servers one more point want to ignore some of the tables from database also also what are caveats of this type of replication
17533 wonder to know if it is necessary to write commit after insert delete update in function procedure example create or replace function test fun return number is begin delete from return end or procedure create or replace procedure aud clear pro as begin delete from end does it need commit after delete cannot understand the following situation if call the function procedure from sql window then it requires commit but if schedule function procedure using dbms scheduler and run the job delete statement is automatically committed why
17615 suppose have parent table parent referenced by child table child the table parent is populated but child is not attempting to truncate parent results in ora unique primary keys in table referenced by foreign keys is there way to hint to the dbms that child is empty so that the foreign key constraint doesnt need to be disabled
17653 want to update tables my be having 10s of millions of records each the problem is that it is taking too much time for the update process and also at that time cpu usage also goes very high want to do in such way that it can not use much cpu while processing the data if the processing time is increased then it will not be problem for me but it should use limited cpu resources for processing updating the table am using postgresql as database and server operating system is linux sample query of mine can be like this update temp set customername select customername from user where user customerid temp customerid
17691 was just checking out the propaganda page for postgresql and found this little piece of art it is the turtle style postgresql logo how old is this and did postgresql ever officially use turtle instead of an elephant what is the story here
17711 need to store bit array for each record of table supporting the following operations testing if bit is set and setting bit using sql querying and setting the value using ado not ado net indexing in order to benefit from the covering index feature the maximum number of bits to be stored in this array is fixed but may exceed that is simple int column doesnt always work from what ive seen so far my options are use several int columns use bigint works as long as the number of bits is use binary the first option would work but require quite bit of refactoring in the code that accesses the data the second option is temporary relief only and from my searches so far im not too sure if ado works that well with bigint have no experience with binary and im not aware of any other options which data type would you choose given the requirements
17761 take look at the following sqlfiddle http sqlfiddle com dacb5 create table contacts id int auto increment primary key name varchar20 network id int network contact id int insert into contacts name network id network contact id values john alex bob jeff bill walter jessie have basic table of contacts the network id and network contact id fields contain id numbers that link to other tables want to be able to run insert ignore queries to this table but want to use the combination of the network id and network contact id as the unique key to match against so for example if tried to insert contact that had network id and network contact id the insert ignore query would see that entry already exists and ignore any error that was thrown so basically network id is not unique network contact id is not unique but the combination of the two is unique how do set this up would have to have single other field that is the concatenated values of the two other fields or is there way to setup the keys for this table so it will do what need
17790 must be missing something with regards to setting up postgresql what id like to do is create multiple databases and users that are isolated from each other so that specific user only has access to the databases specify however from what can determine any created user has access to all databases without any specific grants being given here is what do on an ubuntu server apt get install postgresql sudo postgres createuser drsp mike1 specifying the password for the new user sudo postgres createdb data1 psql localhost mike1 data1 specifying the password for the user mike1 to login it seems that new user mike1 has no problem connecting to database data1 and creating tables etc and this without running any grant command at all and the owner of data1 is postgres since didnt specify an owner in step is this really how it is supposed to work what id like to do is grant mike1 full access to data1 and then repeat this for more users and databases making sure that the users only have access to one or possibly several databases of my choice
17808 there are two tables deal and dealcategories one deal can have many deal categories so the proper way should be to make table called dealcategories with the following structure dealcategoryid pk dealid fk dealcategoryid fk however our outsource team stored the multiple categories in the deal table this way dealid pk dealcategory in here they store multiple deal ids separated by commas like this feel that what they did is wrong but dont know how to clearly explain why this is not right how should explain to them that this is wrong or maybe im the one whos wrong and this is acceptable
17853 inspired by django modeling question database modeling with multiple many to many relations in django the db design is something like create table book bookid int not null booktitle varchar200 not null primary key bookid create table tag tagid int not null tagname varchar50 not null primary key tagid create table booktag bookid int not null tagid int not null primary key bookid tagid foreign key bookid references book bookid foreign key tagid references tag tagid create table aspect aspectid int not null aspectname varchar50 not null primary key aspectid create table tagaspect tagid int not null aspectid int not null primary key tagid aspectid foreign key tagid references tag tagid foreign key aspectid references aspect aspectid and the issue is how to define the bookaspectrating table and to enforce referential integrity so one cannot add rating for book aspect combination that is invalid afaik complex check constraints or assertions that involve subqueries and more than one table that could possibly solve this are not available in any dbms another idea is to use pseudocode view create view bookaspect view as select distinct bt bookid ta aspectid from booktag as bt join tag as on tagid bt tagid join tagaspect as ta on ta tagid bt tagid with primary key bookid aspectid and table that has foreign key to the above view create table bookaspectrating bookid int not null aspectid int not null personid int not null rating int not null primary key bookid aspectid personid foreign key personid references person personid foreign key bookid aspectid references bookaspect view bookid aspectid three questions are there dbms that allow possibly materialized view with primary key are there dbms that allow foreign key that references view and not only base table could this integrity problem be solved otherwise with available dbms features clarification since there is probably no satisfying solution and the django question is not even mine im more interested in general strategy of possible attack on the problem not detailed solution so an answer like in dbms this can be done with triggers on table is perfectly acceptable
17893 given that the optimizer cannot take all the time it needs it has to minimize the execution time and not contribute to it to explore all possible execution plans it sometimes get cut off was wondering if this can be overridden so that you can give the optimizer all the time in needs or certain amount of milliseconds dont have need for this atm but can imagine scenario where complex query is executed in tight loop and you want to come up with the optimal plan and cache it before hand of course it you have tight loop you should rewrite the query so it goes away but bear with me this is more question out of curiosity and also to see if there is sometimes difference between short circuited optimization and full one it turns out that you can give the optimizer more time with trace flag its not exactly what was asking but it comes close the best information found on this is in query processor modelling extensions in sql server sp1 by ian jose use this trace flag with care but it can be useful when coming up with better plans see also articles tagged optimization level by grant fritchey before you upgrade to sql server by brent ozar tuning options for sql server when running in high performance workloads by microsoft support was thinking about queries with lots of joins where the solution space for join order explodes exponentially the heuristics that sql server uses are pretty good but was wondering if the optimizer would propose different order if it had more time in the range of seconds or even minutes
17904 in sql server have varchar column with value such as may may may may may how do convert this column to datetime column with real datetimes if this is not possible how do just create new column and copy this values into it but as datetime values
17921 ive got some customer comments split out into multiple rows due to database design and for report need to combine the comments from each unique id into one row previously tried something working with this delimited list from select clause and coalesce trick but cant recall it and must not have saved it cant seem to get it to work in this case either only seems to work on single row the data looks like this id row num customer code comments dilbert hard dilbert worker wally lazy my results need to look like this id customer code comments dilbert hard worker wally lazy so for each row num theres really only one row of results the comments should be combined in the order of row num the above linked select trick works to get all the values for specific query as one row but cant figure out how to make it work as part of select statement that spits all these rows out my query has to go through the whole table on its own and output these rows im not combining them into multiple columns one for each row so pivot doesnt seem applicable
17926 everyone knows that in tables that use innodb as engine queries like select count from mytable are very inexact and very slow especially when the table gets bigger and there are constant row insertions deletions while that query executes as understood it innodb doesnt store the row count in an internal variable which is the reason for this problem my question is why is this so would it be so hard to store such information its an important information to know in so many situations the only difficulty see if such an internal count would be implemented is when transactions are involved if the transaction is uncommitted do you count the rows inserted by it or not ps im not an expert on dbs im just someone who has mysql as simple hobby so if just asked something stupid dont be excessively critical
18024 have large in the tens of millions of records database that am going to perform full database backup on however the database is large enough that transactions can start before and during as well as commit during and after the backup takes place for example t0 transaction start t1 full database backup start t2 transaction start will not deadlock with t3 transaction commit rollback does not matter does it t4 full database backup end t5 transaction commit rollback again does not matter does it t0 t1 t2 t3 t4 t6 my understanding is that no locks are used during backup although other performance problems may arise due to say high but im not sure what can guarantee what will be committed or not also my concern isnt that the database will be in an inconsistent state but rather what that state will be even if its not deterministic if theres set of rules that can be consistently applied and how it got there for example how much of the data file is used along with the transaction log to create backup file
18042 in my sql server database have datetime column what is good way to create new column that represents the long value for the datetime column the long would represent number of seconds thought if can convert it to longs it would make it easier to do group by queries over time periods as could just divide the long number by fixed amounts the table is static wont be updating or deleting data
18059 using some methods when you create copy of table you lose indexes pk fk etc for example in sql server can say select into dbo table2 from dbo table1 this is just simple copy of the table all of the indexes constraints are missing how can copy table structure without using backup am primarily looking to do this manually but if thats not possible ill accept any solution
18208 over the past few weeks ive been raging against an old firebird database this database is crappy for all sorts of reasons but one thing noticed was that every single field of every single table has two indexes each one with single segment one in asc order and one in desc order apart from the wtfness of having an index for every field in every table it got me thinking is there any advantage for single segment indexes to having two indexes with the same index segments but one in desc and one in asc is there anything to be gained or would modern dbms simple use the asc index and start from the end and work its way backwards if required
18215 added new column in the table this column cn has to be unique and mandatory but old data dont have any value how to update the existing records with sequecely or random unique data thank you
18239 is there better way to rewrite select clause where multiple columns use the same case when conditions so that the conditions are only checked once see the example below select case teststatus when then authorized when then completed when then in progress when then cancelled end as status case teststatus when then authtime when then cmpltime when then strttime when then canctime end as lasteventtime case teststatus when then authby when then cmplby when then strtby when then cancby end as lasteventuser from test in non sql psuedo code the code might look like case teststatus when statuscol authorized lasteventtimecol authtime lasteventusercol authuser when statuscol completed lasteventtimecol cmpltime lasteventusercol cmpluser end note am aware of the obvious normalization issues implied by the query only wanted to demonstrate the issue
18300 asking this question specifically for postgres as it has good supoort for tree spatial indexes we have the following table with tree structure nested set model of words and their frequencies lexikon id integer primary key word text frequency integer lset integer unique key rset integer unique key and the query select word from lexikon where lset between low and high order by frequency desc limit suppose covering index on lset frequency word would be useful but feel it may not perform well if there are too many lset values in the high low range simple index on frequency desc may also be sufficient sometimes when search using that index yields early the rows that match the range condition but it seems that performance depends lot on the parameter values is there way to make it perform fast regardless of whether the range low high is wide or narrow and regardless of whether the top frequency words are luckily in the narrow selected range would an tree spatial index help adding indexes rewriting the query re designing the table there is no limitation
18315 context we are developing system with large ish database in the bottom it is an ms sql database running on sql server r2 the total size of the database is about gb out of these approximately gb is in single table binarycontent as the name suggests this is table where we store simple files of any kind directly in the table as blob recently weve been testing the possibility to move all these files out of the database to the file system using filestream we did the necessary modifications to our database without any problems and our system is still working fine after the migration the binarycontent table looks roughly like this create table dbo binarycontent binarycontentid int identity11 not null filename varchar not null binarycontentrowguid uniqueidentifier rowguidcol not null on primary filestream on filestreamcontentfg alter table dbo binarycontent add filecontentbinary varbinary max filestream null alter table dbo binarycontent add constraint dfbinarycontentrowguid default newsequentialid for binarycontentrowguid with everything residing in the primary file group except the field filebinarycontent which is in separate file group filestreamcontentfg scenario from developers point of view we would often like fresh copy of the database from our production environment to be able to work the the latest data in those cases we are rarely interested in the files stored in binarycontent now using filestream we have this almost working as wed like we back up the database without the file stream like this backup database filestreamdb filegroup primary to disk backup filestreamdb withoutfs bak with init and restore it like this restore database filestreamdb from disk backup filestreamdb withoutfs bak this seems to be working ok and our system works as long as we avoid the parts that use the filebinarycontent field we can for instance run the following query without problem select top binarycontentid filename binarycontentrowguid filecontentbinary from dbo binarycontent naturally if un comment the line above including filecontentbinary in the query get an error large object lob data for table dbo binarycontent resides on an offline filegroup filestreamcontentfg that cannot be accessed our system handles files where the content is set to null so what would like to do is something like this update dbo binarycontent set filecontentbinary null but this of course gives me the same error as above at this point im stuck question is there any way can restore the database without having to also restore everything from the filestreamcontentfg file group either by updating the values to null as im trying above or default to to null when the file is missing or something or am perhaps approaching the problem in the wrong way im developer by nature and does not have much knowledge as dba so do excuse me if im overlooking some trivial thing here
18339 believe understand the reasons behind fenced and unfenced stored procedures fenced run outside of the database in our case db2 so as to prevent possible corruption of the database engine should there be issues with things like pointers unfenced runs inside of the database which means that performance is better from what have also researched sql pl is always basically unfenced because it is sql and therefore cannot access memory like programming languages can and java procedures can run fenced or unfenced but since they can possibly access memory there should be consideration for running them fenced unless there is certainty on the quality of the code to not crash and it needs performance first of all am correct in my understand of the above next is it generally best practice to start out with all stored procedures even those defined as sql pl as fenced first any other best practices for stored procedures especially as related to fencing and or security edit further research has shown that sql pl procedures cannot run fenced since they do not contain any code that could harm the database engine such as pointers or file db2 knows they are safe and runs them inside the engine ie unfenced that being said am still looking for best practices regarding all other stored procedures
18372 have used alter index rebuild to remove index fragmentation in some cases rebuild does not seem to remove this fragmentation what are the reasons why rebuild does not remove fragmentation it seems that this happens especially with small indices
18399 this was suggested to be be repost here from stackoverflow currently have table and need to start adding new data columns to it not every record even going forward with new data after adding the new data columns will have data so am wondering if this is more suited for new table since it is really an extension of some of the data rows and not applicable to every row in other words since there will be lot of unused columns for those new data elements it seems like this would be more suited for new table the first table is record of page views currently 2million records id ip address times viewed created at timestamp date for every ip address record is made per day and consecutive pageviews are added to the times views per day additional fields would be for point of origin tracking ie google analytics source medium campaign not every visit will have that info im would assume about of the rows will have the data as it is usually only attributed on the first visit the main use for the data would be to attribute where people came from this may wind up being used more frequently which then seems to lend itself to the single table appreciate the feedback can add more if needed
18433 have master slave replication setup and it looks like that it is running fine below is result of show slave status command show slave status row slave io state waiting for master to send event master host master user repliv1 master port connect retry master log file mysql bin read master log pos relay log file mysqld relay bin relay log pos relay master log file mysql bin slave io running yes slave sql running yes replicate do db data1 replicate ignore db replicate do table replicate ignore table replicate wild do table replicate wild ignore table last errno last error skip counter exec master log pos relay log space until condition none until log file until log pos master ssl allowed no master ssl ca file master ssl ca path master ssl cert master ssl cipher master ssl key seconds behind master master ssl verify server cert no last io errno last io error last sql errno last sql error would like to understand further about the relay log file relay log pos and relay master log file my questions are is it true that the relay log file is one which is being read and stored locally for the replication to run what about the relay master log file then how is it different from the master log file what are both of these values viz read master log pos and relay log pos why are they showing up even though the replication is complete and in sync is it true that these files are in binary format and hence cannot view them
18463 is it possible to view delete statements that have recently occurred in the transaction log
18495 am following this solution here https stackoverflow com questions howto clean mysql innodb storage engine comment14041132 and tried to increase my innodb buffer pool size to 4g and later 1g also 1024m in addition to the log file size but mysql wont start with those values if put it back to 512m mysql starts fine how can solve this my server is 16gb one and according to webmin sysinfo real memory gb total gb used meanwhile found the error log as well mysqld safe mysqld from pid file var run mysqld mysqld pid ended mysqld safe starting mysqld daemon with databases from var lib mysql note plugin federated is disabled innodb the innodb memory heap is disabled innodb mutexes and rw locks use gcc atomic builtins innodb compressed tables use zlib innodb using linux native aio innodb initializing buffer pool size 0g innodb completed initialization of buffer pool innodb error log file ib logfile0 is of different size bytes innodb than specified in the cnf file bytes
18593 am about to embark on migrating database files to new san from an old san abd have couple of options to implement this it was suggested that look into the level of effort of restoring full backup to new database on the server however my original plan was to copy the files from the old san to the new san by detaching and then reattaching the database my gut tells me that id rather detach copy and attach since it seems more fail safe but that may just be my na vety dont want to miss transaction or somehow break something in the process of renaming databases guess my question is whether or not am justified in my skepticism of the backup restore replay option and what are other merits or risks of that option
18610 today while troubleshooting service broker problem discovered that the database owner was the windows login of an employee who had left the company his login had been removed and thus the query notifications were failing supposedly the best practice for dealing with this is to make sa the database owner we changed it and that cleared out the queue my very elementary question what is the database owner and what is its purpose
18637 ive got query like the following delete from tblfestatsbrowsers where browserid not in select distinct browserid from tblfestatspaperhits with nolock where browserid is not null tblfestatsbrowsers has got rows tblfestatspaperhits has got rows tblfestatsbrowsers create table dbo tblfestatsbrowsers browserid smallint identity11 not null browser varchar not null name varchar not null version varchar not null constraint pk tblfestatsbrowsers primary key clustered browserid asc tblfestatspaperhits create table dbo tblfestatspaperhits paperid int not null created smalldatetime not null ip binary null platformid tinyint null browserid smallint null referrerid int null userlanguage char null theres clustered index on tblfestatspaperhits that does not include browserid performing the inner query will thus require full table scan of tblfestatspaperhits which is totally ok currently full scan is executed for each row in tblfestatsbrowsers meaning ive got full table scans of tblfestatspaperhits rewriting to just where exists doesnt change the plan delete from tblfestatsbrowsers where not exists select from tblfestatspaperhits with nolock where browserid tblfestatsbrowsers browserid however as suggested by adam machanic adding hash join option does result in the optimal execution plan just single scan of tblfestatspaperhits delete from tblfestatsbrowsers where not exists select from tblfestatspaperhits with nolock where browserid tblfestatsbrowsers browserid option hash join now this isnt as much question of how to fix this can either use the option hash join or create temp table manually im more wondering why the query optimizer would ever use the plan it currently does since the qo doesnt have any stats on the browserid column im guessing its assuming the worst million distinct values thus requiring quite large in memory tempdb worktable as such the safest way is to perform scans for each row in tblfestatsbrowsers there is no foreign key relationship between the browserid columns in the two tables so the qo cant deduct any info from tblfestatsbrowsers is this as simple as it sounds the reason update to give couple of stats option hash join logical reads scans option loop join hash group logical reads scan per browserid no options logical reads scan per browserid update excellent answers all of you thanks tough to pick just one though martin was first and remus provides an excellent solution have to give it to the kiwi for going mental on the details
18664 im using postgresql on ubuntu are scheduled vacuum analyze still recommended or is autovacuum enough to take care of all needs if the answer is it depends then have largish database gib compressed dump size gib data directory do etl into the database importing close to million rows per week the tables with the most frequent changes are all inherited from master table with no data in the master table data is partitioned by week create hourly rollups and from there daily weekly and monthly reports im asking because the scheduled vacuum analyze is impacting my reporting it runs for more than hours and ive had to kill it twice this week because it was impacting regular database imports check postgres doesnt report any significant bloat on the database so thats not really an issue from the docs autovacuum should take care of transaction id wrap around as well the question stands do still need vacuum analyze
18669 does anybody knows how google or yahoo perform searches for keywords against very very huge amounts of data what sort of database or technologies do they employ for this it takes few milliseconds but they have more than billion pages indexed
18700 there was lot of discussion in this question what database technologies do big search engines use so much discussion that it made me confused so what is database anyway are only relational databases databases are object oriented databases databases is any system that allows me to store and retrieve information like map list etc database or does database have to store retrieve information and also have some administration features like users and privileges was dbase iii plus database since it wasnt really relational
18864 fairly simple question probably answered somewhere but cant seem to form the right search question for google do the number of columns in particular table affect the performance of query when querying on subset of that table for example if table foo has columns but my query only selects of those columns does having versus say columns affect query performance assume for simplicity that anything in the where clause is included in those columns im concerned about the usage of postgres buffer cache in addition to the operating systems disk cache have very lose understanding of postgres physical storage design tables are stored across several pages defaulting to 8k in size per page but dont quite understand how tuples are arranged from there is smart enough to only fetch from disk the data that comprises those columns
18877 when insert into tables using instead of triggers identity ident currenttable and scope identity return null how can get the last identity of inserted row
18887 im working with table where the date and time of an event is stored as char in the format yyymmddhhmmss or need to compare this to the current timestamp to see if its been more than minutes since the event happened but that seems to be more difficult than anticipated is it possible to do this or if not is there different approach that works
18934 am in the process of improving performance on our database application while am not dba am pretty comfortable with sql am after book that can help me understand how queries written in different ways affect performance and also understanding things like table scans indexes an statistics dont just want to blindly add everything the tuning advisor says so am looking for the knowledge required to assess these recommendations in the context our environment and how to fully utilise the tools available am working with mssql2008 so book the utilises this environment specific would be good thanks in advance
18943 want to create the best indexes for each table in my database is there query or tool in sql server to help with this process
19077 today had an issue with stored procedure timing out took longer than seconds when it was run from an asp net web page but executed quickly when run from ssms took seconds after suspecting parameter sniffing as the culprit masked the input parameters and the query executed faster my question is why did this happen this system has been in production for more than years and this is the first time weve seen anything like this on our stored procedures is this database wear and tear weve resolved the issue so it isnt big deal but im just curious as to why this was happening
19111 need to be able to pull the column names which allow null value know that show columns from table will give show me the table properties and whether or not the column allows null values but is there way to just return only the columnnames which allow null show columns from table where null yes doesnt work but it explains what need to accomplish and of course its easy to just pull everything and sort it out later on but if there is way to do what im asking id like to learn it
19130 ive inherited maintenance plans that does the following cleanup old data checks db integrity performs database and transaction log backups reorganizes our indexes updates statistics delete old backups and maintenance plan files of the minute maintenance plan updating the statistics takes staggering minutes during this minute period access to the database is blocked or at least replication from this db to our others is paused my question is when should we be updating the statistics and why this seems like the kind of thing we should do less frequently than every day im trying to get us out of the just because mind set of doing unnecessary maintenance
19135 in mysql error logs see these quite few warnings like these warning aborted connection to db db name user user name host webapp hostname got an error reading communication packets havent noticed any loss of data per se so am wondering what this warning means or what causes it and if how one might address the issue causing these this is on rhel and mysql enterprise
19159 im storing sensor data in table sensorvalues the table and primary key is as follows create table dbo sensorvalues deviceid int not null sensorid int not null sensorvalue int not null date int not null constraint pk sensorvalues primary key clustered deviceid asc sensorid asc date desc with fillfactor data compression page pad index off statistics norecompute off sort in tempdb off ignore dup key off online off allow row locks on allow page locks on on mypartitioningscheme date yet when select the sensor value valid for specific time the execution plan tells me it is doing sort why is that would have thought that since store the values sorted by the date column the sorting would not occure or is it because the index isnt solely sorted by the date column it cant assume that the result set is sorted select top sensorvalue from sensorvalues where sensorid and deviceid and date order by date desc edit can do this instead since the table is sorted deviceid sensorid date and do select specifying only one deviceid and one sensorid the output set should already be sorted by date desc so wonder if the following question would yield the same result in all cases select top sensorvalue from sensorvalues where sensorid and deviceid and date according to catcall below the sort order is not the same as the storage order we cant assume that the returned values are already in sorted order edit ive tried this cross apply solution no luck martin smith suggested id try to outer apply my result against the partitions found blog post aligned non clustered indexes on partitioned table describing this similar problem and tried the somewhat similar solution to what smith suggested however no luck here the execution time is on par with my original solution with boundariesboundary id as select boundary id from sys partition functions pf join sys partition range values prf on pf function id prf function id where pf name pf and prf value union all select maxboundary id from sys partition functions pf join sys partition range values prf on pf function id prf function id where pf name pf and prf value top1sensorvalue as select top sensorvalue from boundaries cross apply select top sensorvalue from sensorvalues where sensorid and deviceid and date and partition pfdate boundary id order by date desc order by date desc select sensorvalue from top1
19164 have 170gb of innodb index and data have to readjust the innodb buffer pool size for better performance the max table size of innodb tableindex data is 28gb so what should be the optimal size of innodb buffer pool update we are going to migrate our this local database to ec2 so will set the ram according to the current statistics of innodb thats why need the size of buffer pool so we can have available ram there file per table is enabled am using linux machine
19165 does anyone know of ready made1 command line tool that would allow me to connect from linux client to sql server want to be able to run arbitrary queries but most of the time want to be able to take database dump and then restore it this has to be scriptable as it will integrate in an automated build environment 1fyi my only other alternative is to write something ill probably use perl and dbi
19240 am very curious have business tables now think will have to create separate table location table that separate table should be myisam but why would do so why cant innodb store points
19242 realized that my company uses an elt extract load transform process instead of using an etl extract transform load process what are the differences in the two approaches and in which situations would one be better than the other it would be great if you could provide some examples
19291 im running plpgsql script in postgres would like to pass arguments to this script via psql im currently executing the script like psql database user update file sql came across this link which explains pgoptions environment variable but that doesnt work for custom arguments receive an error because the setting isnt listed in the postgres conf file bash export pgoptions pretend true bash psql my db update database sql psql fatal unrecognized configuration parameter pretend any other ideas ideally id like to avoid environment variables
19344 this is an issue come up against periodically and have not yet found good solution for supposing the following table structure create table int primary key char1000 null char1000 null and the requirement is to determine whether either of the nullable columns or actually contain any null values and if so which ones also assume the table contains millions of rows and that no column statistics are available that could be peeked at as am interested in more generic solution for this class of queries can think of few ways of approaching this but all have weaknesses two separate exists statements this would have the advantage of allowing the queries to stop scanning early as soon as null is found but if both columns in fact contain no nulls then two full scans will result single aggregate query select maxcase when is null then else end as maxcase when is null then else end as from this could process both columns at the same time so have worst case of one full scan the disadvantage is that even if it encounters null in both columns very early on the query will still end up scanning the whole of the rest of the table user variables can think of third way of doing this begin try declare int int int select case when is null then else end case when is null then else end divide by zero error if both and are might happen next row as no guarantee of order of assignments from option maxdop end try begin catch if error number divide by zero begin select bc both contain nulls return end else return end catch select isnull b0 isnull c0 but this is not suitable for production code as the correct behavior for an aggregate concatenation query is undefined and terminating the scan by throwing an error is quite horrible solution anyway is there another option that combines the strengths of the approaches above edit just to update this with the results get in terms of reads for the answers submitted so far using ypercubes test data exists case kejser kejser kejser ypercube 8kb maxdop hash group maxdop no nulls one null two null for thomass answer changed top to top to potentially allow it to exit earlier got parallel plan by default for that answer so also tried it with maxdop hint in order to make the number of reads more comparable to the other plans was somewhat surprised by the results as in my earlier test had seen that query short circuit without reading the whole table the plan for my test data that short circuits is below the plan for ypercubes data is so it adds blocking sort operator to the plan also tried with the hash group hint but that still ends up reading all the rows so the key seems to be to get hash match flow distinct operator to allow this plan to short circuit as the other alternatives will block and consume all rows anyway dont think there is hint to force this specifically but apparently in general the optimiser chooses flow distinct where it determines that fewer output rows are required than there are distinct values in the input set ypercubes data only has row in each column with null values table cardinality and the estimated rows going into and out of the operator are both by making the predicate bit more opaque to the optimiser it generated plan with the flow distinct operator select top from select distinct case when is null then null else foo end as case when is null then null else bar end as from test where leftb1 leftc1 is null as dt edit one last tweak that occurred to me is that the query above could still end up processing more rows than necessary in the event that the first row it encounters with null has nulls in both column and it will continue scanning rather than exiting immediately one way of avoiding this would be to unpivot the rows as they are scanned so my final amend to thomas kejsers answer is below select distinct top nullexists from test cross apply valuescase when is null then end case when is null then end vnullexists where nullexists is not null it would probably be better for the predicate to be where is null or is null and nullexists is not null but against the previous test data that one doesnt give me plan with flow distinct whereas the nullexists is not null one does plan below
19360 my tables structure is below tbdoc id int tbdocactions id int docid int date datetime col1 int col2 int want to have indexed view to get last tbdocactions columns for each tbdoc record result of this view must be such as below docid col1 col2 for get this result with view can use below query select docid from select docid maxid as maxactionid from tbdocactions group by docid inner join tbdocactions on id maxactionid but want indexed view to have better performance and in indexed view cant use max aggregate function
19456 am trying to delete principal from the database but cant because it owns schema when go to edit the user however the box to uncheck schemae is blue and unremovable how can remove the principal from these schemas
19491 is it enough to have the entire index in memory ram or does mongodb even try to allocate as much ram as possible to store even the data for fast reads id like to run mongodb other applications and it looks like mongodb is the only one which does not allow me to define range of ram to lets say max memory allocated or reserved 8gb if there is no way to do so should explain to oom killer that mongod is the bad process which is not best practise in my opinion
19525 have requirement for tracking some usage on table that will be getting retired in the next year and feel that could get the pertinent data stored procedures and in line sql being used against it from the transaction logs ive see some expensive purchased options out there for reading the logs but was wondering if anybody knew of any opensource solutions or some sample code of how to parse these logs
19544 am profiling an instance of sql server and via perfmons sqlserver sql statistics sql compilations sec metric see that the average is about or so whipped out sql profiler and looked for sp compile or sql compile events apparently they do not exist did find stored procedure sp recompile and tsql sql stmtrecompile events the amount of data see in the profiler suggests that these are the wrong events to look at though am not sure so my questions answers to any of these would be great how can see what exactly is compiling in sql server did pick the wrong metrics to look at in either perfmon or sql profiler with regards to stored procedure sp recompile and tsql sql stmtrecompile events in sql profiler they do not include the duration metric how can gauge the impact of these events to the system if they provide no way to see the timing impact to the system
19564 is it possible to pass in the name of table into stored procedure for example suppose you have several views of the same table they all have the exact same structure you want stored procedure that can be run for any of the views something like create procedure myprocedure tablename varchar50 select blah from tablename where blah blah2 when try to do this get must declare the table variable tablename any ideas how can do this
19632 can create column in db table postgresql which have default value random string and how if is not possible please let me know that
19670 im running search query in mysql to return items from products and pricelist table when user does search query need to get products from the products table left joined with all authorized unlocked sellers from 2nd table any number of sellers have the basic search query working but cant get the dynamic left join to work was told to do this in prep statement which im struggling mightly first time and which dont know where to put when working my search query will look like this select articles as art need to left join here from bigtable as bt where lot of other criteria this is the prep statement came up with set sql text declare strcount int default select sid ifnullpricelist base count as recs from buyerlist as left join sellerlist as on sid sid and pass pass where bid set string left join preislisten as lj loop set string concat string on iln iln and preisliste sid and ean ean and iln pricelist or set strcount strcount if strcount recs then leave lj end if end loop lj set string concat string set param iln param iln prepare stmt from sql text execute stmt using param iln deallocate prepare stmt so im basically finding all sellers pricelist name default base and then try to construct string like this left join pricelists on sid sid and pricelist foo and ean ean and iln or sid sid and pricelist bar and ean ean and iln or my questions if there are sellers will do loops but will the correct seller and pricelist be inserted into my string like this how do insert this into my actual search query if execute may get string back but cant just put the string into the query can thanks for being easy on me first time prep statement 2nd week mysql edit so this is what came up with bigtable query select counta id as gesamt datensaetze nos nos anzeige from artikelstammdaten as join colors left join farbenzuordnung as zu on farbe zu farbe and param filter and or zu iln param filter new part left join preislisten as on iln iln and ean ean and preisliste new get pricelists select ifnullklhs preisliste standard as pricelistid from kundenliste haendler as klhd left join kundenliste hersteller as klhs on klhs iln klhd iln verkaeufer and klhs plz klhd plz and klhs cid klhd cid where klhd iln kaeufer param iln group by pricelistid and iln get seller select klhd iln verkaeufer as sellerid from kundenliste haendler as klhd left join kundenliste hersteller as klhs on klhs iln klhd iln verkaeufer and klhs plz klhd plz and klhs cid klhd cid where klhd iln kaeufer param iln group by pricelistid active where aktiv ja more criteria stil trying to find out if this works correctly if anyone can shed some insights on whether can combine both my identical nested selects into single nested select it would be greatly appreciated
19719 am unable to see the copy database option in sql server management studio r2 can anyone tell me whats going on please
19749 normally when create stored procedure use the following as template of sort create procedure procedurename param1 type param2 type etc as begin procedure end is there way to include granting execute permission on only that stored procedure while im at it for instance like grant execute user execute but only for this stored procedure ive seen some other similar questions but they seem to all refer to all of the stored procedures and not just one nor have seen one where you can specify permissions inside of the create procedure script even answers about how can set permissions without the gui for specific stored procedures would be welcome edit the top answer certainly pointed me in the right direction this is is essentially what was looking for didnt think about batching the commands which is what ended up doing batching the command along with my stored procedure anyway think its pretty slick create procedure procedurename param1 type param2 type etc as begin procedure end go grant execute on procedurename to username go
19870 would like to know how to identify the exact query or stored proc which is actually filling up the transactional log of tempdb database
19894 am looking for alternative to create index on long column create table line field key integer not null value varchar4000 create index key value idx on line field key value results db2 sql error sqlcode sqlstate documentation says the sum of the stored lengths of the specified columns must not be greater than for such cases in mysql there is syntax create index key value idx on line field key value1000 and hsqldb just works without any limitations what is the analogue for db2
19912 ive been brought up old school where we learned to design the database schema before the applications business layer or using ooad for everything else ive been pretty good with designing schemas imho and normalized only to remove unnecessary redundancy but not where it impacted speed if joins were performance hit the redundancy was left in place but mostly it wasnt with the advent of some orm frameworks like rubys activerecord or activejdbc and few others cant remember but im sure there are plenty it seems they prefer having surrogate key for every table even if some have primary keys like email breaking 2nf outright okay understand not too much but it gets on my nerves almost when some of these orms or programmers dont acknowledge or to or they stipulate that its just better to have everything as one big table no matter if it has ton of nulls todays systems can handle it is the comment ive heard more often agree that memory constraints did bear direct correlation to normalization there are other benefits too but in todays time with cheap memory and quad core machines is the concept of db normalization just left to the texts as dbas do you still practice normalization to 3nf if not bcnf does it matter is dirty schema design good for production systems just how should one make the case for normalization if its still relevant note im not talking about datawarehouses star snowflake schemas which have redundancy as part need of the design but commercial systems with backend database like stackexchange for example
19924 have problem with my application that works on my dev sql server developer edition but not on production sql server is there an easy way to export the settings from the production server and compare it to my server configuration what found out is that can export facets in sql server management studio to xml files and compare them in diff tool is there any other better way to export and compare settings of two sql server instances
19959 had question about what the expected and actual behaviour for the following scenario is the scenario is one where database table table1 is cleared and reloaded every day the table has an id column which is reference by several other tables using foreign keys if set the foreign key on delete action to cascade obviously this will delete rows in the other tables if issue single delete from table1 command on this table however what would happen if were to delete all the rows and then re insert the same rows with the same ids under the same transaction will this trigger the cascade mid transaction or will the foreign key reconciliation happen once ive called commit obviously im thinking sql server here but im wondering if this behaviour is consistent across other dbs as well in the event that the cascade is triggered even in the middle of the transaction what would be the best way to go about managing foreign keys and relationships with table which is completely cleared and re loaded every day
20068 is there way to temporarily suppress sql server management studios auto complete while typing query dont want to disable auto complete completely just say hold down some key while typing in particular word so that it doesnt get in the way for example say had the following query select foo foo2 from sometable as type foo and then hit space bar sql server management studios auto complete kicks in and completes foo to foobar
20070 have sql server table called brittney spears marriages and it has the following columns marrigeid tinyint husbandname varchar500 marrigelength int now have another table brittney spears marriage stories storyid int marriageid tinyint storytext nvarcharmax the problem is we want to update marrigeid column to an int from tinyint we just feel that brittney is going to have lots of marriages before everything is said and done now the brittney spears marriage stories table has million rows in it hey the girl has some issues so when we go to do the update the transaction log fills up and our sql server box dies how can we get around this is there anyway to say hey sql server im going to update this column and make it bigger trust me on this sql server please dont fill up the transaction log while you attempt to validate everything
20099 have an issue that face every time decide to build cube and havent found way to overcome it yet the issue is how to allow the user to define range of things automatically without having the need to hardcode them in the dimension will explain my problem in an example have table called customers this is the data in the table want to display the data in pivot style and group up the salary and age in defined ranges like below wrote this script and defined the ranges select custid custname age salary salaryrange case when castsalary as float then when castsalary as float between and then when castsalary as float between and then when castsalary as float then end agerange case when castage as float then below when castage as float between and then when castage as float between and then when castage as float between and then when castage as float then end from customers go my ranges are hard coded and defined when copy the data to excel and view it in pivot table it appears like below my problem is want to create cube by converting the customers table into fact table and create dimension tables salarydim agedim the salarydim table has columns salarykeysalaryrange and the agedim table is similar agekeyagerange my customer fact table has customer custid custname agekey foreign key to agedim salarykey foreign key to salarydim still have to define my ranges inside these dimensions every time connect an excel pivot to my cube can only see these hardcoded defined ranges my question is how to define ranges dynamically from the pivot table directly without creating the range dimensions like agedim and salarydim dont want to only be stuck to the ranges defined in the dimension the range defined is might want to change it to and so on and users request different ranges every time every time change it have to change the dimension how can improve this process it would be great to have solution implemented in the cube so that whatever bi client tool that connects to the cube can define the ranges but wouldnt mind if there is good way using excel only
20117 background this is for the construction of some views well be using for reporting have an table of locations the key fields being location and parent the structure that these two fields create level wise are along the lines of company name campus name building name floor name room name company name remains the same and campus name remains the same in this case the structure of locations generally looks like this org name campus name bldg bldg bldg grounds floor basement room room every location links back to its parent location which is ultimately the organization name currently there is only one organization and one campus goals would like to be able to query all locations beneath any given location at the building level this is so can return things like how many workorders have been performed for any location within given building would like to be able to determine which sub location belongs to which building essentially the reverse would like to go from any level beneath the building level and trace back up to what the building is would like this to be in view that means would like to have table that for every item at the building level lists the building in the left hand column and all possible locations under that building in the right hand column this way id have list that could query at any time to find which locations are part of which building attempts and doing it right ive attempted to do this through horribly constructed views union queries etc which all have seemed like bad idea know oracle possesses mechanism for this through connect by im just not sure how to make use of it
20145 basically part of our postgresql table is used to keep server access logs and as such sometimes during production this can get pretty large is there any way of setting in postgresql to have maximum number of records table can have and to push off the oldest record
20217 how do set timestamp column whose default value is the current utc time mysql uses utc timestamp function for utc timestamp mysql select utc timestamp utc timestamp row in set sec so ive tried create table blah creation time timestamp default utc timestamp and other variations like utc timestamp but without success
20275 existing setup sql server with standard eav table that users want to slice and dice with bi tool cognos is there any hope that we can transform this data into format that we can report off of google has led me to believe there is no hope id like to believe that some kind of solution is out there
20283 one of my developers has written sql function that works like the vb net function lastindexof and wants to publish it my question is what would be the reason to put this in central database versus putting it in each user database the developer was trying to put it in sys schema on his master db so he wouldnt have to qualify calls to it from user databases sigh but wasnt sure what the valid excuse would be to centralize it obviously not master database versus each user database
20335 am planning on storing scans from mass spectrometer in mysql database and would like to know whether storing and analyzing this amount of data is remotely feasible know performance varies wildly depending on the environment but im looking for the rough order of magnitude will queries take days or milliseconds input format each input file contains single run of the spectrometer each run is comprised of set of scans and each scan has an ordered array of datapoints there is bit of metadata but the majority of the file is comprised of arrays or bit ints or floats host system os windows bit mysql version x86 cpu 2x xeon e5420 cores total ram 8gb ssd filesystem gib hdd raid tib there are some other services running on the server using negligible processor time file statistics number of files total size tib min size bytes max size gib mean mib median mib total datapoints billion the total number of datapoints is very rough estimate proposed schema im planning on doing things right normalizing the data like crazy and so would have runs table spectra table with foreign key to runs and datapoints table with foreign key to spectra the billion datapoint question am going to be analyzing across multiple spectra and possibly even multiple runs resulting in queries which could touch millions of rows assuming index everything properly which is topic for another question and am not trying to shuffle hundreds of mib across the network is it remotely plausible for mysql to handle this additional info the scan data will be coming from files in the xml based mzml format the meat of this format is in the binarydataarraylist elements where the data is stored each scan produces binarydataarray elements which taken together form dimensional or more array of the form these data are write once so update performance and transaction safety are not concerns my na ve plan for database schema is runs table column name type id primary key start time timestamp name varchar spectra table column name type id primary key name varchar index int spectrum type int representation int run id foreign key datapoints table column name type id primary key spectrum id foreign key mz double num counts double index int is this reasonable so as you may have been able to infer am the programmer not the biologist in the lab so dont know the science nearly as well as the actual scientists heres plot of single spectrum scan of the kind of data with which ill be dealing the goal of the software is to figure out where and how significant the peaks are we use proprietary software package to figure this out now but we want to write our own analysis program in so we know what the heck is going on under the sheets as you can see the vast majority of the data are uninteresting but we dont want to throw out potentially useful data which our algorithm missed once we have list of probable peaks with which were satisfied the rest of the pipeline will use that peak list rather than the raw list of datapoints suppose that it would be sufficient to store the raw datapoints as big blob so they can be reanalyzed if need be but keep only the peaks as distinct database entries in that case there would be only couple dozen peaks per spectrum so the crazy scaling stuff shouldnt be as much of an issue
20355 am working on documenting my databases and would like to create list of all of the indexes in my database the reason want to do this is so that can track changes to my indexes overtime currently have spreadsheet with all of the indexes that have changed since created that spreadsheet but it doesnt have all of the indexes instead of having to script out each index would like to be able to just generate the list played with the system views but wasnt able to figure it out how can generate list of indexes and the create statement for each index
20410 am more experienced with sql server and sybase than oracle and understand those products well ive been asked to look for ways to reduce the server estate running oracle understand that an instance in oracle maps to database hosting many tablespaces have fairly good grasp of the fundamentals however if wanted to consolidate server1 server4 running oracle database into one server what would be the best way to do it physically am considering virtual as well using dbaas database as service model but am curious if it can should be done physically is it possible to have four separate instances point to four separate databases on one machine or would have to merge the four databases into one database on the consolidated server and manage the schemas to ensure there are no name conflicts if did that would have one instance or four have read the documentation but im still not sure about this area
20416 am just starting to learn about memory usage on sql server when using the query in the answer to the question sql server r2 ghost memory discovered that single database is taking up the lions share of space in the buffer pool looking further using sys allocation units and sys indexes confirmed this is likely caused by the heavy use of indexes in the database most indexes are clustered another database developer believes we are having memory issues on the server that queries are starting to run long because there is no available memory my question here is does the use of these indexes and their existence in the buffer pool take away memory available for other processes
20419 this is continuation of do indexes consume memory fellow database developer believes we are having memory issues he has seen increased run times in some standard queries going from under seconds to about two and half minutes he looked at task manager on the server and found high memory usage and now wants to take some of the memory currently allocated to the os and free it up for sql server we are on sql server bit machine awe is not enabled minimum mb max mb found brent ozars sysadmin guide to microsoft sql server memory which indicates task manager is not reliable have also found that our page life expectancy is not indicating memory pressure checked via pinal daves query where else should look what else should check id like to report back to the database developer to either confirm his suspicions or prove them incorrect edit modified my actual question appreciate and agree that these queries are overwhelming more likely slower for reasons other than memory problems am in situation however that need to prove memory is not the culprit across the board that is proving that handful of queries are slower for other reasons wont accomplish my task id like to understand where can get such information and what metrics should check
20455 using sql server r2 how can write to the sql server error log have rollback statement that id like to couple with statement written to the error log for external monitoring example begin tran insert into table1 select from table2 if error begin rollback tran write to log return end commit tran edit id like to clarify want to write to the sql server logs current log under the management folder in the object explorer
20499 need to update million records in single table in effect normalizing the table by replacing the varchar value of column with simply an id say replacing but really im writing the id into another column what im trying to achieve is to normalize the dataset the not yet normalized data has no indexing my thought was that would not build indexes on the raw values waiting instead to index the foreign keys that will be replacing the varchar values with tinyint values after the update completes update set autoclassid autoclassid from autodataimportstaging dbo automobile as join autodata dbo autoclass as on autoclassname autoclassname background using mssql r2 on server r2 server has gb ram server has one raid10 rpm sata not great know in production this will only read data and not write data plus recent hd shortage made this necessary for cost server has dual quad core xeon cpu the machine is not doing anything else currently dedicated to dev only this process simple logging turned on but does it still log so that it can rollback note that the query references two different dbs for what thats worth width of record in table getting updated is bytes resources during execution physical ram is maxed out disk is maxed out cpu is hardly doing anything choke point is run time has been hours and counting suspect few things like need an index on the raw data even though will be dropping the column autoclassname after the normalization updates also wonder if should just loop down the table one record at time instead of the join which seemed ridiculous at the time started this but now it seems that would have been faster how should change my methodology for my remaining normalization updates similar to this one more quickly
20566 there is something in the mysql console that drives me nuts when hit ctrl to cancel the current command being typed the terminal exits in every terminal know nix terminals python postgresql ctrl cancels the current command and ctrl exits the terminal this issue has been reported at and bumped several times since is there way to change this behaviour or convince the mysql dev team that this is really annoying
20583 have records one details when the stop clock was stopped and the other started only have the one date value in each record im trying to work out the difference between the stop and start dates im looking for pointers thought by doing something like select e1 id mine1 eventtime as stoptotalminutes maxe2 eventtime as starttotalminutes from event as e1 join event as e2 on e1 id e2 id would have been able to work something out but im failing so table and some sample data is id eventtime action stop start stop stop start stop
20619 during maintenance job im trying to get list of fragmented indexes but the query is extremely slow and takes over minutes to execute think this is due to remote scan on sys dm db index physical stats is there any way to speed up the following query select object namei object id as tablename name as tableindexname from sys dm db index physical statsdb id null null null detailed phystat inner join sys indexes on object id phystat object id and index id phystat index id where phystat avg fragmentation in percent and object namei object id is not null order by phystat avg fragmentation in percent desc im not dba and could be making an obvious mistake in the query above or maybe there are some indexes or statistics that would help maybe its just the size of the database around 20gb with about tables the reason ask is that we only have very small window for maintenance during the night and this is taking up most of the time
20706 so we can do windows login or mixed mode but can we configure sql server to only use internal logins and to block all windows logins is the only solution to add all potential windows logins and set them to restricted privileges as preventative or reactive procedure
20714 this is duplicate of the question asked on stackoverflow but was advised that someone here could have better idea what is happening have sporadic problem when upgrading sql server in single user mode using net sqlconnection some other application somehow logs into the database while the sql code is being executed and kicks my process out sqlconnection is not closed or disposed in any way but some other application somehow ends up connected to the database and that kicks my connection out when run sp who could see that process that took control of the database is command task manager anyone could tell me what is this process what is its purpose and how in the world it could get into database which is in single user mode and there is an active connection
20734 two things id like to know how do you safely move tempdb with minimal downtime how many tempdb files do you need is it file per core so quad core tempdb files creating three new ones
20759 we process routine data feed from client who just refactored their database from form that seems familiar one row per entity one column per attribute to one that seems unfamiliar to me one row per entity per attribute before one column per attribute id ht cm wt kg age yr after one column for all attributes id metric value ht cm wt kg age yr ht cm wt kg age yr ht cm wt kg age yr is there name for this database structure what are the relative advantages the old way seems easier to place validity constraints on specific attributes non null non negative etc and easier to calculate averages but can see how it might be easier to add new attributes without refactoring the database is this standard preferred way of structuring data
20832 have about tables in mysql innodb one of them has million records in it used this command to back things up mysqldump username dbname gzip path to file dbname sql gz and this command to import things on the new server use dbname source path to file dbname sql here is the pain am afflicted with the time is going up with each query what can do to speed things up is something wrong with my mysqldump command
20867 did some googling and couldnt find an answer to this question more recent than few years ago so thought id ask oracles rac feature offers load balancing for both read and write transactions as well as scale out and high availability without downtime at least as understand it were about to deploy our first databases that use rac so well see how it goes is there any sql server feature set or third party component you could install on top that delivers equivalent functionality weve always used windows clustering where failover event causes about seconds of sql downtime always tolerable but not ideal now with alwayson in sql sql server shrinks that to about seconds and adds the concept of read only secondary databases but they still require that write transactions are choked through single connection point much improved since many transactions are just read but still not really load balancing and in the case of node failure or the need to patch theres still downtime suppose its just more curiosity feel like this is the only area that sql server falls behind oracle at least among the features ive personally seen used wanted to see if there are any options out there to close that gap and possibly improve our own sql server deployment while we wait for microsofts equivalent feature to be added maybe in sql
20959 im trying to set up an openstreetmap server on an ubuntu machine using the ubuntu packages listed at switch2osm org initially installed and set up everything using northeast us only map extract but now want to install the entire planet of maps downloaded planet latest osm bz2 and ran osm2pgsql slim planet latest osm bz2 as user with write permission to the database this was the same command that worked to install us northeast osm pbf earlier came back the next day to find this command appeared to finish successfully but for some reason the rendering daemon wasnt generating new tiles from the new data tried restarting renderd and when that had no effect tried restarting the postgresql server with sudo etc init postgresql restart however server startup failed with the following errors in the log utc warning page of relation base was uninitialized utc warning page of relation base was uninitialized more lines like this utc warning page of relation base was uninitialized utc panic wal contains references to invalid pages utc log startup process pid was terminated by signal aborted pastebin of entire log here there isnt much information on these kinds of errors on the internet but from what can find it seems to mean that either my indexes are corrupted or my write ahead log is the only way to fix corrupted indexes though is to start the database in single user mode and rebuild them and cant even do that because get the same fatal errors even when start in single user mode with indexing disabled is there any way for me to delete the write ahead log and force the server to start up from scratch or fix for this kind of corruption that doesnt require first starting the database successfully alternatively is there way for me to delete the database and just re import all the planet data given that cant start the server to execute the drop database command update following craig ringers suggestion went and looked through the database logs from before the wal errors started occurring to see if could find any suspicious behavior in the log from immediately before the first instance of wal errors found these suspicious looking lines utc log received fast shutdown request utc log aborting any active transactions utc fatal terminating connection due to administrator command utc fatal terminating connection due to administrator command utc fatal terminating connection due to administrator command utc fatal terminating connection due to administrator command utc fatal terminating connection due to administrator command utc statement create table planet osm polygon tmp as select from planet osm polygon order by way utc fatal terminating connection due to administrator command utc statement create index planet osm ways nodes on planet osm ways using gin nodes with fastupdate off utc fatal terminating connection due to administrator command utc statement create table planet osm line tmp as select from planet osm line order by way utc log received immediate shutdown request utc warning terminating connection because of crash of another server process utc detail the postmaster has commanded this server process to roll back the current transaction and exit because another server process exited abnormally and possibly corrupted shared memory utc hint in moment you should be able to reconnect to the database and repeat your command utc log could not send data to client broken pipe utc warning terminating connection because of crash of another server process utc detail the postmaster has commanded this server process to roll back the current transaction and exit because another server process exited abnormally and possibly corrupted shared memory utc hint in moment you should be able to reconnect to the database and repeat your command utc log could not send data to client broken pipe pastebin of the entire log is here when it says terminating connection due to administrator command assume that was my command to restart the database server but it looks like the shutdown somehow failed horribly resulting in corruption of shared memory this doesnt make sense because restarted it cleanly using the etc init postgres restart script not an abrupt kill or manually logging in as postgres am interpreting this log incorrectly or is there actually problem with using etc init postgres restart to restart postgresql server please note since my question was moved to database admin where im new user no longer have the ability to upvote your answers this doesnt mean dont appreciate the help
20973 with sql server you could look at the task manager and at least get cursory look at how much memory is allocated to sql server with sql server the working set or commit size never really goes above mb even though the sqlserver memory manager total server memory kb perf counter states is there setting where it will actually show the server memory in the task manager or is it result of them changing how memory is used in sql server
20974 according to postgresqls docs theres no performance difference between varchar varcharn and text should add an arbitrary length limit to name or address column edit not dupe of would index lookup be noticeably faster with char vs varchar when all values are chars know the char type is relic of the past and im interested not only in performance but other pros and cons like erwin stated in his amazing answer
21014 there are parts to my question is there way of specifying the initial size of database in postgresql if there isnt how do you deal with fragmentation when the database grows over time ive recently migrated from mssql to postgres and one of the things we did in the mssql world when creating database was to specify the initial size of the database and transaction log this reduced fragmentation and increased performance especially if the normal size of the database is known beforehand the performance of my database drops as the size grows for example the workload im putting it through normally takes minutes as the database grows this time increases doing vacuum vacuum full and vacuum full analyse do not appear to solve the issue what does solve the performance problem is stopping the database de fragmenting the drive and then doing vacuum full analyse takes the performance of my test back to the original minutes this leads me to suspect that fragmentation is whats causing me pain ive not been able to find any reference to reserving tablespace database space in postgres either im using the wrong terminology and thus finding nothing or there is different way of mitigating filesystem fragmentation in postgres any pointers the solution the supplied answers helped confirm what id begun to suspect postgresql stores the database across multiple files and this is what allows the database to grow without worry of fragmentation the default behaviour is to pack these files to the brim with table data which is good for tables that rarely change but is bad for tables that frequently updated postgresql utilizes mvcc to provide concurrent access to table data under this scheme each update creates new version of the row that was updated this could be via time stamp or version number who knows the old data is not immediately deleted but marked for deletion the actual deletion occurs when vacuum operation is performed how does this relate to the fill factor the table default fill factor of fully packs the table pages which in turn means that there is no space within the table page to hold updated rows updated rows will be placed in different table page from the original row this is bad for performance as my experience shows as my summary tables get updated very frequently up to rows sec opted to set fill factor of of the table will be for inserted row data and for update data while this may seem excessive the large amount of space reserved for updated rows means that the updated rows stay within the same page as the original and theres the table page isnt full by the time the autovacuum daemon runs to remove obsolete rows to fix my database did the following set the fill factor of my summary tables to you can do this at creation time by passing parameter to create table or after the fact via alter table issued the following plpgsql command alter table my summary table set fillfactor issued vacuum full as this writes completely new version of the table file and thus by implication writes new table file with the new fill factor rerunning my tests see no performance degradation even when the database is as large as need it to be with many millions of rows tl dr file fragmentation wasnt the cause it was table space fragmentation this is mitigated by tweaking the tables fill factor to suit your particular use case
21031 currently im just trying to delete objects with the name column being null from temp table with the following delete from pulleddata where pulleddata name0 null ive also tried null null etc but nothing seems to work id appreciate some help on the matter
21044 how can concatenate two psql postgresql client variables want to generate an absolute path by concatenating directory path variable and filename variable ive tried this set path tmp set file foo echo path file but psql puts space between the path and the file and outputs tmp foo
21065 ive been considering this for quite long time now the basic question is how to unit test stored procedures see that can set up unit tests relatively easily for functions in the classic sense mean they get zero or more arguments and return value but if consider real life example of seemingly simple procedure inserting row somewhere with few triggers doing this and that before or after the insert even defining the boundaries of unit is quite difficult should test only the insert itself thats fairly straightforward think with relatively low value should test the result of the whole chain of events apart from the question whether this is unit test or not designing suitable test can be quite strenuous job with lots of additional question marks arising on the way and then comes the problem of constantly changing data in the case of an update affecting more than just few rows every potentially affected row must be included somehow in the test cases further difficulties with deletes and so on and so on so how do you unit test your stored procedures is there treshold in complexity where it gets completely hopeless what resources are needed for maintenance edit one more small question based on alexkuznetsovs answer or is there treshold under which it is completely useless
21075 weve encountered problem after moving the database of our customer to an extra server this should have had positive effects on the sites performance but there is problem with table locking in myisam ive heard of using innodb instead of myisam but we cannot change the engine in the near future we could spot it to an update query which is performed when moderator activates comment on the articlesite this is the process update query is processed set status where id index is set the cached files of the page are deleted at this point the whole page becomes slow the database itself is busy for minutes fetched the processlist few times and saw about entries of different select queries which were all on the state waiting for table level lock dont unterstand why this update on the table article comments can affect select statements for table article to wait for table level lock in processlist almost all waiting queries were from this table ive read about the fact that updates inserts are preferred to selects and that this can cause such problems but the articles table itself isnt updated when comments become activated so the selects shouldnt wait did missunterstand that is there something besides changing to innodb to prevent this behaviour or at least to get better balance im very irritated about the fact that this problem did not appear before moving the database to the new server guess there is some misconfiguration but dont know how to identify
21087 have master slave configuration where the master failed ive been able to reset the old slave to be master and the old master to slave from it fine what cant seem to do is to remove the master information on the old slave which is now the new master see mysql show slave status row slave io state master host master user replicationslave master port slave io running no slave sql running no ive read lot of mysql documentation but still havent found way to clear the slave information from the new master ive tried reset slave which does not seem to clear those settings actually it does remove the master info file but not the memory settings see below change master to master host which just spits on an error since it was deprecated recently checking my cnf which does not have the master information since they were added programmatically reset master because some mysql docs recommended it that only resets the bin logs poking around in the internal mysql tables to see if can find the fields to clear what is the proper way to do this on mysql thanks for any help edit so it turns out that reset slave removes the master info file as rolandomysqldba implied however you still need to restart the server before the slave information is removed is there any way to remove this slave information without having to restart mysqld
21152 have table in the name of ips as below create table ips id int10 unsigned not null default begin ip num int11 unsigned default null end ip num int11 unsigned default null iso varchar3 default null country varchar150 default null engine innodb lets assume have countryid field on this table from country table which is as below create table country countryid tinyint3 unsigned not null auto increment name varchar50 character set utf8 collate utf8 unicode ci not null ordering smallint5 unsigned not null default iso char2 not null primary key countryid engine innodb there is about records in ips table is there any query for the following scenario check if ips iso is equal to country iso if its equal then add country coutryid to that record couldnt think of any way to do it do you have any idea how to do that
21181 when do single row insert to table that has an auto increment column id like to use the last insert id function to return the new auto incremented value stored for that row as many microsoft sql server devs and admins no doubt are aware the equivalent functionality in sql server scope identity and identity hasnt been without its problems know the mysql docs state the id that was generated is maintained in the server on per connection basis this means that the value returned by the function to given client is the first auto incrementvalue generated for most recent statement affecting an auto increment column by that client this value cannot be affected by other clients even if they generate auto increment values of their own this behavior ensures that each client can retrieve its own id without concern for the activity of other clients and without the need for locks or transactions source and even go so far as to say using last insert id and auto increment columns simultaneously from multiple clients is perfectly valid source are there any known risks or scenarios that may cause last insert id not to return the correct value im using mysql on centos x64 and fedora x64 and the innodb engine
21186 am aware of when adding new fields to large tables it is recommended to add them to the end of the fields rather than somewhere in the middle and wondering if something like this applies when changing field types have table with about million records that has several varchar type fields would like to change these to nvarchar but as understand it this will take some time and resources as the fields are in the middle of the table and sql server has to do bunch of copying re ordering what is an efficient way of accomplishing this
21189 am running sql server and need to back up the udf scripts queries that have created how can this be done in timely manner tried right clicking on the folder icon where the udf are saved table valued functions to copy the folder but am not bale to the only thing can think up is using the modify command on each and every single one my udf scrips and copying and pasting to separate text files even if if there is only simple way to save all the scripts in one text file instead of separate txt files would be more than happy to do that instead of having to copy and paste every single file one at time any suggestions
21220 have unique compound key like frfromidtoid in the table when run the query with explain get the following result impossible where noticed after reading const tables the query ran explain select rid from relationship where fromid and toid any help edit1 when use the below query explain select rid from relationship where fromid and toid and is approved or is approved or is approved see using where instead of the previous message but when use the below query explain select rid from relationship where fromid and toid and is approved or is approved or is approved again get the first impossible message what these parenthesis do here edit2 create table relationship rid int10 unsigned not null auto increment fromid mediumint8 unsigned not null toid mediumint8 unsigned not null type tinyint3 unsigned not null is approved char1 not null primary key rid unique key fromid fromidtoid key toid toid constraint relationship ibfk foreign key fromid references user uid on delete cascade on update cascade constraint relationship ibfk foreign key toid references user uid on delete cascade on update cascade engine innodb edit3 as mysql site say impossible where noticed after reading const tables mysql has read all const and system tables and notice that the where clause is always false but in the query get the result want the where part is not false is there someone who could explain this and shed some light on the subject
21226 am trying to make the following sql statement work but get syntax error select countb foo from table1 left join table2 on pkey fkey group by here is wide table with columns and would like to avoid listing each column name in the group by clause if possible have many such tables over which have to run similar query so will have to write stored procedure whats the best way to approach this am using ms sql server
21302 on one of our production server log files the following message is being observed on daily basis what does it mean is it serious issue sql server has encountered occurrences of cachestore flush for the sql plans cachestore part of plan cache due to some database maintenance or reconfigure operations
21319 have created very basic sql table as following create table dbo tickdata date varchar null time varchar not null symbol varchar not null side varchar not null depth varchar not null quote varchar not null size varchar not null on primary then performed gig bulk insert bulk insert tickdata from sumo csv go then ram usage for sql server went skyrocking eating up 30go of ram prefer to think this is an abnormal behavior and that action can be taken to avoid this edit ok this seems to be the default behavior fair enough however why isnt memory freed up long after the bulk insert is finished couple of extra considerations as of the comments concerning sql server freeing the memory when it is told to by the os my hands on experience on core gb xeon server proves this to be inexact once memory voracious bcp extract is over have pool of net instances of my data processing application that need to process the extracted data and they are left choking fighting to share the remaining memory to try to perform their jobs which take faaaaar longer that when sql server is turned off and memory is available for all applications to share have to stop the sql server agent to make everything go smoothly and prevent apps from crashing for articiallt caused outofmemmroy exception as to artificial brutal memory capping limitation if free memory is available why not use it ideally it would rather be dinamically set to adapt to what is available rather than just being forcibly limited randomly but guess this is by design so case closed on this last point
21342 as follow up to my previous question on perf troubleshooting sharepoint site was wondering if could do something about the cxpacket waits know the knee jerk solution is to turn off all parallelism by setting maxdop to sounds like bad idea but another idea is to increase the cost threshold before parallelism kicks in the default of for the cost of an execution plan is fairly low so was wondering if theres query out there already written that would find me the queries with the highest execution plan cost know you can find those with the highest duration of execution and so on but is the execution plan cost retrievable somewhere too and that would also tell me if such query has been executed in parallel does anyone have such script at hand or can point me in the direction of the relevant dmv dmf or other system catalog views to find this out
21414 using sql enterprise edition following instructions here for restoring backup http msdn microsoft com en us library ms186390 aspx restrictions need to be able to rename the database because it is the second copy on the instance testing purposes but cannot rename it nor change the folders that it will restore the mdf ldf files to for whatever reason the relocate all files to folder checkbox is not available restoring form sql backup
21426 googled and found this old web page which said just to change the name of the directory tried it but it didnt work pg lsclusters version cluster port status owner data directory log file main down postgres var lib postgresql main var log postgresql postgresql main log main online postgres var lib postgresql main var log postgresql postgresql main log pg ctlcluster main stop pwd var lib postgresql ls main mv main oldmain pg ctlcluster oldmain start error specified cluster does not exist postgresql ubuntu
21434 understand that you cannot have order by in view at least in sql server am working with also understand that the correct way of sorting view is by putting an order by around the select statement querying the view but being relatively new to practical sql and the usages of views would like to understand why this is done so by design if ive followed the history correctly this was once possible and was explicitly removed from sql server and so on dont quote me on the exact version however the best reason can come up with as to why microsoft removed this feature is because view is an unsorted collection of data am assuming there is good logical reason as to why view should be unsorted why cant view just be flattened out collection of data why specifically un sorted it doesnt seem that hard to come up with situations where at least to me imho it seems perfectly intuitive to have sorted view
21443 my mysql slave got stuck on an error and built up to being seconds behind master should rebuild it and start from scratch or let it catch up by itself
21480 ive been wondering lately what if any are the improvements available in mariadb over conventional mysql understand that where platform interoperability and or backwards compatibility may be an issue then sticking with the tried and trusted mysql is best but for stand alone db on stand alone web site application are there any benefits to be had by using maria will maria work with common web platforms such as wordpress drupal joomla etc expect that some of this is going to come down to choice preference of storage engines but to be honest still dont know half the time if when should use myisam innodb or any of the others which is better or faster or whatever the only thing get is that if want true table relationships foreign keys etc use innodb thanks for any help or clarity people can offer me
21483 have large myisam table on mysql 5windows xp x64 on which will have to run delete low priority queries does delete low priority make the rows invisible to select statements immediately and actually delete the rows from disk when no clients are accessing the table or is the point delaying removal of visibility
21542 im in situation where want to get the minimum value from of columns ive found three ways so far to accomplish this but have concerns with the performance of these methods and would like to know which would be better for performance the first method is to use big case statement heres an example with columns based on the example in the link above my case statement would be much longer since will be looking at columns select id case when col1 col2 and col1 col3 then col1 when col2 col3 then col2 else col3 end as themin from mytable the second option is to use the union operator with multiple select statements would put this in an udf that accepts an id parameter select id dbo getminimumfrommytableid from mytable and select mincol from select col1 col from mytable where id id union all select col2 from mytable where id id union all select col3 from mytable where id id as and the 3rd option found was to use the unpivot operator which didnt even know existed until just now with cte id col1 col2 col3 as select id col1 col2 col3 from testtable select cte id col1 col2 col3 themin from cte join select id minamount as themin from cte unpivot amount for amountcol in col1 col2 col3 as unpvt group by id as minvalues on cte id minvalues id because of the table size and frequency in which this table is queried and updated am concerned about the performance impact these queries would have on the database this query will actually be used in join to table with few million records however the records returned will be reduced to around hundred records at time it will get run many times throughout the day and the columns am querying are frequently updated they contain daily stats do not think there are any indexes on the columns am querying which of these methods is better for performance when trying to get the minimum of multiple columns or is there another better method that dont know of am using sql server sample data results if my data contained records like this id col1 col2 col3 col4 col5 col6 the end result should be id value
21567 this question regards the proper use of null and utilizing check constraints for business logic vs stored procedures have the following tables setup normalized the tables to avoid using nulls the problem is that some of these tables depend on each other due to business processes some devices must be sanitized and some are tracked in another system all devices will eventually be disposed in the disposal table the issue is that need to perform checks such as if the boolean field requiressantization is true then the disposaldate cannot be entered until the sanitize fields are entered also if the boolean value istrackedinother is true then the officialoutofservice fields must be entered before the disposaldate can be entered if merge all of these columns into the archive device table then will have null fields but will be able to manage all of the business rules using check constraints the alternative is to leave the tables as they are and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors is this case where null can be used appropriately the boolean fields istrackedinother and requiressanitization basically give meaning to the null fields if istrackedinother is false then the device is not tracked in the other system and sectionid and specialdevicecode are null and know that they should be null becuase it is not tracked in the other system likewise officialoutofservicedate and ooslogpath know will be null aswell and disposaldate can be entered at any time if istrackedinother is true then sectionid and specialdevicecode will be required and if officialoutofservicedate and ooslogpath are null then know they have not been officially removed from that system yet and thus cannot have disposaldate until they are entered so its question between separate tables no nulls enforce rules in stored procedures vs combined table nulls enforce rules in check constraints understand that querying with nulls in the picture can be complex and have somewhat undefined behavior so separate tables and stored procedures seem beneficial in that sense alternatively being able to use check constraints and have the rules built into the table seems equally beneficial any thoughts thanks for reading please ask for clarification where needed update example table if they were merged and allowed nulls allow null archive device table deviceid serialnumber devicetypeid istrackedinother sectionid specialdevicecode officialoutofservicedate ooslogpath oosremarks requiressanitization sanitizemethodid sanitizelogpath sanitizedate sanitizeremarks location originalinventorydate archivedate lastupdated reasonid storagelocation archiveremarks categorycode example check istrackedinother and sectionid is null and specialdevicecode is null and officialoutofservicedate is null and ooslogpath is null and oosremarks is null or istrackedinother and sectionid is not null and specialdevicecode is not null does this seem natural way to handle this or is this design problem
21587 recently updated my machine from mac os lion to mountain lion and think it borked my postgresql installation it was installed originally via homebrew im not dba but hoping someone can tell me how to troubleshoot this am unable to connect but was able to before pre mountain lion psql rails myapp development psql could not connect to server no such file or directory is the server running locally and accepting connections on unix domain socket var pgsql socket pgsql but postgres is still clearly running ps aux grep postgres meltemi ss wed01pm postgres rails myapp development local idle meltemi ss wed12pm postgres stats collector process meltemi ss wed12pm postgres autovacuum launcher process meltemi ss wed12pm postgres wal writer process meltemi ss wed12pm postgres writer process meltemi wed12pm usr local bin postgres usr local varpostgres usr local var postgres server log and its responding to queries both to test db and the development db from local rails app user load 2ms select users from users rendered users index html haml within layouts application 3ms there appears to be no var pgsql socket directory let alone the var pgsql socket pgsql socket file mentioned above maybe the install of mountain lion wiped that out ls var grep pg drwxr postgres postgres jun pgsql socket alt how can troubleshoot this
21650 as the title says even tried select convertnumeric which also returned am using sql server
21724 assuming that the following query is legitimate and executes successfully is there any way to speed it up select foo from t1 t1 is huge 5e6 rows table with no indexes union select bar from t1 union select baz from t1 union select qux from t1 the four columns shown above are all of type nvarchar128 non sparse and nullable am using sql server thanks edit provided more info cannot add indices because it is stage table also removing the distincts totally forgot about that phil how do consolidate this into single query
21749 fairly simple question that cant seem to find an answer to im working with unions and differences and would like to perform count on the results currently im having to pipe out to file and wc the file minus for the postgres printing theres got to be way to include count select tbl1 id except select tbl2 id union tbl3 id would like to know the number of results after the set difference thanks for any input
21779 when you drag the columns folder to editor window the full list of columns are added without brackets if you drag columns individually they have brackets is there anyway to turn that off im unable to find anything in options and theyre really distraction
21837 sql server introduced the concept of contained databases where everything well mostly everything the database needs is contained within the database itself this offers big advantages when moving databases between servers would like to know then if this should be my default strategy when designing new database msdn lists several disadvantages to contained databases and the big ones are lack of support for change tracking and replication are there others if im not going to use these features is there any reason not to use contained databases
21895 im quite new to sql server would be grateful if someone can help have restored copy of huge database to sql server and tried to run some simple queries against it im trying to run select query against database table of lines this select query only has simple where clause every time run this query it fails because the system disk the partition where windows is installed run out of space this partition has only 6gb free space and dont understand why defined my tempdb to be on different drive which has more than terabytes of free space of course that my database is located on different drive too what makes my system partition run out of space is it the page file
21897 have postgresql database in one table there are three columns first name last name display name is it possible to set the default value of display name to be first name last name
21913 hope this is question with shorter answer than read page book but then if thats the real situation then hit me with it am not real dba im software developer who is realizing we need dba and yet the shop work in has zero dbas however our ms sql database design including several core stored procedures is giant mess the stored procedures are slow we suspect they have bugs but we dont even know how they are expected to work so we dont know how to fix them as start ive decided well document how its supposed to all work then well start unit testing and building up set of unit tests that help prove that the stored procedures actually do work the logic that they perform is key part of our application you could say its the crown jewels of our companys main product and the way it works is completely undocumented im looking for the specific technical documentation that professional dba might expect to have existing or might write themselves if they had to to understand giant web of stored procedures that call each other what is the usual format for documenting large stored procedure description of expected values for each in parameter ie preconditions postconditions ie for boolean parameters what changes when you turn it on or off etc how does one usually document it sql comments only external tooling that is specific to the purpose external documentation we have no sql tools other than ms sql management studio but we are wondering if there is tool that would make understanding documenting and testing our environment better maybe that is better way to ask my question what tool do need to solve our mess our goal is to be able to use the documentation we generate or whatever tools we add to our environment to help understand how the procedures are supposed to work so we can then go on to create unit test coverage for the stored procedures show the client app developers how to properly call each of these complex stored procedures unit test our stored procedures
21925 currently have tables in my db stores and products each product has its own id and each store has its own id my question on db design is how do keep track of which products each store has example store has products store has products store has products etc should have column in my stores table that has ids stored as string separated by column such as with the possibility of thousands of entries in the string should have another table that is just id storeid productid with the possibility of storesxproducts in the table
21962 my current project involves lot of database shuffling as we upgrade from sql re task hardware repeat one minor annoyance is the connection dialog in ssms re shuffling the server order to put the most recent on top given that on any particular day im connecting to db servers and want to quickly find specific one in the list this behavior is much more of hindrance than help is there way to disable this behavior having the servers listed alphabetically or in the order they were added or anything that doesnt change every time connect to server barring that is there an entirely different way to better manage connecting to specific server from list farm of them im using ssms for sql though im curious about solutions as well
21965 while trying to write query found out the hard way that sql server parses wheres in query long before parsing the selects when executing query the msdn docs say that the general logical parsing order is such that select is parsed nearly last thus resulting in no such object alias errors when trying to use column alias in other clauses there was even suggestion to allow for aliases to be used anywhere which was shot down by the microsoft team citing ansi standards compliance issues which suggests that this behavior is part of the ansi standard as programmer not dba found this behavior somewhat confusing since it seems to me that it largely defeats the purpose of having column aliases or at the very least column aliases could be made significantly more powerful if they were parsed earlier in the query execution since the only place you can actually use the aliases is in order by as programmer it seems like its missing huge opportunity for making queries more powerful convenient and dry it looks like its such glaring issue that it stands to reason then that there are other reasons for deciding that column aliases shouldnt be allowed in anything other than select and order by but what are those reasons
22002 on my development machine ive installed sql server express but it has the instance name sqlexpress want it to be the default instance im pretty sure that selected the default instance option in the setup but apparently it didnt work is there any way can change the instance name now added it seems that this is known bug in the installer it will be fixed when pcu public update comes out but theres not telling when thats gonna happen in the mean time there are two workarounds when installing choose the named instance option and enter mssqlserver install named instance then install another instance and this time the default instance option will work still the question remains is there any way to rename an instance without reinstalling because the setup was slow pain xd
22026 would it be possible to suppress all sqlcmd messages output to the cmd window am running large script and it seems to be slowed down lot with all the processed xxxx total records messages the script is over 4gb so there is large amount of data
22067 ive created user but forgotten the password mysql create user blayo identified by right which linux command line tool can encrypt the password the same way mysql does mysql select passworduser from mysql user 920018161824b14a1067a69626595e68cb8284cb blayo to be sure use the right one tool right 920018161824b14a1067a69626595e68cb8284cb
22148 select counttitle idas algodata from titles where pub id select counttitle idas binnet from titles where pub id select counttitle idas newmoon from titles where pub id the database used is pubs in sql server can use single query to show the count of records for each publisher here there are publishers in single record
22189 have query which uses three lookup tables to get all the information need need to have distinct values for one column however also need the rest of the data associated with it my sql code select acss lookup id as acss lookupid acss lookup product lookupid as acssproduct lookupid acss lookup region lookupid as acssregion lookupid acss lookup document lookupid as acssdocument lookupid product id as product id product parent productid as productparent product id product label as product label product displayheading as product displayheading product displayorder as product displayorder product display as product display product ignorenewupdate as product ignorenewupdate product directlink as product directlink product directlinkurl as product directlinkurl product shortdescription as product shortdescription product logo as product logo product thumbnail as product thumbnail product content as product content product pdf as product pdf product language lookupid as product language lookupid document id as document id document shortdescription as document shortdescription document language lookupid as document language lookupid document document note as document document note document displayheading as document displayheading from acss lookup inner join product on acss lookup product lookupid product id inner join document on acss lookup document lookupid document id order by product displayheading asc want to get all the products from this query but only want to get them once because im populating drop down menu for search application want the user to be able to select from the products that are in that table thats why only need them once is this too complicated should use more simplified approach
22250 im trying to execute the following script in sql server management studio use master go create database test1 on primary name ntest1 filename nc program files microsoft sql server mssql10 sqlexpress mssql data test1 mdf size 70656kb maxsize unlimited filegrowth 1024kb log on name ntest1 log filename nc program files microsoft sql server mssql10 sqlexpress mssql data test1 log ldf size 164672kb maxsize 2048gb filegrowth but im getting the error msg level state line create file encountered operating system error access is denied while attempting to open or create the physical file program files microsoft sql server mssql10 sqlexpress mssql data test1 mdf msg level state line create database failed some file names listed could not be created check related errors already have all role permissions for my user any ideas on whats wrong
22362 im looking for precise piece of information in database which have no knowledge about its 3rd party product they are slow on answering some questions and know the data is lying inside that db so want to do little of retro engineering given one table is it possible to have list of the names of the columns for this table for example in sqlserver its possible to dump table into reusable create statements that textually lists all the columns the table is composed of
22385 so it seems that the company who is hosting our sql servers has been having some trouble with the replication from the production server to the backup server believe some of the tables have been replicating correctly though replication is done daily after hours is there way can compare of the same tables from the backup vs from the production server to see if last nights replication worked the only way could find was to run the following query on both servers and seeing if the result matched which might mean that to tables contain the same information select checksum aggbinary checksum from select from table to compare t1 using the above code it seems that the table did replicate successfully as the checksum values are the same but im not sure how reliable this method is does anyone know of better method to check this out or if this is good way im running sql server on windows server computer thanks
22459 have database where am using inline tvfs table value functions instead of views for example might have two tables called car model and car manufacturer that im joining together inside the tvf fncarbrands these tvfs are then called by other tvfs to do further processing and reporting so might take my function fncarbrands and join to the table purchase year to form function fncarbrandhistory and so on for several layers of tvfs could probably get the same functionality using views since my inline tvfs are really just joins of tables and other tvfs how does the performance of inline tvfs written in this way compare with views
22460 would like to use code that developed in clr to be used in all of the databases on the system so that dont have to set each to trustworthy and turn clr on and keep bunch of the same code inside each one is there best way to do this from an administrative and security standpoint the clr functions are very basic like string breakers email validation url en decode base64 and etc would like only the dbo schema in each database to be able to access the functions is there any simple way to do this also am not clear if the clr dll is embedded and if move the database it tags along or do have to move the dll as well thanks
22512 would like to be able to generate random bytea fields of arbitrary length 1gb for populating test data what is the best way of doing this
22638 in sql server r2 how would determine what partition record currently lives in
22697 whenever user updates his profile we conduct set of queries like so delete from user likes where id id insert into user likes values default id interest id interest name is there way to do this all in one step there has to be way because the interest ids are all different for each user keep in mind that the data never actually has to be deleted its set that is always growing
22726 have the following customers table customer id int company name nvarchar street nvarchar city nvarchar comments nvarchar the app will only be used in part of one small country something like cities friend told me should seperate city into different table cities and use only city id in customers table personally didnt see much of benefit from it except for saving some space on customers table which seems insignificant to me in this case for the cost of creating another table he also mentioned that because have duplicate columns city foo city bar city foo few customers in the same city this is not considered normalized is this true whos right any enlightenment on the issue
22771 in microsoft sql server is it ok to shrink the log file of an online database or will it cause an interruption of service
22779 am subscriber to transactional replication subscription have no control over the publisher have read the following article about schema changes to the publication database make schema changes on publication databases however am trying to change the schema on the subscription end if possible am wondering if transactional replication supports the addition of persistent computed columns to the subscriber tables would much prefer this solution versus adding new view for every table that requires computed value
22799 we are planning to use postgresql in our application but are concerned about crash safety and recovery cant find any database crash recovery method or process in postgresql know there must be something in postgresql advice so if anyone can help me out to find this then that would be great
22803 know we can check the logins and the users that are defined using gui in sql server but am wondering how we can do this check using script ran the query below but it shows principal id which im not sure how to map to get the permission level select from sys login token so is there any built in stored proc that can list the logins and the users with their permission level thank you
22809 while rebuilding index have used option sort in tempdb on in order to avoid unnecessary growing your user database files what does it exactly means is complete process done on tempdb and does not grow mdf ldf file size for example before reindex database is in simple recovery mode the largest index size in the database is 25gb mdf size 100gb allocated and used no freespace ldf size 20gb tempdb has 200gb freespace what will be the approximate size for mdf ldf files are they grow or not
22833 would like to update my development machine to sql server but still manage some very old sql server machines wil my ssms be able to connect to those sql server machines sql server r2 works fine what have now
22909 dont know if this question better suits here or in so this is script that id like to launch the code of the function was copied from question on so mydb create or replace function truncate tablesusername in varchar returns void as declare stmt record statements cursor for select tablename from pg tables where tableowner username begin for stmt in statements loop execute truncate table quote identstmt tablename cascade end loop end language plpgsql get the following error error syntax at or near line1 query context sql statement in pl pgsql function truncate tables near line am new to postgres and pl pgsql and dont know what this error message means
22962 have about rows that im needing to update the information from column to column for example customers need their arrival time to match their departure time understand that can do this with one row and it work with nested select statement in an update statement when trying to update multiple rows though believe im getting stuck on having to have unique key identifiers this was what used to update of the records im using oracle sql update patient set discharge dt select admit dt from patient where pat seq xxxxxx where facility id and pat seq xxxxxx apologize for the confusing description not really database administrator myself any help on this would be greatly appreciated
22979 have query that is used for getting internet traffic statistics of certain ip addresses there are separate ip address fields for hosts and blocks of ips called assignments the data is stored in minute intervals the query results are grouped on the time column and the total sums in and out of these minute intervals are used to plot graph the table is called traffic and contains at the end of the month around million records show create table traffic create table traffic type enumv4 assignmentv4 hostv6 subnetv6 assignmentv6 host not null type id int11 unsigned not null time int32 unsigned not null bytesin bigint20 unsigned not null default bytesout bigint20 unsigned not null default key basic select type idtimetype engine innodb default charset latin1 select traffic time sumtraffic bytesin sumtraffic bytesout from traffic where traffic type v4 assignment and type id in between to ids265 or traffic type v4 host and type id in lot of ids and traffic time and traffic time group by traffic time order by traffic time the following is explain output for the query above id select type table type possible keys key key len ref rows extra simple traffic range basic select basic select null using where using temporary using filesort show indexes from traffic table non unique key name seq in index column name collation cardinality sub part packed null index type comment traffic basic select type id null null btree traffic basic select time null null btree traffic basic select type null null btree this query takes from seconds to minutes to complete hope can improve things using better indexes or maybe using different query but im unable to figure it out update following the advise of the helpful commentors ive created primary key and added the index traffic pk time type type id id unfortunately it turns out the cardinality of this new index is equal lower than my original index basic select and mysql still uses my original key update dropped my original index basic select and now the explain shows higher rows value but less steps in the extra fields also the query execution time went down to below minute still bit too slow but major improvement mysql show create table traffic test row table traffic test create table create table traffic test traffic id int10 unsigned not null auto increment type enumv4 assignmentv4 hostv6 subnetv6 assignmentv6 host not null type id int11 unsigned not null time int32 unsigned not null bytesin bigint20 unsigned not null default bytesout bigint20 unsigned not null default primary key timetypetype idtraffic id key traffic id idx traffic id engine innodb auto increment default charset latin1 the indexes on the table mysql show index from traffic table non unique key name seq in index column name collation cardinality sub part packed null index type comment traffic test primary time null null btree traffic test primary type null null btree traffic test primary type id null null btree traffic test primary traffic id null null btree traffic test traffic id idx traffic id null null btree also simplified the query by not using the or select sql no cache traffic time sumtraffic bytesin sumtraffic bytesout from traffic where traffic type like v4 host and type id in and traffic time and traffic time group by traffic time asc old execution of this query rows in set min sec new executiontime rows in set sec explain output id select type table type possible keys key key len ref rows extra simple traffic range primary primary null using where the rows value is still quite high think can improve on this by switching the order of type and type id in the index since there are only types possible and many more type ids is this correct assumption
22989 sql select kill from tbl pvporderview problem is that end up with incorrect syntax near the keyword kill because kill is sql command any way to bypass it cant change the column name because its used by the software lot and cant change the software thats using the database so it simply fails if use sqlserv to select data from that column or wont help the complete statement would be sql select serialkill from tbl pvporderview where kill order by kill desc
23036 im attempting to import csv file into database via the copy command however get the what seems common error that need to be superuser and that should use copy instead however when using copy get syntax error error syntax error at or near line copy with the caret pointing to heres my query copy tablenamecolumn2 column3 column4 column5 from home uploads data csv with delimiter csv header tried both copy and copy the first giving me superuser error the latter giving me that syntax error any idea on how to fix it make it work im executing the command via mypgadmins sql input field the only other question have is concerning the importing of the columns via tablenamecolumn2 column3 and so on is that the correct syntax for that
23041 id like to be able to predict whether delete will run into constraint violation without actually performing the delete what are my options for doing this is there simple way to do dry run of delete
23053 am migrating my server application from the existing system to another system unfortunately the existing system is also the database server it has data stored is it possible to copy this data to the other system have copy of all the table schemas have no idea on how to proceed am using postgresql
23058 lets say we have principal server mirroring server the one that serves as the failover backup and witness server does it matter if were to bring the mirroring server not the principal offline for short while five minutes lets say is this going to break the mirror is there anything need to do such as turning the mirroring off temporarily on the mirrored databases before take the server offline there are plenty of resources online to explain what needs to happen when the principal goes down but nothing can find about this scenario
23068 count records with queries like select countcol1 from table1 where col1 like something select countcol1 from table1 where col1 like another select countcol1 from table1 where col1 like word for each count mysql needs to walk thoughout the table and this is big problem if having long table and numerous queries wonder if there is way to make all counts in one query in this case when mysql walks over each row it will process all counts and no need to scanning the entire table over and over again
23124 lets imagine web site that is directory of people for each person there may be profile photo and biography ill admit my sql queries could be better but in general what would be faster and use less processing power to check if file exists and then open it or check against mysql to see if bio exists and display it im pretty sure in the above case the filesystem will smoke the mysql database what if make the database read only delimited txt file whats faster in this case is there certain point where if the txt file has too many records its better to use mysql
23129 in mysql we can create queries with or without the backtick symbol example select from test select from test both works fine in mysql console is there any technical difference between them is there any benefit using over over simple queries
23135 if apply static data in where clause is it faster then applying sub query example the query returns records as result set select startdate enddate from test where startdate is faster then select startdate enddate from test where startdate select startdate from test2 limit or they are same in any case sub query executes each time comparison perform by query with each record or only for time
23147 have to do some project and need to estimate the db size im going to use oracle in the project because we will have lot of transactions and data but in the presentation need to specify in years how much expect the db to grow so have pdf with the way of calculating the row size and another stuff but for sql server want to do it for oracle the pdf is an estimation formula without having the real db so cant do queries to the db to check the actual size thats what ive seen in other posts here when googled the formula for sql server row is row size fixed data size variable data size null bitmap this formula works in oracle too also is the size of the header of row dont know how to translate the meaning in english so have to do this in oracle what will be the value in oracle the other things think think ive solved them
23163 have correlated subquery like this from bol select distinct lastname firstname businessentityid from person person as join humanresources employee as on businessentityid businessentityid where in select bonus from sales salesperson sp where businessentityid sp businessentityid go when rewrite this query using joins select lastname firstname businessentityid bonus from person person as inner join humanresources employee as on businessentityid businessentityid inner join sales salesperson as on businessentityid businessentityid where bonus and look the actual execution plan it looks exactly the same in both queries why was thinking that correlated subquery is much slower because of the nested loop and the execution plan looks different is it because there is not much data in these tables
23192 have table which stores the sequence counter of data received from devices out in the field at any rate these sequences need to be in order within configurable time span but can come into the system out of order if device is reset then its sequence number is set back to create table dbo tapgapdetail id bigint not null fk to another table deviceesn varchar collate sql latin1 general cp1 ci as not null tapdateutc datetime not null date event occurred on device createddateutc datetime not null counter int not null create clustered index cx tapgapdetail on dbo tapgapdetail deviceesn createddateutc counter on primary go ive tried to switch the order the cx by deviceesn and createddateutc but on my system it doesnt seem to make much of difference in the io there are millions of rows in this table example insert would be insert into tapgapdetail 00am 10am insert into tapgapdetail 05am 15am out of order by insert date insert into tapgapdetail 00am 05am back in order insert into tapgapdetail 00am 05am missing insert into tapgapdetail 00am 05am in order outside tolerance insert into tapgapdetail 00am 05am device reset the report needs to report that gap has occurred once for this data there is also detail but one step at time want to get it to run in less than seconds but seems to take at least minutes on my local have stat io if needed so the proc created to get the gaps is like so create proc dbo getvalidatortapgapsummary validator varchar100 filterdateutc datetime tolerancehours int as temp table select rowid row number over order by deviceesn createddateutc counter deviceesn tapdateutc createddateutc counter into taps from tapgapdetail where createddateutc filterdateutc order by cx create clustered index cx1 on tapsrowid results select deviceesn as validator sum case they are in sequence when t2 counter counter then reset has occured when t2 counter counter then gap exists find the difference else t2 counter counter end as tapgaps case gets the last tap date per validator when maxt tapdateutc maxt2 tapdateutc then maxt tapdateutc else maxt2 tapdateutc end as maxtapdate case gets the last tap date per validator when maxt createddateutc maxt2 createddateutc then maxt createddateutc else maxt2 createddateutc end as maxcreateddate from taps inner join taps t2 on deviceesn t2 deviceesn and t2 rowid rowid where t2 counter counter and t2 counter counter and createddateutc filterdateutc and t2 createddateutc filterdateutc and deviceesn validator or validator and not exists edge case for when there is gap at the end tried with left join and stats are the same so this is easier to read think select top null from tapgapdetail tgd where tgd deviceesn deviceesn and counter tgd counter and tgd createddateutc dateaddday tolerancehours createddateutc and tgd createddateutc dateaddday tolerancehours createddateutc group by deviceesn order by maxcreateddate desc deviceesn go an example statio would be table tapgapdetail scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected rows affected table taps scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected table taps scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table tapgapdetail scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads ive tried various indexes and what not on the temp table but suspect that even just filling is taking too long any suggestions are appreciated
23203 im trying to store lot of large numbers that truncate when stored as int values unfortunately they are too large for this type im thinking about using base36 encoding to accomplish this are there any other ways to go about fixing this problem
23259 if remember correctly last time microsoft sqlcmd require each command to end with and hit enter key to execute the command however currently we need to type the go each time to execute the command so are there other alternative such that when we hit enter key after ending the command with it will execute the command
23265 mysqls show grants shows the permissions of the current user is there way to log in as root and show the permissions of all users
23280 im new to oracle databases if have understood correctly materialized view is view which result set is saved as physical table in the database and this view table is refreshed bases on some parameter if view is saved as physical table why not store the data into table in the first place so what is the benefit of using materialized view instead of table
23305 have been monitoring file growth via the data collector in sql server r2 for two weeks the database is growing consistently at around 35mb day the db has not yet hit the initial size of gb the db files auto growth is set to 5mb and would like to try different approach so am looking for suggestions and or comments there is tuning task that runs every week on sunday night at am the task will check database integrity shrink the log file this is ok because logging mode is simple shrink database reorganize index rebuild index update statistics clean up history would like to add two more steps to the weekly tuning plan grow the database file by mb if the used space reaches certain threshold or total size grow the log file by mb after the shrink if the used space reaches certain threshold of total size by placing the growth burden in offline hours hope to gain performance by reducing the number of auto growth events during heavy loads have two questions relating to auto growing files the best place to put the file grow steps would be prior to the current steps or after if use the alter database modify file to grow the file then how can determine if spaceusedinfile totalfilespace allowancethreshold
23335 im having problems with database can run basic queries albeit much slower than normal when attempt to view the hierarchy trees for tables views or procedures in ssms object explorer get lock request time out period exceeded my ssrs reports that run on objects in this database are no longer completing jobs associated with procedures stored on this database also do not run tried using sp who2 to find and kill all connections on the database however this has not solved the problem what is going on here how can resolve this
23355 its possible when using the pgadmin or plsql to get hold of query plan for sql statement executed inside user defined function udf using explain so how do get hold of the query plan for particular invocation of udf see the udf abstracted away into single operation in pgadmin have looked at documentation but couldnt find anything currently im pulling out the statements and running them manually but this isnt going to cut it for large queries for example consider the udf below this udf even though it has the ability to print out its query string will not work with copy paste as it has local created temporary table which does not exist when you paste and execute it create or replace function get paginated search results forum id integer query character varying from date timestamp without time zone default null to date timestamp without time zone default null in categories integer default returns setof post result entry as declare join string character varying from where date character varying to where date character varying query string character varying begin if not from date is null then from where date and fp posted at from date end if if not to date is null then to where date and fp posted at to date end if create local temp table un catid on commit drop as select from unnestin categories if in categories then join string inner join forum topics ft on fp topic id ft id inner join un cat uc on uc id ft category id end if query string select indexposted atpost textnamejoin datequotes from forum posts fp inner join forum user fu on fu forum id fp forum id and fu id fp user id join string where fu forum id forum id and to tsvectorenglishfp post text to tsqueryenglish query from where date to where date raise notice query string return query execute query string end language plpgsql
23371 was just looking at post on stackoverflow where aaron bertrand proposes using cte instead of numbers table which is an elegant way of performing the task at hand my question is why does the first line of the cte begin with semi colon with as select top from select row number over order by s1 object id from sys all objects as s1 cross join sys all objects as s2 as order by select from order by look ma no gaps is this to ensure the with statement does not get parsed into previous select or something see nothing in sql server bol about using semi colon prior to the with
23399 ive got tdf sql server profiler trace template someone wants me to run but the template is targeting ss r2 while my ssms is r2 the server need to trace is ss when attempting to trace the server the server type is locked generated from the actual server so cant just select the template need while its marked as different ss version how can change which server type the trade template is targeting ive tried file templates edit templates but there doesnt seem to be an option to change this is it possible to change the target server version or does the whole trace have to be made again from scratch
23456 want to force the appdomain being used by sqlclr to be reset how can do that besides restarting the sql server instance
23465 have query that is currently taking an average of 2500ms to complete my table is very narrow but there are million rows what options do have to improve performance or is this as good as it gets the query select top from cia wiz dbo heartbeats where dateentered between and the table create table dbo heartbeats id int identity11 not null deviceid int not null ispup bit not null iswebup bit not null ispingup bit not null dateentered datetime not null constraint pk heartbeats primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary the index create nonclustered index commonqueryindex on dbo heartbeats dateentered asc deviceid asc with pad index off statistics norecompute off sort in tempdb off ignore dup key off drop existing off online off allow row locks on allow page locks on on primary would adding additional indexes help if so what would they look like the current performance is acceptable because the query is only run occasionally but im wondering as learning exercise is there anything can do to make this faster update when change the query to use force index hint the query executes in 50ms select top from cia wiz dbo heartbeats withindexcommonqueryindex where dateentered between and adding correctly selective deviceid clause also hits the 50ms range select top from cia wiz dbo heartbeats where dateentered between and and deviceid if add order by dateentered deviceid to the original query am in the 50ms range select top from cia wiz dbo heartbeats where dateentered between and order by dateentered deviceid these all use the index was expecting commonqueryindex so suppose my question is now is there way to force this index to be used on queries like this or is the size of my table throwing off the optimizer too much and must just use an order by or hint
23467 have the following procedure sql server r2 create procedure usp savecompanyuserdata companyid bigint userid bigint datatable tt couserdata readonly as begin set nocount xact abort on merge companyuser with holdlock as using select companyid as companyid userid as userid mykey myvalue from datatable as newdata on companyid newdata companyid and userid newdata userid and mykey newdata mykey when not matched then insert companyid userid mykey myvalue values companyid userid newdata mykey newdata myvalue end companyid userid mykey form the composite key for the target table companyid is foreign key to parent table also there is non clustered index on companyid asc userid asc it is called from many different threads and am consistently getting deadlocks between different processes calling this same statement my understanding was that the with holdlock was necessary to prevent insert update race condition errors assume that two different threads are locking rows or pages in different orders when they are validating the constraints and thus are deadlocking is this correct assumption what is the best way to resolve this situation no deadlocks minimum impact on multi threaded performance if you view the image in new tab it is readable sorry for the small size there are at most rows are in the datatable have traced back through the code and cannot see anywhere that we start transaction here the foreign key is set up to cascade only on delete and there were no deletions from the parent table
23475 am using postgresql but figure most of the top end dbs must have some similar capabilities and moreover that solutions for them may inspire solutions for me so dont consider this postgresql specific know am not the first to try to solve this problem so figure it is worth asking here but am trying to evaluate the costs of modelling accounting data such that every transaction is fundamentally balanced the accounting data is append only the overall constraint written in pseudo code here might look roughly like create table journal entry id bigserial not null unique artificial candidate key journal type id int references journal typeid reference text source document identifier unique per journal date posted date not null primary key journal type id reference create table journal line entry id bigint references journal entryid account id int not null references accountid amount numeric not null line id bigserial not null unique check sumamount over partition by entry id this wont work obviously such check constraint will never work it operates per row and might check over the entire db so it will always fail and be slow doing it so my question is what is the best way to model this constraint have basically looked at two ideas so far wondering if these are the only ones or if someone has better way other than leave it to the app level or stored proc could borrow page from the accounting worlds concept of the difference between book of original entry and book of final entry general journal vs general ledger in this regard could model this as an array of journal lines attached to the journal entry enforce the constraint on the array in postgresql terms select sumamount from unnestje line items trigger could expand and save these to line items table where individual column constraints could more easily be enforced and where indexes etc could be more useful this is the direction am leaning could try to code constraint trigger that would enforce this per transaction with the idea that the sum of series of 0s will always be am weighing these against the current approach of enforcing the logic in stored procedure the complexity cost is being weighed against the idea that mathematical proof of constraints are superior to unit tests the major drawback of above is that types as tuples is one of those areas in postgresql where one runs into inconsistent behavior and changes in assumptions regularly and so would even hope that behavior in this area might change over time designing future safe version is not so easy are there other ways to solve this problem that will scale up to millions of records in each table am missing something is there tradeoff have missed in response to craigs point below about versions at minimum this will have to run on postgresql and higher maybe and higher but probably we can go with straight
23509 as follow up to this question about increasing query performance id like to know if there is way to make my index used by default this query runs in about seconds select top from cia wiz dbo heartbeats where dateentered between and this one runs in about 33ms select top from cia wiz dbo heartbeats where dateentered between and order by dateentered deviceid there is clustered index on the id field pk and there is non clustered index on dateentered deviceid the first query uses the clustered index the second query uses my non clustered index my question is two parts why since both queries have where clause on the dateentered field does the server use the clustered index on the first but not the second how can make the non clustered index be used by default on this query even without the orderby or why would not want that behavior
23532 have been trying to compare the features of the sql server express oracle xe and db2 express but find it difficult to compile all the differences into one place especially the current differences as this is constantly shifting battle field anyone have this information on hand or know of someone who tracks this to the best of my knowledge db2 express cores mem 16g 4g before size terabytes of user data per database oracle express edition 11g cores mem 1g size 11g sql server express cores lesser of socket or cores mem 1g size 10g other than these obvious measures what am missing know could go with open source as well and mysql and postgresql are both certainly viable solutions but for the sake of argument lets limit it to just these options in my view db2 is the clear cut winner another article on this subject can be found here
23547 have found that when table has both clustered and non clustered indexes on different columns the leaf level non clustered pages instead of pointing to the data row point to the node of the clustered index from where another search is instituted to find the data row what is the point of this extra level of indirection if the clustered index has say levels then the indirection from the nci leaf page to the ci root would have to traverse through these levels to reach the data why not store the normal rid in the nci leaf page so that we can access the data at once without going through the ci index structure
23566 we need to write the select query results to csv file how can it be done using sql in sql server r2 know that it can be done in ssis but for some reasons we dont have this option tried to use the suggested proc in the article below but when run the proc sql complains that cant run sys sp oacreate and sys sp oadestroy which are called in this proc do you know how we can turn on these components or know any better way to write to file using sql thanks in advance
23637 found myself writing the following select yes where existsselect from foo where val and not existsselect from foo where val and wondering if there is more concise way without sacrificing too much readability found one way which am posting as an answer but im not entirely happy with it and would be very interested in alternatives in this case val is unique within foo there are no duplicates
23704 for troubleshooting purposes would like to be able to check if client can connect to sql server instance independent of the application that possibly cant connect to the sql server is there an easy way that means not having to install 3rd party software to do this using the default windows system tools perhaps using scripts or network applications
23718 we have an instance in oracle 10g only one schema is used for web application internet facing which is only user for reading information no adding or modifying data the rest of schemas are not used archived redo logs is activated what is strange for us is that archived files are generated every day including sundays and holidays when nobody comes to work thought archived redo logs are generated when inserting deleting or updating data never because of select statements is that correct
23747 how do you test mysql credentials from the command line on linux server
23772 cant seem to find the option to set the server property that will enable compression on sql database backup to be on by default so dont have to manually set it every time can anyone point me at that please
23782 one of our products supports both oracle and sql server as database backend we have customer who wishes to switch from an oracle backend to microsoft sql server which isnt typical transition for us what is the easiest way to get all the data from the entire oracle schema into the sql server database the schema only contains plain old tables and nothing fancy there might be one or two stored procedures that well have no problem migrating by hand could use oracles sqldeveloper to export the table data as create and insert statements but these wont match the syntax used on sql server and am not looking forward to having to manually fix the syntax errors
23786 what is the best practice for data obfuscation in sql server wed like to use masked production data in our uat system if we want to do it quickly and with higher level of obsfucation what approach should be taken im thinking about character scrambling for peoples given name and family name but how should create function myself or there is any predefined functions available to use dont want to spend time re inventing the wheel how about for date fields for example should date of birth be randomly picked from the whole table and assigned to record or there is better way of doing that
23794 can anybody please summarize the differences between http www postgresql org docs static xfunc sql html and http www postgresql org docs static plpgsql html main points conceptional differences given problem family convenience of use political issues
23879 have postgresql table select is very slow whereas select id is nice and quick think it may be that the size of the row is very large and its taking while to transport or it may be some other factor need all of the fields or nearly all of them so selecting just subset isnt quick fix selecting the fields that want is still slow heres my table schema minus the names integer not null default nextvalcore page id seq regclass character varying255 not null character varying64 not null text default text character varying255 integer not null default text default text text timestamp with time zone integer timestamp with time zone integer the size of the text field may be any size but still no more than few kilobytes in the worst case questions is there anything about this that screams crazy inefficient is there way to measure page size at the postgres command line to help me debug this
23908 suppose need to encrypt certain table fields of mysql database additionally need to search some of those fields did encrypt how would one search those fields anyway decrypting each record step by step is no option suppose have multiple of thousands of records it would take too much time and space to decrypt each record and check if each single record matches the search update adding further details to the database schema would be ok since im about to implement new application furthermore need to extend applications currently running in production but even for those application adding further details would be ok update encryption is the kernel of this question access restrictions as proposed by some answers already apply but do not fit the formal requirement to encrypt data this formal requirement is not payment card industry data security standard pci
23983 have created table testtable inside the database testbase that have the following structure product no int not null product name varchar30 not null price money null expire date date null expire time time7 null which used the microsoft sql server management studio created stored procedure testtable pricesmaller as follows use testbase go create procedure testtable pricesmaller pricelimit money as select from testtable where price pricelimit go and are able to view the stored procedures on the object explorer of the microsoft sql server management studio it is listed in the following tree structure of the object explorer databases testbase tables dbo testtable programmability stored procedures dbo testtable pricesmaller find it very strange when receive the following error could not find the stored procedure dbo testtable pricesmaller when execute the following sql statement execute dbo testtable pricesmaller what could it be missing
24014 have innodb table idtimes mysql log with columns id int11 not null time int20 not null with compound unique key unique key id time idtime so there can be multiple timestamps per id and multiple ids per timestamp im trying to set up query where get all entries plus the next greater time for each entry if it exists so it should return id time nexttime null null right now am so far select id time time from idtimes as left join idtimes as on id id where time time order by id asc time asc but of course this returns all rows with time time and not only the first one guess ill need subselect like select outer id outer time select time from idtimes where id outer id and time outer time order by time asc limit from idtimes as outer order by outer id asc outer time asc but dont know how to refer to the current time know the above is not valid sql how do do this with single query and id prefer not to use variables that depend on stepping though the table one row at time and remembering the last value
24091 my senior dba told me that sql query execution by default doesnt lock the table was having some issues with my sql server reporting services ssrs report which seems to be getting some issues with locking and getting some errors did some googling but fell short of finding anything do ssrs reports lock the tables that are being queried is there any msdn documentation that document this behavior specifically
24165 when connecting to sql server r2 from net client application on different server in the same lan one can set three different network protocols tcp named pipes dont set anything in the connection string and use the default what is best practice what to choose additional information both tcp and named pipes are enabled both on the server and on the client the application is using database mirroring client and server communicate over fast lan we are investigating this because we have rare and spurious connectivity and timeout problems but regardless of that id like to know the best practice there is an article on this subject on msdn but it is very generic and vague it does not advise or recommend anything useful
24280 am trying to duplicate the business logic embodied an intranet web application in the database so that other databases can access it and work under the same rules this rule seems difficult to implement without using hacks create table case stage id number9 primary key not null stage id number9 not null case phase id number9 not null date created timestamp6 default current timestamp not null end reason id number9 previous case stage id number9 current number1 not null date closed timestamp6 default null and create table case recommendation case id number9 not null recommendation id number9 not null order number9 not null date created timestamp6 default current timestamp not null case stage id number9 not null alter table case recommendation add constraint sys c00000 primary key case id recommendation id the business logic can be summed up as when inserting into case stage if case stage stage id then case stage previous stage id must be found in case recommendation case stage id can this logic be embodied in check constraint or is an ugly trigger the only way edit for all values of case stage stage id the value for previous stage id must be found in case stage id the application does not allow deletions from case recommendation once it is no longer current ie when the value of case stage current is this stage is closed and can no longer be changed when this is the stage or row that is active and can be changed now edit using all the excellent ideas and comments here is working solution to this problem create materialized view log on case stage tablespace users storage buffer pool default nocache logging noparallel with rowid create materialized view log on case recommendation tablespace users storage buffer pool default nocache logging noparallel with rowid create materialized view case recommendation mv refresh fast on commit as select cr rowid cr rowid necessary for fast refresh cs rowid cs rowid necessary for fast refresh cr case id cs stage id cr recommendation id cr case stage id cs previous case stage id from case recommendation cr case stage cs where cs previous case stage id cr case stage id and cs previous case stage id is not null and extract year from cs date created covers non conforming legacy data and cr recommendation id is null and cs stage id this last line excludes everything but problem cases due to the outer join alter table case recommendation mv add constraint case recommendation ck check previous case stage id is not null and case stage id is not null when inserting stage using existing packages without recommendation the error was ora error in materialized view refresh path ora check constraint appbase case recommendation mv c01 violated ora at line job done not what materialized view was intended for but better than trigger
24326 am performing parallel testing we have upgraded the legacy batch jobs to new framework which we need to test against the existing batch jobs in the existing setup have web application which creates or updates records based on user input and these batch jobs will process the records the plan is to have databases primary and secondary with the existing batch job connected to the primary database server and the new batch job connected to the secondary database server the web application will populate data to both primary and secondary databases would like to perform replication for all the tables in the database however would only like to replicate any dml changes done by the specific database account used by the web application is that possible
24386 hi im trying to create login role user with fairly minimal ability to modify the database they should be able to select delete create triggers insert and update would then like to have multiple accounts which are members of user so they have the same permissions but can log edits for individual users im using pgadmin for my database design and my program accesses this with qtsql the error message get is caught by qt when trying to select some data as test when run the same code as the database owner and superuser have no problems bool run queryconst qstring qsqldatabase db qsqldatabase database default qsqlquery querydb query execq if query isactive qmessagebox warning0 qobject tr database error query lasterror text return false while query next qstring title query value0 tostring std cerr qprintabletitle std endl return true run query select forename from contacts error permission denied for relation contacts qpsql unable to create query so far ive created the role user role user drop role user create role user login encrypted password md54d45974e13472b5a0be3533de4666414 nosuperuser inherit nocreatedb nocreaterole noreplication comment on role user is low level user as this gave the previous problems tried adding permissions for public for the database but this didnt work either now ive no idea where to look for solution any help is much appreciated thanks
24410 have two tables one is paymentdetail and another is ledger here want to prioritize paymodetype in manner that cash should be considered first for ra and then fundtype then credit card then dd and then cheque want that amount received in cash should first be considered for then and lastly for fundtype another example here is sqlfiddle as well can it be solved by sql only dont want to use cursor edit have updated the sqlfiddle with what have tried am trying to use join due to which multiple rows are obtained in my result with respect to payment detail want to span amount of one row of payment detail over ledger then second row should be used how can achieve that result of current sqlfiddle is
24496 one of the most perplexing issues with which ive had to deal has to do with stored procedure groups given stored procedure usp dosomethingawesome can create that proc in another group by calling it usp dosomethingawesome discovered this when troubleshooting some replication issues publisher sql ent dist sub r2 ent that arose with some of the system generated insert update and delete replication stored procedures what is the purpose thought behind having this grouping ability
24507 have database that an application connects to using db owner permissions how do effectively determine the minimum set of requirements actually needed by this user application to run without causing service interruption ie without trial and error
24518 lets consider the following example from the start of psql script db to run on truncate the most important table tried to avoid similarities to anything that exists out there now if it is run this by the command psql connection details db to run on dev database then it just runs and the user is happy but what if she decides to specify db to run on production database lets assume that this can happen just like people run rm rf dont try this at home ocassionally hopefully there is fresh backup of that table so the question arises how to check the variables passed to script and stop further processing based on their value
24531 is there way to create an index in mysql if it does not exist mysql does not support the obvious format create index if not exists index name on tablecolumn error you have an error in your sql syntax mysql version mysql is but think that mysql lacks the create index if not exist ability in all of its versions whats the right way to create an index only if it does not already exist in mysql
24583 here is structure of both tables category table create table category catid bigint20 not null auto increment title varchar60 not null created at timestamp not null default current timestamp primary key catid engine innodb auto increment default charset latin1 and the second table projects create table projects id int11 not null auto increment catid int11 not null industryid int11 not null dsid int11 not null countryid int11 not null title varchar255 not null primary key id key catid catid key industryid industryid key dsid dsid key countryid countryid engine innodb default charset latin1 now to add fk to tables here is the query but keep getting the error cant create table errno alter table projects add constraint fk catid foreign key projects catid references category catid
24617 im fixing performance issues on multistatement stored procedure in sql server want to know which parts should spend time on understand from how do read query cost and is it always percentage that even when ssms is told to include actual execution plan the query cost relative to the batch figures is still based on cost estimates which can be far off actuals understand from measuring query performance execution plan query cost vs time taken that can surround invocation of the stored procedure with set statistics time statements and will then get list like this in the messages pane sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms etc sql server execution times cpu time ms elapsed time ms with one output message for each statement can easily though not conveniently associate the time statistics output with the statement by statement execution plans in the execution plan pane by counting them the fourth sql server execution times message output corresponds to query in the execution plan pane and so on but is there better way
24711 note work with mssql but guess its valid for many others db engines have this table users userid user countryid user user user user user user and this table countries countryid country mx usa can england as you can see every user belongs to country if want to know all the different countries where have at least one user on the users table right now do this query select distinct country from users inner join countries on users countryid countries countryid and achieve the next result set can england mx usa which is indeed all the different countries where have at least one user on muy table users my doubt is is possible to achieve the above result set with out using distinct mean only using joins and conditions here its de ddl scripts use test go object table dbo users script date set ansi nulls on go set quoted identifier on go create table dbo users userid int null user nvarchar null countryid int null on primary go insert dbo users userid user countryid values nuser insert dbo users userid user countryid values nuser insert dbo users userid user countryid values nuser insert dbo users userid user countryid values nuser insert dbo users userid user countryid values nuser insert dbo users userid user countryid values nuser object table dbo countries script date set ansi nulls on go set quoted identifier on go create table dbo countries countryid int null country nvarchar null on primary go insert dbo countries countryid country values nmx insert dbo countries countryid country values nusa insert dbo countries countryid country values ncan insert dbo countries countryid country values nengland insert dbo countries countryid country values nbrazil
24714 am writing some dynamic sql to identify and perhaps if im feeling crazy enough automatically convert my nonclustered indexes into clustered indexes the line order by desc in the sql below is designed to output drop index statements before alter table statements in order to drop the nonclustered index first and then add clustered index had to add the desc after column to get the drop first followed by the alter this is backwards unless im losing it declare server nvarcharmax declare database nvarcharmax declare cmd nvarcharmax declare indextype int set indextype is clustered is nonclustered set server myserver set database mydatabase set cmd declare cmd nvarcharmax set cmd set nocount on declare indexinfo table tablename nvarchar255 indexname nvarchar255 indexcolumnname nvarchar255 insert into indexinfo tablename indexname indexcolumnname select name as tablename name as indexname name as indexcolumnname create date ic from sys tables left join sys indexes on object id object id left join sys index columns ic on object id ic object id and index id ic index id left join sys columns on object id object id and ic column id column id where is primary key and type cast indextype as nvarcharmax order by create date desc declare t1 nvarcharmax declare t2 nvarcharmax declare t3 nvarcharmax declare cmd nvarcharmax declare cur cursor for select tablename indexname as execorder drop index indexname on tablename from indexinfo union all select tablename indexname as execorder alter table tablename add constraint pk tablename indexcolumnname primary key clustered indexcolumnname from indexinfo order by desc open cur fetch next from cur into t1 t2 t3 cmd while fetch status begin print cmd fetch next from cur into t1 t2 t3 cmd end close cur deallocate cur exec server database sys sp executesql cmd print cmd
24748 learned about normalization recently and understand how important it is when implementing new schema how can check if my database is 2nf or 3nf compliant manual review is sure option but im looking for an automated tool here im not looking for point and click tool more something that would highlight possible optimizations to make table 3nf compliant guess it might use statistics based on good sample data and or semantic analysis of columns names
24758 when you create your primary key and foreign key constraints in create table script there are named constraints that are created such as fk recentlyvi scid 2764765d are these constraints predictable ie if you run the same creation script on another server will the constraint name be the same ask because with entity framework when you have multiple references to secondary table you get properties like foreign foreign1 foreign2 etc and sometimes when re generating the entity model the order is different ive come up with the following to work around this but want to know if the default constraint names will work even though im now using named constraints my workaround is included below if need this again in the future may refactor out getting the property of an entity based on the foreign key and object type name private contact getcontactmatchforforeignkeystring foreignkeyname var props typeoforder getproperties whereprop attribute isdefinedprop typeofedmrelationshipnavigationpropertyattribute foreach var prop in props var attrs prop getcustomattributestypeofedmrelationshipnavigationpropertyattribute true foreach var attr in attrs var edmrelationshipnavigationpropertyattributeattr if relationshipname foreignkeyname targetrolename contact return contactprop getvaluethis null return null private void setcontactmatchforforeignkeystring foreignkeyname contact value var props typeofcontact getproperties whereprop attribute isdefinedprop typeofedmrelationshipnavigationpropertyattribute foreach var prop in props var attrs prop getcustomattributestypeofedmrelationshipnavigationpropertyattribute true foreach var attr in attrs var edmrelationshipnavigationpropertyattributeattr if relationshipname foreignkeyname targetrolename contact prop setvaluethis value null return public contact purchaser get return getcontactmatchforforeignkey fk set setcontactmatchforforeignkey fk value public contact seller get return getcontactmatchforforeignkey fk set setcontactmatchforforeignkey fk value
24774 installed postgresql on ec2 machine and now want to change the password of user postgres do sudo postgres psql psql type help for help postgres alter user postgres with password newpasswd alter role then exit the shell and try to login with new password psql postgres password for user postgres psql fatal peer authentication failed for user postgres my postgresql version is psql version psql postgresql contains support for command line editing what is that am doing wrong thank you update made changes in pg hba conf and this is how it looks now database administrative login by unix domain socket local all postgres peer type database user address method local is for unix domain socket connections only local all all peer local all all md5 ipv4 local connections host all all md5 ipv6 local connections host all all md5 then restarted the postgres sudo etc init postgresql restart restarting postgresql database server ok tried logging in again but failed psql postgres password for user postgres psql fatal peer authentication failed for user postgres
24832 for moderately complex query am trying to optimize noticed that removing the top clause changes the execution plan would have guessed that when query includes top the database engine would run the query ignoring the the top clause and then at the end just shrink that result set down to the number of rows that was requested the graphical execution plan seems to indicate this is the case top is the last step but it appears there is more going on my question is how and why does top clause impact the execution plan of query here is simplified version of what is going on in my case the query is matching rows from two tables and without the top clause the optimizer estimates there will be 19k rows from table and 46k rows from table the actual number of rows returned is 16k for and 13k for hash match is used to join these two results sets for total of rows then sort is applied this query happens very quickly when add top the optimizer does not use hash match instead it first sorts the results from table same estimate actual of 19k 16k and does nested loop against table the estimated number of rows for table is now and the strange thing is that the top directly affects the estimated number of executions index seek against it appears to always be 2n or in my case this estimate changes accordingly if change top of course since this is nested join the actual number of executions is 16k the number of rows from table and this slows down the query the actual scenario is bit more complex but this captures the basic idea behavior both tables are searched using index seeks this is sql server r2 enterprise edition
24883 have some alter table statements that am running not all of them work they are the result of running sql data compare and want to group them in some transactions and roll back the statements if something goes wrong is this possible or is it only data that can be rolled back
24894 an address can belong to either the mailing or billing categories when user adds an address to table should the address category be an independent integer value set by constant somewhere in the source code or should it be foreign key to table that lists the categories available im concerned about performance and best practice the chances of adding or removing categories are slim to none
24937 im new to oracle databases have used to sql server and the description fields ms description for tables and columns for documentation purposes is there an equivalent for oracle what is the best practice for documenting oracle database
24991 so was asking in chat about how secure sql server backup files are im told that if an attacker has access to the bak file unencrypted that they can have access to the data so lets look at this scenario open symmetric key mysymetrickey decryption by certificate mycertificate notice there is no password here and then we encrypt our tables by update tbl1 set namepass encryptbykeykey guidmysymetrickeyname go now lets say hacker got my bak file all he has to do in order to view the data in his own computer sql server is select convert nvarcharmax decryptbykeynamepass from tbl1 would he still be able to access the data
25047 how should rely on sql server database engine tuning advisor for creating indexes and statistics find more often than not it generates overly complex indexes that think would be more costly in index maintenance than the benefit of having this index if cannot solely rely on database engine tuning advisor is there straightforward method book an article perhaps the missing index dmvs are not particularly useful because am trying to think of indexes for brand new tables
25103 want to be able to script the schema of given database into sql file from the windows command prompt basically want to execute the generate scripts feature of management studio programmatically know this is possible using net and smo but is there something built in to do this background to this question this is meant as simple auditing instrument we want to capture the schema every night any primitive low tech solution works for us
25124 in traditional sql server cluster when failover occurs all clients that are connected to the sql server failed instance lose connections and each client must re establish new connection to failover cluster instance does alwayson availability group mitigate this problem is failover in case of sql server alwayson availability group transparent to clients connecting to the sql server
25161 what are the cases when oracle advanced queuing is the preferred mechanism for implementing functional requirements for example money transfer from bank account to bank account might theoretically be considered as two different operations and might be implemented separately first enqueue money transfer from account update then enqueue money trasfer to account update however its obvious it cant be done like that because those two operation should be done in one consistent operation in transaction maybe advanced queuing should be only considered when developing stored procedures functions that perform some logic that is both done internally by doing dml operations and calling other local stored proc fnc and externally by calling some webservices when using such calls to webservices we cant wrap it all into consistent transaction so the only way is to use some queuing mechanism any real life detailed examples would be appreciated what im curious about is not the details of the technology itself but rather what are the real life cases of using such messaging because havent done that before like why do need to pass some data message payload around
25177 according to this site http www dbspecialists com oracle11glinux html oracle is only supported on the following linux distributions oracle enterprise linux red hat suse asianux is there any technical reason that oracle would perform poorly on debian based install ubuntu or is this more about dealing with oracle from an enterprise support standpoint
25226 are there any tools out there comparable to ssms tools pack the licensing per machine for any number of machines for months leaves lot to be desired and wasnt sure of what other options are available for example one thing that really miss is the save every query you run when tinkering and researching its been invaluable to have running backup of different versions of queries as run them or when realize dont have backup of query was working on months ago clarification sql server management studios have no official addon support but there are some tools out there ssms tools pack is one that ive been very fond of versions but the licensing fee for is horrible would pay for reasonable license but thats not the question here ive found ssms boost for example that has some cool additions to ssms that seem worth it what other addins are available for sql server id love to have something that saves query when hit f5 like ssms tools pack does or anything out there besides the two tools listed
25291 have tickets table with an id that need to associate to lookup table where the counterpart of that data is another id that is controlled from an outside source tickets id sutff lookup outside data id ticket id just need to know if the association exists hence the lookup table and has one to many relationship know nothing else about the outside data id except the id itself and the related ticket id also dont want to alter the tickets table as this will only affect small amount of ticket ids example outside data id can be related to ticket id ticket id ticket id would the below lookup table structure work or is there better way create table lookup outside data id integer ticket id integer
25304 we use sql server r2 standard edition we cant afford enterprise edition on all our production database servers and have to periodically maintain the critical indexes rebuilding reorganizing in our production servers the problem is that since we are using standard edition we dont have access to the rebuild index online option if we dont limit access to the database while these critical indexes are rebuilt we get many timeout errors because the index is essentially disabled during rebuild ive used scripts such as ola hallengrens index optimize script however this doesnt seem to solve my problem what im thinking to do to work around this problem is instead of rebuilding an index when an index is fragmented create separate index copy ix some name temp once that is done drop the original fragmented index and finally rename the copy index back to the original name what im hoping is that while the new index is being built sql server can use the original somewhat fragmented index then after the new temp index is built it can start using that one then we drop the fragmented index and rename and were back to the original state for the next time we have to run our scheduled job my question is does this approach make sense will sql server be able to utilize my new temp index copy while im dropping the original fragmented index any tips or possibly other strategies are appreciated
25305 the tablespace parameter is optional when creating tables upon execution of create table statement oracle assigns the default one if it was not defined in the default database there is users tablespace if there were several tablespaces defined which one would be assigned
25382 am trying to figure out how to store formula method in the table could create category formula table each category will have different way of formula method to calculate the point and need to assign formula method on each category what the best way to do this for example cpucategory id to calculate the point the formula would be commision fulfilment bonus cost hard drivecategory id to calculate the point the formula would be commision cost and formula would be different for other categories mysql select from category id name cpu hard drive mysql select from items id category id name cost commision fulfilment bonus intel cpu core i7 3770k 160gb samsung spinpoint rows in set sec update would use php to do the calculation for example admin on the website can select formula setting on each category and save it the formula setting could be saved in category formula table but how category formula table should be designed create table structure for table category create table if not exists category id int11 not null auto increment name varchar100 not null primary key id engine innodb default charset latin1 auto increment dumping data for table category insert into category id name values cpu hard drive table structure for table items create table if not exists items id int11 not null auto increment category id int11 not null name varchar100 not null cost decimal62 not null commision decimal62 not null fulfilment float not null bonus float not null primary key id engine innodb default charset latin1 auto increment dumping data for table items insert into items id category id name cost commision fulfilment bonus values intel cpu core i7 3770k 160gb samsung spinpoint
25425 recently discovered that large swath of the finance department is using excel to connect to my sql server instance with an account in the sysadmin role what are my current risks that should immediately communicate to the powers that be
25448 my query to create new table from existing table takes very long time so ive set up remote database in my office more ram there can connect to my database from home as usual with psql how can tell the remote server to execute my query from the terminal without having to wait for response postgresql linux enviroment edit im open to other solutions its not necessary to use psql
25461 well its hypothetical scenario but what im trying to understand is the path to go from post morten log say an sql server profiler trace to identify the code on orm situation to make it not too vague consider an scenario like that sql server entity framework as orm so in that scenario what is the common path to dba which is also an vb net developer make from the log to diagnosing which codes in this case linq queries are creating the trouble in this case the app is ok but is affecting other apps response time using the same db server that would be absurdly different from java hiberate process edit want to understand the path from the trace to the culprit metaquery if the app have sql in it this means that an find in files session maybe with some regex in extreme cases could reduce the inspect task targets to some dozen suspects instead of tenths or even hundreds of source files using an orm how to get to that stage using an orm in this case ef
25472 have very high traffic website where it is possible that 1000s of new records are inserted every hour this one error is crippling the site pdoexception sqlstate serialization failure deadlock found when trying to get lock try restarting transaction insert into location instance nid vid uid genid lid values db insert placeholder db insert placeholder db insert placeholder db insert placeholder db insert placeholder array db insert placeholder db insert placeholder db insert placeholder db insert placeholder cck field item location db insert placeholder would be very surprised if mysql could not handle this type of load so my questions are then is this database issue and how can configure mysql to be able to handle this much traffic have copy of my website set up on development server with scripts that simulate the load of content being added to the website am running ubuntu lamp stack with 16gb of ram admittedly am not very knowledgeable about databases in fact am starting with the default my cnf that comes with it after apt get install finishes tables are all innodb what starting configuration settings and approach would you recommend to begin solving this problem let me know what more information you may need thanks
25513 im having difficulty finding lay explanations of how indexes are cached in postgresql so id like reality check on any or all of these assumptions postgresql indexes like rows live on disk but may be cached an index may be entirely in the cache or not at all whether it is cached or not depends on how often it is used as defined by the query planner for this reason most sensible indexes are going to be in the cache all the time the indexes live in the same cache the buffer cache as rows and therefore cache space used by an index is not available to rows my motivation for understanding this follows on from another question asked where it was suggested that partial indexes can be used on tables where majority of the data will never be accessed before undertaking this id like to be clear that employing partial index yields two advantages we reduce the size of the index in the cache freeing up more space for rows themselves in the cache we reduce the size of the tree resulting in faster query response
25531 found this script sql server reaching table row size limit that seems to return the row size per defined data type lengths need script that would give me all the rows in table that their max data size is over the recommended whatever ms recommends
25627 problem definition our users need the ability to query database that is mostly up to date the data can be stale up to hours and that is acceptable what would be the lowest cost approach to getting and keeping second database up to date with production copy is there an approach im not thinking of workload we have third party application that we use to monitor stock trading activity during the day lots of little changes occur as part of various work flows yes this trade was valid no this is suspicious etc at night we perform large set based operations load the previous days trades the current solution and problem we make use of database snapshots at we drop and recreate the snapshot the etl processing then begins this is obviously taxing on our disk but allows our users the ability to query the database without locking the database they use an access front end they use it late into the night and early in the morning so they will notice downtime the problem with this approach is two fold the first is that in the event the nightly processing fails and thats not terribly uncommon we get to restore the database which results in the snapshot being dropped the other problem is our processing times are slipping past our sla we are attempting to address this by working with the vendor after having identified poorly written queries and lack of indexing the database snapshot is also culprit in this slowdown as evidenced by the speed difference when it is present versus not shocking know approaches considered clustering we had database clustering turned on but that didnt address the needs of making the data available and just generally complicated the admins lives it has since been turned off sql server replication we started looking at replication last week our theory is that we can get second catalog stood up and synchronized with the production database prior to etl beginning well sever the connection and only re enable it once the etl process has completed the admin started with snapshot replication but hes concerned that its taking multiple days of high cpu usage to generate the snapshot as well as the disk consumption required he indicates that it appears to write all the data out to physical files prior to ever shipping to the subscriber so our 6tb database will cost 8tb in storage costs also if itll take multiple days to generate snap then it wouldnt fit in the desired sla after reading the fine article it seems like snapshot might be the way to initialize the subscribers but then wed want to switch to transactional replication to keep it in sync after that assume turning the transactional replication on off wont force full reinitialization otherwise well blow our time window database mirroring our database is in full recovery mode so database mirroring is an option but know even less about it than replication did find the so answer that indicated database mirroring prevents data to be accessed directly mirrored data are only accessible through database snapshot log shipping it sounds like log shipping might also be an option but this is another of those things know nothing about would it be an lower cost solution implementation and maintenance than anything else based on remuss comment log shipping allows read only access to the replica copy but will disconnect all users when applying the next backup log received eg every minutes im not sure how long that downtime would translate into so that might cause the users some angst ms sync only heard about using sync this past weekend and have not yet investigated it id hate to introduce new technology for something with high visibility like this problem has but if its the best approach so be it ssis we do plenty of ssis here so generating few hundred ssis packages to keep the secondary synchronized is an option for us albeit an ugly one am not fan of doing this as thats lot of maintenance overhead id rather my team not take on san magic snapshot in the past ive heard of our admins using some san technology to make instant backups of entire disks perhaps theres some emc magic that could be used to make uberquick copies of the mdf ldf and we can then detach attach the target database backup and restore think we take full backups once week differentials nightly and tlogs every minutes if the users could live with the hour outage for the full restore suppose this might be an approach constraints windows r2 sql server r2 enterprise edition vmware v5 enterprise edition emc san storage with drives mapped to vmdk files commvault handling backups and 6tb of data in the source catalog this is third party application we host in house modifying their structure is generally frowned upon the users cannot go without querying the database and refuse to be constrained by proactively identifying the tables they monitor to do their work our dbas are purely contractors at the moment the full timers have set sail and we have not replaced them yet the application admins are not well versed on sql server matters and we have team of storage vm admins that could help hinder this effort development teams are not currently involved but can be enlisted based on the approach so simpler to implement and maintain solution would be preferable me im on the development side of the hosue so can only propose approaches and have not had to deal with the administration side of things so with no time in the admin saddle im hesitant to say one approach would be superior to another it all looks great according to the papers im fully willing to run any direction yall suggest because as see it its only going to make me more valuable as db professional have wheelbarrow but no holocaust cloak available related questions https stackoverflow com questions what are the scenarios for using mirroring log shipping replication and cluste https stackoverflow com questions mirroring vs replication https stackoverflow com questions sync databases mirroring replication log shipping https stackoverflow com questions sync databases mirroring replication log shipping http nilebride wordpress com log shipping vs mirroring vs replication edits to address onpnts questions data latency acceptance the users currently view data that is up to hours behind the data is only current as of amount of data change in given minute hour and day not sure how to quantify that business hours maybe hundreds of changes per hour nightly processing millions of rows per business day connectivity to the secondary internal network separate virtual host and dedicated storage read requirements on the secondary instance windows group will have read access to the secondary all tables up time of the secondary instance there is no strong definition of an up time requirement users want it always available but are they willing to pay for that probably not so much realistically id say hours out of the day would suffice alterations to existing schema and all objects infrequent modifications maybe once per quarter for table objects maybe once per month for code objects security no special security needs the production permissions would match the copys permissions although as think about it we could revoke the users read access to prod and only allow them to read the copy not requirement though darin strait reverting to the snapshot could be an option but think there was some reason they didnt pursue it ill check with the admin cfradenburg my assumption was that wed only use one of these approaches but that is good point that restores would break the other sync technologies they are investigating doing using the emc snapshot magic as the admin described it they would take snapshot at and migrate the image over to the secondarys zone that should complete by and then theyd perform detach and reattach of the secondary database wrap up we evaluated the emc snapshot magic and some other replication options but the dbas decided they could best figure out mirroring upvoted the answers because they all helped out and gave me plenty of options as well as homework to investigate
25642 in sql server r2 have nonclustered covering index on multiple tables with 100m rows the table has few thousand insertion points where all new inserts happen this means that regardless of fill factor ill quickly end up with page splits and fragmentation at every insertion point and no fragmentation or splits anywhere else in the table unfortunately queries always include new rows and hence fragmented areas of the index what happens when theres page split but inserts continue sequentially after the split is there way to tell sql server to do the split with lots of extra room for subsequent inserts without wasting space on existing pages with large fill factor that for most pages will never be filled what are good index maintenance strategies to use for indexes like this is there good automated way to identify tables like this where fragmentation is severe but not uniform these tables dont show up as more than fragmented overall are there index schema changes should be considering heres more info about the problem the indexes all look like this pattern simplifying for clarity below create table foo id int identity11 primary key clustered foreign key int log time datetime create nonclustered index on foo foreign key log time include queries on this table are always in this form where log time getdate and foreign key in select other facts there are about foreign key values each with 10000s of rows for each average row size is bytes meaning around rows per page the in filter usually includes of foreign key values rows and the date filter includes of the rows the average is about of total rows selected the index is covering index for the queries so no clustered index access is needed
25667 am looking for the best way to disable access to the sys tables information schema for user group in sql server found this thread from it shows way how to deny access on sys something like so deny select on sys columns to denysystemtableselectrole go deny select on sys tables to denysystemtableselectrole go deny select on sys syscolumns to denysystemtableselectrole go deny select on sys sysobjects to denysystemtableselectrole go but no way how to disable access on the information schema deny select on information schema tables to denysystemtableselectrole this seems not to work how can disable access to information schema and is there an easier way disable access to all sys information schema update actually can not run both ot the following statements deny select on sys to reduceddbo go deny select on information schema to reduceddbo go tried to run them on the specific db where the user exists and also tried on the master still can run select from information schema tables still returns results select from sys tables no results anymore including schema in the query made it possible to create the securables deny select on schema sys to reduceddbo go deny select on schema information schema to reduceddbo go but now still can select all the information from the db had look at the securables tab in the users property window in management studio it looks like this entry that does block the selecion of sys tables schema sys name tables type view permissions for sys tables permission select grantor dbo deny is checked entry that do not block any selection schema name information schema type schema permissions for information schema permission select grantor dbo deny is not checked tried to check it but no chance permission select grantor information schema deny is checked tried to set the permissions over the gui but then get the same error that setting permissions would be possible only on the master db but not have the user login added to the master dbs security solution the only way could make the deny work for the information schema was to add the user to the master db and run the deny select on the master deny select on sys tables to reduceddbo go deny select on information schema tables to reduceddbo go and as in this code it can only be executed for single tables
25683 have log table that captures the datetime stamp of when certain files were exported to another system the exportedlog table currently has three fields id primary key messageid int exporteddatetime datetime reviewing this found that the id field serves no purpose as there are no joins to this table the only thing working on this table is the insert of the batch job that processes the messages and inserts into this log table should remove the id field should have primary key on either messageid or exporteddatetime or both
25730 our current setup is principal database used in production with manual failover mirrored database for ha need to setup reporting database as well what would be the best approach shall setup sql job to drop and create the snapshot of the mirrored database and refresh this every night does anyone have any other suggestions thanks
25809 lets say have table bookingsperperson person id arrivaldate departuredate what need to achieve with view is the following person id arrivaldate departuredate jan jan jan jan jan jan jan the system is for events so each hotel booking could take anything between to days but no more than that any ideas would be very much appreciated
27045 ive been writing basic web apps for year for an oracle db and since the functions are pretty simple most of us stick with regular for loops to get our data for in select from students loop htp prni student last name student first name student dob end loop but cursors seem to be the right way to do things can find lots of information on what cursors are and different ways to loop through them but cant find solid reason why to use them over regular for loops is it dependent on the needs of the procedure are there inherent advantages should be aware of
27100 are these two queries logically equivalent declare datetime datetime getdate query select from mytable where datediffday loginserttime datetime query select from mytable where loginserttime datetime if they are not logically equivalent can you give me the logical equivalent of the first query so that the where clause can effectively use an index eliminate function wrapping
27153 how do most popular mysql postgres database system handle altering tables on live production databases like adding deleting or changing the type of colums know the correct way is to backup everything schedule downtime and do then do the changes but does any current database system support doing these things on line without stopping anything maybe just delaying the queries that reference column that is just being changed deleted and what does it happen when just do an alter table on live running database does everything stop when this happens can data get corrupted etc again im mostly referring to postgres or mysql as these are what encounter and yes anytime had to do this before did it the right way backing things up scheduling downtine etc but just want to know if its possible to do this sort and things quick and dirty or if there is any db system that actually has support for quick live and dirty schema changes someone just suggested online schema change for mysql from facebook script with tutorial here and source here seems like nice way to automate the set of hacky ways to do it has anyone ever used it in something resemblig production
27255 need to know if the enable disable history of table trigger is natively tracked by sql server ive reviewed the system views sys triggers contains modify date field sys trigger events focuses on the trigger insert update delete events can you recommend any other sources of information about trigger history
27265 sorry for the confusing title wasnt sure what to write there have table of few hundred records need to assign each record of this table to much smaller dynamic table of users and the users should alternate as to what records they get assigned for example if tablea is row number id and tableb is row number id need an end result set that is userid recordid ive managed to do something bit messily using the mod operator but was curious if this same query could be run without the temp table and the variable the temp table is used because tablea is actually user defined function that converts comma delimited string to table and need the count of the objects from the udf converts comma delimited string into table select num as userid row number over order by select as rowno into tmptest from dbo stringtonumset231 declare test int select test count from tmptest select from tmptest as t1 inner join select top id row number over order by somedatetime as rowno from tablea with nolock as t2 on t1 rowno t2 rowno test its important that the userids alternate too cannot assign the top of the records to user1 second of the records to user2 and 3rd of the records to user3 also the userids need to maintain the order in which they were originally entered in which is why have row number over order by select in the users table is there way of joining these tables in single query so wont need to use temp table and variable im using sql server
27310 when doing count aggregate sql query what can speed up the execution time in these database systems im sure many things could speed it up hardware for one but im just novice dba so im sure ill be getting few answers here migrated about million rows to sql server database and this query is taking forever but in my source netezza database it takes seconds for example netezza select count from databasename mytable oracle 11g select count from mytable sql server select count from databasename dbo mytable
27328 have busy database with solely innodb tables which is about 5gb in size the database runs on debian server using ssd disks and ive set max connections which sometimes saturate and grind the server to halt the average query per second is about 5k so need to optimize memory usage to make room for maximum possible connections ive seen suggestions that innodb buffer pool size should be up to of the total memory on the other hand get this warning from tuning primer script max memory ever allocated configured max per thread buffers configured max global buffers configured max memory limit physical memory here are my current innodb variables innodb adaptive flushing on innodb adaptive hash index on innodb additional mem pool size innodb autoextend increment innodb autoinc lock mode innodb buffer pool instances innodb buffer pool size innodb change buffering all innodb checksums on innodb commit concurrency innodb concurrency tickets innodb data file path ibdata1 10m autoextend innodb data home dir innodb doublewrite on innodb fast shutdown innodb file format antelope innodb file format check on innodb file format max antelope innodb file per table on innodb flush log at trx commit innodb flush method direct innodb force load corrupted off innodb force recovery innodb io capacity innodb large prefix off innodb lock wait timeout innodb locks unsafe for binlog off innodb log buffer size innodb log file size innodb log files in group innodb log group home dir innodb max dirty pages pct innodb max purge lag innodb mirrored log groups innodb old blocks pct innodb old blocks time innodb open files innodb purge batch size innodb purge threads innodb random read ahead off innodb read ahead threshold innodb read io threads innodb replication delay innodb rollback on timeout off innodb rollback segments innodb spin wait delay innodb stats method nulls equal innodb stats on metadata on innodb stats sample pages innodb strict mode off innodb support xa on innodb sync spin loops innodb table locks on innodb thread concurrency innodb thread sleep delay innodb use native aio on innodb use sys malloc on innodb version innodb write io threads side note that might be relevant see that when try to insert large post say over 10kb from drupal which sits on separate web server to database it lasts forever and the page does not return correctly regarding these im wondering what should be my innodb buffer pool size for optimal performance appreciate your suggestions to set this and other parameters optimally for this scenario
27383 how much does an index need to narrow the results of search in order to be useful in speeding up queries some examples all across the spectrum column for storing true false values obviously has only two unique values last name column probably has many unique values although it may not primary key column has all unique values think that the goal of an index is to quickly narrow search to few rows and that therefore the last case is the best the second is ok and the first is useless am correct if so roughly where is the line of usefulness for example if an index can narrow the results to of the rows is that useful what about or
27450 am getting the following sqlexception on calling stored procedure attempt to fetch logical page in database failed it belongs to allocation unit not to system data sqlclient sqlexception occurred message attempt to fetch logical page in database failed it belongs to allocation unit not to source net sqlclient data provider errorcode class linenumber number procedure ispdisplaycount server state what does this exception mean is there any resolution to the above issue although the database referenced in the error above indicates tempdb similar errors referencing message may be fixed using the answers below msg level state line attempt to fetch logical page in database failed it belongs to allocation unit not to
27467 consider the sql query select name row number overorder by name asc from footable as here observe the results being returned sorted by name if change the sort column defined in the row number function to another column then again the results become sorted by that column was expecting the row number to be assigned to the rows but was not expecting the rows to come back sorted by that same criteria is this simply side effect of how the query is being executed in my case on sql server r2 or is this behaviour guaranteed could find no reference to such guarantee
27477 have table which stores historical temperatures for the past days stored in an integer column based on city the question is how can query for entries which have value great than or less than specified value heres an example table city temps seattle miami lets say want to query for cities which have experienced temp higher than in this example only miami has experienced temps higher than so only that row should be returned ive tried few queries with no success looked into intarray but that doesnt seem to solve my problem either thanks much im running postgresql
27558 this relates to counting the number of records that match certain condition invoice amount tend to prefer countcase when invoice amount then end however this is just as valid sumcase when invoice amount then else end would have thought count is preferable for reasons conveys the intention which is to count count probably involves simple operation somewhere whereas sum cannot count on its expression to be simple integer value does anyone have specific facts about the difference on specific rdbms
27637 the dbconsole is configured and started but when navigate to it internet explorer shows the page stating that internet explorer cannot display the webpage this is the error that means that ie received no response from the server at all the firewall is completely disabled ive tried deconfig and config to no avail any thoughts that could possibly lead me down the path to fixing this are appreciated os windows server bit db 11gr2 enterprise edition em whatever comes with db 11gr2 update im using the link that is automatically created for you when you run emctl config dbconsole db the url is https corp svr xxxxxx xxxxx em where represents an alphanumeric character removed for privacy the reason it configures at port is that there is 10g instance on the same server that already has port we recently updated it to 11g but havent uninstalled 10g yet ive tried changing the url to use localhost and the network ipv4 address to no avail
27659 have table that looks like this name kode jum aman kode1 aman kode2 jhon kode1 amir kode2 how can make view like this with mysql kode1 kode2 count aman jhon amir
27725 is there an equivalent to mysql show databases statement is it possible to find databases in cluster databases present on the network on some other system could analyze the files present on an oracle installation to find the same given complete access credentials to an oracle system how would you go about enumerating all the databases that exist
27732 is it possible to rename default f1 f2 f3 names when using row to json function for only some columns can do row to jsoncustomers returning id customer first name bla last name second bla but if want only names without id customer have to use row to jsonrowfirst name last name and then get f1 bla f2 second bla and would like to get this result with either default column names or my own know can create my own composite type and use row to jsonrowfirst name last name my custom type but isnt it possible to do it right in the query without creating that type
27759 suppose have table friends with columns friend1id friend2id chose to represent each friendship with two records say john jeff and jeff john thus each pair of friends should show up exactly twice in the table sometimes this constraint is violated pair of friends shows up only once in the table how do write query that will identify all such cases ideally using reasonably standard sql in other words would like the query to return the list of rows in this table for which there is no corresponding row with the swapped fields an additional question is there any way to enforce this referential integrity in mysql
27879 the system stored procedure sp dboption is not available in sql server anymore how could it be replaced
27893 have postgresql instance running on rhel core machine with 16gb of ram the server is dedicated to this database given that the default postgresql conf is quite conservative regarding memory settings thought it might be good idea to allow postgres to use more memory to my surprise following advice on wiki postgresql org wiki tuning your postgresql server significantly slowed down practically every query run but its obviously more noticeable on the more complex queries also tried running pgtune which gave the following recommendation with more parameters tuned but that didnt change anything it suggests shared buffers of of ram size which seems to in line with advice elsewhere and on pg wiki in particular default statistics target maintenance work mem 960mb constraint exclusion on checkpoint completion target effective cache size 11gb work mem 96mb wal buffers 8mb checkpoint segments shared buffers 3840mb max connections tried reindexing the whole database after changing the settings using reindex database but that didnt help either played around with shared buffers and work mem gradually changing them from the very conservative default values 128k 1mb gradually decreased performance ran explain analyzebuffers on few queries and the culprit seems to be that hash join is significantly slower its not clear to me why to give some specific example have the following query it runs in 2100ms on the default configuration and 3300ms on the configuration with increased buffer sizes select count from contest left outer join contestparticipant cp on id cp contestid left outer join teammember tm on tm contestparticipantid cp id left outer join staffmember sm on cp id sm contestparticipantid left outer join person on id cp personid left outer join personinfo pi on pi id cp personinfoid where pi lastname like or pi firstname like explain analyzebuffers for the query above default buffers http explain depesz com xahj bigger buffers http explain depesz com plk the question is why am observing decreased performance when increase buffer sizes the machine is definitely not running out of memory allocation if shared memory in os is shmmax and shmall is set to very large values that should not be problem im not getting any errors in the postgres log either im running autovacuum in the default configuration but dont expect that has anything to do with it all queries were run on the same machine few seconds apart just with changed configuration and restarted pg edit just found one particularly interesting fact when perform the same test on my mid imac osx also with postgres and 16gb ram dont experience the slow down specifically set work mem 1mb select running time is ms set work mem 96mb select running time is ms when do exactly the same query the one above with exactly the same data on the server get ms with work mem 1mb and ms with mb the mac has ssd so its understandably faster but it exhibits behavior would expect see also the follow up discussion on pgsql performance
27949 use postgresql have table with million rows and columns the table does not change at all replace it once year users query this table with all kinds of filters on any some of the columns select from table where and is not null select from table where is null and and and for performance plan to create an index on every column of the table some feeling in my stomach tells me to ask the experts first is it good design for the above described use case to create an index on every column update have to speculate about real use cases cant measure the exact queries yet this is in design phase the server is well equipped with ram and ssd storage so queries are already fast now and can feel the effect of caching when fire similar queries in sequence the columns are of types double integer timestamp and geometry which explicitly gets gist index the queries will include from to columns usually results will usually be 20k rows queries on column will never relate to another column thanks for all the explanations what will do select 4th of the columns that think will be most used and create indexes wait for more testing usage and start measuring analysing the queries and use cases then thank you
27950 say you want to query database to discover the types of trigger it contains one way to do this is to use the objectproperty function on all trigger objects in the database sometimes the objectproperty function produces confusing result its output seems to depend on the database context the following example query returns row for each of the sysmail triggers in msdb select object id name objectpropertyobject id execisinserttrigger as isinserttrigger objectpropertyobject id execisupdatetrigger as isupdatetrigger objectpropertyobject id execisdeletetrigger as isdeletetrigger from msdb sys objects where type tr and name like trig sysmail go the intent is to find out what dml action will fire each trigger for example the isinserttrigger column contains if the trigger is defined as after insert and otherwise when execute the query in the context of msdb the result set contains or in each of the computed columns it looks like this object id name isinserttrigger isupdatetrigger isdeletetrigger trig sysmail profile trig sysmail account trig sysmail profileaccount trig sysmail profile delete trig sysmail servertype trig sysmail server trig sysmail configuration trig sysmail mailitems trig sysmail attachments trig sysmail log when execute the query in the context of master the result set contains null in each of the computed columns it looks like this object id name isinserttrigger isupdatetrigger isdeletetrigger trig sysmail profile null null null trig sysmail account null null null trig sysmail profileaccount null null null trig sysmail profile delete null null null trig sysmail servertype null null null trig sysmail server null null null trig sysmail configuration null null null trig sysmail mailitems null null null trig sysmail attachments null null null trig sysmail log null null null msdn notes that the objectproperty function returns null when the property name is not valid the object id is not valid id is an unsupported object type for the specified property the caller does not have permission to view the objects metadata can rule out reasons and because the query returns the correct result in the context of msdb at first thought it might be cross database permissions issue reason but am sysadmin on the server that leaves reason which leaves me with these questions is the object id invalid in cross database query which databases objectproperty function is being called
28055 why does the truncate table statement hang sometimes what are the reasons for this type of issue am migrating from mysql to mariadb this problem doesnt happen with mysql only with mariadb the hanging statement is simply truncate table sampledb datatable what can cause this to happen and how could fix it another one observation is if the table have some data may be one or two rows then the truncate query works successfully else the table have lot of data query becomes hang
28187 currently use the following to get local datetime from utc datetime set offset datediffminute getutcdate getdate set localdatetime dateaddminute offset utcdatetime my problem is that if daylight savings time occurs between getutcdate and utcdatetime the localdatetime ends up being an hour off is there an easy way to convert from utc to local time for date that is not the current date im using sql server
28213 how can profile sql server database to see code thats being executed on particular database remember using the sql server profiler but dont see it in sql server management studio after downloading sql server r2 express where can download that tool and install it do need the full version of sql server in order to see this option
28326 have installed oracle 11g and can connect as sysman to the oracle database but there is no tnsnames ora file that can find do need to generate the tnsnames ora file myself if so where do place it if not how does oracle generate it for me if do need to generate it what is the appropriate syntax for the file
28360 im no good in sql but ive got database to maintain theres almost no place left for it so ive decided to delete all the data for lets say year after executing delete query had about rows cleaned and cleaning transaction log ive found out that my actions had no effect on database size is there anything else have to do
28370 am currently doing some data imports into legacy system and discovered that this system does not use single clustered index quick google search introduced me to the concept of heap tables and now am curious in what usage scenarios heap table should be preferred over clustered table as far as understood heap table would only be useful for audit tables and or where inserts happen far more often than selects it would save disk space and disk since there is no clustered index to maintain and the additional fragmentation wouldn be problem because of the very rare reads
28406 want to count two columns but need to get the results of first column in one row select country count from table1 group by country type this query gives me country type count canada first canada second canada third australia second australia third but need to get country type first type second type third canada australia as want to update another table with these values and with this row structure can update the country table row by row as get from the above query update country set first second third note type column is enum with predefined values
28431 have two tables with foreign key from t1 t2 in one to many relationship that is tuple in table t1 is associated with tuples in t2 to create simple example lets say t1 is cars and t2 is table of imperfections so car can have imperfections and we store these imperfections in t2 as integers would like to select from only those cars in cars that contain imperfections i1 and i2 performing an or instead is pretty easy select from cars as t1 where exists select imperfection from imperfections as t2 where t1 uid t2 uid and imperfection or imperfection ive been trying set logic using intersection but at this point im wondering if im over complicating it
28459 how do modify the user interface language in sql server management studio for instance if installed in english but wish to view in japanese to name but one possible example am running this environment in non english as first language environment and would prefer all my windows to read in my natural language my particular natural language isnt really important is it
28512 first off im not dba im software engineer and have been building applications which are database backed for my entire career one of the things remember maybe incorrectly is that when designing the erd that you take the real world into account as it serves as the context for your design have situation where we have the concept of customer who can have two and only two phone numbers customer can have zero to many addresses as well and each address can have only two phone numbers and flag indicating if the number is mobile number the numbers on an address would only be used to contact someone at that particular address while the numbers for the customer are used to contact the customer whos address may not be in the system the design came up with was to have phone1 phone2 as columns on the customer table as well as on the address table ive had others suggest that this isnt good design and that should have created phonenumber table instead im not sure how they were suggesting relate the numbers to the other records certainly can see that being valid as well but either still need to have phone1id and phone2id in the customer and address table or have something in the phone table that tells me which record owns the number the issue have though is that of course makes the appliction logic more complex since now need to add remove or update the record in the phone table instead of just blanking or nulling the value in the respective tables is my design also acceptable is one or the other preferable or are both just as valid
28544 want to do the following in my db related to patients when patient inserts his birthday it shows his age category as infants children adult elderly etc to help me classify my patients how can translate the birthday into age category how to design that age category
28584 think would need some help to simplify my problem lets assume that have two tables first called topics with this structure idtopic author deleted false false and second callledd texts which looks like this idtopic idtext dateposted and what need is to select the data for every topic with the data of newest text of this topic so my output should be like this idtopic author deleted idtext dateposted false false so how should my sql query looks was trying to do some inner join with second table grouped by idtopic but it didnt work thanks
28730 have stored procedure that takes about seconds that im trying to understand so want an execution plan for it when run it in sql server management studio with execution plan enabled it takes over minutes and then get tab that says execution plan but the tab is empty any further attempts to run any sql at all even select from foo no longer works have broken sql server management studio app must shut it down and try again ive done it three times and wasted minutes and am now ready to learn about some alternatives first is there reliable command line way to generate an sqlplan file for particular sql script maybe from the command line and then could investigate using some other tool than ssms to generate my plans secondly if there isnt built in reliable command line way to generate and store an sql execution plan as text or xml then id like to know if there exists some other third party tools that would do good job on very large sqlplans in particular not choking and dying when the gui drawing parts of it get overwhelmed what do you do when ssms wont generate and show you an execution plan im using sql r2 standard and its included ssms version and dont have any extra plugins update im invoking stored procedure which creates cursor which does subqueries and does great evil in loop generating in excess of 10k individual subquery statements it looks like really need to refactor this down to generate less of storm of output update2 it really looks like server side tracing to zero in on problem areas and then return to profiling then return to query plans might be required am zoomed in far too deep in big picture and need to zoom the heck out bit
28751 have an application which generates lot of data which needs to be inserted quickly something around 13million records use jpa hibernate with postgres and managed to achieve quite good performance around 25k inserts per second with multi threading and batching of inserts every few thousand inserts or so completing whole run in around 8mins however noticed that had few of the foreign keys which had an index missing which would really wish to have both from an analysis point of view to drill down in the data and also to delete data to specific run unfortunately when added in these indexes to the table that is getting most inserts performance dropped down drastically to around 3k per second is there any way to avoid this performance slow down know that one option is to drop the indexes before run and recreate them in the end another more clumsy option is to generate the data of the biggest table in file instead and use copy guess can only do it on the largest table in the relation due to the foreign key values which would need to know generated through sequences both alternatives seem to be hacks is there any other solution maybe bit less intrusive on the application some setting to tell postgres to defer indexing or something of that sort any ideas welcome
28801 on windows r2 running sql server r2 how imporatant is the ntfs allocation unit size on disk io performance it appears to me that server admin who built the few servers for mission critical app left ntfs allocation unit size cluster size to default kb instead of kb sql server is already installed does it worth to take pain to uninstall sql format the drive with kb cluster size and reinstall sql server
28926 am using aws as my cloud environment installed postgresql on the same drive as my root instance volume have attached and mounted the second drive to my instance now want to move all my postgresql data to the different drive am still in development mode so can delete the old data if it makes transfer easier at this point of time what is the best way to do this is postgresql tablespace something which should look at
29055 in sql server im getting the following error the query has been canceled because the estimated cost of this query exceeds the configured threshold of contact the system administrator this is the result of an execution of stored procedure which is pretty complex havent run into this for other stored procedures only this one is it possible to change the query cost for this one procedure somehow can do that in the stored procedure itself upon execution or do have to define this on the server only im using ado net command to execute the stored procedure thanks
29064 we have customer who is switching from san storage to directly attached storage our sql server database has several filegroups all of which has been deployed on the same lun in order to determine the best setup for the filegroups on the directly attached storage want to test the loading on the existing file groups for directly attached storage would normally have tested loading by looking at performance counters like disk queues etc but this assumes that the file groups have been setup on separate arrays for sans we would normally have got the io throughput for each lun from the san vendor however because all the filegroups are on the same lun how can test how heavily each filegroup is loaded is there any way to do this with sql profiler thanks chris
29116 need to run sql server logging db with main tables in seperate datacentres writing to both at the same time had the idea of restoring the db to the new datacentre and then reseeding the identity column to and setting the increment to that way there would never be any chance of duplicate ids when the data needs to be combined datacentre1 would be positive integers datacentre2 negative integers would an increment of cause any issues
29194 up to mysql the official rpm was shipping with these files usr share mysql my innodb heavy 4g cnf usr share mysql my medium cnf usr share mysql my huge cnf usr share mysql my large cnf usr share mysql my small cnf in these files are gone and the only one left is usr share mysql my default cnf im not mysql tuning expert and was used to always start with my innodb heavy 4g cnf now am left with single configuration option remove leading and set to the amount of ram for the most important data cache in mysql start at of total ram for dedicated server else innodb buffer pool size 128m is tuning this only variable the magic key to most optimizations or are there other relevant parameters that should tune when installing new server in which case where can find an equivalent file that would contain sensible defaults for innodb only 4gb ram dedicated mysql server
29284 am using sql server and was wondering how to profile stored procedures for instance can profiler capture each individual sql statement in stored procedure what it is and how long it takes to run etc am trying to diagnose merge replication stored procedures and this must be captured part of full run of the merge agent it doesnt seem possible to grab the stored procedure with performance issues and run it again because at that point it is not slow
29289 some context at first we wrote reports just straight up without any locking hints in the queries with the larger reports this would sometimes cause locking problems at first we remedied this by using the with nolock hint for tables in the query because its quite obtrusive and its easy to forget the hint for one of the tables we moved to second approach setting transaction isolation level to read uncommitted which is fine at the top of each datasets query as you may guess its still easy to forget the hint for one of the datasets so this leads to the question question what are the options for sending nolock hints along with report queries ps realize this is to some extent an xy problem with lot of my other options for such as optimizing the query not doing reporting on the operational database etc but tried to make this valid question on itself nonetheless options here are the options mentioned above with added options about which im curious if they would work set with nolock hint for each table obtrusive very easy to forget set isolation level to read uncommitted for the entire query still easy to forget is it possible to specify this at the report level make sure all dataset queries for one report will be run without locking is it possible to specify this at some other ssrs level perhaps set this for certain report folder or by utilizing an extension is it possible to specify this at the data source connection string level have all relevant reports use certain no lock data source related to the previous option perhaps its possible to specify default locking hint for specific no lock sql user the one thats used in the connection which options are viable are there options ive missed
29309 take the following code declare integer set set cast as binary set cast as binary select cast as integer now shouldnt the output of this code be have an integer set it to so in binary its all 0s then or it with so it becomes then or it with in binary so it should become which in decimal is the output actually get is edit for more clarity im trying to accomplish with tsql what the following code does in include stdio include iostream using namespace std int main int cout 0b100 cout endl
29328 have been working with sql server on and off since sql server the old advice that still rings in my head was never to do an in place upgrade im currently upgrading my r2 dev and test systems to sql server and need to use the same hardware the thought of not having to restore my reporting services configuration is very attractive and im really up against the wall time wise there is no analysis services involved or anything unusual or non standard only the database engine and reporting services are installed has anyone experienced serious issues with in place upgrades or should reevaluate my position about in place upgrades
29352 field is meta value and current value is like want to change it to string so thought could get the value and update it with the characters in the correct order ie update wp postmeta set meta value meta value meta value meta value meta value
29363 we have database that was set up to use full recovery and my guess is that the reasoning was only to prevent data loss if failure happened between full backups we make daily full backups of the database and we have no need to recover to any point in time previous to our last full backup our data files and log files are in the same hard drive from my experience as programmer im not dba most database failures ive seen were related to disk failures and so wonder if this setup makes any sense imagine that if the hard drive fails we wouldnt be able to recover using the transaction logs so my question is twofold is the most likely cause of database failure hard drive failure or are there other common reasons that would justify this setup would it make more sense to switch to the simple recovery model and convince the business that in the worst case scenario they would have to re input data for the day
29403 we have many systems that have the same configuration one server virtual or physical running sql server sql and sql server analysis server as multiple cores 16gb ram each night the sql server will do about hours processing followed by hours of as processing then throughout the day only the as are queried assuming this is dedicated server and no other apps are of concern and that the two sets of processing are completely synchronous no overlaps always one after the other how can best set the sql and as server memory limits the reason for asking is that if dont set limit for sql it will grab all the memory it can however my understanding is that sql will happily relinquish this memory if its not using it and another service program requests it so from logical perspective believe allow sql to take as much as it needs but im not so sure about as totalmemorylimit im not sure if as will relinquish its memory in fact reading more leads me to believe that it is wrong to let it take it all does this mean that need actually set limits for both im confused as to what the best practises should be and what we need to be measuring considering the processes dont overlap hope this makes sense
29415 have table called slot as follows with default data 1st table day time venue free rm rm rm rm rm rm rm rm there is another table booking with data that might come and go anytime but the column header is fixed 2nd table day time venue user rm jill rm jill rm jack rm mary rm mary rm jill rm ken rm ken based on the example data in the table booking how can derive the following table 3rd table this is what wanted day time venue free used rm rm rm rm rm rm rm rm am able to retrieve the following table 4th table day time venue used rm rm rm by using the following command select day time venue count as used from booking group by day time venue order by day asc time asc time asc but find it hard to merge and get the 3rd table that wanted
29468 am in the process of creating test environment for our sql server development staff in production we have sql servers sql01 contains several databases that are mirrored to sql02 sql03 acts as the witness in high safety with automatic failover or synchronous configuration ive used vmware p2v to virtualize all three machines onto separate hardware reconfigured the sids of the machines and blackholed the ip addresses of our production servers from these new machines had initially forgotten to blackhole the production witness machine so the databases on the test machines were still using the sql03 machine as the witness noticing the issue decided to reconfigure the databases on test to point to the newly virtualized test witness call it test03 to reconfigure the database to use the new witness entered the following command on the primary server test01 alter database testdb set witness tcp test03 domain inet the response was unexpected the alter database command could not be sent to the remote server instance tcp test03 domain inet the database mirroring configuration was not changed verify that the server is connected and try again was very perplexed at this error message since the configuration does work on the production machines and has not been modified in any way on the test machines in order to get this to work needed to create login on the test witness create login domain sqlserviceaccount for windows with default database master and grant it connect rights on the endpoint in question grant connect on endpoint mirroring to domain sqlserviceaccount was then able to successfully point the mirrored databases on the test environment to the new test witness how can inspect the production witness endpoint to see what security is associated with it assume there must be some system catalog that can inspect however books on line does not seem to have anything specific for endpoints and bing is well bingless additional info select ep endpoint id class desc permission name ep name sp name from sys server permissions inner join sys endpoints ep on major id ep endpoint id inner join sys server principals sp on grantee principal id sp principal id where class returns endpoint id class desc permission name endpoint name principal name endpoint connect tsql local machine public endpoint connect tsql named pipes public endpoint connect tsql default tcp public endpoint connect tsql default via public and select name endpoint id protocol desc type desc role desc from sys database mirroring endpoints returns name endpoint id protocol desc type desc role desc mirroring tcp database mirroring witness there appears to be no entry in sys server permissions for the mirroring endpoint object no major id and no minor id matches also none of the system databases contain any reference to the endpoint im at loss
29489 have table with million tuples which looks like this table public methods column type attributes id integer not null default nextvalmethodkey regclass hash character varying32 not null string character varying not null method character varying not null file character varying not null type character varying not null indexes methods pkey primary key btree id methodhash btree hash now want to select some values but the query is incredibly slow db explain select hash string countmethod from methods where hash not in select hash from nostring group by hash string order by countmethod desc query plan sort cost rows width sort key countmethods method groupaggregate cost rows width sort cost rows width sort key methods hash methods string seq scan on methods cost rows width filter not subplan subplan materialize cost rows width seq scan on nostring cost rows width the hash column is the md5 hash of string and has an index so think my problem is that the whole table is sorted by id and not by hash so it takes while to sort it first and then group it the table nostring contains only list of hashes dont want to have but need both tables to have all values so its not an option to delete these additional info none of the columns can be null fixed that in the table definition and im using postgresql
29522 have table with 64m rows taking gb on disk for its data each row is about bytes of integer columns plus variable nvarchar255 column for text added nullable column with data type datetimeoffset0 then updated this column for every row and made sure all new inserts place value in this column once there were no null entries then ran this command to make my new field mandatory alter table tblcheckresult alter column dtodatetime datetimeoffset not null the result was huge growth in the transaction log size from 6gb to over 36gb until it ran out of space does anyone have any idea what on earth sql server r2 is doing for this simple command to result in such huge growth
29532 have the sql queries below that combine into one result set at the end am guessing that since am operating on the same tables in each of the sql sections there is cool way to get my results out of one unified sql query currently am doing it like this select date convertdate crdate tally sent to smc attachment count total mb size sumcastad size as decimal into tmp attachments sent to smc from attachmentdetail ad inner join messageattachment ma on ad attachmentid ma attachmentid inner join messagerecipient mr on ma messageid mr messageid inner join message on mr messageid id where ad isinline and mr recipienttypeid and leftmr emailaddress4 smc group by convertdate crdate order by date desc select from tmp attachments sent to smc select date convertdate crdate tally sent from smc attachment count total mb size sumcastad size as decimal into tmp attachments sent from smc from attachmentdetail ad inner join messageattachment ma on ad attachmentid ma attachmentid inner join messagerecipient mr on ma messageid mr messageid inner join message on mr messageid id where ad isinline and mr recipienttypeid and leftmr emailaddress4 smc group by convertdate crdate order by date desc select date convertdate crdate grand total sent to smc count total mb size sent to smc sumcastsize as decimal into tmp sent to smc from message where messagesourceid group by convertdate crdate order by date desc select date convertdate crdate grand total sent from smc count total mb size sent from smc sumcastsize as decimal into tmp sent from smc from message where messagesourceid group by convertdate crdate order by date desc select tally sent from smc attachment total mb size grand total sent from smc total mb size sent from smc grand total sent to smc total mb size sent to smc from tmp attachments sent to smc join tmp attachments sent from smc on date date join tmp sent from smc on date date join tmp sent to smc on date date drop table tmp attachments sent to smc drop table tmp attachments sent from smc drop table tmp sent from smc drop table tmp sent to smc
29543 we are using databases in total for an application and we can only share 4tb of space among all auto grow databases via san storage id like to write query report for single database indicating the currently allocated space and available free space attributes under the tasks shrink database option in sql server management studio then id like to convert those numbers to tb and total each database to get rough estimate of how much space we have left can these fields be accessed via sql query if so what would the query look like
29544 working in sql server r2 am trying to rollback set of ddl statements as group think of an upgrade script for database but am running into trouble take the following code begin try begin tran create table foo int alter table foo add dog insert into foo select insert into foo select commit tran end try begin catch rollback tran print error end catch im expecting the try to fail on the alter table statement drop to the catch rollback the transaction and print the error message however if you check your objects tables youll see that foo is still there so the create table didnt rollback properly select from sys objects where name foo what am doing wrong here
29596 in my test network there is database created with sql server have to update sql server to sql server the data base should be used with sql server so it is to be moved what is the best way to transfer the database during after sql server update
29613 message that sql server log file viewer shows login failed for user user error severity state what it actually means failed to open the explicitly specified database my question is there list somewhere of all the variations of the errors login failed for each combination of severity and state with the helpful description text ive had google but cant find anything other than specific combinations
29636 need to make heavy statistical analysis to deliver some data to users currently catch the data from mysql and process them via php arrays however mysql temporary tables are quite better extremely more efficient and faster than php arrays obviously due to their mechanism my tentative solution is to form temporary table upon each request connection to import and process the data however am not sure if there is drawback for creating many temporary tables simultaneously can this make problem for the mysql server or can use it as an alternative to php arrays in numerous simultaneous requests
29668 alright im trying to make query for library that will show which students have never borrowed book for this have done the following select leerlingen llnr leerlingen voornaam leerlingen tussenvoegsel leerlingen achternaam leerlingen klas count as aantal from uitleningen inner join leerlingen on uitleningen llnr leerlingen llnr group by leerlingen llnr leerlingen voornaam leerlingen tussenvoegsel leerlingen achternaam leerlingen klas having count this doesnt seem to work for some reason as all it does is create an empty table when click execute what did do wrong here
29712 if have 8gb of ram in server and run instances of sql server express will the total memory limit used by sql server be 1gb or 4gb would it be advisable to run multiple instances like this to enable each database to make better use of resources assuming that the server has plenty of resources
29723 have sql server user that has the ability to drop any database ive been running the code below to check the rights that the user has in sql server but have not been able to identify how the user has the ability to drop databases is there sql script that can help me identify how this user can drop dbs is there command to deny them dropping any databases ssms is not showing the user as part of the dbcreator role select user namep grantee principal id as principal name dp type desc as principal type desc class desc object namep major id as object name permission name state desc as permission state desc from sys database permissions inner join sys database principals dp on grantee principal id dp principal id order by principal name the output of the query above provides the following three records for the user if it is helpful class desc object name permission name permission state desc object or column xp cmdshell execute grant database null connect grant database null create database grant
29767 when try to use on file that is not in the psql exe folder it says permission denied for example have file with sql command at users work desktop school work load database sql and when type users work desktop school work load database sql it says permission denied how can fix this found work around here where you copy the sql file to the same folder as psql exe is in by the way does stand for import
29775 input have table containing any number of user supplied strings value to an and table of user details userid username tom ann dina mark need query to return all records from the users table where username is partial string match for any value record in the input table expected output in this case is userid username tom ann
29801 want to make an alter table expression which adds new column and sets default value and additionaly defines the allowed values for that column its text column and allowed should be only value1 value2 and value3 default should be value1 according to following syntax diagrams im getting to this point alter table exampletable add column new column varchar20 default value1 but im absolutely not sure how to set the allowed values is it possible to make somethin like constraint check new column in value1 value2 value3 must admit the search condition diagram is quite confusing me
29829 this one seems to be common question in most forums and all over the web it is asked here in many formats that typically sound like this in sql server what are some reasons the transaction log grows so large why is my log file so big what are some ways to prevent this problem from occurring what do do when get myself on track with the underlying cause and want to put my transaction log file to healthy size
29903 we have servers in an alwayson group while the user accounts within each synchronized database exist on both servers the database instance level logins only exist on one of the servers ie dbinstance security logins are missing on one server therefore when there is failover get login failures on the second server which doesnt have the corresponding instance level logins how do overcome this issue was supposed to set up the user account in special way
29913 im graduate school student researching olap with mondrian olap so want to insert data into innodb mysql faster at the initial loading in this environment the only user is me so think can allow more loose settings for insertion speed at the moment im using the following techniques disable log bin enable skip innodb doublewrite set transaction isolation to read committed or read uncommitted actually read commited set innodb flush log at trx commit to or actually set innodb buffer pool size to 5gb system has 6gb ram are there any more techniques for faster insertion on innodb and do have to modify innodb io read thread and innodb io write thread if you need more information please tell me
29961 using pgadmin iii can right click on database navigate to the variables tab and put variable name value property on the database itself is there way to customize these saw an application name variable but id like to have an application version variable
29963 this is related to this question it does help to get better performance for innodb tables according to mysql manual innodb flush log at trx commit is global dynamic variable thus can change it using set global command and it seems to be working mysql set global innodb flush log at trx commit query ok rows affected mysql show variables like innodb flush log at trx commit variable name value innodb flush log at trx commit row in set but it did not make the actual mysql setting changed when updated my cnf and restarted the mysql server it did work so cannot change the global variable at run time prefer the default value innodb flush log at trx commit but need to change it to before run restore process for large database to get faster but when the process done want to change the value back to is it possible to do this at run time dont have access to my cnf on my shared hosting server
29985 have table with the following structure id int identity pk fooid int fk barid int fk quxid int fk other fields the following indexes have been defined pk id clustered index1 fooid index2 fooid barid index3 fooid barid id quxid is it redundant to keep both index1 and index2 considering id is the pk does it make any sense to have it as the third column in index3 before quxid would it make more sense to have just one index fooid barid quxid would there be any benefit in including id as sidenote im developer not dba
30019 if an index has more than one attribute in it is there any speed gained in select statement whose where clause uses one of the attributes in the index for example take table with an index on attributes and is the index useful for the query select from where foo ask because the book im reading has the following statement which have trouble understanding if the key for the multiattribute index is really the concatenation of the attributes in some order then we can even use this index to find all the tuples with given value in the first of the attributes
30061 can list all tables in all schemas using dt but that also lists system tables that greatly outnumber my tables that care about id like all the tables and possibly views created by me in the public schema and any schemas ive defined im hoping to find way to do this without having to explicitly add schemas to the search path as create them as described here https stackoverflow com edit based on the accepted answer ive created the following view create view my tables as select table catalog table schema table name table type from information schema tables where table schema not in pg catalog information schema and now the following command gives me what wanted select from my tables
30091 im using sql server r2 logged in to ssms which in turn connects to remote sql server machine im writing query which writes file need to know which windows permissions should grant to myfolder is there any select query which can provide me the windows account who is actually finally writes the file
30119 have sql server instance that has linked server to an oracle server there is table on the oracle server called personoptions which contains the following data personid optionid need to pivot that data so the results are personid optiona option option any suggestions
30210 the new offset fetch model introduces with sql server offers simple and faster paging why are there any differences at all considering that the two forms are semantically identical and very common one would assume that the optimizer recognizes both and optimizes them trivially to the fullest here is very simple case where offset fetch is 2x faster according to the cost estimate select into objects from sys objects select from select row number over order by object id from objects where and order by object id select from objects order by object id offset rows fetch next rows only one can vary this test case by creating ci on object id or adding filters but it is impossible to remove all plan differences offset fetch is always faster because it does less work at execution time
30317 start with an object id and database id inside user defined function want to get the full name and id of the database schema and object how can get the schema id without using dynamic sql which is prohibited inside udfs note can get the database name with db name can get the object name by using object name because it accepts the db id as second parameter can get the schema name by using object schema name because it also accespts db id as second parameter its easy to get the schema id using dynamic sql to select from db name want sys schemas but this is not allowed in udf update purpose for possible solution to db id context from farther up call stack im adapting some call stack functions by gabriel mcadams to work accross multiple databases his version just pushes the proc id at each call level onto the context info stream ive modified this to also push the db id could push the schema id as well but then things start to get crowded because context info is limited to bytes so was hoping to be able to reconstruct the schema id from the db id and object id in the function the creates view of the call stack callstackview code db id proc id are saved in logging function declare db name nvarchar128 db name db id ok declare obj name nvarchar128 object name proc id db id ok declare schema name nvarchar128 object schema name proc id db id ok declare schema id int what can do here
30374 have several databases for legacy applications which run in vms that are currently off until someone needs to look at some history have set each of the databases used by these applications to offline but wanted to know is there performance benefit to having these unused databases in the offline state instead of sitting online but unused without any connections to them or queries run against them
30383 im the administrator and simply run the command sqllocaldb start v11 result start of localdb instance v11 failed because of the following error error occurred during localdb instance startup sql server process failed to sta rt event viewer log event id windows api call waitformultipleobjects returned error code windows system error message is application error the application was unable to start correctly 0x lx click ok to close the application reported at line tried another user and administrator accounts there were no problems with them uninstalled reinstalled version of sqllocaldb msi but had no luck do you have any idea fix
30477 have found deadlock that appears to show something thought was impossible there are two processes involved in the deadlock process8cf948 spid performing an alter table on temporary table pb cost excp process invoices work owns ix lock on table pb cost excp process invoices work with object id process4cb3708 spid performing in update on temporary table pb cost excp process invoices work which is supposed to be its own unique copy of the table owns sch lock on pb cost excp process invoices work with the same object id this is supposed to be impossible am missing something did temporary table really get reused between these two spids this is on sql server r2 service pack with cumulative update version the full unaltered deadlock trace is below note how the two processes are both operating on the same object id with the same table name pb cost excp process invoices work snip 0000000d8519 03spid23sunknownwaiter id process8cf948 mode requesttype wait 03spid23sunknownwaiter list 03spid23sunknownowner id process4cb3708 mode sch 03spid23sunknownowner list 03spid23sunknownobjectlock lockpartition objid subresource full dbid objectname tempdb dbo pb cost excp process invoices work 0000000d8519 id lock371705d00 mode sch associatedobjectid 03spid23sunknownwaiter id process4cb3708 mode sch requesttype wait 03spid23sunknownwaiter list 03spid23sunknownowner id process8cf948 mode ix 03spid23sunknownowner list 03spid23sunknownobjectlock lockpartition objid subresource full dbid objectname tempdb dbo pb cost excp process invoices work 0000000d8519 id lock3139b4780 mode ix associatedobjectid 03spid23sunknownresource list 03spid23sunknownproc database id object id 03spid23sunknowninputbuf 03spid23sunknownexec pb processexc costs submit sp sitekey pwdate 03spid23sunknownframe procname pdicompany dbo dr submitpaperwork sp line stmtstart stmtend sqlhandle 0x03000800cb72be6e500434018da000000100000000000000 03spid23sunknownexec pb processexc costs create sp clean up work table 03spid23sunknownframe procname pdicompany dbo pb processexc costs submit sp line stmtstart stmtend sqlhandle 0x03000800428c1f1950f833018da000000100000000000000 03spid23sunknownupdate pb cost excp process invoices work set pbceprcinv rtlpkg item quantity rtlpkg item quantity from pb cost excp process invoices work inner join item packages nolock on pbceprcinv itempkg key itempkg key inner join retail packages nolock on itempkg rtlpkg key rtlpkg key lookup pricebook cost 03spid23sunknownframe procname pdicompany dbo pb processexc costs create sp line stmtstart stmtend sqlhandle 0x030008003a082846321f46018da000000100000000000000 03spid23sunknownexecutionstack 03spid23sunknownprocess id process8cf948 taskpriority logused waitresource object waittime ownerid transactionname update lasttranstarted 14t13 xdes 0x3c4502930 lockmode schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 14t13 lastbatchcompleted 14t13 clientapp pdi wcf services pdidb01 pdimaster cfg hostname pdiweb01 hostpid loginname pdiuser isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 03spid23sunknownproc database id object id 03spid23sunknowninputbuf 03spid23sunknownexec pb processexc costs submit sp sitekey pwdate 03spid23sunknownframe procname pdicompany dbo dr submitpaperwork sp line stmtstart stmtend sqlhandle 0x03000800cb72be6e500434018da000000100000000000000 03spid23sunknownexec dbo pb processexc costs createinvoiceworktable sp 03spid23sunknownframe procname pdicompany dbo pb processexc costs submit sp line stmtstart stmtend sqlhandle 0x03000800428c1f1950f833018da000000100000000000000 03spid23sunknownalter table pb cost excp process invoices work drop column pbceprcinv filler 03spid23sunknownframe procname pdicompany dbo pb processexc costs createinvoiceworktable sp line stmtstart stmtend sqlhandle 0x0300080025d75a14ffff4701969f00000100000000000000 03spid23sunknownexecutionstack 03spid23sunknownprocess id process4cb3708 taskpriority logused waitresource object waittime ownerid transactionname alter table lasttranstarted 14t13 xdes 0x5f48bce80 lockmode sch schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 14t13 lastbatchcompleted 14t13 clientapp pdi wcf services pdidb01 pdimaster cfg hostname pdiweb01 hostpid loginname pdiuser isolationlevel read committed xactid currentdb locktimeout clientoption1 clientoption2 03spid23sunknownprocess list 03spid23sunknowndeadlock victim process4cb3708 03spid23sunknowndeadlock list update the machine in question shows processors in task manager and device manager so lock partitioning is enabled and the two locks are on different lock partitions dont know if lock partitioning is contributing cause here or not also found this intriguing post on the css sql server engineers blog update the temporary tables are dropped at the end of every stored procedure they are created with the pattern create table modify schema insert update select and then drop there are multiple entry points to common procedure that uses this temp table so we have central proc that sets up the columns needed to call the common proc otherwise wed have to replicate the same table definition in all the entry point procs the process is invoked frequently from multiple client applications some of the client applications call this process from multiple threads others run it one at time think inventory accounting software where the home office is processing data for thousands of stores in parallel while the stores also run the same process themselves so if this is rare issue when lock partitioning is enabled it is not going to be so rare on our larger customer databases update another customer is having the same issue on sql server build did not see any mention of fix for this issue in the cumulative update descriptions researching update microsoft has released the fix for this bug in the following updates cumulative update package for sql server r2 sp2 cumulative update package for sql server sp1
30551 am trying to add new column in table and the query giving me this error my rds instance size is small gb memory ecu virtual core with ecu table am trying to modify is having million rows and 7gb in size disk size of instance is 15gb free its critical time for us as we are stuck aws rds not allowing access to all parameters of mysql configurations how do get rid of this problem
30609 our established replication has broken requested wal segment has already been removed during downtime we cannot easily stop the master again can we do pg start backup rsync pgdata master to slave pg stop backup while the master postgresql is still under full load or will pg start backup lead to table locks blocks inconsistencies fire alarm slow db response in other words will pg start backup affect our application
30621 in the last release of my app added command that tells it to wait when something arrives in the service broker queue waitfor receive convertint message body as message from myqueue the dbas tell me that since the addition the log sizes have gone through the roof could this be correct or should be looking elsewhere
30626 disclaimer admittedly havent tried this yet but im not sure would know if it wasnt working correctly so wanted to ask would like to run nightly backup job via pg dumpall from hot standby server running streaming replication to avoid putting that load on the primary ive only seen mention of some gotchas people have run into here and here but very little guidance its okay if the backup lags behind the primary slightly as long as its consistent which it should be my questions are do really want to do this or should the backup be done on the primary server why when doing dump on the standby what settings do need and procedure should use to do that correctly must stop replication for the duration of the backup
30637 an old computer with oracle 8i is dead we have the daily backup of the database oradata we are unable to install this old oracle version anywhere because we dont have the install cds for it can we read this database with for example oracle express if yes how
30692 weve had reports of queries running slowly or timing out early in the morning and the only job see running that think could affect this is our database backup job the database itself is about 300gb and the backup job starts at 30am and doesnt finish until little after 00am the current syntax of our backup job is backup database databasename to disk ne database backups databasename bak with init nounload name ndatabasename bak noskip stats noformat is partition on the server which holds both the databases and the database backups it should also probably be noted that this is virtual server not dedicated standalone server we started getting complaints about slowdowns during the backup process right after we switched to virtual server so think it may be related is there way to run this backup job so it doesnt affect the query performance while its running we are using sql server
30696 have the following sql query select event id event iata device name eventtype description event data1 event data2 event plctimestamp event eventtypeid from event inner join eventtype on eventtype id event eventtypeid inner join device on device id event deviceid where event eventtypeid in and event plctimestamp between and and event iata like order by event id also have an index on the event table for the column timestamp my understanding is that this index is not used because of the in statement so my question is is there way to make an index for this particular in statement to speed up this query also tried adding event eventtypeid in as filter for the index on timestamp but when looking at the execution plan it doesnt appear to be using this index any suggestions or insight into this would be greatly appreciated below is the graphical plan and here is link to the sqlplan file
30734 im currently working on project which bulk import data from flat files csv about different files each linking to specific table through some stored procedure followed the steps as advised in data loading performance guide the database is in bulklogged recovery mode to minimize the logging when executing the stored procedure below on file containing rows get an error msg level state procedure sp import declarationclearancehistory fromcsv line the transaction log for database is full to find out why space in the log cannot be reused see the log reuse wait desc column in sys databases for testing purposes do full backup before starting the import looking at the log reuse wait desc see the following log reuse wait desc checkpoint all other import get imported successfully any input in solving this would be welcomed procedure dbo sp import declarationclearancehistory fromcsv filepath nvarchar as begin creating temproary table for importing the data from csv file dbcc traceon610 create table declarationclearancehistory itemid int identity1 not null cmsdeclarationid bigint not null statuscode nvarchar not null substatus nvarchar null departmentcode nvarchar null startdate datetime null enddate datetime null primary key clustered itemid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary inserting all the from csv to temproary table using bulk insert exec bulk insert declarationclearancehistory from filepath with fieldterminator rowterminator firstrow keepidentity codepage acp order itemid asc by using merge statement inserting the record if not present and updating if exist merge dbo declarationclearancehistory as targettable inserting or updating the table using declarationclearancehistory as sourcetable records from the temproary table records from csv file on targettable itemid sourcetable itemid defining condition to decide which records are alredy present when not matched by target then insert itemid cmsdeclarationid statuscode substatus departmentcode startdate enddate values sourcetable itemid sourcetable cmsdeclarationid sourcetable statuscode sourcetable substatus sourcetable departmentcode sourcetable startdate sourcetable enddate when matched if matched then update then update set targettable itemid sourcetable itemid targettable cmsdeclarationid sourcetable cmsdeclarationid targettable statuscode sourcetable statuscode targettable substatus sourcetable substatus targettable departmentcode sourcetable departmentcode targettable startdate sourcetable startdate targettable enddate sourcetable enddate dbcc traceoff610 end
30787 was answering this stackoverflow question and found strange result select from pg timezone names where name europe berlin name abbrev utc offset is dst europe berlin cet and next query select id timestampwithtimezone timestampwithtimezone at time zone europe berlin as berlin timestampwithtimezone at time zone cet as cet from data id timestampwithtimezone berlin cet im using postgresql and ubuntu just checked that on result is the same according to documentation it doesnt matter if use name or abbreviation is this bug am doing something wrong can someone explain this result edit for the comment that cet is not europe berlin im just selecting values from pg timezone names select from pg timezone names where abbrev cest name abbrev utc offset is dst and select from pg timezone names where abbrev cet name abbrev utc offset is dst africa tunis cet africa algiers cet africa ceuta cet cet cet atlantic jan mayen cet arctic longyearbyen cet poland cet during winter europe berlin is during summer it is edit2 in timezone has change from summer time to winter time at this two records have the same value in europe berlin this suggest that if use one of abbreviations cet or cest for big data range summer time and winter time result will be wrong for some of records will be good if use europe berlin changed the system time to and pg timezone names has changed also select from pg timezone names where name europe berlin name abbrev utc offset is dst europe berlin cest
30788 have two tables sources sname sid apple1 apple2 banks banksb bankerly prefixes pname pid app bank banker my goal to find the longest prefix that matches each of sources the results would look like this sname sid pname pid apple1 app apple2 app banks bank banksb bank bankerly banker constraints am using sql server and cannot upgrade know how to solve this problem using analytic functions but they are not available in sql server as far as know
30824 sql server newbie here im mysql guy im having look at something for client in their sql server and need some advice whoever designed the database chose to log insane amounts of stuff and never flush those log tables the largest table stores complete xml documents from transactions between the app and apis of sites like ebay can only assume that the database being about gigabytes hurts performance im guessing these tables are not queried on in the app but even so dont like the idea of such huge database after purging the log tables would anticipate total remaining size of about 30gb id like some advice on how to go about this from the little ive read on the subject after deleting bunch of data the database file will not automatically shrink in size also read that shrinking and re indexing are bad is this large database hurting performance of other tables should do something about it how can safely do something about it that will yield performance increase
30862 executing the query from here to pull the deadlock events out of the default extended events session select cast replace replace xeventdata xevent value data value varcharmax victim list deadlock victim list process list victim list process list as xml as deadlockgraph from select cast target data as xml as targetdata from sys dm xe session targets st join sys dm xe sessions on address st event session address where name system health as data cross apply targetdata nodes ringbuffertarget event as xeventdata xevent where xeventdata xevent value name varchar4000 xml deadlock report takes about minutes to complete on my machine the stats reported are table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server execution times cpu time ms elapsed time ms slow plan xml if remove the where clause it completes in less than second returning rows similarly if add option maxdop to the original query that speeds things up too with the stats now showing massively fewer lob reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server execution times cpu time ms elapsed time ms faster plan xml so my question is can anyone explain whats going on why is the original plan so catastrophically worse and is there any reliable way of avoiding the problem addition ive also found that changing the query to inner hash join improves things to some extent but it still takes mins as the dmv results are so small doubt that the join type itself is responsible though and presume something else must have changed stats for that table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server execution times cpu time ms elapsed time ms and plan after filling up the extended events ring buffer datalength of the xml was bytes and it contained events and testing cut down version of the original query with and without the maxdop hint select count from select cast target data as xml as targetdata from sys dm xe session targets st join sys dm xe sessions on address st event session address where name system health as data cross apply targetdata nodes ringbuffertarget event as xeventdata xevent where xeventdata xevent value name varchar4000 xml deadlock report select from sys dm db task space usage where session id spid gave the following results fast slow internal objects alloc page count internal objects dealloc page count elapsed time ms lob logical reads there is clear difference in tempdb allocations with the faster one showing pages were allocated and deallocated this is the same amount of pages used when the xml is put into variable too for the slow plan these page allocation counts are into the millions polling dm db task space usage whilst the query is running shows it seems to be constantly allocating and deallocating pages in tempdb with anywhere between and pages allocated at any one time
30932 since im dba and in many cases the de facto sysadmin sql server is installed on pretty much every server have to work with regularly realized recently that ive been using the sql agent as the job scheduler in pretty much every case rather than the native windows task scheduler from my perspective the sql agent has number of advantages over the native windows task scheduler remote from my workstation start stop monitoring of tasks shared schedules rather than each task on its own multiple steps and control flow different types of tasks alerts on failure completion can be configured to act as different users moderately descriptive error messages rather than just an error code however cant escape the feeling that this is bad practice the sql agent should be reserved for just database related tasks and should leave os level tasks running in the windows task scheduler despite my dislike of its usability is it okay to rely on the sql agent in this way if not should consider third party windows task scheduler to get some of the functionality im looking for
31018 have database accessed by around clients via tds over tcp which does not seem to be releasing log space the number of processes stays around the expected and some of them are quite long lived days the database now has gb in log space it only has gb data gb free due to space limitations on the drive would like to shrink to something more reasonable 10gb ish when execute dbcc shrinkfiledb log it returns an error that the end of the log is in use in order to free access to the end of the log attempted to place the database in single user mode with the following alter database db set single user with rollback immediate go alter database db set multi user go but the script is returning the following message repeated hundreds of times nonqualified transactions are being rolled back estimated rollback completion which leads me to believe that somewhere am leaving some transactions uncommitted am not aware of any process that would intentionally open this many transactions at one time so think they must accumulate over time never being closed question how do locate the offending process or script or why is the log not being released sys dm tran active transactions is showing reasonable transactions with understandable purposes sp who shows only the processes am aware of sql server version microsoft sql server r2 rtm x64 apr copyright microsoft corporation enterprise edition bit on windows nt x64 build service pack hypervisor server version windows server r2 x64 datacenter vcpus 16gb memory pass through disk for data and log os disk is vhd on hyper windows server r2 sp1 x64 datacenter dual intel x5650 core thread at 67ghz gb memory hypervisor only has three vms and does not show high resource use sql server vm shows cpu under load and cache hits
31045 have table of about 106k rows where about 11k rows have the values in columns swapped want to run query to fix it but dont think can do this update game set homescore awayscore awayscore homescore where awayscore homescore and winner hometeam or awayscore homescore and winner awayteam or can im worried the scores will end up the same value also please validate that my query will do what intend to do need to swap the home and away scores on the rows where the recorded winner which is correct doesnt match the scores recorded they were accidentally swapped by coding mistake now fixed saying who the winner is team with more points
31087 have developed java program which works with sql server as its database have installed sql server enterprise edition on my windows and designed the database via sql server management studio for example have designed its tables have set backup schedule and it takes backup from itself every 24hs once and other features which version of sql server express should be installed on users windows computer so that not only can my program connect to local db and work with that but also the other proscribed database features like backup schedule would work the database and java program are going to be on the same computer the criteria have for chosing database edition are my database has some tables the java program is going to insert update delete select and add columns to tables ireport will be used to report on the data in addition to the java application the database file is set so that it can not be deleted it has scheduled backup and nothing else
31104 is it possible to call sql server job to run within another job know we can add all steps of job to job but prefer not to do that first the job is already quite big and second couldnt find copy paste option to copy steps between jobs so it would be time consuming to add the steps manually any suggestion is appreciated
31348 have question regarding transaction log let just call it ldf for short contents am assuming database with full recovery model have read that ldf file contains logs each and every operation to the database that is in full recovery mode how is it different from logging during begin tran commands commit am asking because apparently you can roll back transactions but you cannot roll back standard commands in full recovery mode guess that during transaction the contents being logged into the ldf file are different than in regular full recovery logging is that right how is it different is it only the inclusion of undo operations for each action on related note have heard that there are commercial tools to rollback undo standard queries using full recovery ldf file how do they do it do they analyze the ldf contents and try to come up with inverse undo operations
31366 realize this is somewhat more subjective question but am looking to the community for guidance our company is fairly new to having dbas we used to use db2 for on the ibms iseries servers so there really wasnt need for dba at least operationally as the os pretty much managed everything for us within the last few years we started to move to db2 luw on aix thus need for dbas was born three of us were interested in making the career change from developers to dbas of which am one of them because we are on aix we defaulted to using ksh as our scripting language for database management functions creation deployment operations etc have found ksh to be quite powerful yet some of the functions need to perform get rather complex an example of this is my question over on unix and linux stackexchange based on that question as well as reading blogs by other db2 administrators see general suggestion that perl is better suited for administration scripting than ksh is realize that the unix linux forum would have their own thoughts on such thing but wish to approach this from database administration viewpoint regardless of dbms which scripting language seems better suited for administration on the unix linux platform perl ksh or perhaps other what strengths and weaknesses in your use of scripting languages did you encounter as dba and why did you pick the language you did what dont want is highly subjective flame war on my language is better than your language kind of stuff like used to see in the development community regarding say java vs ruby etc am looking for honest as objective as possible information regarding what best suits the needs of database administrators
31368 have two questions have database in full recovery mode lets call transaction log an ldf file for short sql server allows me to restore to point in time in the past when restoring from database to itself does this even make sense when tried it did not yield expected results why is this even allowed if it doesn work can it have any practical purposes when playing with the ldf files discovered curiosity consider having database in simple mode then switch it to full recovery mode then insert some rows into the database check the ldf and find out that the ldf file size did not increase when take full backup and then insert rows again into the database only then the ldf file increases is this some sort of optimization note had ldf file shrinked before both operations thanks for all explanations
31378 scenario each time data is inserted updated deleted into in from table multiple unrelated items of business logic need to be executed question given that the solution is to use database triggers which option is better single trigger per operation insert update delete table executed on an insert table executed on an update table executed on delete each trigger would handle multiple unrelated items of business logic or multiple triggers per operation that were segregated by concern table logging for logging on an insert table ri for enforcing referential integrity on an insert table bl for executing business logic on an insert table logging for logging on an update table ri for enforcing referential integrity on an update table bl for executing business logic on an update table logging for logging on an delete table ri for enforcing referential integrity on an delete table bl for executing business logic on an delete prefer option because single unit of code has single concern am not dba and know enough about sql server to make me dangerous the architecture and schema of the database must not be changed there are several places in the schema that should be enforced with explicit relationships but unfortunately cannot be changed yet that is the reason need to manually enforce referential integrity instead of relying on an explicit relationship are there any compelling reasons to handle all of the concerns in single trigger im specifically concerned about performance order of execution and rollback
31383 can anyone explain the main differences any help appreciated
31460 am moving records from one database to another as part of archiving process want to copy the rows to destination table and then delete the same rows from the source table my question is what is the most efficient way to do check if the first insert was successful before deleting the rows my idea is this but feel there is better way num records select countid from source table where criteria for eligible rows insert into destination table where criteria for eligible rows if select countid from destination table where criteria numrecords delete from source table where criteria is it better possible to combine it with raiserror function thank you
31511 copied this code from here create table records email text references usersemail lat decimal lon decimal depth text upload date timestamp comment text primary key upload dateemail create table samples date taken timestamp temp decimal intensity decimal upload date timestamp email text primary keydate takenupload dateemail foreign key upload dateemail references recordsupload dateemail the first thing that caught my eyes was the use of natural composite keys as primary keys for both tables things was able to extract from this piece of code the users table not shown here uses email as primary key of type text the records table uses composite key of text timestamp the samples table uses composite key of fields of type text timestamp timestamp now in this case wouldnt surrogate key be better of identification mean performance wise indexing an int should be better than indexing text is there something that could make surrogate key bad choice
31514 is there way can figure out the best way to know which indexes to create for table
31561 have run into some pre existing sql that im having hard time uderstanding select maxi symbol symbol maxi ticker cusip maxi name name sumh quantity totalquantity sumh marketvalue totalmarketvalue maxh pricelc price maxi categorycode5 buy sell maxi equivfactor1 pricetgt maxp lastprice currprice maxi assetclass target maxi industry industry maxi categorycode1 risk from holdings secure investments price where symbol symbol and product stock and quantity and categorycode5 not in and symbol symbol group by symbol symbol ticker name categorycode5 assest class industry and categorycode1 are all varchar fields the remaining fields are decimals my best educated guess is that somehow max is being used to avoid multiple grouping columns but how can this return the correct results
31593 have two databases database1 and database2 both databases contain table that has similar structure exemplified as follows id name phonenoformat dialingcountrycode internationaldialingcode internettld however due to some reason one of the tables in one of the databases has data that is not exactly the same as that contained in the other table in the another database so how can compare database1 table1 against database2 table1 tried using the following query but nothing happened so was wondering if have to rewrite it select mintablename as tablename id name phonenoformat dialingcountrycode internationaldialingcode internettld from select table as tablename id name phonenoformat dialingcountrycode internationaldialingcode internettld from database1 mdf dbo table1 as union all select table as tablename id name phonenoformat dialingcountrycode internationaldialingcode internettld from database2 mdf dbo table1 as tmp group by id name phonenoformat dialingcountrycode internationaldialingcode internettld having count order by id
31701 encountered new issue yesterday with one of my mysql slave dbs that runs on ec2 in aws the db was created from snapshot of another slave the data is correct but for at least one table secondary index is returning incomplete results querying child table by the parent id was returning rows when it should have returned querying the missing rows by primary key worked and the correct parent id was returned so the problem is with the secondary index this problem is highly concerning to me since presumably even if all of the data on the slave matches the master will still get incorrect results from some queries run on the slave my brute force solution was to rebuild the entire table like this alter table my table engine innodb that resolved the issue for this specific table but im left with the following questions how can determine if other tables have similar index corruption whats the most efficient way to fix the corrupt indexes ive found some good resources online for finding and fixing innodb data corruption but havent found anything relevant for innodb index corruption looked in the mysql error log and didnt find smoking gun did find some troubling innodb errors im assuming this is separate issue but it could be related innodb unable to find record to delete mark innodb tuple data tuple fields len hex 04af1f21 asc len hex 0a1c03bd asc innodb record physical record fields compact format info bits len hex 04af1f21 asc len hex 0a052a77 asc
31720 want way to establish which columns in given database are joined via pk fk relationships can return the pk fk information for given table via select from information schema key column usage as cu where exists select tc from information schema table constraints as tc where tc constraint catalog mydatabase and tc table name mytable and tc constraint type primary key and tc constraint name cu constraint name go but for pk returned from such query how do establish the associated fk assuming there is one know you can also get the referenced tables via select constraint name name foreign schema object schema nameparent object id foreign table object nameparent object id referenced schema object schema namereferenced object id referenced table object namereferenced object id from sys foreign keys where object namereferenced object id mytable go but am struggling now to get the explicit column references am creating script generator for qlikview to generate the script need the constraints and the associated links need all of the constraint information for any given column if any want to construct database class that holds all the information for given database this class structure database table column constraints will then be used to get the matches between different columns on pk fks clearly some columns will have fks only and in this case also want to retrieve the pk information of the corresponding key some will have only pks and then want the reverse some of course can have both
31752 im loading 100gb file via load data infile ive had good success with myisam few hours and done im trying it now using innodb the load starts fast at over 10mb sec watching the table file growth file per table is turned on but after about 5gb of data it slows down to the 4mb sec range as get over 20gb it was down around 2mb sec innodb buffer pools size is 8g and ive done the following prior to running the load data infile command set session sql log bin set autocommit set unique checks set foreign key checks alter table item load disable keys run load data infile cant see the reason why its starting off well and slowing down over time also using the same settings ran the same load data infile command with the table using innodb and myisam and 5gb test dataset myisam was 20x faster innodb mysql load data concurrent local infile tmp item replace into table item load query ok rows affected warnings min sec records deleted skipped warnings myisam mysql load data concurrent local infile tmp item replace into table item load query ok rows affected warnings min sec records deleted skipped warnings anything else should consider trying the myisam engine is able to keep up the load rate much better additional details ive tried loading the files individually no difference incidentally have files of 500mb each within each file the keys are sorted after getting 40gb in overnight 12h later the load rate was down to 5mb sec meaning the operation is practically speaking impossible havent found any other answers to similar questions on other forums its seeming to me that innodb doesnt support loading large amounts of data into tables over few gb in size
31773 have inherited table that contains column that is used to chain between various related rows within the same table for instance id bar reference id foo foo foo have no idea what to call this structure in order to search for how should deal with it the idea is that the table maintains sort of living history within itself my problem is that may have row or but need to get to row somehow that is agnostic to the number of levels that need to traverse
31805 there was rather innocuous question about adding dates and times in sql server that set off rather fascinating taxonomic debate so how do we differentiate between these related terms and how we use them properly row record
31823 2m rows inserted using load data infile to innodb takes 5min and profiling shows of that time is in system lock does this tell me anything useful mysql set profiling query ok rows affected sec mysql load data concurrent local infile tmp item replace into table item load query ok rows affected warnings min sec records deleted skipped warnings mysql show profile for query status duration starting checking permissions opening tables system lock waiting for query cache lock query end closing tables freeing items logging slow query logging slow query cleaning up rows in set sec
32906 im still struggling to understand sqls backup jobs we currently have sql server instance using the full recovery model we do full database backup every week backup database mydatabase to disk ne database backups mydatabase bak with init name nmydatabase bak and differential backup once day backup database mydatabase to disk ne database backups mydatabase diff bak with init differential name nmydatabase diff bak now am trying to figure out how to schedule transaction log backups every hour so at most we would lose an hours worth of work backup log mydatabase to disk ne database backups mydatabase log bak my problem is our storage space is limited and dont want the log file to grow too large can use with init on the transaction log backup to force it to create new file every hour or do need all the transaction log backups since the last differential backup to restore to specific point in time during the day and if do need to keep all copies since the last differential is there way to tell it to reset anytime do backup
32919 were working on migrating our database tables from sql r2 to sql azure were currently in the proof of concept stage figuring out the process well be following we have process that we think is solid but id like to perform some validation diffs on both schema and data to confirm that we have successful migration ive never done this before with sql azure and am looking for good way to do this how can perform this verification effort on both the schema and data ultimately this is one time migration well do it few times but the real migration will only be done once
32975 am trying to run the below sql command select array select column name from information schema columns where table name gis field configuration stage and get the below error error could not find array type for datatype information schema sql identifier
33021 what will be easier way to group records by range of percents here what try to accomplish select top percents between and from mytable order by pki here is way did this but have felling that it cloud be much much easier select top25 percent from mytable where id not in select top25 percent id from mytable order by pki order by pki this should be top percents of my table is there better nicer way to do this
33074 ive seen several people call set transaction isolation level read uncommitted before reading system dmvs is there ever any reason to do this assuming you arent mixing calls to dmvs and tables in the same transaction
33085 ive found lot of resources that mention that adding an index to table makes searches faster and inserts slower but only if the table is large this creates tradeoff which is design decision but there should be an approximate table size before which using an index is absurd rows for example is probably way beneath that limit does anybody know about where this limit would be or know of resource that would point me in the right direction
33196 have table with multicolumn index and have doubts about the proper sorting of the indexes to get the maximum performance on the queries the scenario postgresql table with about one million rows values in column c1 can have about different values we can assume the values are evenly distributed so we have about rows for every possible value column c2 can have different values we have rows for every possible value when searching data the condition always includes values for these two columns so the table has multicolumn index combining c1 and c2 have read about the importance of properly ordering the columns in multicolumn index if you have queries using just one column for filtering this is not the case in our scenario my question is this one given the fact that one of the filters selects much smaller set of data could improve performance if the first index is the most selective one the one which allows smaller set had never considered this question until saw the graphics from the referenced article image taken from the referenced article about multicolumn indexes the queries use values from the two columns for filtering have no queries using just one column for filtering all of them are where c1 parametera and c2 parameterb there are also conditions like this where c1 abc and c2 like ab
33285 how do give user account in postgresql the ability to create and drop databases is there way to do this with grant
33297 am running sql server r2 and need to create new job that will basically run query at the start of each month at the 1st of each month at am here is the query insert into supporttracker dbo dashboardrecords select bg id bg short desc bg reported date bg status updated date us firstname us lastname lastupdateduserfirstname lastupdateduserlastname st name pr name ct name pj name assigneduserfirstname assigneduserlastname bg project no of hours bugtype subtype device pj parent id from supporttracker dbo viewissuelistwbugtypendevice where bg reported date between and and bg id not in select bg id from supporttracker dbo dashboardrecords order by bg reported date asc my problem is at the where clause these two dates have to change every month if we are on the need these dates to be and if we are on the need these dates to be and basically need to capture parts of the database and save it in the table as it was at the end of each month thanks
33319 have function in postgresql called fun test it has composite type as input parameter and keep getting casting error when call it create or replace function netcen fun testmyobj netcen testobj returns boolean as body declare tmp code smallint cur member refcursor begin check if the member exists first open cur member for execute select testkey from netcen test where testkey myobj testkey fetch cur member into tmp code close cur member case tmp code when coalescetmp code0 then record not found insert new record will skip user defined validation for now insert into netcen test valuesmyobj testkey myobj tes myobj testname else record found update the record update netcen test set test myobj test testname myobj testname where testkey myobj testkey end case end body language plpgsql below is the type testobj create type netcen testobj as testkey smallint tes text testname text when call the function select netcen fun test3khaendra me comkhaendra netcen testobj get the following error message error operator does not exist smallint boolean line select case variable in coalescetmp code0 hint no operator matches the given name and argument types you might need to add explicit type casts query select case variable in coalescetmp code0 context pl pgsql function fun test line at case where should cast definition of the table netcen test create table netcen test testkey smallint not null default tes netcen dom email validation testname text constraint key primary key testkey erwin thanks for the links read and have modified my function to this please go through it and tell me if it can work well with several clients calling the same function concurrently create or replace function netcen fun test modifiedmyobj netcen test returns boolean as body declare myoutput boolean false begin update netcen test set tes myobj tes testname myobj testname where testkey myobj testkey if found then myoutput true return myoutput end if begin insert into netcen test valuesmyobj testkey myobj tes myobj testname myoutput true exception when others then update netcen test set tes myobj tes testname myobj testname where testkey myobj testkey myoutput true end return myoutput end body language plpgsql have done away with the type test and just used the table test didnt know that could work
33333 in mysql workbench is it possible to search for specific column name in all the tables writing the string to look for in the field at the top right does nothing thank you
33357 im attending the free db course at stanford online and frankly one of the exercises gave me some trouble have feeling that this should be horribly simple so for dba im obviously not very good with sql were working with simplified scenario for rating movies for all cases where the same reviewer rated the same movie twice and gave it higher rating the second time return the reviewers name and the title of the movie heres the schema movie mid title year director reviewer rid name rating rid mid stars ratingdate how should go about this
33448 the article sql server tempdb best practices increase performance suggests that should split tempdb into number of files equal to the number of cores so for cores you get files by having the larger number of files you can increase the number of physical operations that sql server can push to the disk at any one time the more that sql server can push down to the disk level the faster the database will run with standard databases sql server can cache large amount of the data that it needs into memory because of the high write nature of the tempdb the data needs to be written to the disk before it can be cached back up into memory though it sounds good in theory is it really that good as general optimisation is it something that may only apply for specific systems where io is very high
33553 have made draft remote application on top of libpq for postrgresql it behaves well but have profiled the general functioning of the application for each final business result that produce it happens that call something like select clause over tcpip have reminiscences from sql server reminding me to minimize the number of interactions between my remote application and the database having analyzed my selects do think could reduce this number to select clauses using joins but dont remember the syntax for using the result of select in another select select from individual inner join publisher on individual individual id publisher individual id where individual individual id here would like to use the results of another select this other select would be simply of the kind select identifier from another table where something something here is the simplified tables layout declined number of times for different item types totally different types hence the sql queries if optimized table passage id passage pk business field passage bytea table item id item pk id passage fk business field item text table item detail id item detail pk id item fk business field item detail text image content bytea there are several id item for one id passage there are several id item detail for one id item how would you write that what is the name for describing the action of redirecting one select into another if any
33561 have table with this rows stickers id title keywords ts vector sticker case 580h 580h cas stick sticker case 580l 580l cas stick sticker case cas stick sticker case plus cas plus stick well when do search using this script just row return how do return the row and select from stickers where keywords to tsquerycase
33564 have database that has gb transaction log it is caused by the fact that the database is in full recovery mode has been used for over years prior to the first backup want to truncate and shrink the log file with the following command backup log dbname with truncate only go dbcc shrinkfilelogicalfilename100 want to know after ran the above command in the future will be able to do restore to point in time between future full backup and time line pre truncate backup truncate and shrink future full backup future full backup thanks
33596 after running fairly hefty query the execution plan gave me missing index suggestion which was of the form timestamp include customerid eventid id employeeid which seems to be covering index the include column are all either primary keys id or foreign keys however my querys where clause is filtering by timestamp customerid and eventid dont know why these werent included in the main part of the index so my question is is there any difference in using the suggested index above or what think is better alternative timestamp customerid eventid include id employeeid my understanding is that this will still allow timestamp only index seeking but will also further assist my query by having the customer and event ids which are filtered in the main part think this was something to do with the width of the main part fyi timestamp is datetime20 customerid is an int and eventid is byte am testing this myself at the moment but this is huge table over rows and it is taking time to compare the indexes that and id like to learn more about this thanks
33624 was wondering about this example create table cities city varchar80 primary key location point create table weather city varchar80 references citiescity temp lo int temp hi int prcp real date date the definition of city as varchar80 is duplicated here is there postgresql syntax that allows not to duplicate varchar80 only basing itself on references citiescity so as to create the weather table
33669 have seen and heard from multiple sources that it can be good idea performance wise to write your db journal and your data files to separate disks whats the recommended way to do this on ec2 if we write our data files to raid of ebs drives where would most people write the journal to should just use completely separate raided set of drives should use single non raid drive should just leave data and journal on the same raid
33698 so we had long running proc causing problems this morning sec run time we decided to check to see if parameter sniffing was to blame so we rewrote the proc and set the incoming parameters to variables so as to defeat parameter sniffing tried true approach bam query time improved less than sec when looking at the query plan the improvements were found in an index the original wasnt using just to verify that we didnt get false positive we did dbcc freeproccache on the original proc and reran to see if the improved results would be the same but to our surprise the original proc still ran slow we tried again with with recompile still slow we tried recompile on the call to the proc and inside the proc itself we even restarted the server dev box obviously so my question is this how can parameter sniffing be to blame when we get the same slow query on an empty plan cache there shouldnt be any parameters to snif are we instead being affected by table stats not related to the plan cache and if so why would setting the incoming parameters to variables help in further testing we also found that inserting the option optimize for unknown on the internals of the proc did get the expected improved plan so some of you folks smarter than can you give some clues as to whats going on behind the scenes to produce this type of result on another note the slow plan also gets aborted early with reason goodenoughplanfound while the fast plan has no early abort reason in the actual plan in summary creating variables out of incoming parameters sec with recompile sec dbcc freeproccache sec option optimize for uknown sec update see slow execution plan here https www dropbox com cmx2lrsea8q8mr6 plan slow xml see fast execution plan here https www dropbox com b28x6a01w7dxsed plan fast xml note table schema object names changed for security reasons
33700 have large table million rows im trying to bulk insert into sql server and get the error could not allocate space for object mydb in database stroke because the primary filegroup is full create disk space by deleting unneeded files dropping objects in the filegroup adding additional files to the filegroup or setting autogrowth on for existing files in the filegroup there is another table in the database with around million rows this database will only be used on single machine and it will be designed to mine data that already exists and it under no circumstances will ever grow beyond its current size for situation such as this whats the best way to tackle this so sql server doesnt complain will the solution matter that this db wont be exposed to multiple users
33703 is there sql query that shows the last restore datetime for certain database
33737 we are developing platform for prepaid cards which basically holds data about cards and their balance payments etc up until now we had card entity which has collection of account entity and each account has an amount which updates in every deposit withdrawl there is debate now in the team someone has told us that this breaks codds rules and that updating its value on each payment is trouble is this really problem if it is how can we fix this
33760 came across this puzzle in the comments here create table int select from having sql server and postgresql return row mysql and oracle return zero rows which is correct or are both equally valid
33894 have postgresql database where part of it handles agent commissions each agent has his her own formula of calculation how much commission they get have function to generate the amount of commission each agent should get but its becoming impossible to use as the number of agents grow am forced to do some extremely long case statements and repeating code which has made my function very big all the formulas have constant variables days worked that month new nodes accuired loyalty score subagent commission base rate revenue gained the formula can be something like each agent negotiates the payment formula with the hr dept so can store the formula in the agents table then have like small function that just gets the formula from the table and translates it with values and computes the amount
33937 given this question on reddit cleaned up the query to point out where the issue was in the query use comma first and where to make modifying queries easier so my queries generally end up like this select companyname shippeddate od unitprice productname from customers as inner join orders as on customerid customerid inner join order details as od on orderid od orderid inner join products as on productid od productid where and shippeddate between and and productname tofu order by companyname someone basically said that is generally lazy and bad for performance given that dont want to prematurely optimize do want to follow good practices ive looked at the query plans before but generally only to find out what indexes can add or adjust to make my queries run faster the question then really does where cause bad things to happen and if so how can tell minor edit ive always assumed as well that would be optimized out or at worst be negligible never hurts to question mantra like gotos are evil or premature optimization or other assumed facts wasnt sure if and would realistically affect query plans or not what about in subqueries ctes procedures im not one to optimize unless needed but if im doing something that is actually bad id like to minimize the effects or change where applicable
33943 im new to postgres and trying to migrate our mysql databases over in mysql can grant select update insert and delete privileges on low privileged user and enable those grants to apply to all tables in specified database must be missing something in postgres because it looks like have to grant those privileges for each table one at time with many databases and hundreds of tables per database that seems like daunting task just to get off the ground in addition once database is in operation adding tables happens frequently enough that wouldnt want to have to grant permissions each time unless absolutely necessary how is this best accomplished
34047 in sql server the date datatype was added casting datetime column to date is sargable and can use an index on the datetime column select from where castdatetimecol as date the other option you have is to use range instead select from where datetimecol and datetimecol are these queries equally good or should one be preferred over the other
34079 tracking who made the change identified by cdc along the lines of my datetime hack tried the same approach by adding suser sname as new field with default value on the cdc change track table but that seems to return the owner of the cdc process and not the user who initiated the change on the base table also tried original login but that returns the sql service account login again likely associated with the cdc process and not the user who initiated the change found similar question on stack overflow but with no answer other than tracking changes from the front end or via trigger which seems to defeat the purpose of using cdc wouldnt repost but since the original was on stackoverflow thought id give it try here especially if r2 or has introduced better way so in short how do know who made the change in change data capture
34132 want to have an export from my database diagram to pdf or image types how can do this worked with sql server r2
34136 ive got postgres and pgbouncer the version that came with the stack builder have net application connecting via npgsql version on win7 machine my application can happily connect straight to the postgres server but always fails to connect via the pgbouncer my connection string to npgsql after some substitution is sslmode prefer timeout server port user id password database pooling false ive also tried to connect to pgbounce with protocol explicitly and that didnt work the database line from pgbounce ini is databases something host localhost port dbname somethingelse user someone password aaa and in the userlist txt have someone aaa as required when my app runs the npgsql says connection forcibly closed and the pgbouncer error log says log file descriptor limit max client conn max fds possible log listening on log listening on log process up pgbouncer libevent stable win32 adns evdns2 log something someone fe80 997b 396e eacc dd2b login attempt db somethingelse user someone log something someone new connection to server log something someone fe80 997b 396e eacc dd2b closing because client close request age log nodb nouser fe80 997b 396e eacc dd2b closing because bad packet header age warning nodb nouser fe80 997b 396e eacc dd2b pooler error bad packet header the npgsql exception says from log file in app the topmost exception follows down the nested exceptions exception message unable to read data from the transport connection an established connection was aborted by the software in your host machine exception source npgsql exception stacktrace at npgsql npgsqlclosedstate opennpgsqlconnector context in projectdirectory npgsql2 src src npgsql npgsqlclosedstate cs line at npgsql npgsqlconnector open in projectdirectory npgsql2 src src npgsql npgsqlconnector cs line at npgsql npgsqlconnectorpool getnonpooledconnectornpgsqlconnection connection in projectdirectory npgsql2 src src npgsql npgsqlconnectorpool cs line at npgsql npgsqlconnectorpool requestconnectornpgsqlconnection connection in projectdirectory npgsql2 src src npgsql npgsqlconnectorpool cs line at npgsql npgsqlconnection open in projectdirectory npgsql2 src src npgsql npgsqlconnection cs line at snip the thing in my code that goes and opens the connection snip exception message unable to read data from the transport connection an established connection was aborted by the software in your host machine exception source system exception stacktrace at system net sockets networkstream readbyte buffer int32 offset int32 size at system io stream readbyte at npgsql npgsqlclosedstate opennpgsqlconnector context in projectdirectory npgsql2 src src npgsql npgsqlclosedstate cs line exception message an established connection was aborted by the software in your host machine exception source system exception stacktrace at system net sockets socket receivebyte buffer int32 offset int32 size socketflags socketflags at system net sockets networkstream readbyte buffer int32 offset int32 size also window firewall is off and anti virus turned off temporarily the hba conf file for postgres has host all all md5 host all all md5 ipv6 local connections host all all md5 host all all fe80 md5 ive googled around and found no solution help thanks peter
34151 want to have one to many relationship in which for each parent one or zero of the children is marked as favorite however not every parent will have child think of the parents as questions on this site children as answers and favorite as the accepted answer for example tablea id int primary key tableb id int primary key parent int not null foreign key references tablea id the way see it can either add the following column to tablea favoritechild int null foreign key references tableb id or the following column to tableb isfavorite bit not null the problem with the first approach is that it introduces nullable foreign key which understand is not in normalized form the problem with the second approach is that more work needs to be done to ensure that at most one child is the favorite what sort of criteria should use to determine which approach to use or are there other approaches am not considering am using sql server
34173 was wondering if the sql server management studio express mainly for microsoft sql server be installed on standalone machine without any of the sql services and stuff to connect to remote database run sql server have been told by one of my managers that they wish me to create web ui for specified users to run queries update the database this is within my ability of php but was thinking for ease could make proposition to scrap the need for the web ui because this will cost lot of time for our server administrators but install smse on our network machines to connect to manage our databases is this possible or would have to go through the painful task of creating webui which displays schemas tables etc
34313 need to create an alert that will notify me when any query has been blocked for more than seconds for example if someone has transaction open on table and forgets to run commit or rollback is this possible to get from the system tables
34356 were doing an etl process when all is said and done there are bunch of tables that should be identical what is the quickest way to verify that those tables on two different servers are in fact identical im talking both schema and data can do hash on the table its self like would be able to on an individual file or filegroup to compare one to the other we have red gate data compare but since the tables in question contain millions of rows each id like something little more performant one approach that intrigues me is this creative use of the union statement but id like to explore the hash idea little further if possible post answer update for any future vistors here is the exact approach ended up taking it worked so well were doing it on every table in each database thanks to answers below for pointing me in the right direction create procedure dbo usp databasevalidation tablename varchar50 as begin set nocount on parameter if no table name was passed do them all otherwise just check the one create temp table that lists all tables in target database create table chksumtargettables fullname varchar250 name varchar50 chksum int insert into chksumtargettables fullname name chksum select distinct mydatabase name name as fullname name as name as chksum from mydatabase sys tables inner join mydatabase sys schemas on schema id schema id where name like isnull tablename create temp table that lists all tables in source database create table chksumsourcetables fullname varchar250 name varchar50 chksum int insert into chksumsourcetables fullname name chksum select distinct mylinkedserver mydatabase name name as fullname name as name as chksum from mylinkedserver mydatabase sys tables inner join mylinkedserver mydatabase sys schemas on schema id schema id where name like isnull tablename build dynamic sql statement to populate temp tables with the checksums of each table declare targetstmt varcharmax select targetstmt coalesce targetstmt update chksumtargettables set chksum select checksum aggbinary checksum from fullname where name name from chksumtargettables select targetstmt declare sourcestmt varcharmax select sourcestmt coalesce sourcestmt update chksumsourcetables set chksum select checksum aggbinary checksum from fullname where name name from chksumsourcetables execute dynamic statements populate temp tables with checksums exec targetstmt exec sourcestmt compare the two databases to find any checksums that are different select tt fullname as tables whose checksum does not match from chksumtargettables tt left join chksumsourcetables st on tt name st name where isnullst chksum0 isnulltt chksum0 drop the temp tables from the tempdb drop table chksumtargettables drop table chksumsourcetables end
34358 on sql server we do weekly full backup with nightly incremental backups want to know if it is possible to restore single table from backup either to the source database or different one can not find any clear answer online thanks in advance
34381 have table that contains posts and another table that contains meta options for each post in first table the meta options table is key value pare table lets say have the posts tables that look like that posts id other columns data data data data data and the meta options table that look like that meta id post id meta key meta value views maxviews views maxviews publison auhor myuser auhor another author the question is how can get the post with id equals to by making comparison of the meta key values views and maxviews for example like to retrive the post id with id only if the views is lower than the masviews in the meta table any help please
34484 what is the difference between sql batch sql statement and remote procedure call how can tell if part of the sql code is batch or statement
34525 at work we host all our webservers on amazon ec2 and usually have used mysql databases installed on the same box as our apache webserver and communicated with them on localhost we now face need to migrate our database to its own server for one of our systems have choice between two solutions use amazon rds or just launch new amazon ec2 box and install mysql on it rds being dedicated database service provided by the same company as ec2 seems like it ought to be the obviously better option however when look at the pricing for the two options see http aws amazon com ec2 pricing and http aws amazon com rds pricing it seems that an rds server costs almost twice as much as an ec2 server for box with the same specs given that im capable of handling backups myself and that ec2 offers the same ability to scale up the instance as required that rds does cant see any reason at all to use rds instead of ec2 it seems like im probably missing something big though because if were right nobody would use rds what exactly am missing and what are the advantages of rds over installing your own database on an ec2 instance
34603 am in database designer in data warehouse environment am used to dealing with tables with maximum of millions rows and am now faced with tables with more than half billion rows are there any significant differences with the tools in the efficiency toolbox can trust my previous knowledge of indexes partitions and the like or are some of these specific tools more of hindrance than help with such large data any other tips for dealing with the tables already found great post on updating million rows to same value
34633 using xp cmdshell can be quite helpful and sometimes possibly the only answer to some scenarios ive read some posts on the internet that enabling xp cmdshell might jeopardize the security of the database server my question is that is there anything we can do to reduce the risk for example can we set some restrictions applying users role etc providing safeguards to mitigate the risk thanks
34730 my servers default collation is latin1 general ci as as determined by this query select serverpropertycollation as collation was surprised to discover that with this collation can match non digit characters in strings using the predicate like why in the default collation does this happen cant think of case where this would be useful know can work around the behavior using binary collation but it seems like strange way to implement the default collation filtering digits produces non digit caracters can demonstrate the behavior by creating column that contains all possible single byte character values and filtering the values with the digit matching predicate the following statement creates temporary table with rows one for each code point in the current code page with p0 as select union all select p1 as select from p0 as cross join p0 as p2 as select from p1 as cross join p1 as p3 as select from p2 as cross join p2 as tallynumber as select row number over order by select from p3 select number as codepoint charnumber as symbol into codepage from tally where number and number each row contains the integer value of the code point and the character value of the code point not all of the character values are displayable some of the code points are strictly control characters here is selective sample of the output of select codepoint symbol from codepage would expect to be able to filter on the symbol column to find digit characters using like predicate and specifying the range of characters thru select codepoint symbol from codepage where symbol like it produces surprising output codepoint symbol the set of code points thru are the ones expect what surprises me is that the symbols for superscripts and fractions are also included in the result set there might be mathematical reason to think of exponents and fractions as numbers but it seems wrong to call them digits using binary collation as workaround understand that to get the result expect can force the corresponding binary collation latin1 general bin select codepoint symbol from codepage where symbol like collate latin1 general bin the result set includes only the code points thru codepoint symbol
34754 where can find legal copy of the iso sql standard
34765 potential customer wants to evaluate our storage system they run windows 2008r2 x64 with 4kb ntfs on virtual test machine theyve sent us they did not seem to know this at hand so think its reasonable to assume that environment was not tweaked the tests are inserting indexing searching deleting tool is not known to me given that the windows ntfs block size is 4kb and sql write in 64kb chunks is it safe to assume that block size of 64k on the san is good choice they run sql server perhaps standard
34766 have vague understanding of transactions that it means you can help the server understand that series of queries are related scenario suppose have series of queries which creates user inserts user info record creates an empty profile record and initialises few other records etc if for whatever reason the script were to fail half way through this what do need to do to ensure that everything to do with the creating user queries gets undone from what understand reading the mysql documentation it as simple as using start transaction and commit statements if that is so why do so many cmss neglect to do so suppose common argument against it might be that it is unnecessary for short scripts dealing with only few record updates but am not right in thinking that it is also highly beneficial when you are dealing with replication environments too
34947 we have stored procedure that users can run manually to get some updated numbers for report thats used constantly throughout the day have second stored procedure that should be run after the first stored procedure runs since it is based on the numbers obtained from this first stored procedure however it takes longer to run and is for separate process so dont want to make the user wait while this 2nd stored procedure gets ran is there way to have one stored procedure start second stored procedure and return immediately without waiting for results im using sql server
34955 am creating restful api am struggling to decide on the best way to design my database tables around my resources initially though table per resource would be good way to go but im now worried that this will result in exponentially bigger tables the further down the resource chain you go for example imagine have three resources users clients sales users are subscribers to my api clients are the users customers and sales are purchases made by each client to the users account sale resource is accessed as follows get users userid clients clientid sales salesid so if there are users each with customers and for each customer there are sales the table size gets larger the further down the resource chain we go im fairly confident that sql can cope with large tables but im not sure how read and writes will slow things down the example above maybe doesnt illustrate it but my api will have progressively more writes and reads the further down the resource chain we go therefore have the scenario where the biggest tables in my database will be read and written to more times than smaller tables it will also be necessary to join tables before running queries the reason is that allow each user to have client with the same name to avoid getting the wrong client data the users table and clients tables are joined by userid this is also the case for sales will joining large tables and running reads and writes slow things down further
34976 in sql server is there way to find users that either dont exist at the server level an account that was deleted at server level but wasnt disassociated from databases before it was deleted or accounts that arent linked an account may have been deleted at the server level but not db level then readded but the db level was never cleaned up ive got very messy server and it would be awesome if there was query to run to find these
34979 im in the process of testing and populating specific table that leverages the sequence object in this process im testing populating the table with tens of thousands of insert lines as im unfamiliar with how to program this the problem im seeing with this specific table is that when start another population test the sequence does not reset back to the first number want which is when wish to re run new test delete the table in question then run the following drop sequence foo fee go drop schema foo go when want to re run the test run the following schema sequence commands which are fired in the order below create schema foo go create sequence foo fee start with increment by no cycle no cache go then create the table create table foo sample table with data order number bigint primary key not null sample column one nvarcharmax null sample column two nvarcharmax null sample column three nvarcharmax null go once that is completed run the following insert command times insert into foo sample table with data order number sample column one sample column two sample column three values next value for foo fee blah blah blah blah blah blah now there is absolutely no problem with the data entering into the table the challenge im encountering is that when delete the table drop the schema and sequence then re create the table sequence and schema the sequence picks up from the last number in the previous database incarnation and not reset back to one for example if the last number in the sequence is say the next sequence number in the new table is after deleting the table and dropping the schema and sequence run the following to verify removal of the sequence and schema select from information schema schemata go select from sys sequences go im stumped as to why this is happening is there another command that im missing here that would help me localize what exactly is going on here should note that this table belongs to database with other tables all running the sequence command correctly this is sql sp1 enterprise edition installation
35015 have microsoft sql server r2 database have some applications in different programming languages some of them are legacy and would not like to modify them is there way to log at server level all query errors regardless of the application causing it would like to know for each error the query causing it the error type and ideally loginame and hostname
35081 we have databases in our server we want to take databases backup using mysqldump how can ignore remaining databases in mysqldump command is there any option for mysqldump to ignore databases for backup in mysql know the general mysqldump command but it is very lengthy want to ignore only databases and need to take remaining dbs backup
35177 is there best way to automatically restart sql server on regular basis read that could create batch file to net start net stop the agent and service in batch file and schedule that but was curious if there was better way of doing this please nevermind how this is indicative of larger problem of operation ive been arguing this for month and weve ended up here thanks
35182 am becoming somewhat involuntary dba at work at themoment and really need some help on something we have 40gb database in full recovery mode no log backup configured and huge log file of 84gb my plan thus far to salvage this situation is to run full log backup on the database shrink the log file and instigate maintenance plan to run log backup every night with the database backup to help keep it under control my problem is do not want the log file to shrink down to nothing and spend the first morning on monday constantly growing have rough estimate as to what the file should be about of the database and would like to set this from the get go to ensure as much contiguous space as possible is this just case of changing initial size under database properties files would guess as well that the database would need to be offline for this to occur thanks in advance
35219 we need to create hash value of nvarchar data for comparison purposes there are multiple hash algorithms available in sql but which one the best to choose from in this scenario we want to ensure the risk of having duplicate hash value for two different nvarchar value is the minimum based on my research on the internet md5 seems the best one is that right msdn tells us link below about the available algorithms but no description on which one for what conditions hashbytes transact sql we need to join two tables on two nvarcharmax columns as you can imagine the query takes along time to execute we thought it would be better to keep the hash value of each nvarcharmax data and do the join on the hash values rather than the nvarcharmax values which are blobs the question is which hash algorithm provides the uniqueness so that we dont run into the risk of having one hash value for more than one nvarcharmax
35225 when execute this sql use aspstate go if not existsselect from sys sysusers where name r2server aaouser create user r2server aaouser for login r2server aaouser go get the following error the login already has an account under different user name how do know what this different user name is for my login account
35271 im using the built in sp spaceused stored procedure before and after performing operation in our software to see which tables have row inserts and how the size of each table changes what im seeing is that out of all the tables that get rows written to them only handful show that the table has increased in side the others that show rows were added show no change in size from this stored procedure the only case this is not true is on the first transaction after performing truncate on all the tables so to me it appears that instead on storing duplicate data sql server is showing rows are inserted but must be just storing pointers to previous identical rows can anyone confirm this please
35277 there are situations which require having really big query joining several tables together with sub select statements in them to produce the desired results my question is should we consider using multiple smaller queries and bring the logical operations into the application layer by querying the db in more than one calls or its better to have them all in one go for example consider the following query select from users where user id in select f2 friend user id from friends as f1 inner join friends as f2 on f1 friend user id f2 user id where f2 is page and f1 user id and f2 friend user id and f2 friend user id not in select friend user id from friends where user id and user id not in select user id from friend requests where friend user id and user image is not null order by rand limit whats the best way of doing it
35292 is data retrieved from microsoft sql server compressed if this is controlled by the connection string is there any simple way to tell if any particular app is using it im examining analysis tools and the volume of data can take minutes to transmit over our network im wondering whether should expect performance increase if we pull data from compressed data store on the same remote server as long as were on the topic im curious is data transmitted in binary or ascii for example if the value is queried from an int column is it transmitted as the five bytes 0x31 0x32 0x33 0x34 0x35 the two bytes that are required for the value or four bytes as required for the column to be clear understand that there are options regarding storing data with compression and backing it up im asking about how data is transmitted
35316 im wondering why newly created user is allowed to create table after connecting to database have one database project2 core postgres list of databases name owner encoding collate ctype access privileges postgres postgres sql ascii project2 core atm project2 utf8 de de utf de de utf project2 ctc project2 template0 postgres sql ascii postgres postgres ctc postgres template1 postgres sql ascii postgres postgres ctc postgres rows so far so good now create user postgres create role dietrich encrypted password md5xxx login nocreaterole nocreatedb nosuperuser okay when try to connect to the database the user is not allowed to do so psql localhost dietrich project2 core password for user dietrich psql fatal permission denied for database project2 core detail user does not have connect privilege this is what expected now the strange stuff starts grant the user connect postgres grant connect on database project2 core to dietrich grant postgres list of databases name owner encoding collate ctype access privileges postgres postgres sql ascii project2 core atm project2 utf8 de de utf de de utf project2 ctc project2 dietrich project2 template0 postgres sql ascii postgres postgres ctc postgres template1 postgres sql ascii postgres postgres ctc postgres rows and without any further grants the user is allowed to create table psql localhost dietrich project2 core password for user dietrich psql ssl connection cipher dhe rsa aes256 sha bits type help for help project2 core create table adsf create table project2 core list of relations schema name type owner public adsf table dietrich row would have expected that the user is not allowed to do anything before explicitly do grant usageon the schema and then grant select on the tables where is my mistake what am doing wrong how can achieve what want that new user is not allowed to do anything before explicitly granting her the appropriate rights im lost and your help is greatly appreciated edit following the advice by daniel verite now revok all immediately after creating the database the user dietrich is not allowed to create table any more good but now also the owner of the database project2 is not allowed to create table even after issuing grant all privileges on database project2 core to project2 and grant all privileges on schema public to project2 get an error error no schema has been selected to create in and when specifically try to create table public whatever get error permission denied for schema public what am doing wrong
35325 why did postgresql choose an elephant as its logo it wasnt clear from logo wiki update now wiki updated thanks
35327 use software which makes big postgresql database there is table with million rows in it and the developers says should vacuum and analyze periodically but the postgresql database default is autovacuum turned on should vacuum analyze at all what are the benefits whats the difference between automatic and manual vacuum for example in pgadmin3 have this
35380 am trying to construct query in postgresql that gets the longest sequence of continuous rows for specific column consider the following table lap id serial lap no int car type enum race id int fk where lap no is unique for each race id car type would like the query to produce the longest sequence for given race id and car type so it would return an int or long that is highest with the following data red red red red blue red blue green for car type red and race id the query would return as the longest sequence of the lap no field found similar question here however my situation is bit more straightforward would also like to know the longest sequence for given car type for all races but was planning to work that out myself
35424 am using sql server on small business server client is using winxp added user to my active directory security group why cant this user immediately access the database it seems there is delay before the user is recognized in sql server am using ad security groups for permissions expressly so that dont need to add individual users in sql server so effectively dont need to do anything but add the user to the ad security group in order to grant access but for some reason sql server doesnt immediately recognize the addition ive seen this number of times add the user to the group but that user cant access data until the next day it seems that it doesnt query the active directory in real time can you confirm that is so what do need to do so that sql server refreshes the list of users from active directory
35459 have table like the following annotation document term category where document and term are some id while category is an integer the couple document term is not unique could have the same couple with different category document id term id category document id term id category document id term id category would like to design query such that it return only the couple document term for whom there exists only row with category in the previous example the couple document id term id is not returned becouse there exist also other two rows with different values of category can you give me some hints on how to do that
35480 ive always seen and written my column aliases as select as columnname but today came across query that used select columnname is there any difference in how these two queries get executed or is there standard among dbas about which one to use personally think the 2nd would be easier to read maintain for longer column definitions good example here from this article however ive never seen the 2nd syntax used before today so am wondering if there is some reason shouldnt be using it
35497 ive created few new udts in postgresql however now have two problems how to see which udts have been defined how to see the columns defined within these udts unfortunately couldnt find anything on that in the postgresql documentation
35500 ive been reading some great articles regarding sql server plan caching by kimberly tripp such as this one http www sqlskills com blogs kimberly plan cache and optimizing for adhoc workloads why is there even an option to optimize for ad hoc workloads shouldnt this always be on whether the developers are using ad hoc sql or not why would you not have this option enabled on every instance that supports it sql thereby reducing cache bloat
35529 have tables in database dont want to use the following sql on each table in database select from table name so is it possible to display the first records for each table inside the same database using sql
35565 have table called book create table book id smallint not null default bname text btype text bprices numeric112 constraint key primary key id and function save book create or replace function save bookthebook book returns text as body declare myoutput text nothing has occured begin update book set bname thebook bname btype thebook btypebprices thebook bprices where id thebook id if found then myoutput record with pk thebook id successfully updated return myoutput end if begin insert into book valuesthebook idthebook bnamethebook btype thebook bprices myoutput record successfully added end return myoutput end body language plpgsql volatile cost now when call the function select save book179the art of warfiction book get the error error malformed array literal sql state 22p02 character dont understand because dont see any error in the format of the array any help
35579 migrated large website and database from an older server windows sql server gb ram ghz quad core sas disks to newer much better server windows r2 sql server sp1 gb ram ghz core processors ssd disks detached the database files on the old server copied and attached them on the new server everything went very well after that changed to compatibility level to updated statistics rebuild indexes to my huge disappointment noticed that most sql queries are much slower times slower on the new sql server than on the old sql server for example on table with around 700k records on the old server query on index took around 100ms on the new server the same query takes around ms same happens for all queries would appreciate some help here let me know what to check verify because find it very hard to believe that on better server with newer sql server the performance is worse more details memory is set to max have this table and index create table dbo answer details id int identity11 not null userid int not null surveyid int not null customerid int not null default summaryid int not null questionid int not null rowid int not null default optionid int not null default enteredtext ntext null constraint answer details pk primary key nonclustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary textimage on primary create nonclustered index idx answer details summaryid questionid on dbo answer details summaryid asc questionid asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary executed this query set statistics time on select summaryid countsummaryid from answer details group by summaryid order by countsummaryid desc set statistics time off old server sql server execution times cpu time ms elapsed time ms new server sql server execution times cpu time ms elapsed time ms execution plans uploaded here http we tl arbpuvf9t8 later update amd 1ghz opteron core processors look much worse than intel 5ghz quad core processors great improvement changing windows power options from ballanced to high power further improvement changing max degree of parallelism to and cost threshold to now sql server execution times cpu time ms elapsed time ms its still worse than the old server but not that bad if you have any other suggestions other than local query optimizations please feel free to comment
35596 when write query like this select from table1 t1 join table2 t2 on t1 id t2 id does the sql optimizer not sure if that is the correct term translate that to select from table1 t1 table2 t2 where t1 id t2 id essentially is the join statement in sql server just an easier way to write sql or is it actually used at run time edit almost always and will almost always use the join syntax am just curious what happens
35616 am working on function that allows me to add an index if it does not exist am running into the problem that cannot get list of indexes to compare to any thoughts this is similar issue to the column creation one that is solved with this code https stackoverflow com
35755 my background is more in web programming rather than database administration so please correct me if im using the wrong terminology here im trying to figure out the best way to design the database for an application ill be coding the situation ive got reports in one table and recommendations in another table each report can have many recommendations also have separate table for keywords to implement tagging however want to have just one set of keywords that gets applied to both reports and recommendations so that searching on keywords gives you reports and recommendations as results heres the structure started out with reports reportid reportname recommendations recommendationid recommendationname reportid foreign key keywords keywordid keywordname objectkeywords keywordid foreign key reportid foreign key recommendationid foreign key instinctively feel like this isnt optimal and that should have my taggable objects inherit from common parent and have that comment parent be tagged which would give the following structure baseobjects objectid primary key objecttype reports objectid report foreign key reportname recommendations objectid recommendation foreign key recommendationname objectid report foreign key keywords keywordid primary key keywordname objectkeywords objectid foreign key keywordid foreign key should go with this second structure am missing any important concerns here also if do go with the second what should use as non generic name to replace object update im using sql server for this project its an internal application with small number of non concurrent users so dont anticipate high load in terms of usage the keywords will likely be used sparingly its pretty much just for statistical reporting purposes in that sense whatever solution go with will probably only affect any developers that will need to maintain this system down the line but figured its good to implement good practices whenever can thanks for all the insight
35812 both forms of locking cause process to wait for correct copy of the record if its currently in use by another process with pessimistic locking the lock mechanism comes from the db itself native lock object whereas with optimistic locking the lock mechanism is some form of row versioning like timestamp to check whether record is stale or not but both cause 2nd process to hang so ask why is optimistic locking generally considered faster superior than pessimistic locking and are there are use cases where pessimistic is preferred over optimistic thanks in advance
35821 am working in mysql database with table like this table name myfield and need to make lot of queries like this with strings in the list select myfield from table name where myfield in something other stuff some other bit longer there will be around unique rows should use fulltext or and index key for my varchar150 if increase the chars from to or would it make great difference is there any way to calculate it as said they are going to be unique so myfield should be primary key isnt it rare to add primary key to field which is already varchar index fulltext
35847 id like to get some clarification about the flush privileges option for mysqldump heres the description of the option from the mysql docs flush privileges send flush privileges statement to the server after dumping the mysql database this option should be used any time the dump contains the mysql database and any other database that depends on the data in the mysql database for proper restoration when it says that it sends the flush statement after dumping the database read that to mean that the data schema etc is dumped into the backup file and then the flush statement is sent to the database that was just dumped after it was dumped was wondering what dumping the data etc did that required the privileges to be flushed so started searching for an explanation to be sure when and why to use it as read various answers it occurred to me that it would make sense if the flush statement was being included in the dump file then after the contents of the file was loaded into database the flush privileges statement was run to update the settings after the new info was imported so how does it work flush the source database after dumping the data to file if so why is this necessary flush the destination database after importing the contents of the dump file something other than the possibilities ive described
35893 read that error state can help to distinguish between different states locations in the source code where same type of error can occur but it is not really clear to me that how it can be useful msdn states error state returns the state number of the error that caused the catch block of try catch construct to be run how it can be really used can some one give me an example the ones provided in this reference article dont really help explain things well for me
35910 please help me understand this statement database in mysql is schema in oracle have just started to use oracle and find it different from other rdbms softwares have used like mssql mysql and derby for example to create database when use create database ghazals it throws an error error at line ora create database failed ora database already mounted also commands like show databases do not work here
35932 if do this select dv name maxhb dateentered as de from devices as dv inner join heartbeats as hb on hb deviceid dv id where de group by dv name get this error msg level state line invalid column name de if do this select name de from select dv name maxhb dateentered as de from devices as dv inner join heartbeats as hb on hb deviceid dv id group by dv name as tmp where tmp de it works as expected can someone explain why need to nest my main query as subquery to limit my data set also is there maybe better way to achieve the goal here retrieve all records from one table and the single top related record ordered by dateentered descending
36056 need to update sql server database thats about 18gb in size to change significant number of text columns to nvarcharmax the problem im having is after executing all the alter table commands the database ends up being almost 26gb in size understand that from here using nvarcharmax will alow the db to grow more slowly but is there any way for me to prevent this bloating
36073 need to write query that will insert row only once even if the query is run multiple times being new to sql well new ish did an if not exists but friend said he preferred deleting the row if it exists and then adding it again what might be the advantages of deleting over exists or vice versa is there another way of doing this
36081 currently in our sql server database were using varchar and wed like to change that nvarchar ive generated script to do that my question is are there any differences in how sql server writes to varchar columns vs nvarchar columns we have number of backend procedures that im concerned about edit not sure if this helps but the columns dont have indexes or constraints on them
36163 im curious if it is possible to create table with column that can never be changed but the other columns of the table can for instance could imagine createdbyuser column that should never be changed is there built in functionality in sql server for this or is it only possible via triggers or something else
36244 we are using sql server with unique identifier and weve noticed that when doing selects with additional characters added onto the end so not chars it still returns match to uuid for example select from some table where uuid 7da26ecb d599 91d4 f9136ec0b4e8 returns the row with uuid 7da26ecb d599 91d4 f9136ec0b4e8 but if you run select from some table where uuid 7da26ecb d599 91d4 f9136ec0b4e8extrachars it also returns the row with the uuid 7da26ecb d599 91d4 f9136ec0b4e8 sql server seems to ignore all characters beyond the when doing its selects is this bug feature or something that can configured its not massive issue as we have validation on the front end for the length but it doesnt seem correct behaviour to me
36345 client ask me to migrate his mysql db the server doesnt have free space and it also has very big table that is broken so cant dump it cant repair it because of lacking of free space question is there way to physically move mysql db data files to another server and use them for the new mysql
36432 in the visual studio designer you can right click on ssis package and designate it as an entry point package doing search found this page on msdn which states the value of signifies that the package is meant to be started directly the value of signifies that the package is meant to be started by another package with the execute package task the default value is with this flag enabled and disabled have been able to execute package directly what is the purpose of enabling or disabling this flag is it merely to document the intentions of your own ssis packages or does sql server ssis behave differently when it enabled or disabled
36522 when setting up new sql server use the following code to determine good starting point for the maxdop setting this will recommend maxdop setting appropriate for your machines numa memory configuration you will need to evaluate this setting in non production environment before moving it to production maxdop can be configured using exec sp configure max degree of parallelismx reconfigure if this instance is hosting sharepoint database you must specify maxdop url wrapped for readability http blogs msdn com rcormier archive you shall configure your maxdop when using sharepoint aspx biztalk all versions including maxdop is only required on the biztalk message box database servers and must not be changed all other servers hosting other biztalk server databases may return this value to if set http support microsoft com kb declare corecount int declare numanodes int set corecount select cpu count from sys dm os sys info set numanodes select maxc memory node id from sys dm os memory clerks where memory node id if corecount if less than cores dont bother begin declare maxdop int of total cores in machine set maxdop corecount if maxdop is greater than the per numa node core count set maxdop per numa node core count if maxdop corecount numanodes set maxdop corecount numanodes reduce maxdop to an even number set maxdop maxdop maxdop cap maxdop at according to microsoft if maxdop set maxdop print suggested maxdop cast maxdop as varcharmax end else begin print suggested maxdop since you have less than cores total print this is the default setting you likely do not need to do print anything end realize this is bit subjective and can vary based on many things however im attempting to create tight catch all piece of code to use as starting point for new server does anyone have any input on this code
36539 have table that has schema like this create table questions tags id false force true do integer question id integer tag id end add index questions tags question id name index questions tags on question id add index questions tags tag id name index questions tags on tag id would like to remove records that are duplicates they have both the same tag id and question id as another record what does the sql look like for that
36603 have table that is used by legacy application as substitute for identity fields in various other tables each row in the table stores the last used id lastid for the field named in idname occasionally the stored proc gets deadlock believe ive built an appropriate error handler however im interested to see if this methodology works as think it does or if im barking up the wrong tree here im fairly certain there should be way to access this table without any deadlocks at all the database itself is configured with read committed snapshot first here is the table create table dbo tblids idlistid int not null constraint pk tblids primary key clustered identity11 idname nvarchar null lastid int null and the nonclustered index on the idname field create nonclustered index ix tblids idname on dbo tblids idname asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on fillfactor go some sample data insert into tblids idname lastid values sometestid insert into tblids idname lastid values someothertestid go the stored procedure used to update the values stored in the table and return the next id create procedure dbo getnextid idname nvarchar255 as begin description increments and returns the lastid value from tblids for given idname author max vernon date declare retry int declare en int es int et int set retry declare newid int set transaction isolation level serializable set nocount on while retry begin begin try begin transaction set newid coalesceselect lastid from tblids where idname idname0 if select countidname from tblids where idname idname insert into tblids idname lastid values idname newid else update tblids set lastid newid where idname idname commit transaction set retry no need to retry since the operation completed end try begin catch if error number deadlock set retry retry else begin set retry set en error number set es error severity set et error state raiserror en es et end rollback transaction end catch end if retry must have deadlockd times begin set en set es set et raiserror en es et end else select newid as newid end go sample executions of the stored proc exec getnextid sometestid newid exec getnextid sometestid newid exec getnextid someothertestid newid edit ive added new index since the existing index ix tblids name is not being used by the sp assume the query processor is using the clustered index since it needs the value stored in lastid anyway this index is used by the actual execution plan create nonclustered index ix tblids idname lastid on dbo tblids idname asc include lastid with fillfactor online on allow row locks on allow page locks on edit ive taken the advice that aaronbertrand gave and modified it slightly the general idea here is to refine the statement to eliminate unnecessary locking and overall to make the sp more efficient the code below replaces the code above from begin transaction to end transaction begin transaction set newid coalesceselect lastid from dbo tblids where idname idname if newid insert into tblids idname lastid values idname newid else update dbo tblids set lastid newid where idname idname commit transaction since our code never adds record to this table with in lastid we can make the assumption that if newid is then the intention is append new id to the list else we are updating an existing row in the list
36612 installed the oracle 11g server on ubuntu but cant start the server when start up the server the following error occurs sql startup ora failure in processing system parameters lrm could not open parameter file u01 app oracle product xe dbs initxe ora how can solve this issue think the connect failed because target host or object does not exist how can target host be specified running ls latr u01 app oracle product xe dbs initxe ora returns rw oracle dba mar u01 app oracle product xe dbs initxe ora running echo oracle sid displays nothing an empty line
36815 ive been running some index usage reports and im trying to get definition of leaf and non leaf there seem to be both leaf and non leaf inserts updates deletes page merges and page allocations really dont know what it means or if one is better than the other if someone could give simple definition of each and also explain why leaf or non leaf matters it would be appreciated
36828 using sqlalchemy to query postgresql database behind pgbouncer using transaction level pooling what is the best pattern to use for this kind of set up should have one engine per process using connectionpool or should create an engine per request and use nullpool for each one of them is there different pattern altogether that should be using thanks very much let me know if more information is needed and ill update asap
36870 added the user myuserto postgres then added the database mydatabase in the pgadmin iii gui and restored from backup file so the owner of mydatabase is the superuser postgres then tried to give all rights to access and modify mydatabase to myuser logged into psql as user postgres psql template1 postgres and then ran this query grant all privileges on database mydatabase to myuser now can use myuser to log but if try simple query get this error error permission denied for relation table name am missing something can you help me solve that
36875 common need when using database is to access records in order for example if have blog want to be able to reorder my blog posts in arbitrary order these entries often have lots of relationships so relational database seems to make sense the common solution that have seen is to add an integer column order create table as your table id title sort order as values lorem ipsum dolor sit amet consect elit fusce then we can sort the rows by order to get them in the proper order however this seems clumsy if want to move record to the start have to reorder every record if want to insert new record in the middle have to reorder every record after it if want to remove record have to reorder every record after it its easy to imagine situations like two records have the same order there are gaps in the order between records these could happen fairly easily for number of reasons this is the approach that applications like joomla take you could argue that the interface here is bad and that instead of humans directly editing numbers they should use arrows or drag and drop and youd probably be right but behind the scenes the same thing is happening some people have proposed using decimal to store order so that you can use to insert record in between the records at order and and while that helps little its arguably even messier because you can end up with weird decimals where do you stop is there better way to store order in table
36917 recently have been using database abstraction layer built by python web framework called web2py click for their dal syntax they include the option to include your constraints within the create table statement whilst taking stanfords introduction to databases mooc the sql standard was mentioned as supporting any query within the create table statement as constraints essentially replacing major use case for triggers what is best practice below is simple example of including constraints in create table statements rather than through alert table and or create trigger statements create table place address varchar240 constraint place pk primary key address create table company name varchar240 constraint company pk primary key name create table employee name varchar240 tax no number salary number194 sex char birthdate date address varchar240 constraint employee pk primary key tax no constraint address fk foreign key address references placeaddress check address is not null create table companyemployee employee id number company id varchar240 constraint unique employee id uniqueemployee id constraint employee id fk foreign key employee id references employeetax no constraint company id fk foreign key company id references companyc name constraint company employees pk primary key employee id company id btw youll note that im using caps for keywords upper camelcase for table names and lower under score for attribute and trigger names is this good practice feel free to critique my indentation and whitespace usage styles also
36943 have some table with numbers like this status is either free or assigned id set number status assigned free assigned free free assigned assigned free free free assigned assigned assigned free assigned and need to find consecutive numbers so for query would return free free free it should return only first possible group of each id set in fact it would be executed only for id set per query was checking window functions tried some queries like countid number over partition by id set rows unbounded preceding but thats all got couldnt think of logic how to do that in postgres was thinking about creating virtual column using window functions counting preceding rows for every number where status free then select first number where count is equal to my number or maybe group numbers by status but only from one assigned to another assigned and select only groups containing at least numbers edit found this query and changed it little bit with as select row number over partition by id set status order by number as rnd row number over partition by id set order by number as rn from numbers select id set minnumber as first number maxnumber as last number status countnumber as numbers count from group by id set rnd rn status order by first number which produces groups of free assigned numbers but would like to have all numbers from only first group which meets the condition sql fiddle
36979 have big database that need to extract all primary keys and foreign keys from each table have pgadmin iii is there way to do this automatically and not go over each table manually
37014 understand that an character email address is valid but implementations have researched tend to use varchar60 to varchar80 or equivalent for example this sql server recommendation uses varchar80 or this oracle example is there reason to not use the full character maximum doesnt varchar by definition use only as much storage as needed to hold the data are there significant performance implications trade offs which cause so many implementations to use less than the full possible characters
37034 have database on postgresql that has main schema with around tables and variable number of identically structured per client schemas of tables each the client schemas have foreign keys referencing the main schema and not the other way around just started filling the database with some real data taken from the previous version the db had reached about gb its expected to grow to several 10s gb within weeks when had to do bulk delete in very central table in the main schema all concerned foreign keys are marked on delete cascade it was no surprise that this would take long time but after hours it became clear that was better off starting over dropping the db and launching the migration again but what if need to repeat this operation later when the db is live and much larger are there alternative faster methods would it be much faster if wrote script that will browse the dependent tables starting at the table furthest from the central table deleting the dependent rows table by table an important detail is that there are triggers on some of the tables
37038 recently have been struggling with sql server performance and although we have fixed huge multitude of basic errors in the config it is still performing badly to clarify it is not overall performance but rather fairly frequent time outs from the client application previously had looked to the memory as cause but this has now been resolved and we are still getting the same behaviour looking at the graphs from management data warehouse can see that lck ix are causing the majority of our waits around the time user experiences timeout everything am reading states need to look at the queries and processes running but have yet to find anything aimed at level can understand the locks as you can see in the picture below seem to spike which coincides with the error on the users side is there clever dmv or some such that can address to try and work out what query is being run that is creating the lock is it case of trawling through trace to find the details any guidance greatly appreciated and apologies if the information is not clear
37058 there are bunch of databases on one of our sql servers that have no owner generally speaking is it harmful to give them one for example use dbname go exec sp changedbowner sa go know sa may not be the best choice but it is just an example my primary concern is whether or not creating an owner when there is none may cause issues with software that can currently connect ok
37149 do you use sql server developer edition on server class machines in dev and staging environments am working on large project where if it passes the proof of concept stage we will have several large geographically distributed enterprise class database servers running sql server enterprise edition we will have production environment will initially have servers our staging environment will have minimum of servers and our development environment will have server hosting three instances was under the impression that we would only need to acquire enterprise licences for the actual production servers and we could get by with developer edition in our developer and staging environments because they are not production other sources have told me that we need to have an enterprise licence on all of the above mentioned machines and that the developer edition is only meant for single developer on their workstation since developer edition has all the sexy features of enterprise cant really see the value of it on workstation class machine especially for developing and testing the type of high availability system we are building if we have to fork out enterprise licences for dev server that will just about kill our proof of concept stage thus killing the project forcing an enterprise licence on staging environment will make management just want to skip staging altogether
37162 have fairly busy database server running sql server r2 that has the following setup sata raid drives os programs sas raid drives sql database files data and logs sas raid drives tempdb data and logs assuming cant add additional drives into this server have made the best use of the configuration have available or should consider another scheme here where logs are isolated from the data files for example update for those that requested further hardware details the sata drives used for the os program partition are wd rpm gb inch sata the sas drives used in the other arrays are seagate 15k rpm gb inch sas the raid controller used is an lsi 8i sas sata gb port update based upon the feedback ive received it looks like have the following viable options to choose from will award the bounty to someone that can tell me which is likely to be the best in the environment that ive outlined leave everything as is probably wont do much better move my sas raid drives into my existing raid array to have it composed of disks in total move my log files onto the sas raid and or relocate tempdb data or logs back to the raid
37252 we have machine tool application that runs on sql server backend these tools run all day every day and am looking for way to incrementally backup the database without any machine downtime ive seen mirroring solutions with two servers but these solutions would not work for us do to cost and size constraints is it possible to programmatically or through some service to mirror this database to backup hard drive on the main pc some data loss is acceptable
37280 am not exactly qualified dba but yes am in charge of my own db which we use in our social app have recently implemented master slave replication on my database for obvious reasons what wish to know is whether mysql replication kills the performance of my db since for every user writing on my master db creates an additional write on slave db might be wrong here there is no lag between my slave master so replication is almost instantaneous
37354 im attempting to set up sandbox for our report developers to their work in my current plan is to reset the database every evening but im not sure how to go about doing so what mean by reset is that want to essentially drop any user tables views stored procedures etc from all but one database on the server suppose another option would be to drop and recreate the database as well but im pretty sure thatd mean regranting access to all of the appropriate ad groups people too really dont know what would be the best way to go about doing this so im hoping some of you will be able to provide some good ideas suggestions thanks for clarity we essentially want to do this with our database http try discourse org this site is sandbox it is reset every day only difference being is that we dont want to recreate our users every day version sql server edition developer enterprise
37363 im doing an update like this update dbo table1 set birthdate birthdate from table1 join table2 on id id and want to use the output clause to back up my changes update dbo table1 set birthdate birthdate output inserted id inserted birthdate as new birthdate deleted birthdate as old birthdate into outputtable from table1 join table2 on id id what want to know is if there is way for the output clause to create the table outputtable or do have to make sure it already exists before running the statement
37379 have an app to deploy in production that uses honor system security that is all users connect to the db using sql user passwd credential and the app manages permissions itself the latter part doesnt bother me as much as the fact that the connection object contains embedded credentials and can be copied around freely im try to find some way to limit connections to more limited set of clients can create firewall rules to limit by ip of course is there any way to prequalify sql logins either by machine account or domain membership
37427 is it possible to have custom unique column constraint as follows suppose have two cols subset and type both strings though the data types probably doesnt matter if type is true then want the combination of type and subset to be unique otherwise there is no constraint im using postgresql on debian
37491 got really weird annoying problem somehow the instance of sql server r2 running on our server has gotten somewhat corrupted first we noticed that the database we created yesterday was missing so we looked around and found that it was still there but detached so we tried to attach the mdf but got message which was something like the file is currently in use thought that was odd so restarted sql server same thing okay time for drastic measures so stopped the service zipped up the mdf started the service unzipped it and then tried to restore it the above message was gone but then got cannot attach database with the same name as an existing database ouch of course its not showing in the database explorer so no idea whats going on last resort drop database databasename of course that didnt work that tells me the database does not exist so im stuck at one point sql server thinks the database does exist and at another point it thinks the db does not exist obviously its in state of confusion has anyone seen this before got any ideas on how to fix it
37557 have table of millions of rows and growing did the following to improve performance of inserts on database side dropped all indexes and constraints disabled logging on application side switched from jpa managed entities to native insert queries added append oracle hint to the query tried to commit in batches per 1k 2k 3k of rows tried to write in parallel multiple threads thread count to core count on server to one table this gave me about rows per second additionally tried write in parallel in batches to multiple tables to group then back results using union this gave me about 1k rows per second but on empty tables but when filled tables with dummy data of millions each speed of inserts dropped to per second could anyone suggest what else can do to speed up inserts basically want to understand what is what could be the bottleneck first upd table is partitioned by insert date table has about columns most of columns are varchar22000 byte
37600 am currently using tortoise svn to source control net web application what would be the best way to bring our sql server stored procedures into source control am currently using vs as my development environment and connecting to an off premise sql server r2 database using sql server data tools ssdt what have been doing in the past is saving the procs to sql file and keeping this files under source control im sure there must be more efficient way than this is there an extension can install on vs2010 ssdt or even sql server on the production machine
37609 we have recently upgraded from sql server to sql server under sql server there is no option to create compressed backups as there is in if you attempt backup database with compression to file that has already been initialized without compression the backup database command will fail with the following error message error message backup database is terminating abnormally error code how can tell if an existing backup file is initialized for compressed backups
37627 would like to be able to easily check which unique identifiers do not exist in table of those supplied in query to better explain heres what would do now to check which ids of the list do not exist in table select from dbo table where id in lets say the table contains no row with id dump the results into excel run vlookup on the original list that searches for each list value in the result list any vlookup that results in an is on value that did not occur in the table im thinking theres got to be better way to do this im looking ideally for something like list to check query on table to check members of list not in table
38708 have database drop table if exists books create table books isbn varchar255 not null title varchar255 null default null primary key isbn comment books used at this school drop table if exists classes create table classes class id int10 not null auto increment teacher id smallint5 null default null primary key class id comment classes at the school drop table if exists create table isbn varchar255 not null class id int10 not null primary key isbn comment books to classes alter table add foreign key isbn references books isbn on update cascade alter table add foreign key class id references classes class id on update cascade the issue im having is that would like to normalize data as much as possible dont want multiple entries for the same relationship to be entered into the table but would like to only store what data is absolutely pertinent my first idea to deal with this is to just create compound primary key for the table consisting of the fields isbn and class id which would solve the issue of having duplicate relationships in the table however have heard strong opinions on having unique identifier for every row in table like this the justification for having unique identifier for every row seems to be that its useful to be able to specify specific row though dont see situation in which this would become useful can someone offer an example another criticism ive heard is that using compound pks in this way can make joins extremely taxing can someone comment on the performance of these two different methods the question boils down to is it worth it to add an id field to the table or is the use of compound pks enough to properly represent the relationship between the books and classes tables if you have any other comments about the design not directly pertaining to the question would love to hear them and thank you in advance for you help
38722 have script identifying what indices to rebuild select alter index name on dbname dbo object namea object id rebuild from sys dm db index physical stats db id dbname null null null null as inner join sys indexes as on object id object id and index id index id where avg fragmentation in percent which generates alter index fooindex on foodb dbo footable rebuild but after execute the alter statement the index still has high fragmentation value and if execute it again it dont get any lower any input on what could be wrong would be highly appreciated updated manually increased the size of the db which managed to lower the fragmentation on some of the indices but still not all still have couple around daniel
38758 have messages table in database which include sender id and message type and of course many more columns not relevant for this question try to create query which counts how many messages of each type user have send if have the following table id user id message type private public private then want to get the following id private public so in fact want to group by message type and user id but instead of generating multiple rows per user want to create multiple columns one for each message type can achieve this without hardcoding the message types in my query
38793 we are looking at developing tool to capture and analyze netflow data of which we gather tremendous amounts of each day we capture about billion flow records which would look like this in json format tcp flags src as nexthop unix secs src mask tos prot input doctets engine type exaddr engine id srcaddr dst as unix nsecs sysuptime dst mask dstport last srcport dpkts output dstaddr first we would like to be able to do fast searches less than seconds on the data set most likely over narrow slices of time mintes intervals we also want to index the majority of the data points so we can do searches on each of them quickly we would also like to have an up to date view of the data when searches are executed it would be great to stay in the open source world but we are not opposed to looking at proprietary solutions for this project the idea is to keep approximately one month of data which would be billion records rough estimate that each record would contain about bytes of data would equate to terabytes of data in month and maybe three times that with indexes eventually we would like to grow the capacity of this system to store trillions of records we have very basically evaluated couchbase cassandra and mongodb so far as possible candidates for this project however each proposes their own challenges with couchbase the indexing is done at intervals and not during insertion of the data so the views are not up to date cassandras secondary indexes are not very efficient at returning results as they typically require scanning the entire cluster for results and mongodb looks promising but appears to be far more difficult to scale as it is master slave sharded some other candidates we plan to evaluate are elasticsearch mysql not sure if this is even applicable and few column oriented relational databases any suggestions or real world experience would be appreciated
38803 just installed fresh copy of ubuntu lts on new machine logged into mysql as root david server1 mysql root p123 created new user called repl left host blank so the new user can may have access from any location mysql create user repl identified by query ok rows affected sec checked the user table to verify the new user repl was properly created mysql select host user password from mysql user host user password localhost root 23ae809ddacaf96af0fd78ed04b6a265e05aa257 server1 root 23ae809ddacaf96af0fd78ed04b6a265e05aa257 root 23ae809ddacaf96af0fd78ed04b6a265e05aa257 root 23ae809ddacaf96af0fd78ed04b6a265e05aa257 localhost server1 localhost debian sys maint 27f00a6baae5070bcef92df91805028725c30188 repl 23ae809ddacaf96af0fd78ed04b6a265e05aa257 rows in set sec then exit try to login as user repl but access is denied david server1 mysql repl p123 error access denied for user repl localhost using password yes david server1 mysql urepl p123 error access denied for user repl localhost using password yes david server1 why is access denied
38808 the release of sql server integration services ssis has delivered an ssisdb catalog which tracks the operations of packages among other things the default package execution for solutions using the project deployment model will have logging to the ssisdb turned on when package executes the system executioninstanceguid is populated with value that if one were using explicit logging to sys sysdtslog90 sys sysssislog would record all the events for specific package execution what id like to know is how do tie an executioninstanceguid to anything in the ssisdb catalog alternatively is an ssis package executing in the ssisdb privy to the value of its catalog executions execution id ultimately am trying to use the existing custom audit table and link it back to the detailed history in the ssisdb catalog but cant seem to find the link
38992 say have the following setup use tempdb go set nocount on go create table mytest column1 varchar100 column2 text go insert mytest column1 column2 select replicatea replicatea from sys syscolumns sys syscolumns id like to convert each of the columns to varcharmax like this processes every page alter table mytest alter column column1 varcharmax processes only metadata alter table mytest alter column column2 varcharmax how can demonstrate that the first command processes the whole table while the second command only processes metadata was thinking of using set statistics io which reports thousands of logical reads for the first command and nothing for the other was also thinking of using dbcc log or fn dblog but wasnt sure how to interpret or tie the results to the queries issued
39058 is there best practice for whether foreign key between tables should link to natural key or surrogate key the only discussion ive really found unless my google fu is lacking is jack douglas answer in this question and his reasoning seems sound to me im aware of the discussion beyond that that rules change but this would be something that would need to be considered in any situation the main reason for asking is that have legacy application that makes uses of fks with natural keys but there is strong push from devlopers to move to an or nhibernate in our case and fork has already produced some breaking changes so im looking to either push them back on track using the natural key or move the legacy app to use surrogate keys for the fk my gut says to restore the original fk but im honestly not sure if this is really the right path to follow the majority of our tables already have both surrogate and natural key already defined though unique constraint and pk so having to add extra columns is non issue for us in this insance were using sql server but id hope this is generic enough for any db
39081 in standard sql the result of union all is not guaranteed to be in any order so something like select as union all select could return two rows in any order although in practice on any database know of will come before in sql server this turns into an execution plan using concatenation physical operation could easily imagine that the concatenation operation would scan its inputs returning whatever input has records available however found the following statement on the web here the query processor will execute this plan in the order that the operators appear in the plan the first is the top one and the last is the end one question is this true in practice is this guaranteed to be true havent found any reference in microsoft documentation that the inputs are scanned in order from the first to the last on the other hand whenever try running it the results suggest that the inputs are indeed processed in order is there way to have the engine process more than one input at time my tests using much more complicated expressions than constants are on parallel enabled core machine and most queries do take advantage of the parallelism
39128 have sql server job with steps steps and must be run regardless of any failures in the first four steps so these first four jobs are set to skip to step if they fail however if steps and then succeed the whole job is regarded as success have an email notification set up for the job failure but do not receive an email if any of the first four steps fail due to the overall job being considered success would like this to happen it would not be ideal to split out the first four steps into separate job as they must be completed before steps and begin please can anyone give me advice to solve this problem so that steps and run even when any of steps fail steps and begin strictly not before steps complete when any of steps fail an email notification is sent indicating the step that failed thanks very much in advance for your help
39239 intend to be using uniqueidentifier as an access key that users can use to access certain data the key will act as password in that sense need to generate multiple such identifiers as part of an insert select statement for architectural reasons want to generate the identifiers server side in this case how can generate securely random uniqueidentifier note that newid would not be random enough as it does not promise any security properties at all im looking for the sql server equivalent of system security cryptography randomnumbergenerator because need unguessable ids anything based on checksum rand or getutcdate would also not qualify
39344 in simple voting system as create table elections election id int11 not null auto increment title varchar255 create table votes election id int11 user id int11 foreign keys for getting the list of elections user has voted the following join is used select from elections join votes usingelection id where votes user id but how to get the list of elections user has not voted
39421 my local development server is in the middle east but my production server is in the uk need to show the date to the user in their time zone for example if user is in saudi arabia then need to show the time according to saudi arabia format should create new database table called timezone and save the time in utc
39572 have new sql server instance and am migrating database from sql server some of the stored procedures in the db am moving reference another database back on that server what is the best way to handle this would linked server help am having trouble creating link for testing some people recommend using odbc others seem to use the sql server client driver sqlncli10 using the sql server client driver seems better to me are there issues installing it on sql server instance
39589 use postgresql on ubuntu need to select records inside range of time my table time limits has two timestamp fields and one integer property there are additional columns in my actual table that are not involved with this query create table start date time timestamp end date time timestamp id phi integer primary keystart date time end date timeid phi this table contains roughly 2m records queries like the following took enormous amounts of time select from time limits as where id phi and start date time timestamp2010 and end date time timestamp2010 so tried adding another index the inverse of the pk create index idx inversed on time limitsid phi start date time end date time got the impression that performance improved the time for accessing records in the middle of the table seems to be more reasonable somewhere between and seconds but its still several tens of seconds for values in the middle of the time range and twice more when targeting the end of the table chronologically speaking tried explain analyze for the first time to get this query plan bitmap heap scan on time limits cost rows width actual time rows loops recheck cond id phi and start date time timestamp without time zone and end date time timestamp without time zone bitmap index scan on idx time limits phi start end cost rows width actual time rows loops index cond id phi and start date time timestamp without time zone and end date time timestamp without time zone total runtime ms see the results on depesz com what could do to optimize the search you can see all the time is spent scanning the two timestamps columns once id phi is set to and dont understand the big scan 60k rows on the timestamps arent they indexed by the primary key and idx inversed added should change from timestamp types to something else have read little about gist and gin indexes gather they can be more efficient on certain conditions for custom types is it viable option for my use case
39596 is there way to determine if view is no longer being used without removing them ideally would like to know the views usage for sql server and am upgrading some databases and suspect that many of the views are no longer being used also some of the views will be difficult to compile on the new server as they access multiple databases some of which are not being moved to the new server
39611 am self taught ms access programmer main part of my job is programming am now building larger databases still using ms access as the ui but sql server to store all data and do more of the work in essence my question is what subject matters do need to know for sql server that probably didnt learn or need when using access not looking for you to tell me how to do anything more what you think are the most important things should go an research theres lot of subjects and hell of lot of detail dont want to find myself long way down less valuable path brain dump maintenance what are the most important check database reduce database update statistics rebuild etc indexes dont know as much as should is there good book blog etc that can teach me the basics upwards anything else have missed theres probably lots as said am new to sql server if it helps work for mid sized retailer and the databases predominantly work on cover such things as reporting platform summarises sales receipts inventory etc from main system and provides fast reporting reconciling tool between third part and what our stores put through registers imports data from third party and cross references the transaction logs stores all data to do with our promotions product prices projections actual results etc
39652 im considering using cluster to reorder table by an index understand that this recreation of the table data makes all the existing indexes either bloat or be useless ive seen some indications that reindex is required after cluster ive found other references that indicate that cluster does reindex the official documentation says nothing at all about reindex being part of cluster or required although it does suggest running analyze after the cluster can anyone definitively with some sort of reference to official docs say whether or not reindex is required after cluster
39689 dbcc freeproccache doesnt work in azure sql db how else can force plan to kick itself out of the cache in way that wont hurt production system cant just go alter tables willy nilly this is specifically for sql created by entity framework so these arent self managed stored procs its effectively dynamic sql source was bad indexes bad stats etc thats all fixed but bad plan wont go away update selected mrdennys solution as he got there first am however successfully using aaron bertrands script to perform the work thanks to everybody for the help
39703 from time to time my stored procedures looks like create procedure handle data fk int value varchar10 as begin if exists select from my table where id fk begin update my table set value value where id fk end else begin insert into my table fk value select fk value end end probably there is fault in design of app which calls this stored procedure should avoid making apps which do same stored procedure and methods for inserting new data and also updating old ones is there better way to achieve updating or inserting data in one approach am using sql server
39815 can use case to choose which columns to display in select query postgres like so select case when val then column when val then column else end as update is something similar at all possible when performing an update query in postgres choose which columns should be updated assume not since couldnt find anything about this but maybe someone has clever alternative besides using procedure or updating each column using case to determine if the value of the column should be assigned new value or simply reassigned the existing value if there is no easy alternative ill of course accept that as an answer as well extra info in my case have potential columns that may be updated with only one being updated per matching row the table to be updated is joined with another in the query the amount of rows to update will most likely vary could be dozens or hundreds believe indexes are in place for the joining conditions
39833 why are constraint applied in database will it not be more flexible to put it in the code im reading beginners book on implementing databases so im asking this as beginner lets say have designed database including this entity model entity type sub types person employee student student graduate undergraduate employee teacher administrator current constraints registered person on the system can only be student or an employee person entity requires uniqueness of social number which we presume every person holds only single unique one aka good enough primary key see later we decide to remove the number if one day the college decides that the teacher the employee sub type can also be student taking courses in their free time its much harder to change database design which could have thousands millions billions zillions of entries rather than just changing the logic in code just the part which didnt allow person be registered both as student and an employee its very improbable but cant think of anything else right now apparently it is possible why do we care about business rules in database design rather than in code note years later real life example have seen government where because of mistake issued ssns were duplicated multiple people same ssn those designing the original db definitely made that mistake of not applying this uniqueness constraint in the database and later bug in the original application multiple applications using the shared database and not agreeing where to put check and enforce the constraint this bug will go on to live in the system and all the system developed after which rely on that original systems database for many many years to come reading the answers here learned to apply all the constraint as many of them as possible wisely not blindly in the database to represent the real physical world out there as good as can
39844 have sql server agent jobs that periodically perform checkdb and defrag alter index reorganize rebuild of any index that is highly fragmented these are typical maintenance best practices im wondering which of the system databases should apply these jobs to right now run checkdb only on master and vaguely remember setting it up because saw that in book somewhere so for which of the following should perform checkdb and or index defrag master model msdb tempdb
39936 am very new at sql and databases in general only use them for the occasional homework so havent even tried to master them have seats at theater the seats are divided into main areas each area has the same number of rows and the same number of seats per row in my database id like to have row seatnumber as compound primary key and to have one table for each area now dont yet know how ill do my selects but what want to ask if do it this way will my selects be doable want to for example select an exact position within the theater where know the area row and seat number would the tables be hindrance could you give an example of how such select might look this is my first time at the site if the question does not belong here please direct me to more suitable site within stack exchange
39968 have sql server and single database with full recovery model usually the queue length is less than but sometimes it grows up to several thousands for few seconds at this time many of write queries ends up with timeout error using resource monitor found that at this moment sqlserver exe writes large amount of data to the main database file mdf thoughh usually it writes to transaction log ldf using sql server profiler found that heavy queries are not running at that moment think that it is some kind of sql servers background operation but wonder what kind database also has read commited snapshot on and the mirroring synchronous mode enabled can this fact be the cause of my issue update found that writing to log not to data file is default behavior of full recovery mode and log can only be copied to data file by backup transaction log operation still dont understand why sql server copying log every ten minutes
40045 when using postgresql v9 how do list all of the schemas using sql was expecting something along the lines of select something from pg blah
40141 dbcc showcontig scanning mytable table table mytable index id database id table level scan performed pages scanned extents scanned extent switches avg pages per extent scan density best count actual count logical scan fragmentation extent scan fragmentation avg bytes free per page avg page density full have read that scan density is very good and logical scan fragementation is also great extent scan fragmentation troubles me but the internet says to ignore it im analyzing single table slow performing query it runs seconds on first execution then ms on second and subsequent executions can reset this behavior with dbcc dropcleanbuffers is the high extent scan fragmentation an important clue if not ill likely add another question about my single table query
40221 if do the following mysql query select from table where name like john limit that mean the query will search all the mysql database table and return only the top results that match the query where condition but what if want to just search the top records in the database table that match the where condition and return only the first rows because have records in the table and just want to search in the first records what should use this is the wanted query in human readable search the top records from the table which have the name like john and return only the top records that match what is the corresponding in mysql
40272 ive installed and successfully configured our sql server alwayson node servers for our new intranet that is coming out ive gotten alwayson working great and our front end servers for the intranet will be using sharepoint the glitch is that sharepoint is configured to add databases automatically to our sql server back end but not to alwayson in reading about this and in contacting microsoft msdn support the default answer is you must manually find select back up and then add those new databases individually to get them into alwayson but wait that can be quite task constantly checking the sql server back end servers to see what databases were created then having to add them into alwayson im looking for script or process that will check for new databases back those new databases up in full mode for being added to alwayson of course then add those databases to alwayson all automatically or have this run every hours without user intervention what ive come up with so far is this script that actually identifies the newly added databases not yet in alwayson and then backs them up to shared location my next task is to find those newly added databases and through the various processes needed get them added to alwayson this will involve some sort of looping action imagine im not sql scripting guru is there any solution or script that might access that would do this add databases to alwayson automatically please advise im sure im not the first person to have this issue have seen previous posts on various internet sites including this one and the solution is either incorrect or states something like sure go ahead and just script that thanks but need just little more detail there thanks again allen declare name varchar50 database name declare path varchar256 path for backup files declare filename varchar256 filename for backup specify database backup directory set path atel web be2 backups declare db cursor cursor for select name from sys databases where group database id is null and replica id is null and name not inmastermodelmsdbtempdb open db cursor fetch next from db cursor into name while fetch status begin set filename path name bak backup database name to disk filename fetch next from db cursor into name end close db cursor deallocate db cursor
40280 tl dr since this question keeps getting views ill summarize it here so newcomers dont have to suffer the history join table on member value1 or member value2 this is slow as hell join table on member coalesce value1 value2 this is blazing fast note that here if value1 has value value2 is null and vice versa realize this might not be everyones problem but by highlighting the sensitivity of the on clauses it might help you look in the right direction in any case the original text is here for future anthropologists original text consider the following simple query only tables involved select sku id as productid is primary as isprimary v1 category name as category1 v2 category name as category2 v3 category name as category3 v4 category name as category4 v5 category name as category5 from category c4 join category voc v4 on v4 category id c4 category id and v4 language code en join category c3 on c3 category id c4 parent category id join category voc v3 on v3 category id c3 category id and v3 language code en join category c2 on c2 category id c3 category id join category voc v2 on v2 category id c2 category id and v2 language code en join category c1 on c1 category id c2 parent category id join category voc v1 on v1 category id c1 category id and v1 language code en left outer join category c5 on c5 parent category id c4 category id left outer join category voc v5 on v5 category id c5 category id and v5 language code lang join category link on sku id in select value from ids and category id c4 category id or category id c5 category id where c4 level and c4 version id this is pretty simple query the only confusing part is the last category join its this way because category level might or might not exist at the end of the query am looking for category info per product id sku id and the thats where the very large table category link comes in finally the table ids is just temp table containing ids when executed get the following actual execution plan as you can see almost of the time is spent in the nested loops inner join heres extra information on those nested loops note that the table names dont match exactly because edited the query table names for readability but its pretty easy to match ads alt category category is there any way to optimize this query also note that in production the temp table ids doesnt exist its table valued parameter of the same ids passed on to the stored procedure additional info category indices on category id and parent category id category voc index on category id language code category link index on sku id category id edit solved as pointed out by the accepted answer the problem was the or clause in the category link join however the code suggested in the accepted answer is very slow slower even than the original code much faster and also much cleaner solution is simply to replace the current join condition with the following join category link on sku id in select value from p1 and category id coalescec5 category id c4 category id this minute tweak is the fastest solution tested against the double join from the accepted answer and also tested against the cross apply as suggested by valverij
40373 have set up mysql replication with single slave whenever am adding data to master database it is automatically copied to slave database but if add data to slave database it is not available in master database why if it is wrong to add slave database how can tell to the slave database to redirect writes to master is it possible my application is designed as normal application not for replicated database should change my application code for working with replicated database
40441 want to list all the partitions created by dynamic triggers in postgresql was able to generate count of partitions using this related answer by frank heikens have table foo with an insert trigger that creates foo foo etc dynamically the partition for insert is chosen based on the primary key id range based partitioning is it possible to display all partitions currently in place for table foo
40481 turned down the maximum memory of the my sql server instance to mb now cant log in to increase it how can increase the maximum memory without logging in version is sql r2
40488 whenever create brand new database in postgresql maestro it creates the following list of default schemas now from my understanding schemas are like folders for organization etc so the question have is are all these schemas needed when create new db if so what are they used for on pg side as wont ever use them myself can understand information schema as this is default for an install of mysql on server though dont get why database would need its own as opposed to the entire server but to each db type his own guess
40492 im trying to find out whether an sql or no sql solution would be better for creating an events database im creating ticketing system similar to ticket master know that for either database type storage is the simple part the deciding factor is the performance of the following queries select events by location within specific date range select all events in given location city town etc sort by date search for events by keyword within specific date range within specific location events basically have id name location venue start date end date in relational schema would have an events table dates table for storing dates separately because events can occur on more than one date and they are repeatable and venues table from which the event location country city etc can be cross referenced have no experience with no sql databases so if you vote for no sql please suggest how you see the schema being organized and which particular db hope this question is specific enough query performance is the deciding factor after further consideration realize the question more properly distills to the availability of date time functions that can facilitate fast date range queries know mysql and postgresql have such functions postgresql is even looking little better at this point in terms of syntax dont know what nosql solutions have to offer regarding this know the system can certainly be modelled comfortably in relational database also am aware that each no sql solution is different was wondering if anyone with any specific knowledge of particular no sql db could cite why that particular db is good for the solution
40516 am trying to run sqlcmd exe in order to setup new database from command line am using sql server express on windows bits heres the command use sqlcmd mssqlserver08 dp0 aqualogydb sql dp0 databasecreationlog log and heres pieceof the sql file creation script create database aqualogy collate modern spanish ci as with trustworthy on db chaining on go use aqualogy go create table dbo baselayers code nchar100 not null geometry nvarcharmax not null isactive bit not null default exec sp updateextendedproperty name nms description value ncapas de cartograf base de la aplicaic consideramos en galia vil la cartograf level0type schema level0name ndbo level1type table level1name nbaselayers well please check that there are some accents on the words which is the tables description the database is created with no problems collate is understood by the script as you can see in the attached screenshot despite of this accents are not properly shown when examining the table id really appreciate any help thank you very much edit hi all changing the sql file encoding using notepad worked fine thank you very much for you help learned something interesting with this problem
40567 am in the process of designing database for new php mysql based application my problem is that do not and cannot represent what should be saved in the database because it is unlimited and changing here is the problem example the application will be shopping website that has many kind of products all of them have some shared attributes such as title and price but some kinds have specific details such as expiry date some have isbn some non this is just an example but really have many kinds with many different attributes can create table for each kinds but what have is not all the available kinds many kinds of items are unknown at this time is their way to accommodate this problem without over head in the users side
40580 have been tasked with writing an update query to update table with more than million rows of data here are the table structures source tables create table dbo sourcetable1 prodclassid varchar not null pricelistdate varchar not null pricelistversion smallint not null marketid varchar not null modelid varchar not null variantid varchar not null varianttype tinyint null visibility tinyint null constraint pk sourcetable1 primary key clustered variantid asc modelid asc marketid asc prodclassid asc pricelistdate asc pricelistversion asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor create table dbo sourcetable2 id uniqueidentifier not null prodclassid varchar null pricelistdate varchar null pricelistversion smallint null marketid varchar null modelid varchar null constraint pk sourcetable2 primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor on primary on primary textimage on primary sourcetable1 contains million rows of data and sourcetable2 contains rows of data here is the targettable structure create table dbo targettable chassisspecificationid uniqueidentifier not null variantid varchar not null varianttype tinyint null visibility tinyint null constraint pk targettable primary key clustered chassisspecificationid asc variantid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor on primary on primary the relationship between these tables are as follows sourcetable1 variantid is related to targettable variantid sourcetable2 id is related to targettable chassisspecificationid the update requirement is as follows get the values for varianttype and visibility from sourcetable1 for each variantid having the maximum value in the pricelistversion column get the value of the id column from sourcetable2 where the values of modelid prodclassid pricelistdate and marketid match with that of sourcetable1 now update the targettable with the values for varianttype and visibility where the chassisspecificationid matches sourcetable2 id and variantid matches sourcetable1 variantid the challenge is to do this update on live production with minimum locking here is the query have put together check if temp table already exists and drop if it does if exists select null from tempdb sys tables where name like cspec begin drop table cspec end create temp table to assign sequence numbers create table cspec rowid int id uniqueidentifier pricelistdate varchar8 prodclassid varchar10 modelid varchar20 marketid varchar10 populate temp table insert into cspec select row number over order by marketid rowid cs id cs pricelistdate cs prodclassid cs modelid cs marketid from dbo sourcetable2 cs where cs marketid is not null declare variables to hold values used for updates declare min int max int id uniqueidentifier pricelistdate varchar8 prodclassid varchar10 modelid varchar20 marketid varchar10 set minimum and maximum values for looping set min set max select maxrowid from cspec populate other variables in loop while min max begin select id id pricelistdate pricelistdate prodclassid prodclassid modelid modelid marketid marketid from cspec where rowid min use cte to get relevant values from sourcetable1 with variant cte as select variantid varianttype visibility maxv pricelistversion latestpriceversion from sourcetable1 where modelid modelid and prodclassid prodclassid and pricelistdate pricelistdate and marketid marketid group by variantid varianttype visibility update the targettable with the values obtained in the cte update sv set sv varianttype vc varianttype sv visibility vc visibility from spec variant sv inner join targettable vc on sv variantid vc variantid where sv chassisspecificationid id and sv varianttype is null and sv visibility is null increment the value of loop variable set min min end clean up drop table cspec it takes about seconds when set the limit of iterations to by hardcoding the value of max variable however when increase the limit to iterations then it takes almost minutes to complete am concerned that the execution time taken for iterations will run into multiple days on production however that might still be acceptable if the targettable does not get locked down preventing users from accessing it all inputs are welcome thanks raj
40656 if go to mysql shell and type select from users get userid name emailid password user type contact id fkusers company id fkusers cc com admin kshitiz ksharma aaa com asdf admin oracle sqlplus shows userid name emailid password user type contact id fkusers company id fkusers cc com admin cc com admin sqlite shell shows cc com admin kshitiz ksharma aaa com asdf admin is there way to beautify the output from sqlite shell is there an alternative shell thats better than default distribution cli clients only
40673 have the following tables staffname name income group educational level country jack jill jane june country id country country description america japan incomegroup id income range description below us us to us educationallevel id educational level country description phd master my intention is to get all the values from other table to the main table and display it something like the following intended result name income group educational level country john us to us phd america koko us to us master japan kane us to us degree thailand ali us to us college malaysia tried using the following sql select name income group educational level country from staffname country incomegroup ig educationallevel el where income group id and educational level ig id and country el id but it return no results there is no foreign or primary keys for all tables what could have been missing
40703 have temptable that was created using select into temptable from openrowsetmicrosoft ace oledb excel database myfilename xls select from sheet1 is there an easy way to copy the table structure of the temptable to new physical table the temp table contains lot of columns which contain numbers dates decimals and strings and do not want to have to identify and manually define every column am using sql server
40738 most of the forum and example online always suggest to have both allow snapshot isolation and read committed snapshot set to on whenever someone is asking snapshot row versioning or similar question guess the word snapshot in both setting get little confusing thought that in order for database engine to use row versioning instead of locks for read committed default behavior the database read committed snapshot is set to on regardless of what allow snapshot isolation setting the allow snapshot isolation setting is set to on only to allow snapshot isolation when starting transaction set transaction isolation level snapshot regardless of read committed snapshot setting the only reason to have these two settings set to on is when it needs to have read committed row versioning and snapshot isolation my question is is my understanding incorrect in some way and that these two setting have to be always set to on together especially for read committed row versioning
40830 we recently moved to sql server thats on vps and every now and then we get error when trying to do an update rebooting the server fixes the problem changed the maximum server memory in mb to because its 4gb vps but just dont know whats causing the error
40833 im somewhat of new dba and im managing sql server instance that has fair amount of activity im running in full recovery mode because we need point in time recovery right now im taking full backup of the databases and logs every day at 5am some of the log files have ballooned up to 300gb and even after taking backup they dont reduce in size can get them to reduce in size by running something similar to backup log db1 to disk server share db1 log1 trn dbcc shrinkfile db1 log backup log db1 to disk server share db1 log2 trn dbcc shrinkfile db1 log backup log db1 to disk server share db1 log3 trn dbcc shrinkfile db1 log when check the lsns of the backup files see something like restore headeronly from disk server share db1 log1 trn firstlsn secondlsn restore headeronly from disk server share db1 log2 trn firstlsn secondlsn restore headeronly from disk server share db1 log3 trn firstlsn secondlsn dont believe im breaking my log chain by shrinking the log files reading up on this do believe im hurting my performance because those shrunk log files have to re grow themselves questions why doesnt the log file shrink after my backups is it because there are uncommitted transactions at first was thinking should shrink the log files after every am backup after reading up on how thats bad for performance now believe that need to take regular log backups every couple of hours during the day is that correct my normal full backup of the database logs happens every day at am and sometimes takes hours if schedule the log backups to happen every hour what will happen when the log backup collides with the am backup
40980 am facing this problem when connecting to local system db but when connect any server it connects login failed for user domain username microsoft sql server error for help click http go microsoft com fwlinkprodname microsoft sql server evtsrc mssqlserver evtid linkid
40987 am very new to sql server and would like to understand whether the following very simple select statement would take any locks select from student please consider the case where the statement would not be running inside begin tran block
41067 consider this select statement select as query id from players where username foobar it returns the column query id with value along with players other columns how would one make the above sql return at least the query id of even if the select finds no rows that match btw its postgresql
41215 have database which has mb data file mdf and gb log file ldf the recovery model is set to full when try to shrink the log file its not shrinking know shrinking database is not good and it should not be done but still am trying to do it for shrinking the log file when ran dbcc sqlperflogspace found that the log size is mb and log space used is then tried this command use databasename dbcc loginfo now almost all vlfs are status which means all are in use tried to take log backup and then shrink the log file shrinking didnt reduce the size changed the recovery model to simple and tried shrinking again but this also didnt help checked for open transactions dbcc opentran database and found that no transaction is open now what is stopping me from shrinking the log file how can solve this
41223 weve been on dedicated server single quad core gb ram and are moving to new dedicated server 2x hex core gb ram both are windows server sql server the performance on the new server is slightly worse than the old slower server in testing our asp net application runs slower running individual expensive queries with statistics io and statistics time shows greater elapsed time on the new server sql query profile shows higher cpu usage on expensive queries task manager on the new server shows sqlserver exe is consuming gb of ram but the cpu values always stay very low ive updated all statistics rebuilt or reorganized indexes etc execution plans should be stored on the new server at this point given the amount of testing ive done if there are any missing indexes dont think there are they affect the old and new servers equally new has restored backup of the same data on the old id expected that the performance on the new server would be better but of more concern is load if the old server is performing better even under load then what will happen when this new slightly worse server has to take that load what else could be missing here edit maxdop set to old server has os databases and tempdbs on the same physical drives raid total of 15k gb inch sas new server has three drive sets os on raid database on raid tempdb on raid total of 15k gb inch sas old server has intel xeon e5620 ghz quad core threads new server has intel xeon e5 ghz six core threads edit heres the final analysis the power plan was on balanced not high performance switched that over tempdb was on raid not raid added another hd to create two physically distinct raid configs one for tempdb and one for everything else excluded sql related files mdf ldf ndf bak from virus scanning rebuilt all indexes following the move to the new server they were very fragmented possibly as result of backup copy restore and realized that the processor jump was not that big the queries arent going to execute that much faster but with more processors more cores more ram well be more scalable
41239 assuming have the following procedure create procedure foo table1 id in table1 table1 id type table1 val in table1 table1 value type as sql update varchar2500 update table1 set table1 value where table1 id begin execute immediate sql update using foo table1 val foo table1 id update table1 set table1 value foo table1 val where table1 id foo table1 id end beside the style readability is it any performance penalty for using dynamic query compared to in such cases mean when its absolutely avoidable thank you
41343 im using sql server r2 and have this pseudo query sp select from where linkmode is null and mycolumn in very long running query the problem is that the query takes very long time to execute even if execute the sp with linkmode as you noticed the long running query should be executed only if linkmode is null which is not the case here in my case linkmode however if change it to select from where and mycolumn in very long time exeted query the sp does run fast ive heard before that sometimes the optimizer can optimize the order of criteria so ask even if the optimizer chooses different route what can be faster than checking if null mean think that checking if null is much faster than running the other long query how can force sql server to run the query as ive written it the same order
41413 ive given mysql 5gb of memory use innodb but when insert lots of data dumpfile is 1gb hard drive is still bottleneck cpu is not busy and hard drive is is it possible to force mysql not to make the hard drive bottleneck
41476 during the installation of sql server in the collation tab can choose between french ci as and french ci as what do they mean and which one is newer when theres no code is it newer or older than the
41574 there are two tables user address user contains reference to address address contains the columns createdby and modifiedby which is reference to user how do design this database to avoid cyclic dependency
41577 in db2 have table containing large binary data now purged the whole table and ran runstats reorg runstats but the amount of disk space taken does not change what could be wrong here the table resides in its own tablespace which created as follows create bufferpool my bp size automatic pagesize create large tablespace my tbs in database partition group ibmdefaultgroup pagesize managed by automatic storage extentsize prefetchsize bufferpool my bp overhead transferrate file system caching deleted reorged as follows delete from my tbl runstats on table my tbl with distribution and detailed indexes all reorg table my tbl runstats on table my table with distribution and detailed indexes all alter tablespace my tbs reduce the table my tbl took up 5gb before all that and after deleting reorging it uses only mb less fwiw im running db2 nt v9
41608 am creating package where will be exporting data from database into an empty excel file when added only the source and destination components and ran the package got conversion error stating output column and column cannot convert between unicode and non unicode string data types to fix this added data conversion component and converted all the columns to unicode string dt wstr and no longer received the error the only problem is that had about columns where had to go by and select unicode string dt wstr from the drop down list then had to go into the destination component and map the newly converted columns to my excel file my question is if anyone else has come across this is there better more efficient way to get around having to do all the manual data type conversions having to convert and map all the columns one by one doesnt seem to practical especially if you have large number of rows understand excel files are not the best way to go for importing and exporting data but it is what is required in this particular case might look for way to just export to flat text file and then try to convert to excel as last step in the package im hopping this wont trigger the same unicode nonunicode conversion error
41709 im really having trouble tracking down some blocking we are experiencing the root blocking spids status is sleeping the cmd is awaiting command and the sqltext is set transaction isolation level read committed when view the top transactions by blocked transactions count report the blocking sql statement is ive performed trace on the sql and when the blocking happens tracing the root blocking spid but it hasnt really led me anywhere the last trace statement is the same as the sqltext above set transaction isolation level read committed ive checked all the related stored procedures can find to make sure they have try catch begin tran commit tran rollback tran statements we use stored procedures for everything so there are no standalone statements being ran this issue just started happening over the last hours and no one is claiming to have made any changes to the system solution one of our seldomly used stored procedures had an error with an insert number of columns didnt match but we are still confused on what exactly was happening when looking at all the trace information the exec statement for this stored procedure was listed at times but never just before the block happened on the blockking spid it seemed that when it starting blocking the trace didnt record the execution of it or any of the statements within it either however there are other times were the trace did record its execution and no blocking occurred the stored procedure error report came from user and was able to find multiple exec statements in traces and run them in ssms no time when ran them did we have any blocking occur or did they hang they ran as expected the catch block fired and rolled back the transaction after the error after resolving the fixing the stored procedure we have not seen the issue again
41754 we are using sql server for j2ee web application need to maintains the audits for updates for each column with their previous value and new value on one of my table which is very heavy around columns and 10k records and increasing rapidly on daily basis around users work with this application and updates also occur very frequently know handling it via code will require very frequent connections to database want to know which is better approach for doing this is handling it through code will provide better performance than using sql server trigger for that
41765 use the following script to search the text of all stored procedures when want to find specific values select routine name routine type from information schema routines where routine definition like searchtext order by routine name recently discovered that routine definition stops after characters so some procedures were not getting returned when they should have been how can query the full text of stored procedure for value
41854 we have two databases setup for mirroring on single sql server instance test database and production database both get mirrored to another server using the exact same endpoints if go into the database properties for the test database and click the failover button will it fail over the production database as well since both databases share mirror endpoint and their server network addresses properties are the same am concerned because when set up mirroring for the 2nd database did not have to configure anything new it just used all the existing information if use the failover button from the database properties will it result in failing over all databases that use that endpoint or just the specific database that am viewing the properties for
41938 am on os ive install postgresql from macports but ive realized that have not enough space on my system drive for the db am about to populate with data is there way to create new database on another drive partition not the system one where postgres is installed how can do that
41961 how can find all the positions with patindex in table or variable declare name nvarcharmax set name ali reza dar yek shabe barani ba yek dokhtare khoshkel be disco raft va ali baraye saat anja bud va sepas ali select patindex ali name as pos this returns but want all results pos
42180 have two datasets fees payments crscode instno fee regno crscode instno payment ca1 r1 ca1 ca1 r1 ca1 ca1 r1 ca1 ca1 desired output regno crscode instno fee paid diff r1 ca1 r1 ca1 r1 ca1 r1 ca1 output am getting regno crscode instno fee paid diff r1 ca1 r1 ca1 r1 ca1 null null null null null query am using select regnoa crscodeb feea paymentb fee payment from fees as left join payments as on crscode crscode and instno instno please find sqlfiddle
42214 tried to backup the ibm db2 luw database using this command db2 backup database dbemp to home user1 db2bkup but got this message sql1035n the database is currently in use sqlstate then tried this db2 backup database dbemp online to home user1 db2bkup and got this message sql2413n online backup is not allowed because the database is not recoverable or backup pending condition is in effect what does it mean is there alternative way to backup the database online cant stop the database because its being used
42290 our system writes lots of data kind of big data system write performance is good enough for our needs but read performance is really too slow the primary key constraint structure is similar for all our tables timestamptimestamp indexsmallint keyinteger table can have millions of rows even billions of rows and read request is usually for specific period timestamp index and tag its common to have query that returns around 200k lines currently we can read about 15k lines per second but we need to be times faster is this possible and if so how note postgresql is packaged with our software so the hardware is different from one client to another it is vm used for testing the vms host is windows server r2 x64 with gb of ram server spec virtual machine vmware server r2 x64 gb of memory intel xeon w3520 67ghz cores postgresql conf optimisations shared buffers 512mb default 32mb effective cache size 1024mb default 128mb checkpoint segment default checkpoint completion target default default statistics target default work mem 100mb default 1mb maintainance work mem 256mb default 16mb table definition create table analogtransition keytag integer not null timestamp timestamp with time zone not null timestampquality smallint timestampindex smallint not null value numeric quality boolean qualityflags smallint updatetimestamp timestamp without time zone utc constraint pk analogtransition primary key timestamp timestampindex keytag constraint fk analogtransition tag foreign key keytag references tag key match simple on update no action on delete no action with oids false autovacuum enabled true query the query takes about seconds to execute in pgadmin3 but we would like to have the same result under seconds if possible select analogtransition keytag analogtransition timestamp at time zone utc analogtransition timestampquality analogtransition timestampindex analogtransition value analogtransition quality analogtransition qualityflags analogtransition updatetimestamp from analogtransition where analogtransition timestamp and analogtransition timestamp and analogtransition keytag or analogtransition keytag or analogtransition keytag or analogtransition keytag or analogtransition keytag order by analogtransition timestamp desc analogtransition timestampindex desc limit explain limit cost rows width actual time rows loops buffers shared hit index scan backward using pk analogtransition on analogtransition cost rows width actual time rows loops index cond timestamp timestamp with time zone and timestamp timestamp with time zone filter keytag or keytag or keytag or keytag or keytag buffers shared hit total runtime ms explain in my latest test it took minutes to select my data see below limit cost rows width actual time rows loops index scan using pk analogtransition on analogtransition cost rows width actual time rows loops index cond timestamp timestamp with time zone and timestamp timestamp with time zone and keytag total runtime ms
42532 what is the format of the mysql query log in particular for lines like query commit query rollback what does stand for and is it true that each line represents round trip communication to the database they are not batched
42539 my database has 16mb of space left used to just truncate as was taught but found these links that advise against truncating http www sqlskills com blogs paul why you should not shrink your data files http blog sqlauthority com sql server shrinking database is bad increases fragmentation reduces performance is there anything else can do on my database to reduce the size other than deleting table records am new to the dba forum and probably should have looked around for other questions before posting but am desperate as am worried about my database going down
42553 looking at an execution plan of slow running query and noticed that some of the nodes are index seek and some of them are index scan what is the difference between and index seek and an index scan which performs better how does sql choose one over the other realise this is questions but think answering the first one will explain the others
42561 we have several independent databases that have data and code in common not in the sense that it is accessed between the databases but in the sense that the data means the same thing in each database and the code does the same thing examples are configuration settings error codes 50xxx boilerplate text for company name etc procedures and functions that perform common tasks converting csv string into table logging an error formatting an error message based on the error code table structures table for database version history table for logging errors as well as the columns constraints and triggers there are also common procedures and functions that read write the data look up tables date look up table containing dates between these are similar to table structures but quite often the data will be the same across the databases in the case of the date table the start and end dates are configuration settings which are read by function then the date data is generated by procedure all these operations are common between the databases user defined types types for passing tables to functions for maintenance and support reasons think it makes sense for things like error codes procedures functions and types to have single point of truth across all the databases not different truth in each database at the moment each database has its own copy of everything including source repository and we maintain them all independently this is far from ideal because its too easy to fix procedure in and forget to put it in or add an error code to and the same one to but they mean different things etc the databases are not updated at the same time and they dont necessarily reside on the same hardware there are cases where they can read data from each other if the other one exists is there standard way of having single point of truth for data code that is used across more than one database
42721 we know that the memo structure is pruned and some expensive alternative plans are discarded during optimization was wondering if there is any way to prevent this and let the optimizer just consider every possible plan and select the best from all alternatives
42837 why does this case expression select case column when then when then when then when then end col from linkedserver database dbo table produce this result error message msg level state line statements could not be prepared msg level state line case expressions may only be nested to level clearly there isnt nested case expression here though there are more than branches another oddity this inline table valued function produces the same error alter function dbo fn myfunction var varchar20 returns table as return select case column when then when then when then when then end col from linkedserver database dbo table but similar multi statement tvf works fine alter function dbo fn myfunction var varchar20 returns result table value varcharmax as begin insert into result select case column when then when then when then when then end col from linkedserver database dbo table return end
42889 have seen lot of blogs stating that shrinking is not good habit as it will reduce the performance of the system agree with all those things it will lead to side effects like fragmentation etc now the doubt have is what are the scenarios where should use the shrink option in the database have never seen scenario it was stated as the useful one is shrinking always evil
42998 conceptual question are individual queries faster than joins or should try to squeeze every info want on the client side into one select statement or just use as many as seems convenient tl dr if my joined query takes longer than running individual queries is this my fault or is this to be expected first of am not very database savvy so it may be just me but have noticed that when have to get information from multiple tables it is often faster to get this information via multiple queries on individual tables maybe containing simple inner join and patch the data together on the client side that to try to write complex joined query where can get all the data in one query have tried to put one extremely simple example together sql fiddle schema setup create table master id int not null name varchar242 char not null constraint pk master primary key id create table data id int not null master id int not null value number constraint pk data primary key id constraint fk data master foreign key master id references master id insert into master values one insert into master values two insert into master values three create sequence seq data id insert into data values seq data id nextval insert into data values seq data id nextval insert into data values seq data id nextval insert into data values seq data id nextval insert into data values seq data id nextval insert into data values seq data id nextval query select name from master where id results name one query select id value from data where master id results id value query select name id value from master inner join data on id master id where id results name id value one one one of course didnt measure any performance with these but one may observe query returns the same amount of usable information as query has to return 2x3 data cells to the client has to return 3x3 data cells to the client because with the join naturally include some redundancy in the result set generalizing from this as far fetched as it is joined query always has to return more data than the individual queries that receive the same amount of information since the database has to cobble together the data for large datasets one can assume that the database has to do more work on single joined query than on the individual ones since at least it has to return more data to the client would it follow from this that when observe that splitting client side query into multiple queries yield better performance this is just the way to go or would it rather mean that messed up the joined query
43099 the postgresql documentation states any function with side effects must be labeled volatile consider the following function create or replace function count items returns integer as body declare result integer default begin select count id into result from some table return result exception when others then perform error log insert sqlstate sqlerrm current query return end body language plpgsql stable cost since error log insert alters the database performs an insert upon an exception does this mean that the count items function has side effect albeit indirect and thus cannot be declared stable but must be volatile in other words does the stability or volatility of function also depend on the functions it calls within its exception block if that is the case then how would you create stable functions in postgresql that log all exceptions to database table
43254 is it bad practice to always create transaction for example it is good practice to create transaction for nothing but one simple select what is the cost of creating transaction when it is not really necessary even if you are using an isolation level like read uncommitted is it bad practice
43352 does the sql server or specifically case statement evaluate all the when conditions or does it exit once it finds when clause that evaluates to true if it does go through the entire set of conditions does that mean that the last condition evaluating to true overwrites what the first condition that evaluated to true did for example select case when thenyes when then no when then no end the results is yes even though the last when condition should make it evaluate to no it seems that it exits once it finds the first true condition can someone please confirm if this is the case
43452 am kind of curious one of sql enterprise edition with gb of ram size of database is gb and growing amount of memory used by locks objectstore lock manager memory clerk showing kb can also confirm that by looking at perf counter select from sys dm os performance counters where counter name lock memory kb however when run query select count from sys dm tran locks it shows only locks so what is using over gb of locks is there way to find out does that mean if once memory for locks has been allocated sql has yet not yet deallocated it in past hour do not see lock count exceeding but lock memory stays the same edit max server memory is gb we do not use lock pages in memory and do not see any memory pressure or any errors in the error log in past hours avialble mbytes couter shows more than gb of available memory edit activity monitor consistenly shows waiting tasks so obviously no blocking considering sql server lock take about bytes of memory gb is lots of memory and trying to find out who is using it edit run server dash board report top transaction by lock count it says currently no locking transactions are running on the system however lock memory still shows as stated above db is most busy during overnight hours
43457 theres long winded debate going on here so id like to hear other opinions have many tables with uniqueidentifier clustered pk whether this is good idea is out of scope here and its not going to change anytime soon now the database has to be merge published and the devs are advocating the use of separate rowguid column instead of marking the existing pk as the rowguidcol basically they say that the application should never bring into its domain something that is used by replication only its only dba stuff for them from performance standpoint see no reason why should add new column to do something could do with an existing one moreover since its only dba stuff why not let the dba choose kind of understand the devs point but still disagree thoughts edit just want to add that im in the minority in this debate and the devs questioning my position are people respect and trust this is the reason why resorted to asking for opinions might also be missing something and could have misunderstood their point
43490 have seen these extensions used for datafiles dat and dbf while creating and or altering tablespace im not sure what the difference between the extensions is or if dat is incorrect here are examples from oracle database sql reference 10g release dat create tablespace tbs datafile tbs f2 dat size 40m online dbf create tablespace tbs datafile tbs f03 dbf size 20m logging
43580 im an experienced programmer but im new to sql databases so please bare with me one of my current tasks involve editing and refactoring if needed dynamic generated sql statement in stored procedure are there any best practices or design patterns that can follow for writing stored procedures in general or better yet stored procedures that generate long lines dynamic sql statements
43632 while exploring the new function format have come across the following issue would like to know if any one else has faced similar problem and found the fix for this or if it is bug select formatgetdatemm dd yyyyen us select getdate output
43772 would have thought that databases would know enough about what they encounter often and be able to respond to the demands theyre placed under that they could decide to add indexes to highly requested data
43802 where work we have sql server database microsoft sql server r2 that serves as the back end with two different user interfaces net web interface and foxpro interface every month we need to apply updates to both the web and foxpro clients before doing this we are advised to make sure that no one is accessing the database during the update process what it the easiest way to prevent access to the database while we update it follow up question what is the best way to prevent access while we update it
43812 know that in mysql at the command line can reset tables auto increment field to with this alter table tablename auto increment am curious if there is way to do this from within phpmyadmin something like check box to reset the auto increment or something else along those lines not that there is anything wrong with the command line approach more one of those curiosity things keep thinking on thanks in advance
43823 have table which currently has duplicate values in column cannot remove these erroneous duplicates but would like to prevent additional non unique values from being added can create unique that doesnt check for existing compliance have tried using nocheck but was unsuccessful in this case have table which ties licensing information to companyname edit having multiple rows with the same companyname is bad data but we cant remove or update those duplicates at this time one approach is to have the inserts use stored procedure which will fail for duplicates if it was possible to have sql check the uniqueness on its own that would be preferable this data is queried by company name for the few existing duplicates this will mean that multiple rows are returned and displayed while this is wrong its acceptable in our use case the goal is to prevent it in the future it seems to me from the comments that have to do this logic in the stored procedures
43854 have been using sql server r2 for quite while and it is working great now need to use sql server will this effect my existing databases how much risk is involved in installing sql server on machine that has sql server r2 running
43910 have table with an identity column while developing delete the rows from time to time and add them again but the identity values always kept increasing and didnt start from when added them again now my ids go from and this crashes my code how do reset the identity value
43937 would like to call stored procedure on regular basis on oracle would create job for this have found that postgresql can mimic this well by using an external tool cron etc and pgagent do you know of an internal alternative which wouldnt involve the external tool want to avoid security concerns with the password stored on the command line of the pgagent want to avoid any additional system configuration for hiding the password pgpass postgresql linux redhat 64bit
44084 im using an open source rhel based machine running siem software when run the top command see postgres and postmaster both with cpu usage is there way to pin point or see what causing these service to stack up
44188 ive multi tenant db setup and need to add some columns im using schemas and search path to partition my users so im looking for ubiquitous way to apply ddl schema change to all my databases initially id thought might be able to do it as single query cursor on pg catalog but thinking command line invocation of psql might be the preferred way
44498 have been tasked with architecting solution for large retail chain they want to allow each of its million customers to log on to web site to see the distribution of recent purchases current month previous month year to date over about categories data will be updated once every day am thinking of putting up sql server based olap cube and letting the website query this cube directly leveraging features like proactive caching however being developer at heart have next to no experience with the analysis services parts of sql server so am quite concerned about the performance of this solution does connecting web site directly to an olap cube sound like feasible solution do such systems react to the load from multiple users roughly like sql server making this reasonable solution or do they act completely differently dont expect users to check their status very often and will of course be using caching on the webserver etc
44507 we have an exe file on server say server1 which should be run from sql server job that exists on different server server2 how can it be done know if its local file can use xp cmdshell in the job step and run the exe file but in our case this file exists on different server if its doable what security permissions should be set up to achieve it
44533 our ms sql server is using about of the cpu power after server hardware restart or sql service restart the usage is and slowly increases over the course of days depending on how much it is used when its over every query is extremely slow our website is dealing with alot of big queries so some of them takes seconds after restart cpu usage less than it takes seconds for the same query how can fix this ive read online that affinity masks can adjust the cpu usage but the affinity settings are disabled cannot change them is this because only have processor there are plenty of tricks to do with the queries themselves but our websites and services are quite big and there is simply too much to change most of them are already pretty well optimized cannot keep restarting the sql service even though it only takes seconds because we have an alarm service that allows people to call in and record message selected group will then be called and hear the recorded message this system is used by hundreds search and rescue teams and if the sql service restarts during an alarm it will terminate and the person that called it in will not be notified have searched all over the place but found nothing except for stuff about affinity masks which cannot change there must be way to clear out the cpu cache without terminating current queries right sql microsoft sql server os windows server x64 processor ghz ram gb
44585 we have used sql server replication for long time and had some issues with it that sometimes we needed to reinitialize subscriptions to fix some issues other times we needed to destroy the whole replication structure rebuild it again our main concern is that once we have replication issue almost all the time the easy solution will be to reinitialize the replication which is not accepted for our business requirements now we are preparing to release new big project we are trying to look for 3rd party software for doing sql server replication our setup includes servers distributed in branches different countries mobile clients laptops with local sql server databases and we need to replicate data between all of these with the ability to offer article filtering would somebody please suggest some alternate solutions for us
44657 manage big some hundreds of gigs database containing tables with various roles some of them holding millions of records some tables only receive large number of inserts and deletes some other few inserts and large number of updates database runs on postgresql on debian amd64 system with gigabytes of ram the question is sometimes autovacuum process on table takes very long time days to complete want to be able to roughly tell how much time particular vacuum command will take to be able to decide whether to cancel it or not also if there were progress indicator for postgres vacuum operations it would be really helpful edit im not looking for bullet proof solution just rough hint on the number of dead tuples or necessary bytes is enough to decide it is really annoying to have no clue when vacuum will finish whatsoever ive seen that pg catalog pg stat all tables has column for number of dead tuples so it is possible to have an estimation even if it means one has to analyze the table before on the other hand autovacuum vacuum threshold and autovacuum vacuum scale factor settings alone prove that postgres itself knows something about the amount of change on the tables and probably puts it in the hands of the dba too im not sure what query to run because when run vacuum verbose see that not only tables but indexes on them are being processed too
44764 am working on customized maintenance solution using the sys dm db index physical stats view currently have it being referenced from stored procedure now when that stored procedure runs on one of my databases it does what want it to do and pulls down listing of all records regarding any database when place it on different database it pulls down listing of all records relating to only that db for example code at bottom query run against database shows requested information for databases query run against database shows requested information for only database the reason want this procedure specifically on database three is because id prefer to keep all maintenance objects within the same database id like to have this job sit in the maintenance database and work as if it were in that application database code alter procedure dbo getfragstats databasename nvarchar64 null tablename nvarchar64 null indexid int null partnumber int null mode nvarchar64 detailed as begin set nocount on declare databaseid int tableid int if databasename is not null and databasename not in tempdbreportservertempdb begin set databaseid db id databasename end if tablename is not null begin set tableid object id tablename end select name as databasename name as tablename name as indexname index id as indexid avg fragmentation in percent as percentfragment fragment count as totalfrags avg fragment size in pages as pagesperfrag page count as numpages index type desc as indextype from sys dm db index physical stats databaseid tableid indexid partnumber mode as join sys databases as on database id database id join sys tables as on object id object id join sys indexes as on object id object id and index id index id where avg fragmentation in percent order by databasename tablename indexname percentfragment desc end go
44784 have table that has an id value and date there are many ids values and dates in this table records are inserted into this table periodically the id will always stay the same but occasionally the value will change how can write query that will give me the id plus the most recent time the value has changed note the value will always increase from this sample data create table taco taco id int taco value int taco date datetime insert into taco values the result should be taco id taco date because was the last time taco value changed
44785 ive got database with about tables and need to build join query to get specific data from two of them know one but not the other basically need something like select tables from database where exists table column name how can do this
44796 have 8tb sql database mostly data files some 400gb of log files that currently takes around hours to restore this database is used for testing purposes and must be deleted and restored from backup between each run to make sure were always starting from the same point my question is the server currently has cores and 92gb of ram with raid disk subsystem that the database is on what areas usually cause bottlenecks for sql restore processes is it the disk memory or cpu
44908 could somebody provide me with better insight about the compatibility mode feature it is behaving different than expected as far as understand compatibility modes it is about the availability and support of certain language structures between the various versions of sql server it does not affect the inner workings of the database engine version it would try to prevent use of features and constructs that were not yet available in earlier versions just created new database with compat level in sql server r2 created table with single int column and populated it with few rows then executed select statement with row number function my thought was since the row number function was only introduced in this would throw an error in compat mode but to my surprise this worked fine then surely the compat rules are only evaluated once you save something so created stored proc for my row number statement the stored proc creation went fine and can perfectly execute it and obtain results could someone help me to better understand the working of compatibility mode my understanding is obviously flawed
44956 dont design schemas everyday but when do try to setup cascade updates deletes correctly to make administration easier understand how cascades work but can never remember which table is which for example if have two tables parent and child with foreign key on child that references parent and has on delete cascade which records trigger cascade and which records get deleted by the cascade my first guess would be the child records get deleted when parent records are deleted since child records depend on parent records but the on delete is ambiguous it could mean delete the parent record when the child record is deleted or it could mean delete the child record when the parent is deleted so which is it wish the syntax was on parent delete cascade on foreign delete cascade or something similar to remove the ambiguity does anyone have any mnemonics for remembering this
44992 when was exploring the master database in ssms noticed that under the tables folder there is another folder called systems tables that houses bunch of tables is it possible for us to create structure akin to systems tables within our database am looking to organize tables and stored procedures into project specific folders under the new setup when am referring to my table object would have to use the following syntax am guessing here dbname projectname dbo tablename also apart from clearing up the clutter do anybody foresee any performance improvement degradation because of this re organization use microsoft sql server r2
45087 tried all three method explained here to max allowed packet but no one changes its value in my mysql use show variables like max allowed packet to see its current value but it always is only changing its value in my ini is effective what is wrong
45095 have sql server database using the full recovery model if execute the following commands alter database test set recovery simple alter database test set recovery full or backup log test with truncate only then try to run log backup backup log test to disk backupfile bak receive the error message backup log cannot be performed because there is no current database backup question how do check that will not be able to take log backup without running backup log command
45137 im looking for best practice in dealing with scheduled sql server agent jobs in sql server availability groups maybe missed something however at the current state feel that sql server agent is not really integrated with this great sql2012 feature how can make scheduled sql agent job aware of node switch for example have job running on the primary node which loads data each hour now if the primary goes down how can activate the job on the secondary which now becomes primary if schedule the job always on the secondary it fails because then the secondary is read only
45179 have table which includes column of decimal values such as this id value size what need to accomplish is little difficult to describe so please bear with me what am trying to do is create an aggregate value of the size column which increments by each time the preceding rows sum up to when in descending order according to value the result would look something like this id value size bucket my naive first attempt was to keep running sum and then ceiling that value however it doesnt handle the case where some records size end up contributing to the total of two separate buckets the below example might clarify this id value size crude sum crude bucket distinct sum bucket as you can see if were to simply use ceiling on crude sum record would be assigned to bucket this is caused by the size of records and being split across two buckets instead the ideal solution is to reset the sum each time it reaches which then increments the bucket column and begins new sum operation starting at the size value of the current record because the order of the records is important to this operation ive included the value column which is intended to be sorted in descending order my initial attempts have involved making multiple passes over the data once to perform the sum operation once more to ceiling that etc here is an example of what did to create the crude sum column select id value size select top sumsize from table t2 where t2 value t1 value as crude sum from table t1 which was used in an update operation to insert the value into table to work with later edit id like to take another stab at explaining this so here goes imagine each record is physical item that item has value associated with it and physical size less than one have series of buckets with volume capacity of exactly and need to determine how many of these buckets will need and which bucket each item goes in according to the value of the item sorted from highest to lowest physical item cannot exist in two places at once so it must be in one bucket or the other this is why cant do running total ceiling solution because that would allow records to contribute their size to two buckets
45296 have table where the rownumber is of essence it has range of clients and running total for that client which resets every time the client changes client rownr amount runtotal company company company company reset because customer previous row customer company reset because customer previous row customer company reset because customer previous row customer company company tried using partition but it always groups client together and client together it removes the crucial part of the row numbers any help please drop table checks create table checks client varchar32 rownr int amount decimal122 runtotal decimal122 part int insert checksclient rownr amount select company union all select company union all select company union all select company union all select company union all select company union all select company union all select company gets the first entries per client these amounts are the base amounts and the other entries are tallied up with cte as select c1 client c1 amount c1 rownr case when c1 client c2 client then c1 amount else null end amt from checks as c1 left join checks as c2 on c1 rownr c2 rownr select client rownr amount case when isnullamt0 then amt else total end as runtotal from cte cross apply select sumx amount as total from cte as where rownr cte rownr and cte client client as rt drop table checks
45302 suppose have structure like this recipes table recipeid name description recipeingredients table recipeid ingredientid quantity uom the key on recipeingredients is recipeid ingredientid what are some good ways for finding duplicate recipes duplicate recipe is defined as having the exact same set of ingredients and quantities for each ingredient ive thought of using for xml path to combine the ingredients into single column havent fully explored this but it should work if make sure the ingredients uoms quantities are sorted in the same sequence and have proper separator are there better approaches there are 48k recipes and 200k ingredient rows
45461 have the following table in sql server create table mytable id uniqueidentifier not null primary key default newid mygroup int not null want to output table of the form mygroup count max min for example if have rows in mytable where mygroup rows where mygroup and rows where mygroup then mygroup count max min what sort of query would output this information
45579 have common product table in db1 and db2 want to make it one table because it has one inventory stock of products what is the best solution to share product table among two different database have few thoughts writing trigger on update delete insert one table and update another same time creating view on product table and used in second db dont know how replication dont know how can you guide me on the best way
45589 cant seem to find documentation that describes the valid formats of postgresql schema name know that schema name cannot start with number have spaces start with pg what else where should look
45655 im very new to microsoft sql server business intelligence and analysis servicebut im programming for years with sql server can any one describe measures and dimensions in cubes in simple wordsif its possible with images thanks
45665 have mysql in master slave setup and created another slave server stopped the original slave dumped the data copied and reimported and it worked fine noted the master log pos of the original slave and used these commands to set it on the new slave change master to master host ipaddress master user username master password password master port master log file mysql bin master log pos master connect retry when started the new slave got last io error got fatal error from master when reading data from binary log log event entry exceeded max allowed packet increase max allowed packet on master however when started the original slave it caught up just fine and is now in sync so the questions the current value is 16m how do know how big to go would rather avoid trial and error with production server why do need to increase the value on the master when the original slave coped just fine could the problem really be with the new slave update increased the max allowed packet to as rolando suggested on the master old slave and new slave and restarted them set global max allowed packet for some reason didnt seem to take now the last io error is the same as before but now see last sql error relay log read failure could not parse relay log event entry the possible reasons are the masters binary log is corrupted you can check this by running mysqlbinlog on the binary log the slaves relay log is corrupted you can check this by running mysqlbinlog on the relay log network problem or bug in the masters or slaves mysql code if you want to check the masters binary log or slaves relay log you will be able to know their names by issuing show slave status on this slave if do mysqlbinlog on the masters file it scrolls past with commands quite happily for ages the file is 722m if do that for the slave relay log get error error in log event read log event sanity check failed data len event type error could not read entry at offset error in log format or read error checked the variables and the changes worked however mysql show variables like max allowed packet on the new slave showed max allowed packet and slave max allowed packet where as on the master it only has max allowed packet so did version check on the master mysql show variables like version variable name value innodb version protocol version slave type conversions version log version comment mysql community server gpl by remi version compile machine x86 version compile os linux and on the new slave mysql show variables like version variable name value innodb version protocol version slave type conversions version log version comment mysql community server gpl by remi version compile machine x86 version compile os linux are these versions too far apart
45708 have user table with column name email password etc would like to create another user that can only select the name column select name from user ok select email from user not ok can this be done on mysql
45712 want to speed up following piece of code delete from ssn sdo where art id art id and skl id skl id and level or art id art id and skl id skl id and level and tip tip or art id art id and skl id skl id and doc id doc id my server is ms sql server art idskl id and doc id are integers while tip is varchar10 type of want to make an indexes on ssn sdo table so this delete is going to faster what was considering to do is to make three indexes each one for every case doc id asc art id asc skl id asc skl id asc art id asc tip asc skl id asc art id asc or is there better way to make one index which will include all three cases am careful with indexes because do not want to slow down inserts in this table
45846 have single server running net web application and sql server database standard im planning to move the database onto separate server but in order to provision the network hardware id like to benchmark the data throughput between the web application and the database can port be monitored internally if so is there any tool native to windows r2 that can do this or would need some 3rd party application like wireshark my connection string is referencing the database using server localhost is it still possible to tap into to see the bandwidth used on this port essentially im trying to determine if need gbit or mbit connection between the web server and database server any thoughts on this would be much appreciated update 8th july hashed out the above as realise its largely irrelevant for some reason thought there would be big cost difference between mbit and gbit switch only the cheapo home user switches are mbit as others have pointed out wireshark will not pickup activity between iis and the sql server on the same box im putting in 1gbit managed switch for now and ill use either wireshark or the built in monitoring on the switch to see whats going on later dont think it will be anywhere close to the physical transfer limits imposed by the hardware
45870 trying to do an hourly incremental backup of single postgres server win7 have the following setup in postgresql conf max wal senders wal level archive archive mode on archive command copy postgres foo restart did base backup with pg basebackup postgres foo which made big base tar file in the foo folder and added some kb files which assume are wals what dont understand is why the wals in foo dont change the wals in data pg xlog change is pg not supposed to copy them over how does it decide to do so perhaps need to set archive timeout ive seen several sites pgs mailing lists baculas postgres page that say you need to call pg start backup and pg stop backup but believe that those are not required is that true secondary questions how often do the wals in data pg xlog get written what triggers write it seems to update wal if do some dml then in psql or edit table in pgadmin then close the window figured it would write on commit best practices pg basebackup once week archive wals to same machine as pg or remote machine
45876 saw in the sql server central thread does full backup truncate the log that full backup does not truncate the log no neither full or differential backups truncate the transaction log lynn pettis no full backup does not truncate the log chad crawford so what is the difference between full backup and copy only full backup for the log backup there is copy only backup which prevent the log chain from breaking without truncating the log so what is copy only full backup
45926 im doing system testing of product and several tests are severely changing the data of demo database the data is quite complex to create queries which will undo the changes done by the tests moreover those tests are subject to change using backup restore is not an option since restoring the database takes seconds which is too long in context of automated testing using insert into select is complex too again tests are subject to change and can affect tables they dont affect now copying the whole database this way is not an option neither for performance reasons microsoft sql server has snapshots feature but given that dont have enterprise version on localhost cant use it cant use transactions neither since the system tests involve two applications with complex interaction between them of course they dont share the same connection what can be done to quickly undo the last changes of database to make the problem easier the database is in controlled environment it is on localhost so am the only one who can access it the only changes are from system tests all the changes can and are expected to be lost any recovery model can be chosen the preparation for example some sort of backup can take long time or even be manual task since it would be performed very rarely its only the speed of undoing the changes from this point which is important since its done after every test which affects the database
45934 from what can find the version store will only clean up versions that are older than the oldest active transaction question is the oldest transaction database specific or will sql server keep all versions regardless of the database if there is an older transaction still active period backstory sql server sp4 enterprise hosting around databases tempdb is currently gb version store is around gb one of the applications hosted on the database instance has an open transaction that is days old based on sys dm database transactions two separate large databases had extremely heavy use over the last month and we saw consistent tempdb growth coinciding with these operations we expected some growth we did not expect it to keep growing question are the versions stored in tempdbs version store from these two separate databases still there because third independent database has connection that is days old and shows an open transaction state perfmon counters version store is continually growing in the few hours have tracked it this morning version generation rate avg is around kb version cleanup rate is kb plenty of space left for tempdb there are around gb of total data files for all user databases tempdb has grown on average mb per day for each of its data files since the last restart this behavior is abnormal and investigation revealed the large version store answers to comment questions so as not to have long running comment section why auto growth on tempdb tempdb is set to initialize at size we have found to be appropriate for most of the time we allow auto growth in order to handle abnormal database activity we monitor auto growth as well how do you know the transaction is active and not just an active connection transaction state says active in sys dm tran active snapshot database transactions and other stuff activity monitor says each connection has open transaction why is your app so stupid its third party one of many on this instance do not know if the behavior is abnormal or easily fixed resolution the open transactions where preventing any version store cleanup so jon was right version store cleanup is done independent of databases closing the offending transactions allowed version store cleanup to commence current theory behind why is from jon seigel the version store can only clear versions based on the oldest active transaction within the entire instance to support the use of transaction level snapshot isolation across multiple databases simultaneously if anyone knows for certain or can prove this please do referenced question find transactions that are filling up the version store referenced documents tempdb wp teratrax tuning tempdb idera demystify tempdb
46009 use the following query to find performance improvements in queries select top substringqt text qs statement start offset case qs statement end offset when then datalengthqt text else qs statement end offset end qs statement start offset qs execution count qs total logical reads qs last logical reads qs min logical reads qs max logical reads qs total physical reads qs last physical reads qs min physical reads qs max physical reads qs total elapsed time as total elapsed time qs last elapsed time as last elapsed time qs min elapsed time as min elapsed time qs max elapsed time as max elapsed time qs last execution time qs creation time qp query plan from sys dm exec query stats qs cross apply sys dm exec sql textqs sql handle qt cross apply sys dm exec query planqs plan handle qp where qt encrypted and last execution time dateadd minute current timestamp order by qs total logical reads desc order by qs total physical reads desc the issue is that ive found query that appears to be thrown repeatedly from time to time and requires enhancement but im not able to determine where it comes from could be user or program but in any case should have login or username how can get this user or track this query so can apply improvements to it
46011 am setting up mysql master slave replication and am trying to figure out how to handle the failover situation where promote the slave to master in the event that the master goes down my application server needs to direct all writes to the current master but cannot use server level ha between the master and slave heartbeat keepalived since the two db servers are on completely different subnets in different physical locations think this is something that need to handle at the application level can query the two servers and ask which one is master then perform all queries to that one is there query in mysql to see if the current server is master in master slave replica
46074 we have been experiencing memory issues with sql server we first realised we had problem when we started getting timeouts and login errors connection was successfully established with the server but then an error occurred during the login process provider tcp provider error the specified network name is no longer available looking into event viewer on our sqlbox we noticed multitude of insufficient memory errors there is insufficient system memory to run this query for more information see help and support center at http go microsoft com fwlink events asp the only immediate warning prior to this was the following message appdomain alerts dbo runtime unloaded about twenty minutes prior to this we had number of perf related messages and errors info the microsoft operations manager agent on this computer received new rules and configuration settings from its mom server management group ggc warning the configuration information of the performance library windows system32 aspperf dll for the asp service does not match the trusted performance library information stored in the registry the functions in this library will not be treated as trusted error the microsoft operations manager performance provider could not access performance counters on computer blah blah blah microsoft operations manager will not monitor performance counters on this computer until they become available info the microsoft operations manager successfully loaded performance counters on computer blah blah blah after previous failures and will start monitoring them doubt the above perf alerts errors had anything to do with the two hours of insufficient memory exceptions but have included the messages just in case finally after two hours of red memory errors the following info message heralded the end of the insufficient memory alerts sql server has encountered occurrences of cachestore flush for the bound trees cachestore part of plan cache due to dbcc freeproccache or dbcc freesystemcache operations so freeprocache was called by our dba at some point despite eventually fixing the insufficient memory exceptions we noticed that our execution plans were still not being stored this issue has now continued for whole days meaning that apps using queries with complex plans are facing sever performance difficulties there are points where the plans start to get taken again but they dont ever tend to stay in the cache for long im wondering if anyone could help with pinpointing the area of concern part represents the system when the query plans are being kept plans being retained but only for an hour or so and part represents when the plans are not being cached at all checking dm exec query stats part dbcc memorystatus results memory manager kb vm reserved vm committed awe allocated reserved memory reserved memory in use memory node id kb vm reserved vm committed awe allocated multipage allocator singlepage allocator memoryclerk sqlgeneral total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlbufferpool total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlqueryexec total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqloptimizer total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlutilities total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlstoreng total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlconnectionpool total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlclr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlservicebroker total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlhttp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sni total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk fulltext total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlxp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk bhf total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlqereservations total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk host total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sosnode total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore objcp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore phdr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xproc total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore temptables total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore notif total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore viewdefinitions total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbtype total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbelement total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbattribute total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore stackframes total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokertblacs total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerkek total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerdsh total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerusercertlookup total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerrsb total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerreadonly total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerto total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore events total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore systemrowset total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore schemamgr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore dbmetadata total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore tokenperm total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore objperm total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore sxc total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore lbss total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore sni packet total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore service broker total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore lock manager total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator buffer distribution buffers stolen free cached database clean database dirty latched buffer counts buffers committed target hashed stolen potential external reservation min free visible available paging file procedure cache value totalprocs totalpages inusepages global memory objects buffers resource locks xdes setls se dataset allocators subpdesc allocators se schemamanager sqlcache replication serverglobal xp global sorttables query memory objects value grants waiting available buffers maximum buffers limit next request waiting for cost timeout wait time last target small query memory objects value grants waiting available buffers maximum buffers limit optimization queue value overall memory target memory last notification timeout early termination factor small gateway value configured units available units acquires waiters threshold factor threshold medium gateway value configured units available units acquires waiters threshold factor big gateway value configured units available units acquires waiters threshold factor memorybroker for cache value allocations rate target allocations future allocations last notification memorybroker for steal value allocations rate target allocations future allocations last notification memorybroker for reserve value allocations rate target allocations future allocations last notification the available memory and largest free contiguous block total avail mem kb max free size kb part dbcc memorystatus memory manager kb vm reserved vm committed awe allocated reserved memory reserved memory in use memory node id kb vm reserved vm committed awe allocated multipage allocator singlepage allocator memoryclerk sqlgeneral total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlbufferpool total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlqueryexec total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqloptimizer total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlutilities total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlstoreng total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlconnectionpool total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlclr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlservicebroker total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlhttp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sni total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk fulltext total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlxp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk qsrangeprefetch total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk bhf total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sqlqereservations total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk host total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator memoryclerk sosnode total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore objcp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore sqlcp total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore phdr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xproc total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore temptables total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore notif total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore viewdefinitions total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbtype total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbelement total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore xmldbattribute total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore stackframes total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokertblacs total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerkek total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerdsh total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerusercertlookup total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerrsb total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerreadonly total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore brokerto total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore events total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator cachestore systemrowset total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore schemamgr total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore dbmetadata total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore tokenperm total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore objperm total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator userstore sxc total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore lbss total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore sni packet total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore service broker total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator objectstore lock manager total kb vm reserved vm committed awe allocated sm reserved sm commited singlepage allocator multipage allocator buffer distribution buffers stolen free cached database clean database dirty latched buffer counts buffers committed target hashed stolen potential external reservation min free visible available paging file procedure cache value totalprocs totalpages inusepages global memory objects buffers resource locks xdes setls se dataset allocators subpdesc allocators se schemamanager sqlcache replication serverglobal xp global sorttables query memory objects value grants waiting available buffers maximum buffers limit next request waiting for cost timeout wait time last target small query memory objects value grants waiting available buffers maximum buffers limit optimization queue value overall memory target memory last notification timeout early termination factor small gateway value configured units available units acquires waiters threshold factor threshold medium gateway value configured units available units acquires waiters threshold factor threshold big gateway value configured units available units acquires waiters threshold factor memorybroker for cache value allocations rate target allocations future allocations last notification memorybroker for steal value allocations rate target allocations future allocations last notification memorybroker for reserve value allocations rate target allocations future allocations last notification memory left total avail mem kb max free size kb part and part are both taken at points where memory is low the difference is that with it seems that the query plans are not being retained for any period of time at all where in they are being held for an hour or so im hoping that someone can look at the memory statuses and possibly point me in the direction of where the problem resides also we are on sql server server pack update ok was looking at the memorystatus above and noticed that the object cache store was at 297mb in part is this high for running on bit will this not consume the majority of vas wanted to take look at this in bit more detail so ive been running this query get the size of the plan cache cachestore sqlcp is non sp and cachestore objcp is sp select sumsingle pages kb summulti pages kb as plan cache sizegb from sys dm os memory cache counters where type cachestore sqlcp or type cachestore objcp it seems that this is cycling every two minutes or so with the stores being flushed every tim it stats to rise above 200mb the majority of this as in 180mb is in cachestore objcp would be right in thinking can use the following query to then analyse the object cache select top objtype usecounts size in bytes in kb left sql text as text from sys dm exec cached plans outer apply sys dm exec sql text plan handle sql order by in kb desc the above query taken at around about the highest point in cache mb returns around objects more or less the top in terms of size seem to be triggers with the sum size 65mb is this normal am barking up the wrong tree
46102 ive been doing some testing of different methods for compressing and storing sql server backups using sql server r2 enterprise edition and im wondering what the most effective compression algorithm is for long term storage of those backups outside of sqls internal compression algorithms im not worried about the physical storage or tape drives or anything just trying to turn our 3tb of data and log files into the smallest single file can so for example would zip or 7z or are there too many variables within my database to be able to accurately estimate what will be the most effective and ill just need to do some tests or is sql servers internal compression the best ill get
46125 im using django and every once in while get this error integrityerror duplicate key value violates unique constraint myapp mymodel pkey detail key id already exists my postgres database does in fact have myapp mymodel object with the primary key of why would postgres attempt to use that primary key again or is this most likely my application or djangos orm causing this this issue occurred more times in row just now what ive found is that when it does occur it happens one or more times in row for given table then not again it seems to happen for every table before it completely stops for days happening for at least minute or so per table when it does occur and only happening intermittently not all tables right away the fact that this error is so intermittent happened only or so times in weeks no other load on the db just me testing out my application is what makes me so wary of low level problem
46127 have comments table which can be simplified down to this comments id user id text parent id where parent id is nullable but might be key for its parent comment now how can select all descendants of specific comment the comments might be several levels down
46273 installed sql server r2 express successfully then realised need to get full text search so downloaded the advanced services installation package but when run it there is no option in the feature selection part for full text search please dont tell me hav to uninstall and reinstall
46289 suppose have to export data from one server to another through linked servers which statement will be more efficient executing in source server insert into destinationlinkedserver destinationdb dbo table select from dbo udf getexportdata or executing in target server insert into dbo table select from openquery originlinkedserver select from origindb dbo udf getexportdata which one will be faster and consume fewer resourcers in total both source and target server both servers are sql server
46373 we can declare an identity like id num so that id num will have an increment of unique numbers create table new employees id num int identity11 fname varchar minit char1 lname varchar30 is it recommended to use identity as alternative to primary key since identity provided unique number for each row
46408 im designing database which will store data in different languages using utf so think the best way to display the querys results is ordering it according to the users language during the query itself because there are more than one correct ways to do that as follows select collate de de from test1 assuming this is the correct way to work with international data which is the best collation for the database itself postgresql documentation says the and posix collations both specify traditional behavior in which only the ascii letters through are treated as letters and sorting is done strictly by character code byte values think this is the best choice in this case or am wrong bonus question is it too slow to select the collation in the query itself
46410 using postgresql v9 have the following tables create table foo id bigserial not null unique primary key type varchar60 not null unique create table bar id bigserial not null unique primary key description varchar40 not null unique foo id bigint not null references foo on delete restrict say the first table foo is populated like this insert into foo type values red green blue is there any way to insert rows into bar easily by referencing the foo table or must do it in two steps first by looking up the foo type want and then inserting new row into bar here is an example of pseudo code showing what was hoping could be done insert into bar description foo id values testing select id from foo where type blue another row select id from foo where type red
46486 created function that accepts start and end date with the end date being optional then wrote case in the filter to use the start date if no end date is passed case when dateend is null then datestart else dateend end when call the function for the most recent month of the data select from thefunction null the query hangs if specify the end date select from thefunction the result is returned normally took the code out of the function and ran it fine inside query window cant duplicate the issue the fiddle either query like select from thefunction also works fine is there anything in the query below that could be causing the function to hang when null is passed for the end date sql fiddle execution plan for select from thefunction estimated plan for select from thefunction null
46577 consider the following query merge parameter with rowlock as target using select areaid parametertypeid value as source areaid parametertypeid value on target areaid source areaid and target parametertypeid source parametertypeid when matched then update set target value source value updatedid target id when not matched then insert areaid parametertypeid value values source areaid source parametertypeid source value statistics gives the following output table parametertype scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table area scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table parameter scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads worktable appears in the messages tab which makes me think that tempdb is being used by merge am not seeing anything in the execution plan that would indicate need for tempdb does merge always use tempdb is there anything in bol that explains this behavior would using insert update be faster in this situation left right here is the table structure
46672 currently working on an database audit project based on triggers that are fired on update on specific tables the triggers write the changes into table information written are table name updated column timestamp user old value and new value triggers work fine with single updates but when it comes to multi row updates it is not working my code is like this if updatepriority begin set updatedcolumn priority insert into dbo audittable tablename source recordid user timestamp updatedcolumn oldvalue newvalue select nbookingitem tablename nvarcharmax select code from tbl leg source inner join inserted ins on leg source id ins sourceid ins id recordid bigint select username from inserted inner join tbl user on modifiedbyid user id user nvarcharmax getdate timestamp datetime updatedcolumn updatedcolumn nvarcharmax del priority oldvalue nvarcharmax ins priority newvalue nvarcharmax from inserted ins inner join deleted del on ins id del id where ins priority del priority or ins priority is null and del priority is not null or ins priority is not null and del priority is null end error message msg level state procedure mytrigger line subquery returned more than value this is not permitted when the subquery follows or when the subquery is used as an expression any suggestions on how to fix my trigger in order to handle multi row operations
46870 here are two tables school staff school code staff type name last update date time person id abe principal jan abe principal feb persons person id name abc xyz here is my oracle query select maxlast update date time as last update school code person id from school staff where staff type name principal group by school code person id order by school code which gives this results last update school code person id jan abe feb abe want to select the first one for the school which has latest date thanks
46942 have trc file from trace that dba did on one of my databases dont have the sql profiler tool installed on my pc so cant view the contents and analyze the trace log how do read this file without sql profiler installed on my pc
46957 have table that contains records in an un expanded form each record has an associated integer weight that essentially informs us how many times the record should be replicated in order to get the true population say have records in my table sampn ids unique record and weight is the frequency weight the un expanded dataset looks like this sampn weight attrib1 attrib2 attrib3 once expanded the dataset will be like this note removed the weight field but this is not essential sampn attrib1 attrib2 attrib3 have tried to do this using cursors but it is taking really long time to execute is there clever way to do this really fast any predefined sql stored procedure that achieves this update all thanks for the answers really great learning experience performed the expansion operation on my dataset pauls auxiliary table of numbers had the best execution time
46980 given variable that contains stored procedure name declare stored procedure name varchar512 set stored procedure name some stored procedure name how can execute the stored procedure without passing in any arguments
47025 this next year am helping an effort to clean several sql server environments we have about stored procedures and estimate that only about of them are used on regular basis and another or so are used on rare occasion meaning we have lot of work to do since we have multiple departments and teams that can access these databases and procedures we are not always the ones calling the procedures meaning that we must determine what procedures are being called on top of that we want to determine this over few months not in few days which eliminates some possibilities one approach to this is to use the sql server profiler and track what procedures are being called and compare them to the list of what procedures we have while marking whether the procedures are used or not from then we could move the procedures to different schema in case department comes screaming is using the profiler the most effective approach here and or have any of you done something similar and found another way better way to do this
47046 ive used purge binary logs as well as flush logs but the mysql directory still contains these files mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin index is there reason why using the commands are not working these files are taking lot of space would like to get rid of them safely
47051 am aware of the microsoft provided code to transfer logins between sql servers however this only does the account and the password what if that particular account has various roles and permissions assigned to it at server level is there an equivalent piece of code to script these permissions also thanks
47058 our sql server instance is configured to send mail and everything is working correctly but cant figure out how to view the existing configuration in particular the smtp server from ssms can only start the configuration wizard and cant find anything online plenty of info on how to set it up but nothing on how to view the current settings how can view the existing settings
47098 so im currently creating some sql to read through the postgres catalogs to build table definitions however am encountering problem with serial bigserial data types example create table cruft temp id bigserial primary key select from information schema columns where table schema cruft and table name temp db cruft temp id nextvalcruft temp id seq regclass no bigint db pg catalog int8 no no never yes it gives me database name db schema name cruft table name temp column name id default value nextval and data type bigint and int8 not bigserial realize that could just check to see if the default value was sequence but dont believe that would be accurate since could manually create sequence and create non serial column where the default value was that sequence does anyone have suggestion for how might accomplish this anything other than checking the default value for nextval seq edited for sql solution added here in case of tl dr or new users unfamiliar with the pg catalog with sequences as select oid relname as sequencename from pg class where relkind select sch nspname as schemaname tab relname as tablename col attname as columnname col attnum as columnnumber seqs sequencename from pg attribute col join pg class tab on col attrelid tab oid join pg namespace sch on tab relnamespace sch oid left join pg attrdef def on tab oid def adrelid and col attnum def adnum left join pg depend deps on def oid deps objid and deps deptype left join sequences seqs on deps refobjid seqs oid where sch nspname information schema and sch nspname not like pg wont work if you have user schemas matching pg and col attnum and seqs sequencename is not null to only view serial bigserial columns order by sch nspname tab relname col attnum
47127 built the following sql server query but it is encountering the anti semi join defect in sql server which results in inaccurate cardinality estimates urgh and runs forever since it is longstanding production sql server cant easily suggest to upgrade versions and as such cannot force the traceflag hint on this specific query im having hard time refactoring the where and not in select can anyone care to help ive made sure to try and use the best joins based on clustered key pairs select top doc2 id direction cd address type cd external identification hash value publishdate sender address id as d2 sender address id address id as address id message size subject emi employee id from assentor emcsdbuser doc2 dnolock inner join assentor emcsdbuser employee msg index eminolock on processdate emi processdate and doc2 id emi doc2 id inner loop join assentor emcsdbuser doc2 address anolock on emi doc2 id doc2 id and emi address type cd address type cd and emi address id address id inner join sis dbo sis snolock on external identification external identification where publishdate and doc2 id not in select doc2 id from assentor emcsdbuser doc2 address d2anolock where doc2 id d2a doc2 id and d2a address type cd frm option fast note that the employee msg index table is 500m rows doc2 is 5b rows sis is 500m rows any help would be appreciated
47186 given variable that contains stored procedure name declare stored procedure name sysname set stored procedure name some stored procedure name how can drop the stored procedure
47237 today we experienced degradation in performance on our production sql server durring the time this occurred we logged several the query processor could not start the necessary thread resources for parallel query execution errors the reading that ive done suggests that this has to do with how many cpus to use when executing complex query however when checked during the outage our cpu utilization was only at is there something else this could be referring too that havent come across yet is this likely culprit of the performance degradation or am chasing red herring my sp configure values for this are as follows name minimum maximum config value run value cost threshold for parallelism
47266 our users want to refresh qa database from production but want two tables lets call them t1 and t2 to retain the original qa data so copied two tables from qa db1 to temp qa database db2 then refreshed db1 from production after the refresh want to overwrite t1 and t2 data from db2 to db1 so it can contain pre refresh qa values have done the following use select into d1 dbo t1 from d2 dbo t1 then refreshed d1 from prod then truncate t1 with the following step select count as beforetruncatecount from t1 go truncate table t1 go select count as aftertruncatecount from t1 go now when go back to copy data from d2 t1 to d1 t1 get the error that there is already an object named t1 in the database should drop the table and copy or is there any better method for the whole procedure
47267 in previous question how to merge data sets without including redundant rows asked about filtering redundant historical data during import but davidspillett correctly replied that couldnt do what was trying to do instead of filtering the table during import now want to create view on the table that returns only records where the price has changed heres the original scenario rephrased to suite this question we have table of historical prices for items the table contains rows where the same price is recorded for multiple dates want to create view on this data which only shows price changes over time so if price changes from to want to see it but if it changes from to then dont want to see it example if the price yesterday was and the price today is and there were no other price changes then the price today can be inferred from the price yesterday so only need the record from yesterday example http sqlfiddle com c95ff table data effective product kind price 23t00 24t00 redundant implied by record 25t00 26t00 27t00 redundant implied by record 28t00 not redundant price changed back to expected view data effective product kind price 23t00 25t00 26t00 28t00 my initial attempt used row number select effective product kind price from select history row number over partition by product kind price order by effective asc as rownumber from history where rownumber order by effective which returned effective product kind price 23t00 not good 25t00 26t00 not good not bad tried searching for similar question answer but its hard to work out how to phrase the search an example is worth lot of words any suggestions appreciated thanks
47295 tl dr ive got an unfixable corruption in an indexed view here are the details running dbcc checkdb dbname with extended logical checks data purity no infomsgs all errormsgs on one of my databases produces the following error msg level state line the spatial index xml index or indexed view viewname object id contains rows that were not produced by the view definition this does not necessarily represent an integrity issue with the data in this database checkdb found allocation errors and consistency errors in table viewname repair rebuild is the minimum repair level do understand that this message indicates that the materialized data of the indexed view viewname is not identical with what the underlying query produces however manually verifying the data does not turn up any discrepancies select from viewname with noexpand except select from t1 with forcescan join t2 on select from t1 with forcescan join t2 on except select from viewname with noexpand noexpand was used to force use of the only index on viewname forcescan was used to prevent indexed view matching from happening the execution plan confirms both measures to be working no rows are being returned here meaning that the two tables are identical there are only integer and guid columns collations do not come into play the error cannot be fixed by recreating the index on the view or by running dbcc checkdb repair allow data loss repeating the fixes also did not help why does dbcc checkdb report this error how to get rid of it even if rebuilding fixed it my question would still stand why is an error reported although my data checking queries run successfully update the bug has been fixed in some releases can no longer reproduce it in sql server sp2 cu the sp2 kb contains fix without kb article creating non clustered index causes dbcc checkdb with extended logical checks to raise corruption error the two connect bugs about this have been closed https connect microsoft com sqlserver feedback details creating non clustered index causes dbcc checkdb with extended logical checks to raise corruption error https connect microsoft com sqlserver feedback details unfixable dbcc checkdb error that is also false positive and otherwise strange
47304 have bak file created today by someone else manually created through ssms r2 im trying to manually restore the database unfortunately the file isnt appearing when go to browse it can script the restore process but ive seen this problem before and im not sure what could cause the bak to not appear
47345 am migrating part of mysql database into aws the data in question is write and each row has about 1k of varchar fields datetime and ints estimate that we will need between 25k records inserted hour during peak times ran iostat on the current database and it reported around tps how do figure out what type of iops ill need
47356 am looking for way to find out when the windows server was last started using only sql commands am trying to stay within the default security configurations dont want to enable xp cmdshell
47357 can force users queries to always run with the hint nolock they type select from customer but what is executed on the server is select from customer with nolock this question is not about the various pros and cons of nolock respectfully know what they are this is not the place to discuss them
47402 we have an sql server standard database to connect to it we use sql server management studio since ssms is not supported on windows xp the problem is that sql server gives all the time an exception index outside the bounds of the array and its not possible to edit table in visual editor or open table client can work now only with database objects through queries so the question is is there any solution on windows xp to work properly with database
47431 sql server is consuming of my server ram this recently caused lot of performance bottlenecks such as slowness researched this issue one common solution could find on the internet is to set the maximum limit for sql server this was done and much improvement is gained want to know why if the maximum memory value is not set why sql server keeps consuming the resources
47444 im not experienced with dba work but im trying to make case for requesting additional resources for our sql server and was hoping could get some smart folks to provide rough rough estimate of what we should be running im suspecting that the allocation of resources it has given to our production sql server is low hardware software database sql server r2 enterprise database windows windows r2 enterprise bit pretty sure running on vmware processor intelr xeonr cpu e7 27ghz ghz processors installed memory 4gb hard drive for database files 300gb hard drive for backups 150gb hard drive for logs 100gb application we have main databases that add up to about 170gb in data reporting services database ssrs on the same server that houses maybe different reports comprising average of 700k records each that get generated daily our user base is about simultaneous users maybe of those could be considered resource intensive with generating data crunching large reports majority of users interact with database through asp net website and report server website additionally our developers use ssis in bids extensively by remoting directly onto the server remote connections max finally we have fairly involved data warehousing operation that probably brings in million records per day by way of ssis packages that also run on the server current problems we have chronic sever server timeouts and the response time to the website is pretty bad suspect the amount of memory we have 4gb is probably large bottleneck our previous requests for additional memory has been denied with the common response that we need to perform more query optimizations while we arent sql pros or as im sure you can tell by our setup db admin pros want to make sure im not expending all my time trying to squeeze out little potential performance if the hardware is the bottleneck thanks all for the tl dr avoidance
47545 am aware of the concept of bulk insert wherein load data from flat file to database now want to unload data from table to flat file want to export data in table to flat file can anyone tell me how to do this am using sql server r2
47842 when tried to restore all database dump which is in version to version it got restored and after that when tried to reconnect am getting the following error error hy000 connection using old pre authentication protocol ref used client option secure auth enabled have tried adding the following lines in my ini and restarted the servicebut the issue persist till skip grant tables the following link says its bug in mysql https github com santisaez powerstack blob master packages mysql mysql powerstack secure auth patch do anyone have any fixes for this solution
47861 have two queries query select eid item as item sums qty as total sold from tbl sales tbl matentry where eid item group by eid result eid item total sold rupa pan america john player classmate lepakshi lee puma query select eid item as item sums qty as total stock from tbl purchases tbl matentry where eid item group by eid result eid item total stock rupa pan america john player classmate lepakshi lee puma when combine both sql statements into single statement am getting incorrect output so far have tried this select eid item as item sump qty as total stock sums qty as toatl sold sump qty sums qty as balance stock from tbl matentry left outer join tbl purchases on item eid left outer join tbl sales on item eid group by eid order by eid eid item stock sold balance rupa pan america john player classmate lepakshi lee puma how to do get correct result where is mistake error in my query please review my query and help me to get correct data my expected output is eid item total stock total sold balance stock rupa pan america john player classmate lepakshi lee puma
48011 have an existing table with data dbo test col1col2col3 on primary need to change this table to be partitioned like this dbo testcol1col2col3 on ps datecol2 how can achieve this without dropping and recreating the table
48055 suppose you have something like this source table variable values leftid int not null rightid int not null customvalue varchar100 null target table mapping leftid int not null rightid int not null customvalue varchar100 null want to merge values into target with the following rules match on source leftid target leftid and source rightid target rightid when matched in target update customvalue when not matched in target insert delete any unmatched values in the target that do match leftid in the source only delete records that related to the lefids of what im merging that last rule is hard to describe sorry for instance source foo foo target bar foo car merge result result target foo updated foo inserted foo deleted car unchanged so heres what have so far which takes care of update and insert merge mapping as target using select leftid rightid customvalue from values as source leftid rightid customvalue on target leftid source leftid and target rightid source rightid when not matched then insert leftid rightid customvalue values source leftid source rightid source customvalue when matched then update set customvalue source customvalue how do do the delete part of my rule
48072 run an explain mysql explain select last name from employees order by last name id select type table type possible keys key key len ref rows extra simple employees all null null null null using filesort row in set sec the indexes in my table mysql show index from employees table non unique key name seq in index column name collation cardinality sub part packed null index type comment index comment employees primary subsidiary id null null btree employees primary employee id null null btree employees idx last name last name null btree employees date of birth date of birth null null yes btree employees date of birth subsidiary id null null btree rows in set sec there is an index on last name but the optimizer does not use it so do mysql explain select last name from employees force indexidx last name order by last name id select type table type possible keys key key len ref rows extra simple employees all null null null null using filesort row in set sec but still the index is not used what am doing wrong here does it have to do with the fact that the index is non unique btw the last name is varchar1000 update requested by rolandomysqldba mysql select countdistinct last name distinctcount from employees distinctcount row in set sec mysql select count1 from select count1 count500last name from employees group by last name having count1 count1 row in set sec
48108 need to add constraint with two columns that says if any given value is present in one of the columns then it cannot be duplicated in the same column it cannot be duplicated in the other column either the constraint we are looking to make is with primaryemail secondaryemail this would be invalid userid primaryemail secondaryemail joe yahoo com null smo gmail com joey yahoo com because joe yahoo com is present in the first column and therefore it cannot be present in the second column regardless of what row its in is it possible to define this type of constraint in sql server we started by defining table just for emails but weve since reverted from that model in favor of two hard columns for many reasons including query speed query complexity and the probability of user using multiple email accounts actively decreases in order of magnitude after one this defines traditional two column constraint but its on per row basis between the two columns and doesnt give us what we are after create unique nonclustered index idx uniqueemail notnull on userprofile primaryemail secondaryemail where primaryemail is not null and secondaryemail is not null
48166 after fresh install of xampp and import from my linux live db that is working to the windows dev stage started encountering problems with insert everything else seems to work well the errors get in the mysql error log are 16c0 innodb error table mysql innodb table stats not found 16c0 innodb recalculation of persistent statistics requested for table sizaradb pages but the required persistent statistics storage is not present or is corrupted using transient stats instead 16c0 innodb error table mysql innodb table stats not found 16c0 innodb recalculation of persistent statistics requested for table sizaradb translations but the required persistent statistics storage is not present or is corrupted using transient stats instead cannot open table mysql innodb index stats from the internal data dictionary of innodb though the frm file for the table exists tried to fix it by googling the errors and trying to see fixes of other people but it is still not working have been working on this for days now please help me solve it
48365 im working on implementing paul randals method of manually spreading dbcc checkdb over several days for very large databases which basically consists of dividing the tables in the database roughly equally between buckets running dbcc checkalloc twice week running dbcc checkcatalog once week running dbcc checktable on one bucket each day of the week has anyone used this technique any existing scripts out there im concerned this may not actually cover everything that checkdb does the books online documentation for checkdb says that in addition to checkalloc checkcatalog and checktable it also validates the contents of every indexed view in the database validates link level consistency between table metadata and file system directories and files when storing varbinarymax data in the file system using filestream sql only validates the service broker data in the database so here are my questions are these additional checks necessary important indexed views are probably bit more concerning to me dont think we are using service broker or filestream yet if so are there ways to perform these additional checks separately checkalloc and checkcatalog seem to run very quickly even on large dbs any reason not to run these every day note this will be standard routine for thousands of existing databases across hundreds of servers or at least every database over certain size this means that options like restructuring all databases to use checkfilegroup arent really practical for us
48390 were moving from sql instance and db have collation of sql latin1 general cp1 ci as to sql which defaults to latin1 general ci as completed sql r2 installation and used default latin1 general ci as collation with the restoration of the database still on sql latin1 general cp1 ci as the excepted problems occurred the temp tables where in latin1 general ci as whilst the db was in sql latin1 general cp1 ci as and this is where am now need advice on the pitfalls now please on installation of sql r2 have the option on installation to use sql collation used for backwards compatibility where have the option to select the same collation as the database sql latin1 general cp1 ci as this will allow me to not have problems with temp tables but are there pitfalls would lose any functionality or features of any kind by not using current collation of sql what about when we move in years from to sql will have problems then would at some point be forced to go to latin1 general ci as read that some dbas script complete the rows of complete databases and then run the insert script into the database with the new collation im very scared and wary of this would you recommend doing this
48405 restore failed for server servername microsoft sqlserver smoextended system data sqlclient sqlerror restore could not start database ecp microsoft sqlserver smo the database is created but is not accessible using object explorer the database ecp is not accessible objectexplorer thanks for any advice on how can make the database work edit this query select state desc from sys databases where name ecp returns recovery pending deleted the database again and had new try with restore verifyonly from disk path file this returns attempting to restore this backup may encounter storage space problems subsequent messages will provide details the path specified by ecpdata1 ecpdata1 mdf is not in valid directory directory lookup for the file ecpdata2 ecpdata2 ndf failed with the operating system error 3failed to retrieve text for this error reason directory lookup for the file ecpdata3 ecpdata3 ndf failed with the operating system error 3failed to retrieve text for this error reason directory lookup for the file ecpdata4 ecpdata4 ndf failed with the operating system error 3failed to retrieve text for this error reason directory lookup for the file ecplog1 ecplog1 ldf failed with the operating system error 21failed to retrieve text for this error reason the backup set on file is valid since it says that the backup set is valid just have to get the files specified to make it work right but on this server dont have drive or how can adjust this in the backup file or is it possible im not that familiar with windows server r2 to have some kind of symlinks like on linux
48408 have some databases created using entity framework code first the apps are working and in general im pretty happy with what code first lets me do am programmer first and dba second by necessity am reading about dataattributes to further describe in what want the database to do and my question is what penalty will be eating by having these nvarcharmax strings in my table see example below there are several columns in this particular table in they are defined as such key databasegeneratedattributedatabasegeneratedoption identity public int id get set public string name get set public string message get set public string source get set public datetime generated get set public datetime written get set expect to query and or sort based on name source generated and written expect name and source to be in the character length occasionally up to expect this table to start pretty small 100k rows but grow significantly over time 1m rows obviously message could be small or large and will probably not be queried against what want to know is there performance hit for my name and source columns being defined as nvarcharmax when never expect them to be larger than characters
48458 have few very large tables with the same basic strucure each one has rownumber bigint and datadate date column data is loaded using sqlbulkimport every night and no new data is ever loaded its historical record sql standard not enterprise so no partitioning because each bit of data needs to be tied back to other systems and each rownumber datadate combination is unique that is my primary key notice that due to the way defined the pk in ssms table designer rownumber is listed first and datadate second also notice that my fragmentation is always very high now because each datadate only appears once would expect the indexer to just add to the pages each day but wonder if it actually is indexing based on rownumber first and hence having to shift everything else around rownumber is not an identity column its an int generated by an external system sadly it resets at the start of each datadate example data rownumber datadate data is being loaded in rownumber order one datadate per load import process is bcp have tried loading to temp table and then selecting in order from there order by rownumber datadate but still comes out high fragmentation
48462 im maintaining sql server database which hosts approximately 9tb of data 45tb have raw schema and an analysis schema so basically two copies of the data ingested the recovery model is simple and the ldf is at 6gb for whatever reason the mdf is 5tb now there are only maybe additional columns in analysis tables and not many nvarcharmax columns which from what may have mistakenly understood please correct me if im wrong may be causing additional space allocation thats after shrinking the database just now it was at 9tb prior to that any thoughts and please let me know if you have additional questions im very new to database administration and optimization efforts usually dont do this side of the job many thanks andrija
48525 recently ran into situation where after log backup and shrinkfile the ldf file remained the same size dbcc loginfo clearly showed only one active vlf and ssms said that of the space in the ldf was free the first attempt did not shrink nor did the second attempt or the third after the fourth attempt success the log was reduced to the requested size meanwhile dbcc loginfo was saying that after every shrinkfile another vlf became active decided to run test of new database create database logs test use logs test first look at the vlfs for the logs test database dbcc loginfo fileid filesize startoffset fseqno status parity createlsn put the database into full recovery before making backup alter database logs test set recovery full first look at the vlfs for the logs test database dbcc loginfo fileid filesize startoffset fseqno status parity createlsn backup database logs test to disk program files microsoft sql server mssql10 mssqlserver mssql backup logs test bak with init go dbcc loginfo fileid filesize startoffset fseqno status parity createlsn create table to put stuff in create table newtablea int go insert into newtable values10 insert into newtable values20 insert into newtable values30 go run the update script again the log file has grown to accomodate the log records set nocount on declare counter int set counter while counter begin update newtable set set counter counter end dbcc loginfo fileid filesize startoffset fseqno status parity createlsn so far so good vlfs were added and they have status of run backup log backup log logs test to disk program files microsoft sql server mssql10 mssqlserver mssql backup logs test trn with init go then ran dbcc loginfo there was one active vlf exactly what was expecting dbcc loginfo fileid filesize startoffset fseqno status parity createlsn so now run my shrinkfile command thinking that will be left with ldf of 1mb dbcc shrinkfile nlogs test log dbcc loginfo fileid filesize startoffset fseqno status parity createlsn no luck the ldf is still at 100mb and now there are two active vlfs so run shrinkfile again because the amount of free space has not changed dbcc shrinkfile nlogs test log dbcc loginfo fileid filesize startoffset fseqno status parity createlsn still no luck the ldf is still at 100mb and now there are four active vlfs so run shrinkfile one last time and this is what get dbcc shrinkfile nlogs test log dbcc loginfo fileid filesize startoffset fseqno status parity createlsn active vlfs and ldf file at 1mb my questions are this why was it necessary to run shrinkfile more than one time to shrink the ldf with the ldf at 1mb why do still have so many vlfs showing up in dbcc loginfo is it possible to know what is being stored within each vlf have the lsn but would like to know what operation took place thank you update here is what found on reading log content thank you all for your input
48537 recently started using management studio when using mysql workbench handy feature was that could stay all in lower case and any reserved word like select insert would convert to upper case automatically how do replicate this behavior in ssms
48580 in sql server there are so many trace flags why some of them need to be turned off some where saw that trace flag need to be turned off so want to know which trace flag need to be tuned off and why
48630 have query to table in postgres with an order based on date field and number field this table has records the data types of the table are fcv id serial fcv fecha comprobante timestamp without time zone fcv numero comprobante varchar60 the query is select fcv id fcv fecha comprobante from factura venta order by fcv fecha comprobante fcv numero comprobante this query takes about seconds but if take out the order by the query takes only seconds the problem have is that need to run this query in the shortest time possible so search on google what can do and create composite index with the following query create index factura venta orden on factura venta using btree fcv fecha comprobante asc nulls last fcv numero comprobante asc nulls last alter table factura venta cluster on factura venta orden but the query is taking the same time or even more im using postgres here is the explain with rows sort cost rows width actual time rows loops sort key fcv fecha comprobante fcv numero comprobante sort method external merge disk 2928kb seq scan on factura venta cost rows width actual time rows loops total runtime ms postgres is running on phenon ii 1055t cores with gb ram and gb disk how can optimize this query
48643 when have query that checks if column of type uniqueidentifer does not exist in table that has null value then get no results back if the subquery does not return null it works fine and it only happens when using not in know can just do not null check in my subquery but am curious why this does not work query example select guid from tablea where guid not in select guid from tableb as working test select where newid not in select newid broken test select where newid not in select null
48705 we are using sql server with full recovery mode given full backup and series of log backups we would like to be able to check whether the log chain is complete from the last full backup to the current tail log without actually restoring these backups the purpose here is to test the consistency of the backups already know how to do this for the existing backups using restore headeronly get the firstlsn and lastlsn of every file which can be compared for consecutive files in order to determine whether they are compatible however dont know how to check whether the tail log follows the last log backup if had the firstlsn of the tail log could compare it to the lastlsn of the last log backup but how can obtain the firstlsn of the tail log need solution that works from sql server upwards ideally using sql so far have searched google to no avail btw first posted this on stackoverflow but migrated it here since it was flagged off topic there edit tried the two provided solutions on small example sql server backup database testdb to disk temp backup test full bak fire some update queries backup log testdb to disk temp backup test log1 bak fire both queries from the provided answers martin smiths answer yields shawn meltons answer yields restore headeronly from disk temp backup test log1 bak yields so it appears the first one is off by several orders of magnitude then did the same test on sql sp1 where both queries yielded the correct answer
48731 am on sql server where login with domain user on my database can see this login is mapped to the user dbo of that database what dont understand is that domain user is nowhere to be found in the sql servers security logins current user reports dbo system user reports domain user as do suser name cant find domain user in the sys syslogins what is going on can database user have its own login circumventing the sql server
48760 are there any special steps necessary to prevent data corruption when restarting server hosting an ms sql server instance for example recently encountered the recommendation of stopping the sql service manually my understanding is that this is handled by the windows shutdown process im sure there are zillion steps which individual people may recommend like that just mentioned but id like to avoid repeating obsolete or superstitious practices are there any recommendations from microsoft or widespread industry standards this question relates to the short term procedure of rebooting machine theres another question regarding the long term procedure of ensuring that machine is unused before taking it down permanently
48872 have table with millions of rows and column that allows null values however no row currently has null value for that column can verify this fairly quickly with query however when execute the command alter table mytable alter column mycolumn bigint not null the query takes forever relatively speaking it actually takes between and minutes more than twice as long as adding check constraint is there way to instantly update the tables metadata for that column especially since know that no row has null value for that column
48898 using sql server r2 maintenance plans to setup some automated backups cleanups and notifications one thing that cant figure out is what is the difference between success and completion the way think about it is that completion doesnt care if the job failed or succeeded but if it failed then obviously it didnt complete maybe perhaps its left there in case of scripts where there might not be way to tell what the actual outcome was just that the script completed but this still feels like its the same as success and failed couldnt find much documentation for it either
48899 is there best practice or coding design conventions for postgres both ddl and dml sql that you could share and recommend am looking for something similar to what google has for programming thank you in advance
48923 first some background on my setup the server has root access disabled so log in as say john who also belongs to the sudo group and is therefore able to run superuser commands created new password less user santa using the command sudo adduser shell bin bash gecos santa claus disabled password santa then changed the login sessions owner to santa using the command sudo su santa and created new postgresql database createdb myapp db the database got created without asking me for password now given the condition how do make an sql dump of the database myapp db whose owner is santa using the command pg dump this is supposed to work but it isnt john host pg dump myapp db santa localhost no owner myapp db backup sql password when run the above command it asks me for password as you can see what password am supposed to enter here didnt enter any password for the database when creating it nor does the database owner santa have any so tried entering the password of john the sudo user and got this error fatal password authentication failed for user santa also tried which didnt work either john host su santa santa host pg dump myapp db santa localhost no owner myapp db backup sql password this time created password for the user santa and entered it still get the error fatal password authentication failed for user santa what am missing here please let me know if am missing any necessary details more information as requested in the comments john host sudo su postgres postgres host psql list of databases name owner encoding collate ctype access privileges app db santa utf8 en us utf en us utf postgres postgres utf8 en us utf en us utf template0 postgres utf8 en us utf en us utf postgres postgres ctc postgres template1 postgres utf8 en us utf en us utf postgres postgres ctc postgres rows output of du santa john host sudo postgres psql postgres du santa list of roles role name attributes member of santa superuser create role create db replication
48991 ive been doing the ms10775a course this past week and one question that came up that the trainer couldnt answer reliably is does re index update the statistics we found discussions online arguing both that it does and that it doesnt
49140 this is slightly loaded question in that have already assumed that the described scenario is wrong dba is deploying an application have written that includes an ms sql server database he has asked me to take database backup from my development machine so he can restore it to the production server thus deploying it this is greenfield deployment so there is no existing data to be migrated was expecting to provide ddl script which have diligently tested and ensured that it contains everything required if execute it in ssms the database is created in one click to me using the backup facility for deployment does not seem right but without being an expert in sql server cant think of solid reason not to do it would have thought for example that there would be some contamination of the database from the development machine perhaps the computer name directory structure or user names stored in there somewhere is this the case or is backup and restore valid deployment technique
49385 have database on sql server which want to drop currently it is in single user mode and it is currently in use select from sys sysprocesses returns msg level state line database main de is already open and can only have one user at time and do not know how to identify the session have to kill an attempt to set it offline alter database main de set offline with rollback immediate yields msg level state line changes to the state or options of database main de cannot be made at this time the database is in single user mode and user is currently connected to it msg level state line alter database statement failed
49398 weve been having this nightly backup job on sql server but it appends to the bak file instead of overwriting so while googling discovered master dbo xp delete file noticed that can delete several backups at once prior than date parameter im thinking in keeping days thus deleting like this declare dt datetime select dt getdate execute master dbo xp delete file 0nd program files microsoft sql server mssql11 mssqlserver mssql backup nbak dt1 all seems ok but for the backup can create filenames like this note the today in concat declare today varchar10 select today convertchar10 getdate126 backup database perfmaster to disk concatd program files microsoft sql server mssql11 mssqlserver mssql backup perfmaster today bak with noformat noinit name nfull database perfmaster backup skip norewind nounload stats or what methods do you employ to accomplish backups with some days history
49744 when doing an insert an empty string is converted to null insert into test values now there is row with containing null but when query the table cannot use select from test where no rows selected can use null select from test where is null null so it appears that oracle decided that empty strings cannot be used for insert but they remain empty strings when doing queries where is the documentation on when an empty string becomes null and when it remains an empty string
49757 if set compression either page or row on the clustered index of table is that the same as setting compression on the table sql server provides options for doing both which suggests that they are different but was under the impression that clustered index and table were essentially the same thing and my mental model of how clustered indexes work tells me that compressing the clustered index must also compress the table
49803 find the list of privileges provided by mysql to be bit overwhelming im not sure who should have what privileges in my mind there are three typical users for my situation root developer application root is self explanatory for developer this user needs to be able to easily access any database make adjustments to it etc for starters im setting this user to this privilege set select insert update delete create drop file references index alter show databases create temporary tables lock tables execute create view show view create routine alter routine event trigger on application has an even more limited set it should just be limited to manipulating specific database im not sure what reasonable set of privileges is to grant what are reasonable set of privileges to grant developer and an application and why
49947 have recently inherited codebase with large amount of stored procedures the system they are supporting is encountering numerous performance problems which am looking in to number of the stored procedures have pattern like this create temp table build up dynamic sql query to insert bunch of records declare sql varcharmax set sql insert into temptable select somecolumn somecolumn2 somecolumn3 etc from mytable if someparam somevalue set sql sql where somecolumn somevalue if someotherparam someothervalue set sql sql where someothercolum someothervalue execute this dynamic sql exec sql select from the temp table and bring in bunch of additional information to return to the client select from temptable inner join my immediate thoughts are there is dynamic sql so no cache plans meaning plans generated every time there is an insert select pattern so table locking is more likely to be an issue have re written some of the stored procedures in this way instead select from mytable inner join where someparam somevalue or somecolumn somevalue and someotherparam someothervalue or someothercolumn someothervalue from comparing execution plans and client statistics in sql management studio have not sped the stored procedures up so am apprehensive about suggesting wholesale re writes of all stored procedures am trying to set up some profiling of live customer scenario but as yet have been unable to prove my thoughts can anyone offer any confirmation of the theory behind my thoughts or any better ways of proving my suspicions the problem is have read that dynamic sql is not always closed case it depends on how it is used my understanding of locking also falls down at the fact that nowhere can get confirmation of how this type of insert select will lock tables
49984 the situation have postgresql database which is quite heavily updated all the time the system is hence bound and im currently considering making another upgrade just need some directions on where to start improving here is picture of how the situation looked the past months as you can see update operations accounts for most of the disk utilization here is another picture of how the situation looks in more detailed hour window as you can see the peak write rate is around 20mb software the server is running ubuntu and postgresql the type of updates are small updated typically on individual rows identified by id update cars set price some price updated at some time stamp where id some id have removed and optimized indexes as much as think is possible and the servers configuration both linux kernel and postgres conf is pretty optimized as well hardware the hardware is dedicated server with 32gb ecc ram 4x 600gb rpm sas disks in raid array controlled by an lsi raid controller with bbu and intel xeon e3 quadcore processor questions is the performance seen by the graphs reasonable for system of this caliber read writes should hence focus on doing hardware upgrade or investigate deeper into the software kernel tweaking confs queries etc if doing hardware upgrade is the number of disks key to performance update have now upgraded my database server with four intel ssds instead of the old 15k sas disks im using the same raid controller things has improved quite lot as you can see from the following the peak performance has improved around times and thats great however was expecting something more like times improvement according to the answers and the capabilities of the new ssds so here goes another question new question is there something in my current configuration that is limiting the performance of my system where is the bottleneck my configurations etc postgresql main postgresql conf data directory var lib postgresql main hba file etc postgresql main pg hba conf ident file etc postgresql main pg ident conf external pid file var run postgresql main pid listen addresses localhost port unix socket directory var run postgresql wal level hot standby synchronous commit on checkpoint timeout 10min archive mode on archive command rsync postgres var lib postgresql wals dev null max wal senders wal keep segments hot standby on log line prefix datestyle iso mdy lc messages en us utf lc monetary en us utf lc numeric en us utf lc time en us utf default text search config pg catalog english default statistics target maintenance work mem 1920mb checkpoint completion target effective cache size 22gb work mem 160mb wal buffers 16mb checkpoint segments shared buffers 7680mb max connections etc sysctl conf sysctl config net ipv4 ip forward net ipv4 conf all rp filter net ipv4 icmp echo ignore broadcasts ipv6 settings no autoconfiguration net ipv6 conf default autoconf net ipv6 conf default accept dad net ipv6 conf default accept ra net ipv6 conf default accept ra defrtr net ipv6 conf default accept ra rtr pref net ipv6 conf default accept ra pinfo net ipv6 conf default accept source route net ipv6 conf default accept redirects net ipv6 conf default forwarding net ipv6 conf all autoconf net ipv6 conf all accept dad net ipv6 conf all accept ra net ipv6 conf all accept ra defrtr net ipv6 conf all accept ra rtr pref net ipv6 conf all accept ra pinfo net ipv6 conf all accept source route net ipv6 conf all accept redirects net ipv6 conf all forwarding updated according to postgresql tuning vm dirty ratio vm dirty background ratio vm swappiness vm overcommit memory kernel sched autogroup enabled kernel sched migration cost etc sysctl postgresql shm conf shared memory settings for postgresql note that if another program uses shared memory as well you will have to coordinate the size settings between the two maximum size of shared memory segment in bytes kernel shmmax maximum total size of shared memory in pages normally bytes kernel shmall kernel shmmax kernel shmall updated according to postgresql tuning output of megacli64 ldinfo lall aall adapter virtual drive information virtual drive target id name raid level primary secondary raid level qualifier size gb sector size is vd emulated no mirror data gb state optimal strip size kb number of drives per span span depth default cache policy writeback readahead direct write cache ok if bad bbu current cache policy writeback readahead direct write cache ok if bad bbu default access policy read write current access policy read write disk cache policy disks default encryption type none is vd cached no
50016 in table many rows have been deleted how to get the id of missing rows for next insert for example id col1 how can get the value of first available id for next insert here want to get the id something like select id from table where clause pointing to the least available value or id after select insert into table id values
50076 am trying to make an sql server failover cluster have two db machines understand that both machines need nics each in my organization the ip scheme assigned is something like so want to know if assign ip to each nic will that be enough like assigning and or two of the nics in each machine have to have some private network scheme which will allow them to communicate directly
50127 there are two different options in modern sql server for page verify being torn page detection and checksum none is also of course an option believe checksum was introduced in sql server and that upgrading or restoring db from prior version would maintain its previous page verify method there was no implicit upgrade the problem involved is that we have production database that went into production using sql server and has since moved to sql server r2 server page verify is set to none when had been expecting it to be torn page detection going back this amount of time we seem to think the db was originally developed in sql server then migrated to sql server and this may explain the observed result was wondering when torn page detection and checksum became feature of sql server and how they behaved when migrated or upgraded to newer versions edit summing up some of the answers there is little discrpenacy over some of the dates for when torn page detection came into sql server link http support microsoft com kb link http technet microsoft com en us library aa337525v sql aspx the first link indicates sql and the second sql2000 tend to put my faith in the sql7 suggestion and that link two was confused over it being off by default in sql7 and on by default in sql2000
50231 postgresql has the cluster command to group rows physically on disk by grouping on information where neighboring rows for lack of better term are often accessed together performance improves since fewer disk blocks need to be read in given query does oracle have anything similar would it even help performance on large table that is almost never updated if there is such an option
50355 im helping friend with setting up encryption of data on sql server r2 standard edition upon original research thought could use tde but did not realize that it was only available for enterprise or datacenter versions of sql servers upon further research into sql server r2 features saw that it does allow for data encryption and key management but im not sure what it means or how to implement it what is the most efficient and low cost method for data encryption should do something through sql server or just use third party tools to encrypt the whole volume where db and backups are also if someone can point to way to use data encryption that comes as feature in standard edition would really appreciate every time search for encryption on sql server keep ending up on how to use tde and in current scenario it is not feasible for the size of business to purchase enterprise edition reason for encryption hipaa compliance if access is gain to file system for database or backups data is encrypted and can not be of any use this could include physical access to the machine or access through local admin
50391 in sql as far as know the logical query processing order which is the conceptual interpretation order starts with from in the following way from where group by having select order by following this list its easy to see why you cant have select aliases in where clause because the alias hasnt been created yet sql sql server follows this strictly and you cant use select aliases until youve passed select but in mysql its possible to use select aliases in the having clause even though it should logically be processed before the select clause how can this be possible to give an example select yearorderdate count as amount from sales orders group by yearorderdate having amount the statement is invalid in sql because having is referring to the select alias amount msg level state line invalid column name amount but works just fine in mysql based upon this im wondering is mysql taking shortcut in the sql rules to help the user maybe using some kind of pre analysis or is mysql using different conceptual interpretation order than the one though all rdbms were following
50652 is it an acceptable practice to use single sequence as primary key across all tables instead of primary key being unique for given table it is unique for all tables if so is it objectively better than using single primary key sequence across tables im junior software developer not dba so am still learning many of the basics of good database design edit in case anyone is wondering recently read critique of database design by one of our companys dbas who mentioned it was problem that the design didnt use single primary key across the entire database which sounded different than what ive learned so far edit2 to answer question in the comments this is for oracle 11g but was wondering on non database specific level if this question does depend upon the database would be interested to know why but in such case would be looking for an answer specific to oracle
50664 ive recently discovered that our production web servers that run off mysql are not being backed up regularly or at all im used to backing up sql server dbs but dont have ton of experience with mysql dbs any best practices for using mysqldump or any other db backup tools ill probably cron job the schedule so that its done nightly and then backup the files with my backup system thanks
50708 im fan of surrogate keys there is risk my findings are confirmation biased many questions ive seen both here and at http stackoverflow com use natural keys instead of surrogate keys based on identity values my background in computer systems tells me performing any comparative operation on an integer will be faster than comparing strings this comment made me question my beliefs so thought would create system to investigate my thesis that integers are faster than strings for use as keys in sql server since there is likely to be very little discernible difference in small datasets immediately thought of two table setup where the primary table has rows and the secondary table has rows for each row in the primary table for total of rows in the secondary table the premise of my test is to create two sets of tables like this one using natural keys and one using integer keys and run timing tests on simple query like select from table1 inner join table2 on table1 key table2 key the following is the code created as test bed use master if select countdatabase id from sys databases where name naturalkeytest begin alter database naturalkeytest set single user with rollback immediate drop database naturalkeytest end go create database naturalkeytest on name naturalkeytest filename sqlserver data naturalkeytest mdf size 8gb filegrowth 1gb log on name naturalkeytestlog filename sqlserver logs naturalkeytest mdf size 256mb filegrowth 128mb go alter database naturalkeytest set recovery simple go use naturalkeytest go create view getrand as select rand as randomnumber go create function randomstring stringlength int returns nvarcharmax as begin declare cnt int declare str nvarcharmax declare randomnum float while cnt stringlength begin select randomnum randomnumber from getrand set str str castchar randomnum as nvarcharmax set cnt cnt end return str end go create table naturaltable1 naturaltable1key nvarchar255 not null constraint pk naturaltable1 primary key clustered table1testdata nvarchar255 not null create table naturaltable2 naturaltable2key nvarchar255 not null constraint pk naturaltable2 primary key clustered naturaltable1key nvarchar255 not null constraint fk naturaltable2 naturaltable1key foreign key references dbo naturaltable1 naturaltable1key on delete cascade on update cascade table2testdata nvarchar255 not null go insert rows into naturaltable1 insert into naturaltable1 naturaltable1key table1testdata values dbo randomstring25 dbo randomstring100 go insert rows into naturaltable2 insert into naturaltable2 naturaltable2key naturaltable1key table2testdata select dbo randomstring25 t1 naturaltable1key dbo randomstring100 from naturaltable1 t1 go create table idtable1 idtable1key int not null constraint pk idtable1 primary key clustered identity11 table1testdata nvarchar255 not null constraint df idtable1 testdata default dbo randomstring100 create table idtable2 idtable2key int not null constraint pk idtable2 primary key clustered identity11 idtable1key int not null constraint fk idtable2 idtable1key foreign key references dbo idtable1 idtable1key on delete cascade on update cascade table2testdata nvarchar255 not null constraint df idtable2 testdata default dbo randomstring100 go insert into idtable1 default values go insert into idtable2 idtable1key select t1 idtable1key from idtable1 t1 go the code above creates database and tables and fills the tables with data ready to test the test code ran is use naturalkeytest go declare loops int declare maxloops int declare results table finishedat datetime default getdate keytype nvarchar255 elapsedtime float while loops maxloops begin dbcc freeproccache dbcc freesessioncache dbcc freesystemcache all dbcc dropcleanbuffers waitfor delay declare start datetime getdate declare end datetime declare count int select count count from dbo naturaltable1 t1 inner join dbo naturaltable2 t2 on t1 naturaltable1key t2 naturaltable1key set end getdate insert into results keytype elapsedtime select natural pk as keytype cast end start as float as elapsedtime dbcc freeproccache dbcc freesessioncache dbcc freesystemcache all dbcc dropcleanbuffers waitfor delay set start getdate select count count from dbo idtable1 t1 inner join dbo idtable2 t2 on t1 idtable1key t2 idtable1key set end getdate insert into results keytype elapsedtime select identity pk as keytype cast end start as float as elapsedtime set loops loops end select keytype formatcastavgelapsedtime as datetime hh mm ss fff as avgtime from results group by keytype these are the results am doing something wrong here or are int keys times faster than character natural keys note ive written follow up question here
50727 have two tables the first table contains all articles blog posts within cms some of these articles may also appear in magazine in which case they have foreign key relationship with another table that contains magazine specific information here is simplified version of the create table syntax for these two tables with some non essential rows stripped out create table base article id int11 not null auto increment date published datetime default null title varchar255 not null description text content longtext is published int11 not null default primary key id key base article date published date published key base article is published is published engine innodb default charset latin1 create table mag article basearticle ptr id int11 not null issue slug varchar8 default null rubric varchar75 default null primary key basearticle ptr id key mag article issue slug issue slug constraint basearticle ptr id refs id foreign key basearticle ptr id references base article id engine innodb default charset latin1 the cms contains around articles total and have written simple python script that can be used to populate test database with sample data if they want to replicate this issue locally if select from one of these tables mysql has no problem picking an appropriate index or retrieving articles quickly however when the two tables are joined together in simple query such as select from base article inner join mag article on mag article basearticle ptr id base article id where is published order by base article date published desc limit mysql fails to pick an appropriate query and performance plummets here is the relevant explain extended the execution time for which is over second id select type table type possible keys key key len ref rows filtered extra simple mag article all primary null null null using temporary using filesort simple base article eq ref primarybase article is published primary my test mag article basearticle ptr id using where edit sept can remove the where clause from this query but the explain still looks the same and the query is still slow one potential solution is to force an index running the same query with force index base articel date published results in query that executes in around milliseconds id select type table type possible keys key key len ref rows filtered extra simple base article index null base article date published null using where simple mag article eq ref primary primary my test base article id would prefer not to have to force an index on this query if can avoid it for several reasons most notably this basic query can be filtered modified in variety of ways such as filtering by the issue slug after which base article date published may no longer be the best index to use can anyone suggest strategy for improving performance for this query
50741 from what ive read typically for indexing the recommendation is to leave the fill factor at and the logic behind this seems that new data may be inserted into table between existing values thus fill factor shouldnt be set as in hypothetical example of usernames new names may be added to the name field so fill factor of could create problem because even though the pages are full the new data wont be organized appropriately with the other data my question involves table where data are organized by date clustered index and id non clustered index each order has date and an id thus no new orders can occur between existing orders and no new ids can appear between existing ids these happen sequentially for table like this would it be appropriate from performance perspective to set the fill factor to and reindex at meaning are there performance advantages to having the pages full since no new data will appear between existing data
50797 have started sql server in single user mode like this sqlservr when try to connect to it with sqlcmd get the following error msg level state server servername line login failed to user user name reason server is in single user mode only one administrator can connect at this time the user am logged onto the server as is domain administrator before tried this the user am logged on as had nothing to do with this sql server am in fact trying to create new sa as we dont know what the current password is edit it seems that even if do sqlservr sqlcmd am still getting the above error am not sure if there are other sqlcmd calls happening from elsewhere
50826 am making node sql server failover cluster do need to install the msdtc component if yes can both be installed on single shared disk
50837 when execute the two queries below select session id transaction id from sys dm tran session transactions and select session id request id at transaction id from sys dm tran active transactions at join sys dm exec requests on transaction id at transaction id ive read the bol for both and but dont see any clear explanation as to why the difference would occur get different results the former query returns no results but the latter returns active transactions with session and transaction ids the request id is which think means that its the only request made by the session could someone help me understand why there is difference between the two concepts ive queried above edit just reran the queries and now get result for the first dmv which has session id that is not actually contained in the second result set
50867 am attempting to install oracle database 11g on windows 64x laptop get all the way to step of and get fail on environment variable path exceeds recommended length have created odb directory and have set oracle home odb have shortened it as much as possible what is the correct way to install oracle database 11g on windows
50906 have successfully installed postgresql from the apt repository on vms running ubuntu and however cannot get it to install properly on my host machine running ubuntu the install this time seems to have gone ok but perhaps there is an error im not understanding no postgresql clusters exist see man pg createcluster setting up postgresql pgdg12 creating new cluster main config etc postgresql main data var lib postgresql main locale en us utf port update alternatives using usr share postgresql man man1 postmaster gz to provide usr share man man1 postmaster gz postmaster gz in auto mode so then try to add myself as postgresql user but get this createuser could not connect to database postgres could not connect to server no such file or directory is the server running locally and accepting connections on unix domain socket var run postgresql pgsql cannot see postgresql running in system monitor and there is no file in the var run postgresql folder completely empty edit on the vms there is file in var run postgresql called main pid there is nothing on the host machine log file located var log postgresql so whats going on here that isnt going on in my vms like said the other installations on the vms including postgis and pgadmin came in perfect no idea why this host machine isnt going through
50951 have table with composite primary key consisting of columns which is used to ensure no duplicates are entered into the table am now in need of new table which will need to reference the keys in this table as foreign keys my question is which approach is more efficient for lookup speeds do create the new table including all columns and reference them all in foreign key or do create new identity column in the primary key table and use this as foreign key in the new table this database is expected to hold very large amount of data so have built it up until now with view to minimising the amount of data held in each table with this in mind option would be the best approach since will save int columns and datetime column for every row but want to avoid increasing the lookup time if unnecessary
51155 want to create new database with 200gb data file mdf and 50gb log file ldf but it is very slow its about minutes and still hasnt created and it is very time consuming is that normal if yes what does it do that takes time can enhance its creating speed am using sql server r2 windows server 2008r2 16gb ram which limited that to 12gb in ssms and core i7 cpu
51215 sql management studio allows to create scripts for all db objects however so far couldnt find way to correctly script schema or user the permissions of user on schema are not included in the script that is created did make something wrong or is msft her bit sloppy
51219 im running sql server r2 sp1 on windows server box have net script running from visual studio that does the following reaches into the database makes change iterates the total number of times it will iterate is however it is stopping at connections and cant figure out why could adjust my script to just use single thread but id prefer to know where im missing max connection setting as that will be more useful to know for future reference heres where ive checked so far sql connection string in visual studio its set to ssms database instance connection properties its set to infinity user connections googled some information on server it looks like it can handle more than connections stepped through my code alongside sp who2 which gives more information on logical connections seeing that the of connections starts at and the script errors with the max pooled connections reached error at logical connections changed the connection string to use data source perfsql02 initial catalog masked integrated security true max pool size im not sure where else to check know have lot of moving parts here but im getting the feeling im just missing max pool setting somewhere
51340 this question is take off from the excellent one posed here cast to date is sargable but is it good idea in my case am not concerned with the where clause but in joining to an events table which has column of type date one table has datetime2 and the other has date so can effectively join using cast as date or can use traditional range query date and date my question is which is preferable the datetime values will almost never match the predicate date value expect to stay on the order of 2m rows having the datetime and under 5k having the date if this consideration makes difference should expect the same behavior on the join as might using the where clause which should prefer to retain performance with scaling does the answer change with mssql my generalized use case is to treat my events table like calendar table select events columns some aggregationstasks column from events left outer join tasks this appropriately states my intent clearer on casttasks datetimecolumn as date events datecolumn but is this more effective scalable on tasks datetimecolumn events datecolumn and tasks datetimecolumn dateaddday1events datecolumn group by events columns
51349 have the following table create table k1 date k2 int references sid on partitionschemek1 the table will be partitioned by k1 k1 has low selective the data will be appended in the order by k1 which of the following primary keys the column order is different is preferred alter table add primary key clustered k1 k2 alter table add primary key clustered k2 k1 or should the pk be non clustered and create another clustered index lot of the queries look like select from join on id k2 where
51440 given the following components declare date declare time7 what is the best way of combining them to produce datetime27 result with value some things which dont work are listed below select operand data type date is invalid for add operator select cast as datetime27 operand data type datetime2 is invalid for add operator select dateaddnanoseconddatediffnanosecondcast00 as time the datediff function resulted in an overflow the number of dateparts separating two date time instances is too large try to use datediff with less precise datepart the overflow can be avoided in azure sql database and sql server using datediff big select cast as datetime the data types datetime and time are incompatible in the add operator select cast as datetime cast as datetime returns result but loses precision
51524 is there an economic way to license sql server standard edition on test server given the following environment live sql server standard edition test sql server developer edition dev sql server developer edition the problem with this scenario is that developer could code using feature that works in developer edition but is not available when deployed to the live server the client would be the first to discover the problem thoughts host test database on live server doesnt give the live test isolation would like uses live server resource buffers cpu etc could disconnect test database when not in use to minimise this run second database instance on the live server still poor live test isolation more processes using server ram cpu
51945 why does not in performs index scan instead of index seek heres my current index on the table create nonclustered index idx dmcasarms courselist cdesc on dbo courselist desc asc code asc sql statement select code coursecode from courselist where desc bachelor of science in information technology go select code coursecode from courselist where desc not in pre school basic education science high school go and execution plan is there anyway can optimize this
51970 im working on oracle 11g database and need to list all index organized tables is there dbas view to query for that thanks
52005 checked sys databases to find out which databases need log backups it surprised me that had quite few where log reuse wait desc log backup meaning the job have set up to run log backups on these databases isnt actually creating log backup files after doing some searching it had been suggested to change database modes from full to simple then back to full to reset things after doing this am still unable to get log backup to successfully run am using sql server and use the ola hallengren method of backup found here http ola hallengren com sql server backup html thank you
52129 have persisted computed column on table which is simply made up concatenated columns create table dbo id int identity1 not null constraint pk id primary key varchar20 not null varchar20 not null varchar20 not null date null varchar20 null comp as persisted not null in this comp is not unique and is the valid from date of each combination of therefore use the following query to get the end date for each basically the next start date for the same value of comp select t1 id t1 comp t1 d2 select top t2 from dbo t2 where t2 comp t1 comp and t2 t1 order by t2 from dbo t1 where t1 is not null dont care about inactive records order by t1 comp then added an index to the computed column to assist in this query and also others create nonclustered index ix comp on dbo comp where is not null the query plan however surprised me would have thought that since have where clause stating that is not null and am sorting by comp and not referencing any column outside of the index that the index on the computed column could be used to scan t1 and t2 but saw clustered index scan so forced the use of this index to see if it yielded better plan select t1 id t1 comp t1 d2 select top t2 from dbo t2 where t2 comp t1 comp and t2 t1 order by t2 from dbo t1 with index ix comp where t1 is not null order by t1 comp which gave this plan this shows that key lookup is being used the details of which are now according to the sql server documentation you can create an index on computed column that is defined with deterministic but imprecise expression if the column is marked persisted in the create table or alter table statement this means that the database engine stores the computed values in the table and updates them when any other columns on which the computed column depends are updated the database engine uses these persisted values when it creates an index on the column and when the index is referenced in query this option enables you to create an index on computed column when database engine cannot prove with accuracy whether function that returns computed column expressions particularly clr function that is created in the net framework is both deterministic and precise so if as the docs say the database engine stores the computed values in the table and the value is also being stored in my index why is key lookup required to get and when they are not referenced in the query at all assume they are being used to calculate comp but why also why can the query use the index on t2 but not on t1 queries and ddl on sql fiddle have tagged sql server because this is the version that my main problem is on but also get the same behaviour in
52244 assuming table entity eid is auto incrementing want to be able to reference the autoincrement value assigned later in the same transaction the way have been doing this is by doing multiple transactions which think is not optimal start transaction insert into entity insert into t2 eid values new eid ref here commit
52314 am programmer dealing with big table which the following scheme updatetime pk datetime notnull name pk char14 notnull thedata float there is clustered index on name updatetime was wondering what should be faster select maxupdatetime from mytable or select max updatetime as value from select updatetime from mytable group by updatetime as the inserts to this table are in chunks of rows with the same date so thought grouping by might ease the max calculation instead of trying to find max of rows grouping by to rows and then calculation of max would be faster is my assumption correct or group by is also costly
52317 have table in sql server express with lot of unused space need to free up space in the database name rows reserved data index size unused mytablename kb kb kb kb how do get sql to release the 3165104kb ive already tried alter table mytablename rebuild dbcc cleantable mydbname mytablename alter index all on mytablename reorganize alter index pk image on mytablename rebuild with online off here is the table create table dbo mytablename imageid int identity11 not null datescan datetime null scanimage image null constraint pk image primary key clustered imageid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor on primary on primary textimage on primary go the only thing we have done is replaced scanimage on every row with much smaller image this is how so much unused space is there
52358 what are the benefits if any of dropping temporary table in sql server eg drop table temp since temporary tables are automatically dropped by the server this statement would seem to be non functional
52492 have simple mysql table which use as server booking system user can log in to server if he has valid booking for that server sample data might look like this server user start end server01 alicebob server02 carlos server03 danerinfrank to check whether user carlos has booking for server02 use the following sql query select id from booking where user carlos and start now and end now and server server02 but this query does not work if there are multiple comma separated users how can check if alice has booking for server01
52517 we have gb table in postgres with rows in it we are adding uuid guid column to it and am wondering what the best way to populate that column is as we want to add not null constraint to it if understand postgres correctly an update is technically delete and insert so this is basically rebuilding the entire gb table also we have slave running so we dont want that to lag behind is there any way better than writing script that slowly populates it over time
52632 am just confuse about the sharding and replication that how they works according to definition replication replica set in mongodb is group of mongod processes that maintain the same data set sharding sharding is method for storing data across multiple machines as per my understanding if there is data of gb then by replication servers it will store 75gb data on each servers means 75gb on server 75gb on server and 75gb on server correct me if am wrong and by sharding it will be stored as 25gb data on server 25gb data on server and 25gb data on server right but then encountered this line in the tutorial shards store the data to provide high availability and data consistency in production sharded cluster each shard is replica set as replica set is of 75gb but shard is of 25gb then how they can be equivalent this makes me confuse lot think am missing something great in this please help me in this
52641 can use following statement to unlock an account alter user username account unlock but which statement can use to verify that account is currently locked out
52698 have group of about tables and want to know the physical size on disk of all of these tables plus indexes is there an easier way of doing this than through the gui in sql server r2
52766 what is the recommended way to perform minor upgrade from postgresql to using the enterprise db built windows installer should uninstall first or just install over the existing installation the current installation was performed with postgresql windows x64 exe now want to upgrade using postgresql windows x64 exe
52826 am developing user defined function that takes two arguments create or replace function gesio events table in regclass events table out regclass returns void as events table in and events table out have exactly the same schema simply explained loop through the records of events table in manipulate the records and want to append insert the manipulated records into events table out in the following fashion open reccurs for execute formatselect from order by session id event time event table in loop fetch reccurs into rec if not found then exit end if do something with rec insert the rec into events table out end loop how can save the rec into events table out
52828 have sql account with the following permissions on database the db executor role you see this account being member of was created by this script create role db executor authorization dbo go grant execute to db executor go when run select update insert or delete on the table it works fine when try to truncate the table it gives me this error message cannot find the object tablename because it does not exist or you do not have permissions what permission is this account missing
52845 have typical star schema simulated here and am mentioning two queries first query simply joins the fact table with dimension tables and calendar table and the second query joins and aggregates have experimented and have created indexes by studying the execution plan and some by reading the suggested indexes and all of them have improved performance by some extent my question is what can further be done in this case what indexes can be applied or how can the query be modified to gain better performance and to reduce execution time so first the query to create and fill the tables and to create indexes create table facttable id bigint identity primary key fkdim1 bigint not null fkdim2 bigint dateref datetime fact1 money fact2 money create table dim1table id bigint identity primary key dim1name nvarchar20 dim1val1 money dim1val2 money create table dim2table id bigint identity primary key dim2name nvarchar20 dim2val1 money dim2val2 money create table calendartable id bigint identity primary key date datetime unique nonclustered weekday nvarchar10 month nvarchar10 alter table facttable add constraint fk dim1 foreign key fkdim1 references dim1tableid alter table facttable add constraint fk dim2 foreign key fkdim2 references dim1tableid alter table facttable add constraint fk calendar foreign key dateref references calendartable date declare counter int set counter while counter begin insert into dim1tabledim1namedim1val1dim1val2valuesdim1 cast counter as nvarcharrand 10000rand insert into dim2tabledim2namedim2val1dim2val2valuesdim2 cast counter as nvarcharrand 10000rand set counter counter end declare startdate datetime declare enddate datetime set startdate cast1 as datetime set enddate dateaddd startdate while startdate enddate begin insert into calendartable date weekday month select startdate datenamedw startdate datenamemonth startdate set startdate dateadddd startdate end set counter while counter begin insert into facttable fkdim1fkdim2datereffact1fact2values counter counter dateadddd counter cast1 as datetime rand rand set counter counter end code to create indexes create nonclustered index dim1tableindex1 on dbo dim1table dim1name ascinclude id dim1val1 dim1val2 create nonclustered index dim1tableindex2 on dbo dim2table dim2name ascinclude id dim2val1 dim2val2 create nonclustered index facttableindex1 on dbo facttablefkdim1 ascincludefkdim2 dateref fact1 fact2 create nonclustered index facttableindex2 on dbo facttablefkdim2 ascincludefkdim1 dateref fact1 fact2 create unique nonclustered index calnedarindex1 on dbo calendartable date ascinclude id weekday month query simple join of fact table with calendar and dimension tables and where clause select d1 dim1name d2 dim2name date weekday month d1 dim1val1 d2 dim2val2 fact1 fact2 from facttable join dim1table d1 on d1 id fkdim1 join dim2table d2 on d2 id fkdim2 join calendartable on dateref date execution details with indexes turned off all mentioned above rows affected table calendartable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim2table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim1table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table facttable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms and execution plan with indexes enabled rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table facttable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim1table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table calendartable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim2table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms and execution plan second query which aggregates after join select d1 dim1name month sumd1 dim1val1 sumdim1val1 sumd2 dim2val2 sumdim2val2 sumf fact1 sumfact1 avgf fact2 fact2avg from facttable join dim1table d1 on d1 id fkdim1 join dim2table d2 on d2 id fkdim2 join calendartable on dateref date group by d1 dim1name month performance with all indexes turned off rows affected table dim1table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table calendartable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim2table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table facttable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms and execution plan and with indexes enabled rows affected table dim1table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table calendartable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table dim2table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table facttable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms and finally the execution plan the improvements got are not very significant but when consider large number of rows for example remove the where clause from query then the indexes reduce the execution time from around seconds to seconds will restate my questions here how can the indexes be redesigned or new indexes added to enhance performance how can the performance be enhanced by redesigning the queries what can be done other than indexes and redesigning the queries have presented simple examples but have tried to cover some typical scenarios and type of queries in star schema to the concept behind the answers of these specific questions will apply generally as well and using sql server
52929 when connect to my local development database get an error saying the password has expired ive been trying to change it with sqlplus to no avail sqlplus tpmdbo password localhost global sql plus release production on wed nov copyright oracle all rights reserved error ora the password has expired changing password for tpmdbo new password retype new password error ora invalid username password logon denied password unchanged get this error no matter what new password type in my main question is how can reset this password however im also curious if theres way to make the password never expire this is local dev database really dont care about the security or anything and none of the data on it is important
52968 the following documentation describes how to see the refcursor returned from function here like this create function reffuncrefcursor returns refcursor as begin open for select col from test return end language plpgsql begin select reffuncfunccursor fetch all in funccursor commit this works for me however if want to keep the results on my screen have to keep the transaction open when execute commit my result set is discarded when execute both fetch and commit at the same time the first result set is discarded is there way to commit the transaction but keep the result set the version of pgadmin is
53085 is there way to generate create script from an existing table purely in sql that is without using smo since sql does not have access to smo lets say stored procedure that receives table name and returns string that contains the create script for the given table now let me describe the situation im facing as there may be different way to approach this have an instance with several dozen databases these database all have the same schema all the same tables index and so on they were created as part of third party software installation need to have way to work with them so that can aggregate data from them in ad hoc manner nice people at dba se have already helped me here how to create trigger in different database currently need to find way to make select from table across all the databases have recorded all the database names into table called databasees and wrote the following script to execute select statement on all of them if object idtempdb tmp is not null drop table tmp select into tmp from database1 dbo table1 where declare statement nvarcharmax ninsert into tmp select from table1 where column1 and cloumn2 declare lastdatabaseid int set lastdatabaseid declare databasenametohandle varchar60 declare databaseidtohandle int select top databasenametohandle name databaseidtohandle database ref no from databasees where database ref no lastdatabaseid order by database ref no while databaseidtohandle is not null begin declare sql nvarcharmax quotename databasenametohandle dbo sp executesql exec sql statement set lastdatabaseid databaseidtohandle set databaseidtohandle null select top databasenametohandle name databaseidtohandle database ref no from databasees where database ref no lastdatabaseid order by database ref no end select from tmp drop table tmp however the script above fails with the following message an explicit value for the identity column in table tmp can only be specified when column list is used and identity insert is on adding this set identity insert tmp on does not help since cant specify the column list and keep it generic in sql there is no way to switch the identity on given table off you can only drop column and add column which obviously changes the column order and if the column order changes you again need to specify the column list that would be different depending on the table you query so was thinking if could get the create table scrip in my sql code could manipulate it with string manipulation expressions to remove the identity column and also add column for the database name to the result set can anyone think of relatively easy way to achieve what want
53105 ive faced problem when my net web application is running slow even when ive optimized all queries as it turned out another developer who is making reports runs bunch of non optimized queries all the time so ive decided to create separate server for him currently created jobs making backup copying it to another server and restoring it but its temporary solution so in order for second server have up to date data or at least with minimal latency im investigating an opportunity to create read only replica as im not dba ive read bunch or articles regarding sql server replication mechanism the best option for me is when my prod web application isnt affected by replication at all mean lots of lock on syncronizing tables dont need real time sync as mirroring does it also dont need any king of cluster solution just read only syncing copy of my prod database so ive choosed transactional replication mechanismnot updatable with asynchronous distributing policyon schedule so ive some question does transactional replication suit my problem the most among sql server replication mechanisms if have an opportunit to migrate from sql server to sql server would transactional replication have some breaking changes ive read an article on technet that there might occur errors isnt sql server always on mechanism better to solve my problemim considering this option as last one as im still using r2 and yet migration to is planned by not soon
53151 am trying to get some reporting done for employee time records we have two tables specifically for this question employees are listed in the members table and each day they enter time entries of work theyve performed and is stored in the time entry table example setup with sql fiddle http sqlfiddle com e3806 the end result im going for is table which shows all the members in column list and then will show their sum hours for the date queried in the other columns the problem seems to be that if there is no row in the time entry table for particular member there is now row for that member ive tried several different join types left right inner outer full outer etc but none seem to give me what want which would be based on the last example in sql fiddle desired end result member id counttime entry timeentrydate sumhours actual sumhours bill adavis btronton cjones dsmith egirsch frowden what im currently getting when query for specific date of member id counttime entry timeentrydate sumhours actual sumhours bill egirsch which is correct based on the one time entry row that is dated for egirsch but need to see zeros for the other members in order to get reports and eventually web dashboard report for this information this is my first question and while searched for join queries etc im honestly not sure what this function might be called so hope that this isnt duplicate and will help others too trying to find solution to similar problems
53201 we are running site moodle that the users currently find slow think have tracked down the problem to mysql creating temporary tables on disk watch the variable created tmp disk tables in mysql workbench server administration and the number increases with roughly tables after days usage created tmp disk tablesis 100k also the memory does not seem to be released the usage keeps increasing until the system becomes pretty much unusable and we have to re start mysql need to re start it almost every day and it begins with using about of available memory and finishing the day with have no blobs in the database and no control over the queries either so cant attempt to optimise them have also used the percona confirguration wizard to generate configuration file but that my ini didnt solve my problem either questions what should change to stop mysql from creating temporary tables on disk are there settings need to change should throw more memory at it how can stop mysql from eating up my memory edit enabled slow queries log and discovered that the query select get lock was logged as slow quick search revealed that had allowed persistent connections in the php configuration mysqli allow persistent on turned this off this reduced the rate at which mysql consumes memory it is still creating temporary tables though also checked that the key buffer size is large enough looked at the variable key writes this should be zero if not increase the key buffer size have zero key reads and zero key writes so assume that the key buffer size is large enough increased the tmp table size and max heap table size to 1024m as an increase in created tmp disk tables may indicate that the tables cant fit in memory this didnt solve it ref http www mysqlperformanceblog com how much overhead is caused by on disk temporary tables edit if you see many sort merge passes per second in show global status output you can consider increasing the sort buffer size value had sort merge passes in an hour so consider the sort buffer size to be large enough ref mysql manual on sort buffer size edit have modified the sort and join buffers as suggested by rolandomysqldba the result is displayed in the table below but think the created tmp tables on disk is still high restarted the mysql server after changed the value and checked the created tmp tables on disk after day 8h and calculated the average any other suggestions it seems to me that there is something that doesnt fit inside some kind of container but cant work out what it is tmp table size sort buffer join buffer no of created max heap table size tmp tables on disk 125m 256k 256k 100k 125m 512k 512k 100k 125m 1m 1m 100k 125m 4m 4m 100k this is my configuration database server web server windows server r2 windows server r2 mysql iis core cpu core cpu 4gb ram 8gb ram additional information param value num of tables in db size of database 5g database engine innodb read write ratio innodb data read innodb data written avg table size 15k rows max table size 744k rows this setup was given to me so have limited control over it the web server is using very little cpu and ram so have excluded that machine as bottleneck majority of the mysql settings originates from config auto generation tool have monitored the system using perfmon over few representative days from that conclude that it is not the os that is swapping to disk my ini client port mysql default character set utf8 mysqld port basedir program files mysql mysql server datadir dbs data default character set utf8 default storage engine innodb sql mode strict trans tablesno auto create userno engine substitution max connections query cache size 350m table cache tmp table size 125m table definition cache max heap table size 32m thread cache size myisam specific options myisam max sort file size 100g myisam sort buffer size 125m key buffer size 55m read buffer size 1024k read rnd buffer size 256k sort buffer size 1024k join buffer size 1024k innodb specific options innodb data home dir dbs innodb additional mem pool size 32m innodb flush log at trx commit innodb log buffer size 16m innodb buffer pool size 2g innodb log file size 407m innodb thread concurrency
53462 google search spewed forth millions of hits on how to find tables without clustered indexed the pk normally being the clustered index of table however table could easily have natural key as clustered index and non clustered surrogate index like an identity column how do find all tables in db without primary key defined have tables in this db manual inspection is grossly inefficient
53726 want to understand why there would be such huge difference in execution of the same query on uat runs in sec vs prod run in secs both uat and prod are having exactly data and indexes query set statistics io on set statistics time on select conf no de duplicate email address rtrimemail address in maintenance conf target no from conf target ct where conf no and leftinternet user id iconf and registration type and select count1 from portfolio where email address ct email address and deactivated yn or registration type and select count1 from capital market where email address ct email address and deactivated yn on uat sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table portfolio scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table capital market scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table conf target scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms on prod sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms rows affected table portfolio scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table capital market scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table conf target scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms note that on prod the query suggests missing index and that is beneficial as have tested but that is not the point of discussion just want to understand that on uat why does sql server create worker table and on prod it does not it creates table spool on uat and not on prod also why are the execution times so different on uat vs prod note am running sql server r2 rtm on both servers pretty soon going to patch with latest sp uat max memory 8gb maxdop processor affinity and max worker threads is logical to physical processor map physical processor physical processor physical processor physical processor physical processor physical processor physical processor physical processor logical processor to socket map socket socket logical processor to numa node map numa node prod max memory 60gb maxdop processor affinity and max worker threads is logical to physical processor map physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded physical processor hyperthreaded logical processor to socket map socket socket logical processor to numa node map numa node numa node update uat execution plan xml http pastebin com z0pwvw8m prod execution plan xml http pastebin com gwty16yy uat execution plan xml with plan generated fro prod http pastebin com 74u3ntr0 server configuration prod poweredge r720xd intelr xeonr cpu e5 v2 50ghz uat poweredge intelr xeonr cpu x5460 16ghz have posted at answers sqlperformance com update thanks to swasheck for suggestion changing the max memory on prod from 60gb to mb am able to generate the same plan in prod the query completes in same time as uat now need to understand why also by this wont be able to justify this monster server to replace the old server
53815 assuming you need to make sure your application that relies on sql server as its database backend is available around the clock even if one server machine fails as developer and not dba am struggling to understand when to use which scenario for my failover high availability two or more servers in windows failover cluster sql server as clustered instance two or more sql server instances that are kept up to date with transactional replication two or more sql servers in sql server availability group configured in synchronous commit mode which of each of those scenarios works for what kind of workload and what kind of failure outage can be handled by those scenarios are they even comparable exchangable
53995 am sysadmin on my sql server and need to be but would like to setup my security on the production server to prevent me from accidentally restoring to the production database restored databases to test machine often for developers to debug test and while am always really careful after about ten years today wasnt my thought was to put in deny on the restore permission and if ever need to really restore remove the deny but cant find anything like this does anyone know how to do this or maybe have better idea
54059 have the following index created on table in my database create index idx index1 on table1 col1 col2 col3 the server is suggesting the following missing index create index idx index2 on table1 col1 col2 include col3 col4 col5 col6 it seems logical to me to amend the existing index definition to include the suggested columns rather than creating new index that needs to be maintained query that selects on col1 and col2 could use index1 just as effectively as index2 am correct or am maybe missing something
54149 have table with columns when selecting data in sql plus the output wraps making it difficult to read what id rather like is either horizontal scroll bar to appear or somehow send the output to less run following statements in sqlplus set linesize set pagesize set long spool output txt select from big table then in bash run less output txt the output still appears wrapped and unreadable
54261 ive got this query id like to get working im trying to count all the instances of foreign key after certain date that isnt null and then left join that to the distinct of the original table so can see the variables with no entries heres my query select fulllist fk fc id from select distinct fk fc id from data item as fulllist left outer join select temp fk fc id count as number from select from data item where di item value not like null and di timestamp as temp group by fk fc id as lj on lj fc fk id fulllist fk fc id the error is error column lj fc fk id does not exist line on lj fc fk id fulllist fk fc id
54283 have column data that holds json document roughly like this name foo tags foo bar would like to turn the nested tags array into concatenated string foo bar that would be easily possible with the array to string function in theory however this function does not act on json arrays so wonder how to turn this json array into postgres array
54318 have table like this id propinsi kota aceh denpasar aceh banda aceh sumatera asahan this table has many rows the problem is want to replace the space before the text in column kota for all rows like this id propinsi kota aceh denpasar aceh banda aceh sumatera asahan searched google the function replace in mysql only affects one row select replacestring column search replace as kota can someone fix my problem
54377 in sql servers sys dm os memory cache entries it is possible to view both the original cost of an entry in the cache as well as the current cost of the cache entry original cost and current cost respectively the dmv sys dm os buffer descriptors contains record of the pages that are currently in memory as well as some metadata about the pages one interesting chunk of info not available in the dvm are the lru values for the data pages is it possible to get the lru values for data pages in buffer pool in sql server if so how
54403 we have table with million records here is the table structure create table metaplay track id int11 not null default user id int11 default null completed int11 default null skipped int11 default null created int11 default null updated int11 default null id int11 not null auto increment primary key id key created created key updated updated key skipped skipped key track id track id engine myisam default charset latin1 all these data are numeric at this point we have inserts and updates per minute we also pull out daily weekly and monthly track playing records from this table now my question is which of the followings will be better for architectural design and best performance would appreciate if you can highlight hardware details as well mysql in ssd cached storage with 4gb ram mysql in ssd with 4gb ram any nosql solution anything else also do you suggest any mysql specific tuning tips for table like this
54608 upgraded to mysql from and now my logs are littered with such messages on startup found possible solution here but it does not seem official http forums mysql com read php22578559579891 msg 7f87b1d26700 innodb error table mysql innodb table stats not found 7f87b1d26700 innodb recalculation of persistent statistics requested for table drupal sessions but the required persistent statistics storage is not present or is corrupted using transient stats instead 7f903c09c700 innodb error table mysql innodb table stats not found any official solutions or fixes
54893 for reporting purposes need to be able to query dependencies between views and underlying tables at column level via sys sql expression dependencies one way to get sql server to store referenced columns is to use schemabound views as some of my users will create views too would like to enforce those as far as ive seen it isnt possible to allow only views with schemabinding to be created within sql server database though although this seems legit requirement to me are there other ways to force sql server to keep track of which columns are being referenced by which views or is there hidden way to enforce schemabound views in sql server
54943 the msdn online article snapshot isolation in sql server states an isolation level has connection wide scope and once set for connection with the set transaction isolation level statement it remains in effect until the connection is closed or another isolation level is set when connection is closed and returned to the pool the isolation level from the last set transaction isolation level statement is retained subsequent connections reusing pooled connection use the isolation level that was in effect at the time the connection is pooled isnt it self contradictory paragraph until vs retained then if the isolation level from the last set transaction isolation level statement is retained after closing the connection and returning it to pool how it should be understood that the default isolation level will have arbitrary value different connections in the pool will have different isolation levels and its value will depend on the connection being re opened or all the default values on all connection in th pool will be changed to last one but again quite unknown before hand
54954 usually if conditions are not required in our query statements we dont use where clause but ive seen where clause being used in many places even where other conditions are not present why is this done are there specific benefits to the execution time does it enable other functionalities is using where similar to this
55019 in recent versions of postgresql as of dec can we share query between two or more cores to get performance boost or should we get faster cores
55198 ive got zoo of million animals which track on my sql server database about of them are black and about of them are swans wanted to get details of all the black swans and so not wanting to swamp the results page did select top from animal where colour like black and species like swan yes inadvisedly those fields are freetext but they are both indexed it turns out we have no such animals as the query returns an empty set in about milliseconds it would have been about twice as fast if id used rather than like but have premonition the latter is about to save me some typing it turns out the head zookeeper thinks he may have entered some of the swans as blackish so modify the query accordingly select top from animal where colour like black and species like swan turns out there are none of those either and in fact there are no black animals except black ones but the query now takes about seconds to return empty it seems its only the combination of top and like causing trouble though because select count from animal where colour like black and species like swan returns very quickly and even select from animal where colour like black and species like swan returns empty in fraction of second does any one have any idea why top and should conspire to cause such dramatic loss of performance especially in an empty result set edit just to clarify im not using any freetext indexes just meant that the fields are freetext at the point of entry not normalized in the database sorry for the confusion poor wording on my part
55327 have sql server job in the sql server agent jobs that need to move transfer to another updated sql server r2 instance was able to script it out and then use it as new query window in the upgraded dev server it seemsas far as know that the only changes that have to make to this script is to point it to the correct server file path and get it new schedule uid number but not sure how to assign that number this job was built for an ssis package as its pointing to in the file path thanks for any advice script code is use msdb go object job elfcopy script date begin transaction declare returncode int select returncode object jobcategory uncategorized local script date if not exists select name from msdb dbo syscategories where name uncategorized local and category class begin exec returncode msdb dbo sp add category class njob type nlocal name uncategorized local if error or returncode goto quitwithrollback end declare jobid binary16 exec returncode msdb dbo sp add job job name nelfcopy enabled notify level eventlog notify level email notify level netsend notify level page delete level description nthis job copies elf files folders from stage locations to active directories category name uncategorized local owner login name nbio wddocmanagement job id jobid output if error or returncode goto quitwithrollback object step runpackage script date exec returncode msdb dbo sp add jobstep job id jobid step name nrunpackage step id cmdexec success code on success action on success step id on fail action on fail step id retry attempts retry interval os run priority subsystem nssis command file ssis packages elfcopy dtsx connection brorporap1 wddocumentmanagement data source brorporap1 initial catalog wddocumentmanagement provider sqlncli integrated security sspi auto translate false connection rcsmtp smtpserver brutmurex3 bio ri redcross net usewindowsauthentication true enablessl false maxconcurrent checkpointing off reporting database name nmaster flags if error or returncode goto quitwithrollback exec returncode msdb dbo sp update job job id jobid start step id if error or returncode goto quitwithrollback exec returncode msdb dbo sp add jobschedule job id jobid name nweeklyschedule enabled freq type freq interval freq subday type freq subday interval freq relative interval freq recurrence factor active start date active end date active start time active end time schedule uid nc406cae9 49b7 40dd cef44160f562 if error or returncode goto quitwithrollback exec returncode msdb dbo sp add jobserver job id jobid server name nlocal if error or returncode goto quitwithrollback commit transaction goto endsave quitwithrollback if trancount rollback transaction endsave go
55363 need to put some random values into database but dont want to end up with completely randomized text like 7hfg43d3 instead would like to randomly pick one of values supplied by myself
55368 is there simple way of listing the size of every table in every database on mssql server have used query on sys tables to get results for single database but we have databases per server so way of getting the same results but for all databases would be great currently im having to create temporary list of databases from master sys databases and then iterate over that with cursor building query and inserting the results into temp table with exec sp executesql sqlstring
55383 have ssis package that is doing simple select statement and then loading into table have anonymized the sql statement which should be painfully obvious when you look at the table name have also simplified the package to just do row count for the purposes of this demo this issue ive having is that the query produces rows in sql server but produces no rows in ssis the query is as follows declare currentblahdate datetime set currentblahdate select maxblahdate from dbo thiscooltable sl with nolock inner join dbo thatcooltable slf with nolock on sl coolid slf coolid and slf typecode in valuea valueb select from dbo calendar where castbasedate as date cast currentblahdate as date please ignore the as that is just for the purposes of this demo the actual query has the same issue even though it does not have the when executed in sql server this returns row from the calendar table as expected however when executed against the same environment in ssis get no rows returned if turn the query into stored procedure and execute the sproc in ssis get row count of if alter the query to this select from dbo calendar where castbasedate as date cast select maxblahdate from dbo thiscooltable sl with nolock inner join dbo thatcooltable slf with nolock on sl coolid slf coolid and slf typecode in valuea valueb as date then once again receive row is there any reason im seeing this behavior update before receive ton of flack for this have no choice on the nolock nonsense
55498 folks am currently exploring the capabilities of mysql while doing simple exercises with diverse queries was reading the following article that explains how table join works http www codinghorror com blog visual explanation of sql joins html however am stuck at the full outer join example comprised of tablea and tableb tablea field type null key default extra id int11 no pri null auto increment name varchar100 yes null mysql select from tablea id name pirate monkey ninja spaghetti tableb field type null key default extra id int11 no pri null auto increment name varchar100 yes null mysql select from tab id name rutabanga pirate darth vader ninja am trying the same as proposed in the website select from tablea full outer join tableb on tablea name tableb name and receive error does anyone have an idea what the reason could be thank in advance for the assistance
55550 want to delete an oracle instance oracle created in the aix os know can delete all the dbf and ctl files in the terminal but thinks thats no the best way to do it think must be cleaner way to do that thanks in advance
55568 have two database tables one contains hundreds of millions of records lets call that one history the other one is calculated on daily basis and want to copy all of its records into the history one what did was to run insert into history select from daily and it did the trick for while but it started to get slower and slower as the number of records kept growing now have around million records that need to be copied from daily to history in single operation and it takes too long to complete is there another more efficient way of copying data from one table to another
56023 can see the current search path with show search path and can set the search path for the current session with set search path user public postgis as well can permanently set the search path for given database with alter database mydb set search path user public postgis and can permanently set the search path for given role user with alter role johnny set search path user public postgis but would like to know how to determine what the database and role settings are with respect to search path prior to altering them
56045 my sql servers instance name is sqlexpress and sql server service name looks like mssql sqlexpress is there any relation between instance name and service name im trying to check sql server service status by name and wonder can sql server service name be different on another computers
56096 lets say have two postgresql database groups authors and editors and two users maxwell and ernest create role authors create role editors create user maxwell create user ernest grant authors to editors editors can do what authors can do grant editors to maxwell maxwell is an editor grant authors to ernest ernest is an author would like to write performant function that returns list of the roles preferably their oids that maxwell belongs to something like this create or replace function get all roles returns oid it should return the oids for maxwell authors and editors but not ernest but am not sure how to do it when there is inheritance
56130 have table with 250k rows in my test database there are few hundred millions in production we can observe the same issue there the table has an nvarchar250 string identifier not null with unique index on it its not the pk the identifiers are made up of first part that has different values in my test database and about thousand in production then an sign and finally number to digits long for example there could be thousand rows that start with abcd bgx1741f xml and it is followed by thousand different numbers when query for single row based on its identifier the cardinality is estimated as the cost is very low it works fine when query for more than one row with several identifiers in an in expression or an or expression the estimations for the index are completely wrong so full table scan is used if force the index with hint it is very fast the full table scan is actually executed an order of magnitude slower and lot more slower in production so it is an optimizer problem as test duplicated the table in the same schema tablespace with the exact same ddl and exact same content recreated the unique index on the first table for good measure and created the exact same index on the clone table did dbms stats gather schema statsschemanameestimate percent 100cascade true you can even see that the index names are consecutive so now the only difference between the two tables is that the first one was loaded in random order over long time period with blocks scattered on the disk in tablespace together with several other big tables the second was loaded as one batched insert select other than that cant imagine any difference the original table has been shrinked since the last big deletion and there hasnt been single delete after that here are query plans for the sick and the clone table the strings under the black brush are the same all over the picture and also under they gray brush in this example there are rows that start with the identifier that is black brushed row query produces cardinality of row query produces cardinality of etc cant be coincidence oracle seems to not care about the end of the identifiers what could cause this behavior obviously it would be pretty expensive to recreate the table in production user tables http stack imgur com ndwze jpg user indexes http stack imgur com dg9um jpg only changed the schema and tablespace name you can see that the table and index names are the same as on the query plan screenshot
56304 have materialized view in postgres that id like to update with new columns however other materialized views also depend upon this view and the error message indicates that dropping view isnt possible when other objects depend on it error cannot drop materialized view latest charges because other objects depend on it it also appears from the documentation that the replace keyword isnt valid for materialized view is there any shortcut aside from dropping all dependent objects and rebuilding each one
56374 can someone explain the difference between these types of vacuum in postgresql read the doc but it just says that full locks the tables and freeze freezes the tuples think thats the same am wrong
56482 what factors determine the maximum possible number of threads mysqld will create only care about consideration for mysql and not limitations that may be imposed by the operating system further only care about linux though if you want to include information specific to windows feel free as far as can tell threads can be created for replication aspects of innodb one per client connection up to max connections with an additional to handle super connections what else is there or is that it threads max max connections for super for replication slave threads for innodb
56494 after having read this page in the mysql documentation tried to make sense of our current innodb usage currently we allocate 6gb of ram for the buffer pool our database size is about the same heres the output from show engine innodb status were running v5 buffer pool and memory total memory allocated in additional pool allocated dictionary memory allocated buffer pool size free buffers database pages old database pages modified db pages pending reads pending writes lru flush list single page pages made young not young youngs non youngs pages read created written reads creates writes buffer pool hit rate young making rate not pages read ahead evicted without access random read ahead lru len unzip lru len sum cur unzip sum cur wanted to know how well were utilizing the buffer cache after initially glancing at the output it appeared that we are indeed using it based off of the pages made young and not young have numbers in them and buffer pool hit rate is which saw elsewhere on the web that this means its being used pretty heavily true whats throwing me through loop is why the young making rate and not are both at and the young and non young accesses are both at those would all indicate that its not being used at all right can anyone help make sense of this
56527 ive downloaded the adventureworks based in memory sample from here and followed all the steps described in the accompanying doc however when try to run the script in sql server management studio get the error message alter database statement not allowed within multi statement transaction the error points to line which is if not exists select from sys data spaces where type fx alter database current add filegroup adventureworks2012 mod contains memory optimized data go since this is more or less official microsoft documentation im assuming its something im doing wrong but cant figure out what it is
56616 am running sql server the sql server management studio has the option to right click on database then select tasks and generate scripts is there way to automate that via command line somehow want to create script that includes the schema and data of the entire data base tools like scriptdb and sqlpubwiz exe all seem to target sql server what about sql server
56804 in my database there is lot of tables starting with elgg now want to drop all tables with this prefix can anyone give me solution thanks in advance
56844 is there quick way to find all columns in sql server r2 that are encrypted have encrypted data need to nullify the data in all encrypted columns in development server according to our business rules know most of the columns because we use them regularly but want to be thorough and also want to be able to prove that ive found them all ive searched the web looked in information schema and checked the dmvs thought would be useful and also sys columns and sys objects but so far no luck
56876 came across some interesting behaviour on sql server observed in and today that was hoping someone could explain query doing comparison using on an nvarchar field ignored the trailing space in the string or auto trimmed the value before comparison but the same query using the like operator did not ignore the space collation being used is latin1 general ci as in consider this sql fiddle http sqlfiddle com note that the like operator does not return result for the trailing space string but the operator does why is this bonus points am unable to replicate this on varchar field would have thought that space would be handled in the same way in both data types is this true
56893 dont have this kind of id in my table still mysql is giving me result this same result is giving when place in my php code
56897 have postgresql table and need to create view with new column this column needs to be an auto incremental column starting at and going to is this possible to do without effecting the original schema of the legacy data structure
56930 am trying to find out who changed the password for login in sql server r2 have already checked default trace and it does not log that event the default trace will include these security related events audit add db user event audit add login to server role event audit add member to db role event audit add role event audit add login event audit backup restore event audit change database owner audit dbcc event audit database scope gdr event grant deny revoke audit login change property event audit login failed audit login gdr event audit schema object gdr event audit schema object take ownership audit server starts and stops also looked into transaction log backup to find that out but no luck is there any other way to find it out also am aware that server side trace will help but unfortunately in our server side trace we did not include the audit login change password event best article that found is from aaron bertrand tracking login password changes in sql server
57058 postgres can use ispell compatible dictionaries in text search but does not provide the required files
57418 im investigating the big switch from mysql to nosql dbaas and ive run into an issue trying to forecast expenses essentially cant figure out how many queries my current mysql server handles per day to try and estimate the number of requests ill be using with cloudant which charges per puts posts and deletes and per gets and heads ive found lot of information about using show status and show global status to get the stats that mysql collects on itself but theres no timeframe reference for instance show global status returns the following queries which is great except have no idea the timeframe that wraps that number million queries when per month year since the beginning of time the mysql docs dont really elaborate too much queries the number of statements executed by the server this variable includes statements executed within stored programs unlike the questions variable it does not count com ping or com statistics commands this variable was added in mysql thanks in advance for any help
57445 in order to use having in sql queries must there be group by to aggregate the column names are there any special cases where it is possible to use having without group by in sql queries must they co exist at the same time
57723 created superuser portal with the following command create role portal with superuser password portal when tried to login into postgres with portal user am getting the following error in rails fatal role portal is not permitted to log in cant follow what is going on of course can alter user with login command to enable portal user to login would like to understand why super user cant login
57785 would like to create computed index on table if sql server is or newer and simple index if sql server is or older check for sql server version if select castleftcastserverpropertyproductversion as varchar as decimal5 create unique nonclustered index ix1 table on table column1 column2 where column1 is not null and column2 is not null else create nonclustered index ix1 table on table column1 column2 the problem is that the whole statement is evaluated and on sql server this throws an error incorrect syntax near the keyword where is it possible to somehow create different index based on sql server version
58214 im trying to get when my table was modified by checking its file modification date as it is described in this answer but the result is not always correct the file modification date updates in several minute after update my table is it correct behaviour does postgresql store table modifications in some cache and then flush it to the hard drive so how do get the correct last modification date of table lets assume that auto vacuum modifications are ok too use postgresql under linux centos x64
58284 is it possible to drop all empty tables from my huge database mysql im looking for sql command to automatically remove all those empty tables currently have tables in my dataset and about of them are old empty tables that will not be used in my new application just to clarify all tables are of type myisam
58308 is there name for the following database schema design pattern my eventual goal is to find more literature about the subject todays cursory net search was too full of generic words to be able pin down the term if any exists for this kind of thing fruit id farm apple fruit id color fruit id fruit id banana fruit id length fruit id fruit id orange fruit id is seedless fruit id fruit id fruitpack id destination fruitpackfruits fruitpack id fruit id fruit type fruit id fruit id fruit type varchar where fruit type would be varchar column filled with values like apple banana orange cherry its some kind of poor mans referential integrity obviously one the failures of this kind of design is being able to insert values that dont resolve out to useful join ie there are no cherries to speak of here heres another example of such pattern single log id table name record id timestamp table that acts as sort of tracker for modification times in various other tables strictly speaking its got no ref integrity but the table name record id part is supposed to refer to some record in another table requiring join to actually get the full data im going to take for granted that the schema is sufficient caricature of some sort of collection of groups of items for the people here the question is whats this kind of poor mans referential integrity called im not trying to learn about referential integrity want to identify this poor designs name and look further into the lets design database schema aspects ex pros cons opinions teachings etc that have to do with this commonly seen disaster of schema
58312 using database name in postgresql will connect to the named database how can the name of the current database be determined entering my db current database produces error syntax error at or near current database line current database
58341 recently heard that should not host my mysql database on an ssd drive because there is big chance of disk failure and data lost given that of all queries in my database are inserts and updates here are my questions questions what are the disadvantages of using mysql on ssd what solutions or alternatives are available for mysql ssd usage
58455 would like to insert record data type variable new variable into table in trigger what would the sql look like the following attempts were unsuccessful execute insert into my table values new execute insert into my table values new execute insert into my table select from new
58772 as you guys probably know sql server does not provide an out of the box solution to export all the security related statements that are declared for each element in the database both at the database level and also the object level im talking about the ability to come up with this information all the users all the user defined roles all permissions at database level grant create function all permissions at the object level grant select on object xxx you would think that sql server must have something like this but neither the sql server export wizard or the various scripts that are generated as result of right clicking the objects do capture this information have seen online many different possible solutions using non curated scripts that people graciously post but since have to be sure that all security information is captured cant fully rely on those scripts have the option of using those as starting point to write something myself but hate having to re invent the wheel for requirement that you would think many people may have isnt there tool provided by someone either as part of the sql server product or an 3rd party tool that could reliably provide you with this information or at least is there community supported script that most of people would agree will do the job thanks
58888 when running the following queries in ms sql server the second query fails but not the first also when run without the where clauses both queries will fail am at loss why either would fail since both should have empty result sets any help insight is appreciated create table temp id int primary key create table temp2 id int select from temp where id select from temp2 where id
59000 have postgresql table prices with the columns price decimal product id int there are also created at and updated at columns prices get updated regularly and keep old prices in the table for given product the last price in the table is the current price what is the most efficient way to get the last price for specific product index product id and query for the last record add third column active boolean to mark the latest price and create composite index product id and active or something else
59006 there is long and quite elucidating answer on the differences between timestamp with time zone vs timestamp without time zone available in this so post what would like to know is are there any valid use cases for actually using timestamp without time zone or should it be considered an anti pattern
59062 have table not designed by me which has variably named columns that is depending on what type of record you are looking at the applicable name of the column can change the possible column names are stored in another table that can query very easily therefore the query im really looking for goes something like this select col1 as select colname from names where colnum and type type col2 as select colname from names where colnum and type type from tbl1 where type type obviously that doesnt work so how can get similar result ive tried building query string and executeing it but that just returns commands completed successfully and doesnt seem to return rowset it turns out was using an incorrect query to build the dynamic sql and as such built an empty string sql server definitely executed the empty string correctly note that the reason need this to occur rather than simply hard coding the column names is that the column names are user configurable
59128 ran brent ozars sp blitz script and one of the things its complaining about is that my sql server agent account has sysadmin permission removed the sysadmin permission but then the agent wouldnt start see image below theres nothing of note in the event log when restore the sysadmin permission the agent starts with no difficulty my account is managed service account none of the other managed service accounts use one for running reporting services and one for running the sql service have sysadmin permission is the sysadmin permission actually necessary if not what are the minimum permissions this account needs
59170 try to create report for my data but it is really slow on big table the table structure is create table posts id serial not null project id integer moderation character varying255 keyword id integer author id integer created at timestamp without time zone updated at timestamp without time zone server id character varying255 social creation time integer social id character varying255 network character varying255 mood character varying255 default null character varying url text source id integer location character varying255 subject id integer conversation id integer constraint posts pkey primary key id create index index posts on author id on posts author id create index index posts on keyword id on posts keyword id create index index posts on project id and network and social id on posts project id network social id create index index posts on project id and social creation time on posts project id social creation time desc create index index posts on server id on posts server id create index index posts on social id on posts social id the query select date trunchour timestamp epoch posts social creation time interval second creating network count posts from posts where posts project id and posts moderation not in junkspam and posts social creation time between and group by network creating order by creating the count is explain plan groupaggregate cost rows width actual time rows loops sort cost rows width actual time rows loops sort key date trunchour text timestamp without time zone social creation time double precision interval network sort method external merge disk 92032kb seq scan on posts cost rows width actual time rows loops filter moderation text all junkspam text and social creation time and social creation time and project id rows removed by filter total runtime ms rows time ms its seq scan but then force use indexes it not help groupaggregate cost rows width actual time rows loops sort cost rows width actual time rows loops sort key date trunchour text timestamp without time zone social creation time double precision interval network sort method external merge disk 92048kb bitmap heap scan on posts cost rows width actual time rows loops recheck cond project id filter moderation text all junkspam text and social creation time and social creation time rows removed by filter bitmap index scan on index posts on project id and network and social id cost rows width actual time rows loops index cond project id total runtime ms an example row from the table id project id moderation keyword id author id created at updated at server id social creation time social id network mood url source id location subject id conversation id pending vkontakte https vk com wall update real that help me is work mem set to my new plan hashaggregate cost rows width actual time rows loops seq scan on posts cost rows width actual time rows loops filter moderation text all junkspam text and social creation time and social creation time and project id rows removed by filter total runtime ms update think create integer column and save date like yyymmdd stackexchange what you think it performance gains ps sorry all for my bad english
59193 have database where load files into staging table from this staging table have joins to resolve some foreign keys and then insert this rows into the final table which has one partition per month have around billion rows for three months of data what is the fastest way to get these rows from staging into the final table ssis data flow task that uses view as source and has fast load active or an insert into select command tried the data flow task and can get around billion rows in around hours cores gb ram on the server which feels very slow to me
59447 am trying to design grade assignment database and am facing the following dilemma have student table which has the following characteristics matriculation number which know for fact is unique first name last name course of studiesbsc mechanical engineering msc mechanical engineering bsc computer science etc email remark maybe some special notes about particular student however there is also the following condition every student can at the same time belong to two different course of studies that is student can be bsc and msc mechanical engineering at the same same time so after some thought decided to create separate table called course of studies which has two columns an auto increment idpk and course of study which will include all possible course of studies then connect the student and course of studies tables using many to many relationship at the same time thought that another implementation would be to use surrogate key auto increment id on the student table and instead of creating separate course of studies table could simply incorporate it in the student table as column and not have any problem with my primary key since if had student that belongs on two different course of studies would have two different rows with unique id so my question to you is what do you think is the best implementation given the situationplease explain why know that natural primary keys vs surrogate primary keys is highly debated subject which doesnt seem to have universal answer however given the situation what do you think is the best implementation any additional details can be provided edit there are two good answers that are saying more or less the same thing chose to accept the one with the more votes
59472 am looking for something like this where could store the output of sp who2 in table without having to first create the table create table test as exec sp who2
59509 assuming in postgres database you have table called party which can have less than well defined party types such as person or organization would you store the party type in the party table party type person or normalize it party party type and party typeid name person and why
59770 does indexed data affect query performance means if passed value as in ordered form in any in clause can it improve query performance and what happen in case pass value randomly etc example select name from emp where id in ordered values or select name from emp where id in randomly ordered values another way select name from emp where id in select empid from trans order by empty ordered values am asking for mysql as well as sql server database and can any one tell me how database engine get result in above case execution plan
59776 have multiple nvarcharmax columns in my db named shippernameconsigneename proddescbilloflading what need to do is to have unique index for the combination of these columns as am failed to do so because if limit then triedafter reading following link indexing wide keys alter table dbo productdetail add shippernamehash as hashbytessha2 shippername persisted alter table dbo productdetail add consigneenamehash as hashbytessha2 consigneename persisted then got following error on following statement msg level state line string or binary data would be truncated the statement has been terminated alter table dbo productdetail add productdeschash as hashbytessha2 product description persisted but also above statements created column varbinary8000 which of course hasnt solved the problem for me is there any way that can built unique index on the combination these columns
59840 have table with geometry column for one record there is only one point stored spatial index has been created but queries searching for the nearest location do not use this index resulting in bad performance example script create the table create table location locationid int not null identity11 primary key locationpoint geometry add records declare counter int while counter begin set nocount on select set counter counter declare randomlocation geometry geometry pointrand rand insert into locationlocationpoint values randomlocation end create index create spatial index spatial structurebe on dbo locationlocationpoint using geometry grid with bounding box xmin ymin xmax ymax grids level medium level medium level medium level medium cells per object statistics norecompute off allow row locks on allow page locks on search this query should use the index but it doesnt declare currentlocation geometry geometry point2450 select top from location order by locationpoint stdistance currentlocation asc
59916 have streaming replication hot standby setup which seems to be running all good can login execute selects and the result seems to be up to date with the master however in the log of the standby see lot of these cp cannot stat mnt wal drive wals 0000000100004ba800000070 no such file or directory cet log record with zero length at 4ba8 70dd79d0 cet log started streaming wal from primary at 4ba8 on timeline cet fatal could not receive data from wal stream ssl error sslv3 alert unexpected message what does each of these mean and should worry about any of them ps in the master see some of these which expect to be related to the ssl thing cet log ssl renegotiation failure cet log ssl error unexpected record im running postgresql on ubuntu edit regarding the wal logs have the following setup on both my master and my slave have mounted network storage in fstab using cifs command the directory is chowned to be owned by the postgres user on my master have the following settings in postgresql conf archive mode on archive command rsync mnt wal drive wals dev null on the slave have hot standby on and the following recovery conf standby mode on primary conninfo host localhost port user replicator password some pass trigger file tmp pgsql trigger restore command cp mnt wal drive wals dev null archive cleanup command usr lib postgresql bin pg archivecleanup mnt wal drive wals have an ssh tunnel forwarding port of the slave to port on the master so connecting on localhost is actually the master db
60108 am about to setup ola hallengrens database maintenance plan we have our database mirrored and was just wanting to know if need to run the script on both my sql server instances or only the primary
60129 am working on third party database when try to view the definition of view by right clicking create to and then to new query edit window am getting an error this property may not exist for this object or may not be retrievable due to insufficient access rights the text is encrypted
60179 background have query running against sql server r2 that joins and or left joins about different tables the database is fairly large with many tables over million rows and about different tables its for large ish company that has warehouses across the country all the warehouses read and write to the database so its pretty large and pretty busy the query im having trouble with looks something like this select t1 something t2 something etc from table1 t1 inner join table2 t2 on t1 id t2 t1id left outer join select from table t3 on t3 t1id t1 t1id etc where t1 something notice that one of the joins is on non correlated sub query the problem is that starting this morning without any changes that or anybody on my team knows of to the system the query which usually takes about mins to run started taking an hour and half to run when it ran at all the rest of the database is humming along just fine ive taken this query out of the sproc that it usually runs in and ive run it in ssms hard coded parameter variables with the same slowness the strangeness is that when take the non correlated sub query and throw it into temp table and then use that instead of the sub query the query runs fine also and this is the strangest to me if add this piece of code to the end of the query the query runs great and name like ive concluded perhaps incorrectly from these little experiments that the reason for the slow down is due to how sqls cached execution plan is set up when the query is little different it has to create new execution plan my question is this when query that used to run fast suddenly starts running slowly in the middle of the night and nothing else is affected except for this one query how do troubleshoot it and how do keep it from happening in the future how do know what sql is doing internally to make it so slow if the bad query ran could get its execution plan but it wont run maybe the expected execution plan would give me something if this issue is with the execution plan how do keep sql from thinking that really crappy execution plans are good idea also this is not problem with parameter sniffing ive seen that before and this is not it since even when hard code the varaibles in ssms still get slow performance
60342 is postgres way of combining is distinct from with any or some other neat way of getting the same result select count from select foo union all select union all select null where foo anyarray null count row select count from select foo union all select union all select null where foo is distinct from anyarray null error syntax error at or near any line where foo is distinct from anyarray null
60449 have database mydatabase created in sql server r2 have upgraded to sql server was trying to execute below query to calculate percentile select distinct key percentile cont0 within grouporder by eachprice overpartition by key as q1percentile cont0 within grouporder by eachprice overpartition by key as q2 percentile cont0 within grouporder by eachprice overpartition by key as q3 percentile cont1 within grouporder by eachprice overpartition by key as q4 from mydatabase but am getting an error stating that msg level state line the percentile cont function is not allowed in the current compatibility mode it is only allowed in mode or higher can change compatibility mode to what are the implications of changing compatibility mode from to please advice
60465 have my production serverubuntu running with postgresql want to use few features of hence want to upgrade could someone help me with upgrading from to so that there is downtime of not more than mins or so prime concern is preventing data loss or file redundancy
60480 have table in postgresql database defined as following create table public match item id bigint default item id bigint default owner id bigint default owner id bigint default other data varchar100 not null default constraint match pk primary key item id item id it will contain lot of rows there will be lot of queries like the following performed on this table select from match where owner id owner id select from match where owner id owner id was thinking about creating indexes on owner id and owner id since these columns are not keys is this good idea and if yes how should create these should create one index with both columns should create two indexes should include other columns
60590 in another application was struck by bad design multiple threads execute an ensuredatabaseschemaexists method concurrently which looks basically like this if not exists select from sys objects where object id object idnmytable and type nu begin create table mytable end however even if executed in serializable transaction this code does not seem to be thread safe the parallel code tries to create the table multiple times is there any chance to force the select statement to acquire lock which prevents another thread to do the very same select statement is there better pattern for multi threaded ensureschemaexists methods
60651 have stored procedures where the second stored procedure is an improvement of the first one im trying to measure by exactly how much that is an improvement measuring clock time doesnt seem to be an option as get different execution times even worse sometimes rarely but it happens the execution time of the second stored procedure is bigger than the execution time of the first procedure guess due to the server workload at that moment include client statistics also provides different results dbcc dropcleanbuffers dbcc freeproccache are good but the same story set statistics io on could be an option but how could get an overall score as have many tables involved in my stored procedures include actual execution plan could be an option also get an estimated subtreecost of for the first stored procedure and for the second one can say the second stored procedure is faster using reads field from sql server profiler so how can say that the second stored procedure is faster than the first procedure no matter the execution conditions the workload of the server the server where these stored procedures are executed etc if it is not possible how can prove the second stored procedure has better execution time than the first stored procedure
60777 am trying to determine which indexes to use for an sql query with where condition and group by which is currently running very slow my query select group id from counter where ts between timestamp and timestamp group by group id the table currently has rows the execution time of the query increases lot when increase the time frame the table in question looks like this create table counter id bigserial primary key ts timestamp not null group id bigint not null currently have the following indexes but the performance is still slow create index ts index on counter using btree ts create index group id index on counter using btree group id create index comp index on counter using btree ts group id create index comp index on counter using btree group id ts running explain on the query gives the following result query plan hashaggregate cost rows width index scan using ts index on counter cost rows width index cond ts timestamp without time zone and ts timestamp without time zone sql fiddle with example data http sqlfiddle com 7492b the question can the performance of this query be improved by adding better indexes or must increase the processing power edit postgresql version is used edit tried erwin proposal with exists select group id from groups where exists select from counter where group id group id and ts between timestamp and timestamp but unfortunetly this didnt seem to increase the performance the query plan query plan nested loop semi join cost rows width seq scan on groups cost rows width bitmap heap scan on counter cost rows width recheck cond group id id and ts timestamp without time zone and ts timestamp without time zone bitmap index scan on comp index cost rows width index cond group id id and ts timestamp without time zone and ts timestamp without time zone edit the query plan for the lateral query from ypercube query plan nested loop cost rows width seq scan on groups cost rows width result cost rows width one time filter is not null initplan returns limit cost rows width index only scan using comp index on counter cost rows width index cond group id and ts is not null and ts timestamp without time zone and ts timestamp without time zone initplan returns limit cost rows width index only scan backward using comp index on counter cost rows width index cond group id and ts is not null and ts timestamp without time zone and ts timestamp without time zone
60789 given sql server table with large number of rows no columns with large value data types multiple indexes more allocated space than available for the largest possible transaction log size single column primary key with clustered index optional consideration for this question an average record size of 1k optional consideration for this question and an update statement which needs to be run against every row sets value on non indexed column optional consideration for this question what techniques can be employed to reduce the peak disk space consuption including data files log file and tempdb if applicable required to do this update for purposes of this question the following is allowed applying changes in batches run in single user mode change recovery model
60837 while calculating difference between dates as number days by using mysql datediff function am facing some issue the sql code is given below select datediffselect curdate select company createdon from dbname company inner join dbname user on user company id company company id am getting the result as follows error code subquery returns more than row do understand the error but cant use any ids to point out as follows select datediff select curdate select company createdon from dbname company inner join dbname user on user company id company company id where company company id while run this query got the output as follows need to select the users depends on the company which registered time exist more than days need to display all the company which registered time more than days have not mentioned this condition in code need to solve subquery result datediff function should compare with all the company createdon value and results only the companies which exists more than days any help would be grateful
61163 hi am trying to connect to postgres from pgadmin3 using ssh tunneling however am not using standard port how can do ssh tunneling using non standard port
61212 for one of our systems we have sensitive client data and store each clients data in separate database we have about clients for that system however were developing new system that will have clients maybe more am thinking it might be unfeasible to have one database per client in this instance to store sensitive records and audit history however dont know if this is perfectly normal or not or if theres another way of maintaining security any thoughts on this
61246 what does the icon of server with blue database symbol mean
61293 we want to rename column in our mysql version database with something like this alter table t1 change integer is this an on number of rows in the table or an o1 operation cant seem to find an answer in the manual nor on google the table has millions of records and it wouldnt be feasible directly if its on
61520 would like to update selection of rows in table this works update t1 set col1 newvalue where col0 in but how can do the same in plpgsql function the following gives syntax error create or replace function foointarray int returns void as body begin update t1 set col1 newvalue where col0 in intarray end body language plpgsql volatile the error error syntax error at or near intarray line where col0 in intarray edit if we replace in intarray with in intarray the function is recorded but when running select from fooarray the error becomes error operator does not exist integer integer line where col0 in intarray
61616 im attempting to install sql server express sp1 x64 on windows sp1 x64 this machine has previously had sql server express installed this instance was upgraded to sql server standard which has since been uninstalled following are the options selected during the install include sql server product updates in install related to kb install all features to default directories named instance sqlexpress in default instance root directory default services accounts sql server database engine nt service mssql sqlexpress default database engine configuration the install fails with following error title microsoft sql server service pack setup the following error has occurred could not find the database engine startup handle for help click http go microsoft com fwlinklinkid prodname microsoft 20sql 20server evtsrc setup rll evtid prodver evttype 0xd15b4eb2 25400x4bdaf9ba buttons ok ive read that this may be related to corrupt mssql sqlexpress virtual account any ideas on how to fix this thanks jon summary txt overall summary final result failed see details below exit code decimal start time end time requested action install setup completed with required actions for features troubleshooting information for those features next step for sqlengine use the following information to resolve the error uninstall this feature and then run the setup process again next step for replication use the following information to resolve the error uninstall this feature and then run the setup process again machine properties machine name montreal machine processor count os version windows os service pack service pack os region united states os language english united states os architecture x64 process architecture bit os clustered no product features discovered product instance instance id feature language edition version clustered package properties description microsoft sql server service pack productname sql server type rtm version installation location 097a5adf24ea31e1a16d x64 setup installation edition express slipstream true sp level patch level product update status success kb kb product updates selected for installation title service pack knowledge based article kb version architecture x64 language title sql server sp1 gdr product update knowledge based article kb version architecture x64 language all update source mu user input settings action install addcurrentuserassqladmin true agtsvcaccount nt authority network service agtsvcpassword agtsvcstartuptype disabled asbackupdir backup ascollation latin1 general ci as asconfigdir config asdatadir data aslogdir log asprovidermsolap asservermode multidimensional assvcaccount assvcpassword assvcstartuptype automatic assysadminaccounts astempdir temp browsersvcstartuptype disabled cltctlrname cltresultdir cltstartuptype cltsvcaccount cltsvcpassword cltworkingdir commfabricencryption commfabricnetworklevel commfabricport configurationfile ctlrstartuptype ctlrsvcaccount ctlrsvcpassword ctlrusers enableranu true enu true errorreporting false features sqlengine replication snac sdk filestreamlevel filestreamsharename ftsvcaccount ftsvcpassword help false iacceptsqlserverlicenseterms true indicateprogress false installshareddir program files microsoft sql server installsharedwowdir program files x86 microsoft sql server installsqldatadir instancedir program files microsoft sql server instanceid sqlexpress instancename sqlexpress issvcaccount nt authority network service issvcpassword issvcstartuptype automatic matrixcmbrickcommport matrixcmservername matrixname npenabled pid quiet false quietsimple false role allfeatures withdefaults rsinstallmode defaultnativemode rsshpinstallmode defaultsharepointmode rssvcaccount rssvcpassword rssvcstartuptype automatic sapwd securitymode sqlbackupdir sqlcollation latin1 general ci as sqlsvcaccount nt service mssql sqlexpress sqlsvcpassword sqlsvcstartuptype automatic sqlsysadminaccounts montreal jonathan bailey sqltempdbdir sqltempdblogdir sqluserdbdir sqluserdblogdir sqmreporting false tcpenabled uimode autoadvance updateenabled true updatesource mu x86 false configuration file program files microsoft sql server setup bootstrap log configurationfile ini detailed results feature database engine services status failed see logs for details reason for failure an error occurred during the setup process of the feature next step use the following information to resolve the error uninstall this feature and then run the setup process again component name sql server database engine services instance features component error code 0x851a0019 error description could not find the database engine startup handle error help link http go microsoft com fwlinklinkid prodname microsoft sql server evtsrc setup rll evtid prodver evttype 0xd15b4eb2 400x4bdaf9ba evttype 0xd15b4eb2 400x4bdaf9ba feature sql server replication status failed see logs for details reason for failure an error occurred for dependency of the feature causing the setup process for the feature to fail next step use the following information to resolve the error uninstall this feature and then run the setup process again component name sql server database engine services instance features component error code 0x851a0019 error description could not find the database engine startup handle error help link feature sql browser status passed feature sql writer status passed feature sql client connectivity status passed feature sql client connectivity sdk status passed rules with failures global rules scenario specific rules rules report file program files microsoft sql server setup bootstrap log systemconfigurationcheck report htm
61633 disclaimer im very aware this is not supposed to be done but time consistency between tables arent concert here and im trying whatever pops in my mind right now to have alternatives for punctual on demand backups in complement to the more robust scheduled ones pretty much want to know if there will be any reading problem while copy myisam table files the frm myd myi and it gets transaction most tables are small so we could just take the risk but there are couple that worries me becuase of their size dont mind waiting for the transaction to get done what worries me is getting read error and failing to get response from the query thats about it if you know something please let me know
62029 in postgresql explain or explain analyze will show the estimate cost of executing query but explain in mysql doesnt provide this information how can get the estimate cost without installation of other tools im using mysql
62051 this is driving me nuts want to rename column from read more to read more in my blog table tried all this alter table blog rename column read more to read more alter table blog change column read more read more varchar255 not null and always get this error you have an error in your sql syntax check the manual that corresponds to your mysql server version for the right syntax to use near column read more to read more at line im using mysql
62129 am using mysql under wamp server environment now want to log all queries into log file the queries which are running by php or from phpmyadmin want to log them
62144 have database with tables in mssql some of them contain the field userid varchar9 while others do not due to an application redesign need to alter all tables in the database to check if the field exists and if it does need to change it to varchar50 if not then need to add it can someone point me how to do this with some kind of batch procedure dont have much experience with mssql databases so detailed explanation would be appreciated
62165 have setup server that runs windows server and has sql server express installed can connect to the machines sql server express database via the machinename sqlexpress however when we come to connecting through any software or script using an ip address it wont allow the connection have tried turning off the firewall allowing remote connections for the sql database enabling tcp ip within the sql configuration when we attempt to connect via the software sql server management studio we get the following message error message an error has occurred while establishing connection to the server when connecting to sql server this failure may be caused by the fact that under the default settings sql server does not allow remote connections provider tcp provider error no connection could be made because the target machine actively refused it microsoft sql server error connection was successfully established with the server but then an error occurred during the login process provider tcp provider error an established connection was aborted by the software in your host machine microsoft sql server error can you please let me know when your free so we can take look because seem to be getting know where ve amended the details as per some information uk fast sent me but they have said it not within the support remit so they can help any further look forward to hearing from you
62208 each records weighs bytes is it possible in todays technology if someone had the storage needed what kind of db would hold the data and will make it possible to retrieve it an example record 5hphagt65tzzg1ph3csu63k8dbpvd8s5ip4neb3kesreabuatmu 1mshws1bnwmc3tle8g35uxss58fkipzb7a 1q1pe5vpgeemqrcvrmbtbk842y6pzo6nk9 the number of records is
62500 am running postgressql and have column relation with about rows it contains nodes in 3d space each one referencing user who created it to query which user has created how many nodes do the following added explain analyze for more information explain analyze select user id countuser id from treenode where project id group by user id query plan hashaggregate cost rows width actual time rows loops seq scan on treenode cost rows width actual time rows loops filter project id total runtime ms as you can see this takes about seconds this isnt too bad considering the amount of data but wonder if this can be improved tried to add btree index on the user column but this didnt help in any way do you have alternative suggestions for the sake of completeness this is the complete table definition with all its indices without foreign key constraints references and triggers column type modifiers id bigint not null default nextvalconcept id seq regclass user id bigint not null creation time timestamp with time zone not null default now edition time timestamp with time zone not null default now project id bigint not null location double3d not null reviewer id integer not null default review time timestamp with time zone editor id integer parent id bigint radius double precision not null default confidence integer not null default skeleton id bigint indexes treenode pkey primary key btree id treenode id key unique constraint btree id skeleton id treenode index btree skeleton id treenode editor index btree editor id treenode location index btree location treenode location index btree location treenode location index btree location treenode parent id btree parent id treenode user index btree user id edit this is the result when use the query and index proposed by ypercube query takes about seconds without explain analyze explain analyze select id select count from treenode as where project id and user id id as number of nodes from auth user as query plan seq scan on auth user cost rows width actual time rows loops subplan aggregate cost rows width actual time rows loops bitmap heap scan on treenode cost rows width actual time rows loops recheck cond project id and user id id rows removed by index recheck bitmap index scan on treenode user index cost rows width actual time rows loops index cond project id and user id id total runtime ms rows time ms edit this is the result when use an index on project id user id but no schema optimization yet as erwin brandstetter suggested the query runs with seconds at the same speed as my original query explain analyze select user id countuser id as ct from treenode where project id group by user id query plan hashaggregate cost rows width actual time rows loops seq scan on treenode cost rows width actual time rows loops filter project id total runtime ms rows
62576 my knowledge of databases and sql is based in most on university classes anyhow spent few monts almost year in company where was working with databases have read few books and have taken part in few trainings about databases such as mysql postgresql sqlite oracle and also few nonsql dbs such us mongodb redis elasticsearch etc as well as said am begginer with lot of lacks of knowledge but today someone told something what is totally against my begginers knowledge let me explain lets take sql database and create simple table person with few records inside id name age alex brad chris david eric fred greg hubert irvin john karl now its the part would like to focus on id is the index so far thought it works in this way when table is being created the index is empty when am adding new record to my table the index is being recalculated based on some alghortims for example grouping one by one 2n xn 1n so for my example with size elements and it will be like this id name age alex group0 brad group0 chris group0 david group1 eric group1 fred group1 greg group2 hubert group2 irvin group2 john group3 karl group3 so when am using query select from person where id it will do some simple calculation so we have to look for this object in group2 and then this row will be returned hubert this approach works in time ok where size of course an alghoritm to organise rows in groups is for sure much more complicated but think this simple example shows my point of view so now would like to present another approach which has been showed me today lets take once again this table id name age alex brad chris david eric fred greg hubert irvin john karl now we are creating something similar to hashmapin fact literally it is hash map which maps id to address of row with this id lets say id addr so now when am running my query select from person where id it will map directly id to address in memory and the row will be returned of course complexity of this is o1 so now have got few questions what are the adventages and disadventages of both solutions which one is more popular in current database implementations maybe different dbs use different approaches does it exist in nonsql dbs thank you in advance comparison tree hash table one element searching ologn o1 on deleting ologn o1 on inserting ologn o1 on space on on elements searching ologn o1 on deleting ologn o1 on inserting ologn o1 on space on on number of records am right what about cost of rebuilding tree and hash table after each insert delete in case of tree we have to change some pointers but in case of balanced tree it needs more effort also in case of hash table we have to do few operation especially if our operation generate conflicts
62598 look at the following example starting from the top row id and work your way down selecting limit of rows that have secs that we have not yet seen we select id because we dont yet have sec we continue to work our way down like this but when we get to id we skip it because we already have sec from row with id we continue in the same manner and we finally stop at id because we have accumulated rows our desired limit id sec skip already have sec skip already have sec skip already have sec of course the sql algorithm can will be different than described desired result id rows if wanted to increase the limit to rows then the row with id would be included in the results however if increased the limit to rows the row with id would not be added because sec has already been seen note though it shouldnt matter am on postgresql in case you want to quickly build the table to test this out create table my table id serial primary key sec integer default not null insert into my table sec values create index index my table on sec on my table sec
62660 in my application with db running on sql server ive got job scheduled task that periodically executes an expensive query and writes the results to table that can later be queried by the application ideally would like to run that expensive query only if something changed since the query has last executed since the source tables are very big cannot just select checksum over all candidate columns or something like that ive got the following ideas explicitly write last changed timestamp must be queries flag or something like this to tracking table whenever change something in source table use trigger to do the same however id really like to know whether there is lightweight way to detect changes on table without me explicitly tracking the writes can for example get the current rowversion of table or something like that
62812 how can we use while loops in mysql my test script begin select into while do select set end while end but it has syntax errors im running the loop using the sqlyog client in standard query window the syntax errors are of the following form error code you have an error in your sql syntax check the manual that corresponds to your mysql server version for the right syntax to use near ive also tried to use the while loop example provided by https dev mysql com doc refman en while html but it still didnt work which part of the script is wrong using mysql im trying to make month moving average of some data so was hoping that while loop would be able to append the new month average onto the old month average through each iteration of the loop
62884 when turn on my desktop windows sp1 often find or gb of memory is already occupied by mysql processes dont care about how much memory database requires if it actually runs some queries but do if its an app even dont start myself yet for that matter barely use mysql barring when develop some web apps in my local environments so is it possible to stop this gluttony tried to remove mysql from my startup group viamsconfig but couldnt find its name over there here is extra info mysql ver distrib for win64 x86 mysql server mysql workbench
62976 im was playing around with permissions and locked myself out of mongo database im pretty sure did this by trying to explicitly add access to database but instead overwrote only allowing permission to the database so im effectively locked out of my mongo database and everything read tells me how to create super user if have the add user privilege right now dont think have any users that have that privilege is there way to enter the database as all access own the server and have root access
63131 ive been looking into using indexed views to increase performance on few of our most commonly used views however indexed views do not support non unique clustered indexes which goes little against the precedence set by the rest of the database structure for example here is simplified version of couple of our tables groups group id groupname users userkey username fullname groupid the indexes are on groups groupid non clustered and users groupid clustered the clustered key being on groupid in the users table as most commonly range of users from specific group would be retrieved obviously you would have multiple users per group so this clustered index is non unique this leaves me bit uncertain of how to follow this precedence when indexing my views such as this example as cannot have non unique clustered index consumableid consumablevariantid allowthresholdoverwrite fullpath groupid manufacturerid type modelid in actuality the only value on this view which would always be unique is the consumableid column so am left with little choice as of where to place my index why do views not permit non unique clustered indexes when regular tables do
63138 need to know what sql queries are generated by some application is it possible to see it in some kind of sniffer tried to look in management studio but it shows only most expensive queries
63255 have received database file and the instructions for loading it is to install sql server and then attach it using sql server management studio after installing everything tried to attach the mdf file but then it tells me the directory lookup for the file foldername filename ldf failed with the operating system error error not found an ldf file did not come with the database so presumably it should be generated automatically now drive is where my cd drive is so its not going to find anything there nor is it going to have any luck trying to create anything there why is it trying to look for log file at specific path why not where the database file is how can attach this database realized that when select database to attach three entries appear under database details an mdf ndf and ldf the ldfs current file path points to the drive path above so removed it this time when hit ok get different error message database cannot be upgraded because it is read only or has read only files make the database or files writeable and rerun recovery file activation failure the physical name folder file ldf may be incorrect new log file was created microsoft sql server error so now it creates new log file in the same folder as the database file which is great but it seems like there are security issues additional information the instructions require me to use the login name sa which appears to be the sysadmin account am connected to my sql server instance using that login have checked the file properties that it is not read only the directory is not read only either all acls are allowed am unable to attach the database when try to attach it it throws an error message with attach database failed closing ssms and reopening it as an administrator made no difference select serverpropertyproductversion returns theres another file that comes with the database called dbdata ini which says issql2000 so presumably it is meant to be loaded in sql server ill see if can get it working on
63506 have one table of services need to merge two select queries both have different where clauses for example select regn as region countcallid as openservices sumcase when descrption like dfc then else end dfc from oscl where status group by regn order by openservices desc this gives me result region openservices dfc karaci lahore islamabad have another query select regn as region countcallid as closedyesterday from oscl where datediffday closedate getdate group by regn order by closedyesterday desc it gives me result region closedservices karachi lahore islamabad need to merge both results and show closedservices beside the dfc column
63553 there has been change in our db and one of the sps has been deleted we have no idea who did it or when is there any way to find it out or enabling an option to be able to track this in future
63641 ive dealt with ms sql server datetime types for long time but never thought why the following is happening query table that contains smalldatetime column this smalldatetime is always returned in the format yyyy mm dd hh mm ss now write different query on which want to apply smalldatetime filter in the where clause something like where timestamp yyyy mm dd hh mm ss sql server retrieves an error and tells me that was not possible to convert that nvarchar to valid smalldatetime it appears that it only works if change the specified format and write it using the european format like where timestamp dd mm yyyy hh mm ss why is sql server showing me the dates in format that is not covertable or valid when applied back to itself dont have any problem in changing the date format when writing queries but want to play with these dates at an application level java jdbc app and dont want to be applying date format changes all the time could anyone explain me why this is happening and if there is any way to solve it at db level thanks edit please see the screenshot of the error in management studio below
63661 full question re write im looking for first aggregate function here found something that almost works create or replace function public first agg anyelement anyelement returns anyelement language sql immutable strict as select and then wrap an aggregate around it create aggregate public first sfunc public first agg basetype anyelement stype anyelement the problem is that when varcharn column passes through the first function its converted into simple varchar without size trying to return the query in function as returns setof anyelement get the following error error structure of query does not match function result type estado de sql detalhe returned type character varying does not match expected type character varying40 in column contexto pl pgsql function vsr table at timeanyelementtimestamp without time zone line at return query in the same wiki page there is link to version of the function that would replace the above dont know how to install it but wonder if this version could solve my problem meanwhile is there way can change the above function so it returns the exact same type of the input column
63676 have in the sql server lot of databases but one of them has been dropped so need to create trigger to send me an email when someone try to drop another database the mail should contain the user name and the the name of the database
63681 were in the process of phasing out an old system and migrating onto new one the last time that we phased out an old system we ran both systems in parallel and integrated data between both until everything was fully migrated out in our field during that process was able to build an integration between our legacy system and our new system that leveraged sql servers change data capture to track changes and integrate those over incrementally for this next migration the legacy system that we will be phasing out is based on mysql v5 instead of sql server am not familiar with mysql and was wondering if there are any technologies similar to cdc that can be leveraged on mysql in our current version or newer version that would be worth migrating towards
63825 is there are unified way of how to check for the existance of an index for given column irregardless of the actual sql database system used for mysql one could for instance check for the existance using show create table mytable in the result there would be something like this if column mycolumn has an index key index mycolumn is this indicator key unified among all sql database systems are there better ways to check for an index
64013 execute below command line statement to optimize table optimize table tablename is there any command or statement which can optimize all the tables one by one of the selected database
64084 is there way to copy maintenance plans from one sql server to another both servers are not accessible via the same copy of management studio at the same time on different networks
64256 ive been told that theres an oracle in memory option for 12c and that it uses columnar compression to get some great query speeds id like to leverage it for some business intelligence stuff ive been searching google and the only thing can find are announcements and news articles does anyone have any code snippets or any directions on where should look to implement this or learn more about it
64270 have table with column called json of type json within the json there is natural key select json id as id from limit id 63631ff3809de7a17398602f can create unique index on id thus create unique index id on tjson id create index id like to add this as table constraint using index but it fails for both primary key alter table add constraint pkey primary key using index id error index id contains expressions line alter table add constraint pkey detail cannot create primary key or unique constraint using such an index and unique alter table add constraint unique id unique using index id error index id contains expressions line alter table add constraint unique id detail cannot create primary key or unique constraint using such an index should be able to add such constraint
64727 made sql fiddle for this question if that makes things easier for anyone have fantasy sports database of sorts and what im trying to figure out is how to come up with current streak data like w2 if the team has won their last matchups or l1 if they lost their last matchup after winning the previous matchup or t1 if they tied their most recent matchup here is my basic schema create table fantasyteams team id bigint not null create table fantasymatches match id bigint not null home fantasy team id bigint not null away fantasy team id bigint not null fantasy season id bigint not null fantasy league id bigint not null fantasy week id bigint not null winning team id bigint null value of null in the winning team id column indicates tie for that match heres sample dml statement with some sample data for teams and weeks worth of matchups insert into fantasyteams select union select union select union select union select union select insert into fantasymatches select union select union select union select union select union select union select union select union select null go here is an example of the desired output based on the dml above that im having trouble even beginning to figure out how to derive team id steak type streak count ive tried various methods using subqueries and ctes but cant put it together id like to avoid using cursor as could have large dataset to run this against in the future feel like there might be way involving table variables that join this data to itself somehow but im still working on it additional info there could be varying number of teams any even number between and and the total matchups will increase by for each team every week any ideas on how should do this
64876 recently installed sql server express on workstation have and am trying to deploy to sql azure ive done this before on previous workstation using sql server express by doing the following right click the database tasks deploy database to sql azure on sql server the option is completely missing and is instead replaced new option deploy database to windows azure vm cant figure out why the option for deploying to sql azure is missing is there something that needs to be installed separately now for sql server from what ive read online both options should be present for editions did microsoft remove this option for sql server express
65142 have dataset of the following structure target polltime value null null null null null null null would like to build resultset that groups the null values showing the start end time and duration null means the target was unavialble target startdate enddate durationmin the startdate would be the first null value while the enddate would be the next non null value and my duration calcualtion would be based on those two values any clues to build this query would be great as am thinking of joining the table to itself but am not sure where to start
65257 assuming have database column containing table names how do safely delete all of the tables it lists the tables themselves are fairly simple they may have indeces but definitely dont contain other complex items and dont have dependencies on other tables ie foreign keys into out of them etc
65262 have dollar amount value that for historical reasons has always just been stored in an nvarchar field now that we are storing many more rows than we have in the past and are using this field to total up amounts using an application that will automatically refresh frequently im concerned about performance but im also concerned about the cost of making change like this late in the development cycle soon this database will be used by customer that will generate millions of rows all of these rows will not participate in this calculation the rows are divided into groups of around each parent of the group will be processed every minutes or so which is when these totals will be calculated is it worth it now to change the column type to money including the stored procedures udts data layer etc it seems like it could have performance impact but unfortunately cant generate enough volume with the resources have to do performance tests thta would be realistic so im hoping someone has experience with string to number conversion and can give me an idea if doing this conversion on rows at time will be problem
65351 have two identical mysql databases one in an internal server and the other in web hosting server want to update the database on the web host each day with the database on the internal server is there way to automate this process also how can do this manually if im to do it manually does it require me to get sql dump of the database on the internal server and then import it on the database on the web host can someone advice please
65609 feel kind of embarrassed here ive always used the terms column and field completely interchangeably which recently caused some confusion in technical discussion was told though that this wasnt correct that it should be translating each term into spreadsheet terminology ignoring data types and all the other stuff that make databases useful database column like spreadsheet column database record like spreadsheet row database field like spreadsheet cell specific column of specific row is this right could have sworn that column and field are used more interchangeably than that certainly have been so we dont add fields to table we add columns to table and fields are only relevant when talking about data within record other thoughts on column vs field edit to clarify the current context is ms sql server my background before sql server was ms access which might influence my use of these terms
65922 have table that has gotten little out of control am not dba per se but seem to recall that deleting huge volume of rows in one shot can cause transaction log issues hamper overall system performance during the delete etc is there an efficient way for me to create job that deletes records in small batches to over hampering other access performance and prevents problems with the transaction log this process can be quite slow is that makes difference for additional context the delete criteria will be based on something like where like blah also there is one clustered index and non clustered indexes
66014 am trying to connect to remote sql server on vpn in different domain when enter the server name on the sql server and choose additional connection parameters to add some extra stuff needed by my school integrated security sspi user id domain username password password get the following error login failed the login is from an untrusted domain and cannot be used with windows authentication
66118 lets say have table with deck of cards numbered could return the top and bottom cards as if held each side of the unions select query in my left and right hands by doing select top from deckofcards order by cardnumber desc union all select top from deckofcards order by cardnumber asc it would be an even split but how could have sql server intertwine the results returned as if had taken both portions of that union one half in my left hand and the other in my right and shuffled them once like deck of cards ie cardnumber followed by in the following sequence etc this is not homework question just one of those things that passes through my mind when trying to get some shut eye
66249 can anyone help me to find below given details for long running query processid process name database host user process login time query start time and query duration am looking for query or an sp which gives me this data
66387 am using postgres would like to change column type in all tables where column name is description from varchar255 to text if anyone knows that would be very glad for your help
66471 am trying to estimate the space requirements for central database server that will be collecting data from about identical field databases have the average daily row count for each table and now need to estimate the row size including indexes for each table is there such an animal in existence or do need to roll my own if do need to roll my own can you suggest good approach tia
66553 play basketball game which allows to output its statistics as database file so one can calculate statistics from it that are not implemented in the game so far ive had no problem caluclating the statistics wanted but now ive run into problem counting the number of double doubles and or triple doubles player made over the season from his game statistics the definition of double double and triple double is as follows double double double double is defined as performance in which player accumulates double digit number total in two of five statistical categories points rebounds assists steals and blocked shots in game triple double triple double is defined as performance in which player accumulates double digit number total in three of five statistical categories points rebounds assists steals and blocked shots in game quadruple double added for clarification quadruple double is defined as performance in which player accumulates double digit number total in four of five statistical categories points rebounds assists steals and blocked shots in game the playergamestats table stores statistics for each game player plays and looks as follows create table playergamestats as select from values nuggets cavaliers nuggets clippers nuggets trailblazers nuggets mavericks nuggets knicks nuggets jazz nuggets suns nuggets kings nuggets kings nuggets thunder as tidplayer idseasondayteamopponentpointsreboundsassistsstealsblocks the output want to achieve looks like this player id team doubledoubles tripledoubles nuggets the only solution found so far is so awful it makes me puke it looks like this select player id team sumcase whenpoints and rebounds or points and assists or points and steals then else end as doubledoubles from playergamestats group by player id and now youre probably also puking or laughing hard after reading this didnt even write out everything that would be needed to get all double double combinations and omitted the case statement for the triple doubles because its even more ridiculous is there better way to do this either with the table structure have or with new table structure could write script to convert the table can use mysql or postgresql here is link to sqlfiddle with example data and my awful solution posted above http sqlfiddle com af6101 note that im not really interested in quadruple doubles see above since they dont occur in the game play as far as know but it would be plus if the query is easily expandable without much rewrite to account for quadruple doubles
66616 select from aa update aa set city chennailastname vinoth id firstname lastname city abcrdrr vinoth chennai john vinoth chennai joe vinoth chennai raja vinoth chennai johsdfgn vinoth chennai have wrongly updated lastname city columns in all the rows now want to rollback to the old table rows using sql server 2008r2
66741 why does something like this not work select case when nullifcol lengthcustomers somecol is null then null else somecol end as mytest from customers am just checking if the column exists however sql server complains about somecol not existing is there an alternative to this in single statement
66840 trying to add not null constraint to table with billion rows cannot afford table lock for more than couple of seconds is there way to prevent full table scan during the alter table statement created an index on the column hoping it would be used but that doesnt seem to work may be check constraint other options thank you
67852 im trying recreate tables structure inside function by using some dynamic sql execute create table my table name bk like my table name that will be similar to create table my table bk like my table but need to discard all constraints using excluding constraints in the like options it still copy the not null constraints documentation confirms this behavior create table my table bk like my table excluding constraints the question is how can recreate the table structure without the not null constraints or in alternative remove all not null constraints in table
67875 have non forking game daemon written in perl which uses acync queries to write player stats into postgresql database but when need to read something from database like if player is banned or if the player has vip status then use synchronous queries this makes the game stop for short moment until the value has been read from the database can not rewrite my game daemon to use async queries for reading values tried but it required too many changes so my question is would it make sense to combine several unrelated queries that need to make when new player connects to procedure and how could return several values at the same time to my perl program my current queries all take player id as parameter and return value has the player been banned select true from pref ban where id what is the reputation of this player select countnullifnice false countnullifnice true as rep from pref rep where id is he or she special vip player select vip now as vip from pref users where id how many games has the player played to the end select completed from pref match where id to combine the above queries probably need procedure like this one create or replace function get user info id varchar returns xxx as body declare is banned boolean reputation integer is vip boolean completed games integer begin select into is banned from pref ban where id id select countnullifnice false countnullifnice true into reputation from pref rep where id id select vip now into is vip from pref users where id id select completed into completed games from pref match where id id return xxx how to return values here end body language plpgsql please help me to declare the above procedure properly
67985 how do find out which edition is installed without having the management studio installed have server that functions as license manager for another software upon investigation of high ram usage alert found that the sqlservr exe process is taking up almost gb of ram looked through the program menu and found that the configuration manager was installed otherwise it is pretty bare bones clicked on properties of the exe file and found but there is no place that ive found that states whether it is express dev stn ent etc if had to guess this is an express edition but wanted to know if there is an obvious tell tale sign update bob the file tells me what know not the edition valo get the following error when run that command and did verify named pipes was enabled hresult 0x35 level state named pipes provider could not open connection to sql server sqlcmd error microsoft sql server native client network related or instance specific error has occurred while establishing connection to sql server server is not found or not accessible check if instance name is correct and if sql server is configured to allow remote connections for more information see sql server books online sqlcmd error microsoft sql server native client login timeout expired thomas noticed the stock keeping unit name before asked the question but that seemed too easy guess my initial suspicion was correct
68077 pgadmin dialog for adding new database connections asks for maintenance db in order to be able to connect set it to the database want to connect and also have the rights to connect so why is it named maintenance db instead of db or database
68089 im trying to achieve the following california los angeles san francisco sacramento florida jacksonville miami unfortunately im getting los angeles san francisco sacramento jacksonville miami can achieve my desired results using the stuff function but was wondering if theres cleaner way of doing it using coalesce state city california san francisco california los angeles california sacramento florida miami florida jacksonville declare col nvarcharmax select col coalesce col city from tbl where city california select col thanks
68150 in oracle datafile are system files where actual data is stored the collection of datafiles make tablespace and at last database is collection of tablespaces correct me if am wrong on the concepts of datafile tablespace and database would like to understand the difference between schema and database in details online resources are helpful but seemed confusing regarding this difference
68264 is there an official postresql convention regarding capitalization in db table and field names the examples on the official site suggest lowercase and word separation and wonder whether this policy is official create table films code char5 constraint firstkey primary key title varchar40 not null did integer not null date prod date kind varchar10 len interval hour to minute
68266 what would be the right datatype to store email addresses in postgresql can use varchar or even text but wonder if there is more specific data type for emails
68639 its just been few months of programming in sql server for me so my knowledge is not good in many regards in an already existing project at work came across many tables with large composite primary keys with clustered index from what have gathered large column composite column with clustered index hits performance very hard and at times the logical solution is an identity column but at the same time have come across many people flaming the over usage of identity columns but have never came across an example where identity column is bad idea recently we have standardized that every table should have an identity column as the clustered index whether we use it as pk or not as we require it for some export purposes so would like some examples in real life scenarios where using an identity column as clustered index is bad idea though at times it makes our life easy have never encountered scenario where it will be considered bad ps think my question is bit naive but it is bugging me so much so had to ask about it
68700 this may sound like dumb question but am software developer new to database design so the concept makes sense to me but maybe this doesnt carry is it possible for field in record in table to simply point to field in record in table example if have two tables one containing list of all employees and another containing history of those employees coming and going from the office employees id first name last name history event id event time event type entry exit employeeid can establish relationship between history employeeid and employees id with foreign key but that duplicates data so then history employeeid and employees id would contain the same id number which would be stored in memory twice so if went into the database and changed john smiths employee id would need to write script or something to scour the database and do find replace for that id what want is for employees id to contain the real id and history employeeid to simply contain pointer to employees id that way if updated john smiths id the change would be centralized to one field in one table is this possible thanks
68771 am using oracle sql developer to manage my oracle database recently added third party database jdbc drivers to be able to connect to my new mysql and sql server installations need help with the right settings for the sql server connection username host name port including formats
68991 hope you can point me in the right direction im not frequent user of sql but did some googleing and found the script below corrected the script bit want the script to to select all databases except the system dbs to set recovery to simple to shrink the log files for every db ldf except the system db the script use master declare isql varchar2000 dbname varchar64 declare c1 cursor for select name from master sysdatabases where name not in mastermodelmsdbtempdbreportserverreportservertempdb open c1 fetch next from c1 into dbname while fetch status begin select isql alter database dbname set recovery simple select isql replace isql dbname dbname print isql exec isql select isql use dbname checkpoint select isql replace isql dbname dbname print isql exec isql select isql dbcc shrinkfile dbname ldf select isql replace isql dbname dbname print isql exec isql fetch next from c1 into dbname end close c1 deallocate c1
69043 am an accidental dba we have sql cluster that hosts databases suddenly the size of one of the database increased drastically from 100gb to 250gb when we checked the datafile the size had grown more than twice over the last few days we identified the tables and truncated the data and deleted 130gb worth of data the datafile is still showing 250gb how do we reclaim the space thanks lot for all your help
69081 have query that am using to find locations that are within 1km of known point to do this am using the spherical law of cosines formula with my latitude and longitudes currently the query runs over roughly records in about minutes this is acceptable but would like to make it as fast as possible the main query am using select loc base key from locations loc left outer join baseline base on base key in select key from baseline where dbo circledistanceloc lat base latitude loc long base longitude the circle distance scalar value function alter function circledistance add the parameters for the function here lat1 varchar250 lat2 varchar250 lng1 varchar250 lng2 varchar250 returns float as begin declare distance float declare lat1 float float declare lng1 float float declare lat2 float float declare lng2 float float select lat1 float cast lat1 as float select lng1 float cast lng1 as float select lat2 float cast lat2 as float select lng2 float cast lng2 as float select distance acossinradians lat1 float sinradians lat2 float cosradians lat1 float cosradians lat2 float cosradians lng2 float radians lng1 float return distance end go the execution plan with some anonymizing have indexed all columns named in both query and function some caveats am converting to float in the function because the data am getting arrives as string and am unable to change it prior to this point the left outer join is required because need to know locations that do not have known point am using the law of cosines rather than haversine because of its simplicity and dont require large amount of accuracy since am only interested in things within km am not dba myself merely net developer so am not really knowledgeable about improving query performance beyond indexing so any help is appreciated edit rolling in rob and marks answers allowed my to get my query execution down to seconds in case theres any further gains to be had here is the query now select columns from locations loc left outer join baseline base on base key in select key from baseline where base location geo stdistanceloc location geo the is earth radius so dont need to do any math when the query is run the location geo is computed column using the formula geography stgeomfromtextpoint longitude latitude is the spatial id for spherical earth model since im not looking for large distance this seems to make the query quicker to return also have spatial index on the location geo column as suggested here http social technet microsoft com wiki contents articles tuning spatial point data queries in sql server aspx
69354 im trying to get an idea of what recommended best practices are for the sql server cumulative updates currently we run on the idea of do nothing unless an issue fixed by the cu is one we experience that works from an if it aint broke dont fix it approach but im wondering if thats really good idea since so many of the cus have performance enhancements were looking at perhaps adding the cu to the patches applied during our periodic maintenance cycles month or two after the cu is released what do others do and why as an update to the question that affects the answers below on march microsofts sql server team announced they were updating their servicing model microsoft is recommending that all users install all cus released after january as of january cu releases these caution messages have been updated we now recommend ongoing proactive installation of cu as they become available you should plan to install cu with the same level of confidence you plan to install sps service packs as they are released this is because cu are certified and tested to the level of sp also microsoft css data indicates that significant percentage of customer issues are often previously addressed in released cu but not applied proactively more so cu contain added value over and above hotfixes these also may contain supportability logging and reliability updates enhancing the overall experience in addition to messaging and guidance updates we have made updates to the cu acquisition model acquisition changes cus of course have traditionally been made available on the hotfix server accompanied by the cautionary language associated with qfe or hotfix the inconsistency here is that cus are not really simple quick hotfixes anymore the encompassed updates are well tested at individual as well as full system integration levels today therefore we are now placing the latest cu per mainstream supported baseline sp2 sp3 and rtm sp1 today on microsoft com downloads just as is done for service packs today additionally we will soon release and maintain all cus into the windows update catalog to facilitate acquisition and distribution only interim cu on demand fixes will be placed on the hotfix server moving forward to reduce friction downloading cus from the microsoft com downloads will not require providing receiving an email and url we are also evaluating offering the latest cu as an optional update on microsoft update just like service packs today
69361 back in the days of yesteryear it was considered big no no to do select from table or select count from table because of the performance hit is this still the case in later versions of sql server im using but guess the question would apply to edit since people seem to be slating me slightly here im looking at this from benchmark academical point of view not whether its the right thing to do which of course its not
69467 have an old database users membership role that was setup automatically by an asp net application years ago the sql server version currently running is sql server the users database log file is huge the ldf file is approx times the size of the mdf file the recovery model is currently set to full understand what that is and dont need point in time restoration if simply changed the recovery model to simple from within sql server management studio and clicked ok to save the changes would be risking my current database in any way or is sql server fine with making changes like this to live databases and would the log file automatically shrink itself thanks for your advice mark
69655 have query like select id name json aggb as item from join on item id id group by id name how can select the columns in so dont have item id in the json object have read about row but it returns json object like f1 foo f2 bar would need to remap the json object once it is fetched to match the proper column keys id like to avoid that and keep original column names
69656 im using these steps to create table my user that already existed but somehow vanished from my database my db mysql use my db mysql drop table my user mysql error 42s02 unknown table my user mysql create table my user id int auto increment not null username varchar255 group id varchar255 default null primary keyid default character set utf8 collate utf8 unicode ci engine innodb mysql error hy000 cant create table my db my user errno tried mysqladmin flush tables and repeated the steps above but it wasnt helpful also restarted the mysql service but no good any ideas google has failed me so far thanks extra info mysql show engine innodb status latest foreign key error error in foreign key constraint of table my db my user there is no index in the table which would contain the columns as the first columns or the data types in the table do not match the ones in the referenced table or one of the on set null columns is declared not null constraint constraint fk cfbd431e285fac6d foreign key group id references my group id
69953 am working with the harmonized system and try to catalog this in mysql database this works for example like this live animals horses asses mules and hinnies live bovine animals live swine live purebred breeding animals weighing less than lb each weighing lb or more each sheep and goats live chickens ducks geese turkeys and guineas live animals live nesoi nesoi not elsewhere specified of indicated as you can see there is clear parent child relationship this relationship can always be traced back by the numbering that is used from this thread learned that best use text data field for the codes etc my question is should somewhere log the parent child relationship or would this not be necessary after all can do select from accounts where code like and get all the subaccounts just fine and feel that parent child table could make things more complicated but id like to hear what other people feel make sense
71393 ive been struggling little bit around performance troubleshooting including baseline and troubleshooting of sql performance could anyone help with this or point me to where can possibly get helpful information around this topic
71596 for some reason when try to open up my tables that are stored in frm and ibd files whether on mysql or phpmyadmin it gives me syntax error or it says it does not exist ive read the other post that had similar problem to this but dont know how to check if innodb file per table is enabled and im overall just really confused also converted copy of my mysql bin file to txt file so see that the data from my database is not completely lost the database was created last year have of those mysql bin files but for some reason the is the largest right now have the ibd and frm files for all my databases but im at loss as to how can restore it back into mysql or at least into something can read im using wampserver and mysql on windows server also am supposed to download plugin in innodb
71608 scenario youre handed database backup and told to restore it to server thats already hosting other databases but are given no useful information about what the backup contains or whether the source should be trusted question what are the potential implications of restoring backup that could well be malicious question what can you do to protect your server the data in other databases from the impact of restoring potentially malicious backup restore verifyonly would seem to be good first step the ultimate answer is probably restore the database in sandbox vm with no access to the outside world but lets assume that option is off the table what else should be done in this situation
71852 have three tables students table idpk student name nationality teachers table idpk teacher name email classroom table idpk date teacher idfk to teachers id student idfk to students id if was given teachers name david for example and student id for example and asked to insert the teacher id into the classroom table based on the id in the teachers table would do insert into classroom date teacher id student id select id from teachers where teacher name david now what if was not given the students id directly and given only the name of the student suppose was given teachers name david and students name sam how do get the teacher id from teachers table and also student id from the students table and insert both into the classroom table based on their respective names
71961 if use mysqldump single transaction according to the docs it should do flush tables with read lock to get consistent state and then start transaction and no writers should be waiting however have caught the following situation last night excerpt from show full processlist hundreds of those command query time state waiting for table flush info insert into db external notification then this command query time state sending data info select sql no cache from db external notification and the rest of the threads are in sleep does anyone have any idea what are these inserts waiting for dont see any flush tables or ddl or anything mentioned in the manual that can cause the queries to wait full mysqldump command mysqldump quick add drop table single transaction master data uxx pxx dbname guess quick is redundant here probably leftover from earlier times this script is very old but shouldnt hurt anyting
72083 so basically in sql server null means there is no value and thus cant be compared which returns some unexpected results for example the following query doesnt return rows where value is null but want it to select from table where value and date is null and last modified understand can do the following as workaround but seriously having to add parentheses and check if is null for every single field every single time want to include it seems ugly not intuitive and crazy select from table where value or value is null and date is null and last modified mean know null is not value and thus cant be compared but cant you infer that it is in fact definitely not if is something and null is nothing and nothing is not something then null is not seems logical to me does anyone know how can in cleaner way include nulls in my results when using comparisons without having to include an explicit check each and every time also turning off nulls on my tables is definitely not an option edit my real problem of why dont want to do it the way showed was not exposed so here goes am writing program that lets you build queries to database tables and lets the user dynamically create filters and such which essentially at the end of the day constructs sql statement and gets the results to display to the user the fields chosen by the user can be any and or all fields in given database and if have to literally put that isnull check on every single field that would be really inefficient and make looking at the sql super ugly my program is table definition agnostic meaning dont care whats in your table and dont want to know what your tables definition is just want you to pick table choose some fields to filter on with equals not equals in not in etc and then click button to view the results of that request
72134 have large database 16m rows containing perceptual hashes of images id like to be able to search for rows by hamming distance in reasonable timeframe currently as far as properly understand the issue think the best option here would be custom sp gist implementation that implements bk tree but that seems like lot of work and im still fuzzy on the practical details of properly implementing custom index calculating the hamming distance is tractable enough and do know though basically what is the appropriate approach here need to be able to query for matches within certain edit distance of hash as understand it levenshtein distance with strings of equal length is functionally hamming distance so there is at least some existing support for what want though no clear way to create an index from it remember the value im querying for changes cannot pre compute the distance from fixed value since that would only be useful for that one value the hashes are currently stored as char string containing the binary ascii encoding of the hash but can convert them to int64 easily enough the real issue is need to be able to query relatively fast it seems like it could be possible to achieve something along the lines of what want with the pg trgm but im bit unclear on how the trigram matching mechamism works in particular what does the similarity metric it returns actually represent it looks kind of like edit distance insert performance is not critical its very computationally expensive to calculate the hashes for each row so primarily care about searching
72330 it is fairly well documented that scalar udfs force an overall serial plan running functions in parallel given large number of rows coming into point in the pipeline where udf must be calculated why cant the engine just distribute them among the processors if there is no state within udf then the order shouldnt matter there are claims about udfs being black box must use cursor can see that user cursor cannot be parallelized within an sp for the cases where some state is maintained between iterations but seems like it should be parallelizable otherwise extra points for explaining why the engine forces the whole plan to be serial instead of just the udf calculation stage is support for parallel udf reasonable feature to request
72358 have little web application that is using sqlite3 as its db the db is fairly small right now am generating some content to display using the following query select dbid dlstate retreivaltime seriesname snip irrelevant columns from dataitems group by seriesname order by retreivaltime desc limit offset where limit is typically and offset is they drive pagination mechanism anyways right now this one query is completely killing my performance it takes approximately milliseconds to execute on table with 67k rows have indexes on both seriesname and retreivaltime sqlite select name from sqlite master where type index order by name snip irrelevant indexes dataitems seriesname index dataitems time index this is the index on retreivaltime yeah its poorly named however explain query plan seems to indicate theyre not being used sqlite explain query plan select dbid dlstate retreivaltime seriesname from dataitems group by seriesname order by retreivaltime desc limit offset scan table dataitems use temp tree for group by use temp tree for order by the index on seriesname is collate nocase if thats relevant if drop the group by it behaves as expected sqlite explain query plan select dbid dlstate retreivaltime seriesname from dataitems order by retreivaltime desc limit offset scan table dataitems using index dataitems time index basically my naive assumption would be that the best way to perform this query would be to walk backwards from latest value in retreivaltime and every time new value for seriesname is seen append it to temporary list and finally return that value that would have somewhat poor performance for cases where offset is large but that happens very rarely in this application how can optimize this query can provide the raw query operations if needed insert performance is not critical here so if need to create an additional index or two thats fine my current thoughts are commit hook that updates separate table that is used to track only unique items but that seems like overkill
72493 consider table of values and hashes like so field type null key default extra id int11 no pri null auto increment val char9 no null val hashed char50 yes null the following query finishes in seconds select from hashes order by desc limit however this query takes min seconds select val from hashes order by desc limit see that while the query is running the process list shows it as status sorting result the situation is completely reproducible note that there is another process performing insert operations on the table continuously why would the more specific query take longer to run than the query ive always believed that queries should be avoided specifically for performance reasons
72641 this has already been asked on stack overflow but only for mysql im using postgresql unfortunately and surprisingly postgresql does not seem to have something like checksum table postgresql solution would be fine but generic one would be better found http www besttechtools com articles article sql query to check two tables have identical data but dont understand the logic used background re wrote some database generating code so need to check whether the old and new code produce identical results
72750 im struggling with bulk importing quite big innodb table consisting of roughly million rows or 7gb which for me is the biggest table ive worked with so far did some research how to improve innos import speed and for the moment my setup looks like this etc mysql my cnf innodb buffer pool size of memory innodb read io threads innodb write io threads innodb io capacity innodb thread concurrency innodb doublewrite innodb log file size 1g log bin innodb autoinc lock mode innodb flush method direct innodb flush log at trx commit innodb buffer pool instances import is done via bash script here is the mysql code set global sync binlog set sql log bin set foreign key checks set unique checks set autocommit set session tx isolation read uncommitted load data local infile filepath into table monster commit data is provided in csv file currently test my settings with smaller test dumps with million million rows each and use time import script sh to compare performance drawback is only get overall running time so ive to wait for the full import to finish to get result my results so far rows second rows seconds rows seconds million rows minutes million rows minutes million rows cancelled after hours it seems there is no cookbook solution and one has to figure out the optimal mix of settings on their own besides suggestions about what to change in my set up also would really appreciate more information how could better benchmark the importing process gain more insight what is happening and where the bottleneck might be tried to read up the documentation for the settings im changing but then again im not aware of any side effects and if might even decrease performance with badly chosen value for the moment would like to try suggestion from chat to use myisam during import and change table engine afterwards id like to try this but for the moment my drop table query also takes hours to finish which seems another indicator my setting is less then optimal additional information the machine im currently using has 8gb of ram and solid state hybrid hard drive 5400rpm while we also aim to remove obsolete data from the table in question still need somewhat fast import to test automatic data cleanup feature while developing and in case our server crashes wed like to use our 2nd server as replacement which needs up to date data last import took more than hours mysql show create table monster row table monster create table create table monster monster id int11 not null auto increment ext monster id int11 not null default some id int11 not null default email varchar250 not null name varchar100 not null address varchar100 not null postcode varchar20 not null city varchar100 not null country int11 not null default address hash varchar250 not null lon float106 not null lat float106 not null ip address varchar40 not null cookie int11 not null default party id int11 not null status int11 not null default creation date datetime not null someflag tinyint1 not null default someflag2 tinyint4 not null upload id int11 not null default news1 tinyint4 not null default news2 tinyint4 not null someother id int11 not null default note varchar2500 not null referer text not null subscription int11 default hash varchar32 default null thumbs1 int11 not null default thumbs2 int11 not null default thumbs3 int11 not null default neighbours tinyint4 not null default relevance int11 not null primary key monster id key party id party id key creation date creation date key email email4 key hash hash8 key address hash address hash8 key thumbs3 thumbs3 key ext monster id ext monster id key status status key note note4 key postcode postcode key some id some id key cookie cookie key party id party idstatus engine innodb auto increment default charset utf8
72770 default mysql config file etc mysql my cnf installed by some debian package using apt often set log bin variable so binlog are enabled log bin var log mysql mysql bin log when want to disable binary logging on such installation comment out the line in my cnf works of course but wonder if there is way to disable binary logging by setting explicitely log bin to off in debian style mean in an included file like etc mysql conf mycustomfile cnf so default my cnf is not changed and can be easily updated by apt if necessary tried log bin log bin off or log bin but none works
72816 am creating clustered column store index in sql server am getting error as timeout expired the timeout period elapsed prior to completion of the operation or the server is not responding microsoft sql server set exec sp configure remote query timeout reconfigure exec sp configure row count data space mb
72858 use domain accounts for sql server service accounts sometimes when have multiple servers that are logically or thematically related ill use the same set of domain accounts for the service accounts on all of them the user account permissions on each server may be different but theres usually lot of overlap the gist of my question is this can user with access to one instance but not the other exploit the shared service accounts to gain access to the other server heres the specific situation im trying to address have two servers with default sql instances call them org sql and org sql use domain account for each service so id have mydomain orgdbservice mydomain orgagentservice etc although im not sure thats strictly relevant think id have the same question if used single account for all services but reuse these domain accounts on both servers so the database service on both org sql and org sql runs under mydomain orgdbservice for example to date ive never particularly questioned the security implications since these servers have common purpose and user base even if the specific user permissions may differ now were about to add third instance lets call it org sql rpt this one is good thematic fit with the others but theres one key difference were going to allow an external partner not an employee of our org access to this server like so their credentials will own belong to db owner couple databases but they wont belong to any server roles other than public of course they may have admin access to the os desktop for limited time prior to making this server live they wont know the passwords for the service accounts given that should worry about reusing the domain accounts as service accounts for this new instance is there any risk that this person could use their legitimate credentials on org sql rpt to gain access to either org sql or org sql any risk that could be mitigated by using different service account credentials or is this just generally bad idea for other reasons edit the new instance will be hosting database integration and reporting services for now at least no analysis services the external user will have elevated database privileges but no explicit instance permissions they wont be able to create logins jobs or reports for example
72864 know it is normal for the target db to be in restoring state also would like to do few queries as sanity check that the data is all up to date can take it to read only non restoring state temporarily to check the data and then put it back to restoring without breaking the whole restore chain thanks
72974 is there suggested way to rebuild an entire sql server database from another database in sql server sp2 let me explain we have database that we are trying to enable partial containment on and then add it to our alwayson cluster we are getting nothing but deadlock errors while trying to set the containment to partial the database has gone from sql server to 2008r2 to no sp sp1 sp2 and had tde for while microsoft support has been looking at it for couple of days but its not looking very promising and think were now leaning towards some form of corruption our application and everything else seems to be able to still access it just fine its just setting this one little flag isnt working very strange issue the only reference found online that shows the same exact issue is from technet forum post in portuguese translated link but it was never resolved at this point cant wait any longer and just want to rebuild this data in new database suppose could script the entire thing out but this db is around 20gb already so that would be one nasty script does anyone have suggestion on how to recreate new database based off of another without the standard backup restore is scripting it out the right way to go
73226 am developer student looking for some advice on what best approach should follow while creating table to model an entity that has number of optional fields there exists need to model an organization entity with few key fields such as id and name there is also join table from users to organization that specifies who belongs to the organization and what their roles are the question have pertains to number of optional fields that belong to the organization such as website email and social links below are my ideas of approaching the problem thus far add them to the table as optional fields pros easy crud on one table faster than navigating joins etc cons it seems bit dirty to me might it become difficult to do migrations in the future create contact information table that references an organization id references contact type id of website email facebook etc from lookup table and has generic value field for the actual content pros feels cleaner allows for an arbitrary number of contact types in the future cons probably lot slower tons more tables am leaning towards because it is similar approach to what im doing for physical addresses but im unsure whether or not it is the best solution as dba is not my forte if there is 3rd or even 4th option that am unaware of id be really interested to hear of those as well
73635 okay setting the scene have three tables table1 table2 and datatable and want to insert into table1 and table2 using datatable as source so for every row in datatable want row in table1 and table2 and table2 needs to have the inserted id pk from table1 if were to do this insert into table1 select from mytable insert into table2 select identity insert from mytable id get the id of the last inserted record into table1 is cursor or while loop the only ways to do this
73680 the current database assumes all times are local to the user now were changing this so the database stores everything in utc and our application converts the utc time in the database to the correct timezone for each user now we need to update every datetime in the database to utc time is there better way than having our application which has timezone library update each one individually due to daylight savings time bst simple dateadd isnt an option there are roughly million cells to update
73711 am trying to test the speed of sql server queries however once do the query once it becomes lot faster the database is live so can not make any changes to it only to the select query being performed is there any way to do select query without sql server optimizing it
73826 can install sql server management studio on my desktop to access database on sql server instance if so where can find the installer google searches only return express versions whereas am looking for the full sql server management studio
73850 we are using alwayson availability group feature of sql server regular full database backups and transaction log backups are done every day on the secondary database have read here doing the transaction log backup on either the primary replica or the secondary replica will mark both replicas transaction logs as reusable anyway the transaction log backup size is big and can be reduce using shrink file have restore the database locally and perform the shrink operation the log file size was reduces to mb my question is on which database should perform shrink operation over the transaction log file primary secondary or both guess in the past for several years no back ups of the log file are made so it become so huge executing dbcc sqlperf logspace can see that only of the file are used there is no point for me to keep such huge size of the log file in sys database files check that its max size is set to with growth to so guess when it need more space it will get anyway can shrink it to for example in order to prevent future growth am trying to find some confirmation that it is not bad idea to do so actually back ups on the database and the log files are performed only on the secondary databases so it will be easier to perform the shrink file on them but will the primary log file size be reduced as well
73921 before immediately marking as duplicate have read mike walshs why does the transaction log keep growing or run out of space but dont think it gave an answer to my situation looked through dozen or so similar questions but the relevant ones mostly just said duplicate and pointed to mikes question details have bunch of 500mb databases on sql server r2 all in simple recovery mode not my choice nightly full backups with 200mb data files and 300mb log files the log doesnt grow to 300mb immediately but rather slowly over the course of couple months there are no open transactions on any of them at least according to sp who2 and the activity monitor if right click on the database and select properties it tells me there is 50mb free particularly right after backup shouldnt the whole log be free in simple mode shouldnt the log be free as long as there isnt an open transaction log reuse wait desc from sys databases says says nothing which based on the question and answer referenced above says it shouldnt wait on anything to reuse the space if do dbcc shrinkfile the log file shrinks to 1mb so it is willing to reclaim the space can set something up that shrinks the logs weekly and keep things from getting out of control but im confused as to why sql server would make me do that can understand if there was some crazy transaction that needed 300mb to log it but were not doing anything extreme just basic oltp from mikes question answer simple recovery model so with the above introduction it is easiest to talk about simple recovery model first in this model you are telling sql server am fine with you using your transaction log file for crash and restart recovery you really have no choice there look up acid properties and that should make sense quickly but once you no longer need it for that crash restart recovery purpose go ahead and reuse the log file sql server listens to this request in simple recovery and it only keeps the information it needs to do crash restart recovery once sql server is sure it can recover because data is hardened to the data file more or less the data that has been hardened is no longer necessary in the log and is marked for truncation which means it gets re used it keeps saying the log space should be reused but with this slow growth over the course of months it doesnt seem that it is what am missing is something keeping sql server from recognizing the data as hardened and freeing up the log edit the after action report aka little knowledge is dangerous after finding that this is popular question it felt like owed an explanation of what happened months ago and what learned to hopefully save some other people some grief first off the space available you see in ssms when you view the properties on database is the space available in the data file you can view this by running the following on database and youll find the space available reported by ssms is the difference between the filesizemb and the usedspacemb select db name mf physical name mf type desc as filetype mf size as filesizemb filepropertymf name spaceused as usedspacemb mf name logicalname from sys master files mf join sys databases db on db database id mf database id where db name yourdatabasename this did confirm that under normal circumstances we were using very little log space 20mb or less but this leads into the second item second my perception of the logs growing was that of slowly over time however in reality the logs were growing rapidly on the nights the guy responsible for applying patches for this 3rd party application was applying patches the patch was done as single transaction so depending on the patch the 200mb data needed 300mb of log the key in tracking that down was the query from aaron bertrand at https sqlblog org reviewing autogrow events from the default trace declare path nvarchar260 select path reversesubstringreverse path charindex reverse path nlog trc from sys traces where is default select databasename filename spid duration starttime endtime filetype case eventclass when then data when then log end from sys fn trace gettable path default where eventclass in order by starttime desc this showed that log was growing on certain evenings when the customer wasnt using the database that led to the conversation with the guy applying the patches and the answer to the mystery thanks again for people who provided help to get me to the answer
74283 given softwarereleases table id version how do produce this output id version
74306 am using the following command to add constraints to one of the raster image in postgis postgresql alter table schema1 table1 add constraint enforce scalex rast unique rast but getting the following errors error data type raster has no default operator class for access method btree hint you must specify an operator class for the index or define default operator class for the data type kindly someone help me to fix this error up have no basic idea about the operator classes thx zia
74343 have dropped sql server windows login then ran the below code to check for orphaned database users however the database user corresponding to the dropped windows login does not appear as an orphaned user why would this be exec sp change users login action report
74457 tl dr prove that in practice the execution of the alter table table name drop foreign key constraint name statement does not corrupt existing data the important consideration is the execution of the statement itself consider data changes after the fact irrelevant by analogy opening the stable door is the harmful action not the horse bolting my boss and are having difference of opinion about whether or not to use foreign keys in mysql innodb database im for using them for enforcing ri and taking advantage of on delete cascade and on update restrict set null while hes against confident that he can enforce ri at the application level his argument is chiefly drupal doesnt use them and gets along fine without them so why should we theyre too inflexible when you need to change data and or structure hes removed them from existing tables to change things and its caused data corruption that was only noticeable weeks or months later on high traffic high activity sites so hed rather not use them my arguments are chiefly not all of the databases db engines supported by drupal myisam sqlite enforce fks by default so drupal leaves them as documented only whereas this might be different in d8 yes they can be pain to deal with but perhaps the fault lies with poor planning designing on the part of the developer not the fks surely not enforcing ri with fks at the dbms level is more likely to cause data corruption than otherwise case in point is d6s user reference module not restricting changing user status when existing content references user that must have certain status its waste of time and resources to make code do attempt what the dbms already does how can he guarantee that his code will work as well as or better than the dbms essentially hell come over to my way of thinking if can prove that the act of removing foreign key itself regardless of subsequent data inserts updates wont corrupt existing data cant see way clear to prove their usefulness advantage because im not sure how would satisfactorily observe document the effect drop foreign key would have immediately after execution other than comparing data from before execution to data after execution so how do persuade my boss that we should use foreign keys by proving that the act of later removal alteration of them wont corrupt existing data whereas using them will not cause any issues how would best set up practical usage tests note dont so much want to prove that am right from an egotistical point of view to use fks but that it benefits the data and the application code more to use them at the dbms level than at the application code level
74461 want to use parameter within where clause only if its value is provided by strongly typed dataset this is what am trying at the moment get right results when provide parameter3 and no results when dont provide its value what desire is when provide no value for parameter3 it should not use it in the query as its value is null and want to see all of the results in the query not where paramerter3 null ones alter procedure dbo getdata parameter1 varchar256 parameter2 varchar256 parameter3 int null as select from table1 where table1 url like parameter1 and table1 id parameter2 and parameter3 is null or table1 id2 parameter3 order by table1 title edit tried thomas answer and executed like this exec return value dbo getdata parameter1 nasda parameter2 nasda parameter3 null select return value return value go also updated the stored procedure as thomas said
74517 im trying to create postgres trigger to ensure after columns value has been set it cannot be updated basically make it readonly so far ive come up with the following draft trigger first am on the correct track is there value in returning new must preface original id with new create function check id change returns trigger as begin if old original id new original id then raise exception cannot change original id end if return new end language plpgsql create trigger client update trigger after update on client for each row execute procedure check id change
74627 have two tables in mysql database parent child im trying to add foreign key references to my child table based on the parent table is there any significant difference between on update cascade and on delete cascade my parent table create table parent id int not null primary key id engine innodb my question is what is the difference between the following sql queries on delete cascade create table child id int parent id int index par ind parent id foreign key parent id references parentid on delete cascade engine innodb on update cascade create table child id int parent id int index par ind parent id foreign key parent id references parentid on update cascade engine innodb on update cascade on delete cascade create table child id int parent id int index par ind parent id foreign key parent id references parentid on update cascade on delete cascade engine innodb are there any errors in the queries what do these queries mean are they same
74679 it is my understanding that can use the execute as owner clause as part of procedure that create to make the body of that procedure run as different user my goal is to execute command that requires the sysadmin role dbcc traceon1224 this procedure is supposed to be called by an unprivileged user ran the following script under the sa user select user name user id issysadmin is srvrolemembersysadmin dbo if existsselect from sys procedures where name myproc drop procedure myproc go create procedure myproc with execute as owner as select user name user id issysadmin is srvrolemembersysadmin dbo dbcc traceon1224 msg level state procedure myproc line user dbo does not have permission to run dbcc traceon return go exec myproc output is inline in comments it turns out that outside of the procedure seem to have sysadmin membership but not inside the procedure the procedure is owned by the dbo user understand that it is not possible to grant the sysadmin role to database user at least the gui doesnt offer this possibility so dont see how could ever make database user have server role also tried execute as sa which results in cannot execute as the user sa because it does not exist or you do not have permission the documentation states that can only specify user name not login name so understand why that didnt work how can run my procedure with sysadmin role membership
74752 have table employee id name salary city ram c1 sham c2 jadu c1 madhu c4 hari c2 gopal c3 komal c3 bappa c4 query is which city earning the highest tried select city sumsalary as maxsalary from employee group by city order by salary desc limit it works fine but if there are more than one max earning cities then it doesnt output other max cities only the first one so tried this query select city maxtotalsalary maxsalary from select city sumsalary as totalsalary from employee group by city as temptable it is giving city max c1 but correct is city max c4 it means it is outputting the first most city name of the table which is c1 but not the correct max earning city name which is c4 what is the right query
74773 how can group by one column while sorting only by another im trying to do the following select dbidretreivaltime from fileitems where sourcesite something group by seriesname order by retreivaltime desc limit offset want to select the last items from fileitems in descending order with the rows filtered by distinct values of seriesname the above query errors out error column fileitems dbid must appear in the group by clause or be used in an aggregate function need the dbid value in order to then take the output of this query and join it on the source table to get the rest of the columns wasn note this is basically the gestalt of the below question with lot of the extraneous details removed for clarity original question have system im migrating from sqlite3 to postgresql because ive largely outgrown sqlite select dbid dlstate sourcesite snip bunch of rows note from fileitems as join select dbid from fileitems where sourcesite something group by seriesname order by maxretreivaltime desc limit offset as di on di dbid dbid order by retreivaltime desc basically want to select the last distinct items in the database where the distinct constraint is on one column and the sorting order is on different column unfortunately the above query while it works fine in sqlite errors out in postgresql with the error psycopg2 programmingerror column fileitems dbid must appear in the group by clause or be used in an aggregate function unfortunately while adding dbid to the group by clause fixes the issue group by seriesnamedbid it means the distinct filtering on the query results no longer work since dbid is the database primary key and as such all values are distinct from reading the postgres documentation there is select distinct on nnn but that requires that the returned results be sorted by nnn therefore to do what id want via select distinct on id have to query for all distinct nnn and their maxretreivaltime sort again by retreivaltime rather then nnn then take the largest and query using those against the table to get the rest of the rows which id like to avoid as the database has 175k rows and 14k distinct values in the seriesname column only want the latest and this query is somewhat performance critical need query times second my naive assumption here is basically the db needs to just iterate over each row in descending order of retreivaltime and simply stop once its seen limit items so full table query is non ideal but dont pretend to really understand how the database system optimizes internally and may be approaching this completely wrong fwiw do occasionally use different offset values but long query times for cases where offset is completely acceptable basically offset is crappy paging mechanism that lets me get away without needing to dedicate scrolling cursors to each connection and ill probably revisit it at some point ref question asked month ago that lead to this query ok more notes select dbid dlstate sourcesite snip bunch of rows note from fileitems as join select seriesname maxretreivaltime as max retreivaltime from fileitems where sourcesite something group by seriesname order by max retreivaltime desc limit offset as di on di seriesname seriesname and di max retreivaltime retreivaltime order by retreivaltime desc works correctly for the query as described but if remove the group by clause it fails its optional in my application psycopg2 programmingerror column fileitems seriesname must appear in the group by clause or be used in an aggregate function think im fundamentally not understanding how subqueries work in postgresql where am going wrong was under the impression that subquery is basically just inline function where the results are just fed into the main query
74879 postgresql debian have lot of huge indexes in legacy database im trying to optimize thinking about dropping all the useless ones but how can tell how often they are used and if they are not used at all is there any usage statistics somewhere or some trick query to do that
75091 have quite an annoying problem want to use innodb as my main database engine and give up on myisam as need the former for using galera cluster for redundancy copied description follows the newbb post table to new table called newbb innopost and changed that to innodb the tables currently hold entries each running these selects on freshly started database so no caching is involved at this point the database yields the following results omitting the complete output please note that do not even ask the database to sort the results select post postid post attach from newbb post as post where post threadid rows in set sec select post postid post attach from newbb innopost as post where post threadid rows in set min sec seconds to seconds am wondering why this is happening did read some answers here on stackexchange involving innodb and some are suggesting increasing innodb buffer pool size to of installed ram this wont solve the problem that the initial query to particular id will take at least 50x longer and stalling the whole websever queueing up connections and queries for the database afterwards the cache buffer might kick in but there are over threads in this database so it is very likely that the cache will never hold all the relevant queries to be served the queries above are simple no joins and all keys are used explain select post postid post attach from newbb innopost as post where post threadid id select type table type possible keys key key len ref rows extra simple post ref threadidthreadid 2threadid visible dateline threadid const this is the myisam table create table newbb post postid int10 unsigned not null auto increment threadid int10 unsigned not null default parentid int10 unsigned not null default username varchar100 not null default userid int10 unsigned not null default title varchar250 not null default dateline int10 unsigned not null default pagetext mediumtext allowsmilie smallint6 not null default showsignature smallint6 not null default ipaddress varchar15 not null default iconid smallint5 unsigned not null default visible smallint6 not null default attach smallint5 unsigned not null default infraction smallint5 unsigned not null default reportthreadid int10 unsigned not null default importthreadid bigint20 not null default importpostid bigint20 not null default converted utf8 int11 not null htmlstate enumoffonon nl2br not null default on nl2br primary key postid key threadid threadiduserid key importpost index importpostid key dateline dateline key threadid threadidvisibledateline key converted utf8 converted utf8 key threadid visible dateline threadidvisibledatelineuseridpostid key ipaddress ipaddress key userid useridparentid key user date useriddateline engine myisam auto increment default charset latin1 and this is the innodb table its exactly the same create table newbb innopost postid int10 unsigned not null auto increment threadid int10 unsigned not null default parentid int10 unsigned not null default username varchar100 not null default userid int10 unsigned not null default title varchar250 not null default dateline int10 unsigned not null default pagetext mediumtext allowsmilie smallint6 not null default showsignature smallint6 not null default ipaddress varchar15 not null default iconid smallint5 unsigned not null default visible smallint6 not null default attach smallint5 unsigned not null default infraction smallint5 unsigned not null default reportthreadid int10 unsigned not null default importthreadid bigint20 not null default importpostid bigint20 not null default converted utf8 int11 not null htmlstate enumoffonon nl2br not null default on nl2br primary key postid key threadid threadiduserid key importpost index importpostid key dateline dateline key threadid threadidvisibledateline key converted utf8 converted utf8 key threadid visible dateline threadidvisibledatelineuseridpostid key ipaddress ipaddress key userid useridparentid key user date useriddateline engine innodb auto increment default charset latin1 server with 32gb ram server version mariadb trusty wsrep log mariadb org binary distribution wsrep r4002 if you need all of the innodb variables setting can attach that to this post update dropped all indexes apart from the primary index afterwards the result looked like this rows in set sec explain select post postid post attach from newbb innopost as post where post threadid id select type table type possible keys key key len ref rows extra simple post all null null null null using where row in set sec after this just added one index back to the mix threadid the results were the following rows in set sec explain select post postid post attach from newbb innopost as post where post threadid id select type table type possible keys key key len ref rows extra simple post ref threadid threadid const row in set sec strange it is that without any relevant indexes the full scan only took seconds compared to the seconds using indexes with only one perfectly tailored index it is still taking seconds by to complete still far too slow for any real world usage update setup mysql 0ubuntu0 ubuntu on another server with the exact same hardware configuration and exactly the same database tables the results are nearly the same first the myisam table rows in set sec and this is the result of the innodb table rows in set min sec update the contents of my cnf mariadb database server configuration file you can copy this file to one of etc mysql my cnf to set global options my cnf to set user specific options one can use all long options that the program supports run program with help to get list of available options and with print defaults to see which it would actually understand and use for explanations see http dev mysql com doc mysql en server system variables html this will be passed to all mysql clients it has been reported that passwords should be enclosed with ticks quotes escpecially if they contain chars remember to edit etc mysql debian cnf when changing the socket location client port socket var run mysqld mysqld sock here is entries for some specific programs the following values assume you have at least 32m ram this was formally known as safe mysqld both versions are currently parsed mysqld safe socket var run mysqld mysqld sock nice mysqld basic settings user mysql pid file var run mysqld mysqld pid socket var run mysqld mysqld sock port basedir usr datadir var lib mysql tmpdir tmp lc messages dir usr share mysql lc messages en us skip external locking instead of skip networking the default is now to listen only on localhost which is more compatible and is not less secure bind address fine tuning max connections connect timeout wait timeout max allowed packet 16m thread cache size sort buffer size 4m bulk insert buffer size 16m tmp table size 32m max heap table size 32m myisam this replaces the startup script and checks myisam tables if needed the first time they are touched on error make copy and try repair myisam recover backup key buffer size 128m open files limit table open cache myisam sort buffer size 512m concurrent insert read buffer size 2m read rnd buffer size 1m query cache configuration cache only tiny result sets so we can fit more in the query cache query cache limit 128k query cache size 64m for more write intensive setups set to demand or off query cache type demand logging and replication both location gets rotated by the cronjob be aware that this log type is performance killer as of you can enable the log at runtime general log file var log mysql mysql log general log error logging goes to syslog due to etc mysql conf mysqld safe syslog cnf we do want to know about network errors and such log warnings enable the slow query log to see queries with especially long duration slow query log slow query log file var log mysql mariadb slow log long query time log slow rate limit log slow verbosity query plan log queries not using indexes log slow admin statements the following can be used as easy to replay backup logs or for replication note if you are setting up replication slave see readme debian about other settings you may need to change server id report host master1 auto increment increment auto increment offset log bin var log mysql mariadb bin log bin index var log mysql mariadb bin index not fab for performance but safer sync binlog expire logs days max binlog size 100m slaves relay log var log mysql relay bin relay log index var log mysql relay bin index relay log info file var log mysql relay bin info log slave updates read only if applications support it this stricter sql mode prevents some mistakes like inserting invalid dates etc sql mode no engine substitutiontraditional innodb innodb is enabled by default with 10mb datafile in var lib mysql read the manual for more innodb related options there are many default storage engine innodb you cant just change log file size requires special procedure innodb log file size 50m innodb buffer pool size 20g innodb log buffer size 8m innodb file per table innodb open files innodb io capacity innodb flush method direct security features read the manual too if you want chroot chroot var lib mysql for generating ssl certificates recommend the openssl gui tinyca ssl ca etc mysql cacert pem ssl cert etc mysql server cert pem ssl key etc mysql server key pem mysqldump quick quote names max allowed packet 16m mysql no auto rehash faster start of mysql but no tab completition isamchk key buffer 16m important additional settings that can override those from this file the files must end with cnf otherwise theyll be ignored includedir etc mysql conf and the contents of the inno variables mariadb none show variables like inno variable name value innodb adaptive flushing on innodb adaptive flushing lwm innodb adaptive hash index on innodb adaptive hash index partitions innodb adaptive max sleep delay innodb additional mem pool size innodb api bk commit interval innodb api disable rowlock off innodb api enable binlog off innodb api enable mdl off innodb api trx level innodb autoextend increment innodb autoinc lock mode innodb buffer pool dump at shutdown off innodb buffer pool dump now off innodb buffer pool filename ib buffer pool innodb buffer pool instances innodb buffer pool load abort off innodb buffer pool load at startup off innodb buffer pool load now off innodb buffer pool populate off innodb buffer pool size innodb change buffer max size innodb change buffering all innodb checksum algorithm innodb innodb checksums on innodb cleaner lsn age factor high checkpoint innodb cmp per index enabled off innodb commit concurrency innodb compression failure threshold pct innodb compression level innodb compression pad pct max innodb concurrency tickets innodb corrupt table action assert innodb data file path ibdata1 12m autoextend innodb data home dir innodb disable sort file cache off innodb doublewrite on innodb empty free list algorithm backoff innodb fake changes off innodb fast shutdown innodb file format antelope innodb file format check on innodb file format max antelope innodb file per table on innodb flush log at timeout innodb flush log at trx commit innodb flush method direct innodb flush neighbors innodb flushing avg loops innodb force load corrupted off innodb force recovery innodb foreground preflush exponential backoff innodb ft aux table innodb ft cache size innodb ft enable diag print off innodb ft enable stopword on innodb ft max token size innodb ft min token size innodb ft num word optimize innodb ft result cache limit innodb ft server stopword table innodb ft sort pll degree innodb ft total cache size innodb ft user stopword table innodb io capacity innodb io capacity max innodb kill idle transaction innodb large prefix off innodb lock wait timeout innodb locking fake changes on innodb locks unsafe for binlog off innodb log arch dir innodb log arch expire sec innodb log archive off innodb log block size innodb log buffer size innodb log checksum algorithm innodb innodb log compressed pages on innodb log file size innodb log files in group innodb log group home dir innodb lru scan depth innodb max bitmap file size innodb max changed pages innodb max dirty pages pct innodb max dirty pages pct lwm innodb max purge lag innodb max purge lag delay innodb mirrored log groups innodb monitor disable innodb monitor enable innodb monitor reset innodb monitor reset all innodb old blocks pct innodb old blocks time innodb online alter log max size innodb open files innodb optimize fulltext only off innodb page size innodb print all deadlocks off innodb purge batch size innodb purge threads innodb random read ahead off innodb read ahead threshold innodb read io threads innodb read only off innodb replication delay innodb rollback on timeout off innodb rollback segments innodb sched priority cleaner innodb show locks held innodb show verbose locks innodb sort buffer size innodb spin wait delay innodb stats auto recalc on innodb stats method nulls equal innodb stats on metadata off innodb stats persistent on innodb stats persistent sample pages innodb stats sample pages innodb stats transient sample pages innodb status output off innodb status output locks off innodb strict mode off innodb support xa on innodb sync array size innodb sync spin loops innodb table locks on innodb thread concurrency innodb thread sleep delay innodb track changed pages off innodb undo directory innodb undo logs innodb undo tablespaces innodb use atomic writes off innodb use fallocate off innodb use global flush log at trx commit on innodb use native aio on innodb use stacktrace off innodb use sys malloc on innodb version innodb write io threads rows in set sec the number of cores of the machine is its intelr xeonr cpu e3 v3 50ghz as of proc cpuinfo one last note ran the queries with the indexes suggested by rolandomysqldba and the queries took about 20s each do want to point out that it is crucial for me this is the main table of bulletin board that the first query about threadid returns in less than second as there are more than threads and google bots constantly crawl these threads
75142 is there straightforward way to adapt these types of mysql queries to postgresql setting variables in mysql like set aintconst set arealconst it seems not assigning variables from select queries and using those variables subsequently in my sql like select pfid id from platform where bios like intel select clientid id from client where platformid pfid id be very grateful for pointers especially on
75214 the error in its entirety reads psql could not connect to server no such file or directory is the server running locally and accepting connections on unix domain socket tmp pgsql this is my second time setting up postgresql via homebrew on my mac and have no clue what is going on previously it had been working at some point mustve entered command that messed things up im not sure now whenever enter sql command from the command line receive the above message ive run command to check whether the server is running and it apparently is not if attempt to start the server using postgres usr local pgsql data receive the following error postgres cannot access the server configuration file usr local pgsql data postgresql conf no such file or directory ive uninstalled and reinstalled postgresql via homebrew but the problem persists im completely at loss as to how to get this working any help would be appreciated
75335 know that the numeric and decimal data types in sql server work the same the syntax for creating them is the same the ranges of values you can store in them is the same etc however the msdn documentation describes the relationship between the two as this numeric is functionally equivalent to decimal normally when see the the qualifier functionally equivalent it means that the two things arent exactly the same but that they are two different types that are indistinguishable from the outside is this implication true are there differences between numeric and decimal that just happen to behave the same to an outside observer or are they actually equivalent is numeric just legacy synonym for decimal
75408 im investigating the benefits of upgrading from ms sql to one of the big selling points of sql is the memory optimized tables which apparently make queries super fast ive found that there are few limitations on memory optimized tables such as no max sized fields maximum 1kb per row no timestamp fields no computed columns no unique constraints these all qualify as nuisances but if really want to work around them in order to gain the performance benefits can make plan the real kicker is the fact that you cant run an alter table statement and you have to go through this rigmarole every time you so much as add field to the include list of an index moreover it appears that you have to shut users out of the system in order to make any schema changes to mo tables on the live db find this totally outrageous to the extent that actually cannot believe that microsoft could have invested so much development capital into this feature and left it so impractical to maintain this leads me to the conclusion that must have gotten the wrong end of the stick must have misunderstood something about memory optimized tables that has led me to believe that it is far more difficult to maintain them than it actually is so what have misunderstood have you used mo tables is there some kind of secret switch or process that makes them practical to use and maintain
75439 problem an instance of mysql running mostly just database with innodb tables is exhibiting occasional stalls for all update operations for the duration of minutes with all insert update and delete queries remaining in query end state this obviously is most unfortunate the mysql slow query log is logging even the most trivial queries with insane query times hundreds of them with the same timestamp corresponding to the point in time where the stall has been resolved query time lock time rows sent rows examined set timestamp insert into sessions redirect login2 data hostname fk users primary fk users id sessions timestamp values null null null anonymous 64ef367018099de4d4183ffa3bc0848a and the device statistics are showing increased although not excessive load in this time frame in this case updates were stalling according to the timestamps from the statement above sar pm dev tps rd sec wr sec avgrq sz avgqu sz await svctm util pm dev8 pm dev8 pm dev8 pm dev8 pm dev8 sar pm cpu user nice system iowait steal idle pm all pm all pm all pm all pm all more often than not notice in the mysql slow log that the oldest query stalling is an insert into large ish rows table with varchar primary key and full text search index create table files id files varchar32 not null default filename varchar100 not null default content text primary key id files key filename filename fulltext key content content engine innodb default charset latin1 further investigation show engine innodb status has shown that it indeed always is an update to table using full text indexes which is causing the stall the respective transactions section of show engine innodb status has entries like these two for the oldest running transactions transaction active sec doing sync index lock structs heap size row locks undo log entries table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix table lock table vw fts 000000000000224a 00000000000036b9 index trx id lock mode ix transaction active prepared sec committing mysql tables in use locked lock structs heap size row locks undo log entries mysql thread id os thread handle 0x7fe0e239c700 query id root query end insert into files id files filename content values f19e63340fad44841580c0371bc51434 file 70380a686effd6b66592bb5eeb3d9b06 doc table lock table vw files trx id lock mode ix so there is some heavy full text index action going on there doing sync index stopping all subsequent updates to any table from the logs it seems bit like the undo log entries number for doing sync index is advancing at until it reaches at which point the operation is done the fts size of this specific table is quite impressive du fts 000000000000224a 00000000000036b9 fts 000000000000224a 00000000000036b9 index ibd fts 000000000000224a 00000000000036b9 index ibd fts 000000000000224a 00000000000036b9 index ibd fts 000000000000224a 00000000000036b9 index ibd fts 000000000000224a 00000000000036b9 index ibd fts 000000000000224a 00000000000036b9 index ibd total although the issue is also triggered by tables with significantly less massive fts data size like this one du fts 0000000000003a21 index fts 0000000000003a21 index ibd fts 0000000000003a21 index ibd fts 0000000000003a21 index ibd fts 0000000000003a21 index ibd fts 0000000000003a21 index ibd fts 0000000000003a21 index ibd total the time of the stall in those cases is roughly the same too have opened bug on bugs mysql com so the devs could look into this the nature of the stalls first made me suspect log flushing activity to be the culprit and this percona article on log flushing performance issues with mysql is describing very similar symptoms but further occurrences have shown that insert operations into the single myisam table in this database are affected by the stall as well so this does not seem like an innodb only issue nonetheless decided to track the values of log sequence number and pages flushed up to from the log section outputs of show engine innodb status every seconds it indeed does look like flushing activity is ongoing during the stall as the spread between the two values is decreasing mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference mon sep cest lsn pages flushed difference and at the spread has reached its minimum so flushing activity seems to have ceased here just coinciding with the end of the stall but these points made me dismiss the innodb log flushing as the cause for the flushing operation to block all updates to the database it needs to be synchronous which means that of the log space has to be occupied it would be preceded by an asynchronous flushing phase starting at innodb max dirty pages pct fill level which am not seeing the lsns keep increasing even during the the stall so log activity is not ceasing completely myisam table inserts are affected as well the page cleaner thread for adaptive flushing seems to do its work and flush the logs without causing dml queries to stop numbers are log sequence number pages flushed up to from show engine innodb status the issue seems somewhat alleviated by setting innodb adaptive flushing lwm forcing the page cleaner to do more work than before the error log has no entries coinciding with the stalls show innodb status excerpts after approximately hours of operation look like this semaphores os wait array info reservation count os wait array info signal count mutex spin waits rounds os waits rw shared spins rounds os waits rw excl spins rounds os waits spin rounds per wait mutex rw shared rw excl latest detected deadlock 7fe0e2e44700 file os file reads os file writes os fsyncs reads avg bytes read writes fsyncs row operations queries inside innodb queries in queue read views open inside innodb main thread process no id state sleeping number of rows inserted updated deleted read inserts updates deletes reads so yes the database does have deadlocks but they are very infrequent the latest has been handled around hours before the stats have been read tried tracking the semaphores section values over period of time especially in situation of normal operation and during stall wrote small script checking the mysql servers processlist and running couple of diagnostic commands into log output in case of an obvious stall as the numbers have been taken over different time frames have normalized the results to events second normal stall 1h avg 1m avg os wait array info reservation count signal count mutex spin waits rounds os waits rw shared spins rounds os waits rw excl spins rounds os waits am not quite sure about what am seeing here most numbers have dropped by an order of magnitude probably due to ceased update operations mutex spin waits and mutex spin rounds however have both increased by the factor of investigating this further the list of mutexes show engine innodb mutex has mutex entries listed both in normal operation as well as during stall have enabled innodb status output locks to see if it is going to give me more detail configuration variables ive tinkered with most of them without definite success mysql show global variables where variable name like innodb adaptive flush variable name value innodb adaptive flushing on innodb adaptive flushing lwm mysql show global variables where variable name like innodb max dirty pages pct variable name value innodb max dirty pages pct innodb max dirty pages pct lwm mysql show global variables where variable name like innodb log variable name value innodb log buffer size innodb log compressed pages on innodb log file size innodb log files in group innodb log group home dir mysql show global variables where variable name like innodb double variable name value innodb doublewrite on mysql show global variables where variable name like innodb buffer pool variable name value innodb buffer pool dump at shutdown off innodb buffer pool dump now off innodb buffer pool filename ib buffer pool innodb buffer pool instances innodb buffer pool load abort off innodb buffer pool load at startup off innodb buffer pool load now off innodb buffer pool size mysql show global variables where variable name like innodb io capacity variable name value innodb io capacity innodb io capacity max mysql show global variables where variable name like innodb lru scan depth variable name value innodb lru scan depth things already tried disabling the query cache by set global query cache size increasing innodb log buffer size to 128m playing around with innodb adaptive flushing innodb max dirty pages pct and the respective lwm values they were set to defaults prior to my changes increasing innodb io capacity and innodb io capacity max setting innodb flush log at trx commit running with innodb flush method direct yes we do use san with persistent write cache setting the sys block sda queue scheduler to noop or deadline
75451 how to list all tables in the current database together the number of rows of the table in other words can you think of query to come up with something like this in mysql tables in database number of rows database database different approaches are welcome
75550 am fairly new to db design and development my requirement is simple drilldown slicing based on time and language of words in language on particular day however my db is mysql but so far have no luck of running these kind of queries so am manually calculating this data and storing in tables to be more specific of my application need to show charts graphs on iphone so users usage of words per language per day week month dont need realtime but the rows for one month of usage for user is approx want to understand if it is possible for mysql to have warehouse scehemas and execute such queries without performance issues what could be my options postgres mysql enterprise cubes really cant pay for enterprise solutions am ready to write code for this kind of processing in my application which have but want to migrate to correct way guess the most ideal case for me would be that without using any tools can create fact and dimensions in my current db and then shift my api to run these olap queries from what have gathered this is not possible for mysql postgres is showing some promise but still reading
75821 lets say for sqlserver2008r2 and higher with full recovery mode databases always thought when transaction is commited commit the transaction is written to the transaction log in ram when checkpoint occurs after some time and or some transactions and other criterias the transactions between the last checkpoint and the current are written to disk when backup log happens the datas are written to the mdf file am correct some of my collegues says im wrong and its hard to find the correct answer even with the bol thanks
75876 for testing purposes need to get some data from set of database tables in any arbitrary but reproducible order the idea being that can later compare two runs using textual diff tool is there an idiom for that can obviously do select from table with columns order by column column am just asking if there is an idiomatic way to achieve the same effect for my purposes without bothering to list every column in the order by clause any ordering will do so long as it is reproducible with subsequent runs of the query
75894 per the pgsql docs because this is not always needed and there are many choices available on how to index declaration of foreign key constraint does not automatically create an index on the referencing columns yet after create such constraint when run table see that an index does in fact get created indexes table primary key primary key btree id fki table product foreign key btree product what could explain why this gets created do still need to create another index on my product column to improve performance of queries on it
75990 on sql server have multiple databases on server used for training purposes periodically these need to be cleared for fresh start when quantities of databases were small would use adam andersons code to remove all objects and manually change the use statement with databases now id rather not looking to automate this process im trying to use sp msforeachdb without luck so far any suggestions thanks in advance declare command nvarcharmax select command if exists select from sys databases where name and name like learndb we have learndb1 learndb2 etc begin declare stmt nvarcharmax declare char1 set char10 select stmt isnull stmt drop procedure schema nameschema id name from sys procedures select stmt isnull stmt alter table schema nameschema id object name parent object id drop constraint name from sys check constraints select stmt isnull stmt drop function schema nameschema id name from sys objects where type in fn if tf select stmt isnull stmt drop view schema nameschema id name from sys views select stmt isnull stmt alter table schema nameschema id object name parent object id drop constraint name from sys foreign keys select stmt isnull stmt drop table schema nameschema id name from sys tables select stmt isnull stmt drop type schema nameschema id name from sys types where is user defined select stmt isnull stmt drop trigger schema nameschema id name from sys objects where type tf exec sp executesql stmt end exec sp msforeachdb command
76021 when inserting less then about rows to the table it all takes about minutes however when number of inserted rows is bigger then the time needed to insert data grows to about hours the problem is not connected with query or indexes because everything has been working fine for long time and nothing has changed in the structure of query tables or indexes problem appeared for the first time about weeks ago and it appears repeatedly on days when number of inserted rows is bigger than for example on one day number of inserted rows is and the process takes minutes on the other day number of rows is and it takes hours to insert data tried to rebuild indexes but it has not helped
76038 little background recently inherited responsibility for support and development of database application after the original developers left looking through the application the database appears to have several issues weekly shrink and re index plan database account used for web application access has db owner role no foreign keys all tables use guid column as primary key queries built in are not parameterized how can bring these issues up with management normally this should be no trouble but have problem have less than six months of sql experience whereas the managers and former developers were working on this project for several years
76091 want to change mysql database name of the db instance of amazon rds how it can be possible
76130 what is the recommended way to back up large data sets in mongodb lets say we have data size in the order of 10tb how would you back that up were considering hidden possibly delayed replica set node the delay would protect us from accidental drops of the whole database is this viable solution and what other options would you recommend investigating thanks
76207 im using the following code to pull our top queries ordered by cpu select top qs sql handle qs execution count qs total worker time as total cpu qs total worker time as total cpu in seconds qs total worker time qs execution count as average cpu in seconds qs total elapsed time qs total elapsed time as total elapsed time in seconds st text qp query plan from sys dm exec query stats as qs cross apply sys dm exec sql textqs sql handle as st cross apply sys dm exec query planqs plan handle as qp order by qs total worker time desc however im seeing this can anyone shed some light as to why the query plan and sql text are showing up null are they some sort of system process or external application we are running sql r2 thanks as always everyone
76273 have database currently running on ec2 as have to move it to bigger machine the question of using rds came up pricing can get an ec2 c3 large instance with two ssds 16gb each two vcpus and 4gb of memory on demand for per hour the closest considering price rds machine would be db m3 medium for per hour single az this machine has the same amount of memory but only one vcpu additionally would have to pay for storage and io so the prices would be very similar for these two the advantages for ec2 as mentioned already one more vcpu can put the write ahead log on the second disc when using ec2 great performance improvement when writing lot to the db can run pgbouncer on my ec2 instance more performance because can keep connections open can edit the configuration file will eventually improve performance the advantages for rds automatically does daily backups can rds cover the advantages that ec2 has especially is there any other advantages
76302 have setup mirroring of database in sql server standard edition after this tried to see if the data has been synchronized or not was not able to check data by taking database snapshot since database snapshots are not available in sql server standard edition is there any alternative solutions available to verify whether the data is synchronized or not
76375 how can check the progress status when submit an alter index reorganize rebuild
76628 im trying to improve the performance of the following query update temptable set received number from temptable inner join select agentid ruleid countdistinct groupid number from temptable where passed group by agentid ruleid on ruleid temptable ruleid and agentid temptable agentid currently with my test data it takes about minute have limited amount of input into changes on the over all stored procedure where this query resides but can probably get them to modify this one query or add an index tried adding the following index create clustered index ix test on temptableagentid ruleid groupid passed and it actually doubled the amount of time the query takes get the same effect with non clustered index tried re writing it as follows with no effect with as select agentid ruleid countdistinct groupid number from temptable where passed group by agentid ruleid update temptable set received number from temptable inner join on ruleid temptable ruleid and agentid temptable agentid next tried to use windowing function like this update temptable set received countdistinct case when passed then groupid else null end over partition by agentid ruleid from temptable at this point started to get the error msg level state line incorrect syntax near distinct so have two questions first can you not do count distinct with the over clause or did just write it incorrectly and second can anyone suggest an improvement havent already tried fyi this is sql server r2 enterprise instance edit here is link to the original execution plan should also note that my big problem is that this query is being run times https onedrive live com redirresid 4c359af42063bd98 edit2 here is the full loop that the statement is in as requested in the comments im checking with the person who works with this on regular basis as to the purpose of the loop declare counting int select counting begin cascading rule check while counting begin update w1 set passed from temptable w1 temptable w3 where w3 agentid w1 agentid and w3 ruleid w1 cascaderuleid and w3 rulepassed and w1 passed and w1 notflag update w1 set passed from temptable w1 temptable w3 where w3 agentid w1 agentid and w3 ruleid w1 cascaderuleid and w3 rulepassed and w1 passed and w1 notflag update temptable set received number from temptable inner join select agentid ruleid countdistinct groupid number from temptable where passed group by agentid ruleid on ruleid temptable ruleid and agentid temptable agentid update temptable set rulepassed where totalneeded received select counting counting end
76788 im new to mysql and would like to know how can create database with charset utf like did in navicat create mydatabase seems to be using some kind of default charset
76802 want to run job every seconds however in sql server we cannot define an interval of less than seconds the job is used to insert update visitor information and segmentation information into database which is tracked by google search there are up to about rows inserted in or seconds that job inserts and update the table in database is there any way to schedule it using sp job scheduling configuration
76834 the isnumeric function has some unexpected behavior the msdn documentation says isnumeric returns when the input expression evaluates to valid numeric data type otherwise it returns valid numeric data types include the following int bigint smallint tinyint decimal numeric money smallmoney float real and it also has footnote isnumeric returns for some characters that are not numbers such as plus minus and valid currency symbols such as the dollar sign for complete list of currency symbols see money and smallmoney transact sql okay so and listed currency symbols are expected to be considered numeric so far so good now for the odd part first up some of the currency symbols from linked article are not numeric including euro currency sign hex 20a0 naira sign hex 20a6 rial sign hex fdfc this is weird and cant seem to find out why is this version or environment dependent however things get weirder here are few others cant explain is not numeric but is huh replicaten9 is numeric but replicaten9 is not the first and most basic question is what explains the above cases more importantly though what is the logic behind isnumeric so could explain predict all cases myself heres good way to reproduce things declare tbl tabletxt nvarchar1000 insert into tbl txt values nchar8356 nchar8352 nchar8358 nchar65020 ne n1e ne1 n1e1 n1 n1 replicaten9 replicaten9 replicaten9 replicaten9 select unicodelefttxt as firstcharasint lentxt as txtlength txt as txt isnumerictxt as isnumeric from tbl when run this on my local sql server box get the following results firstcharasint txtlength txt isnumeric null 1e e1 1e1
77247 getting the error below while trying to create catalog in sql server integration services any idea what missed in installation or anywhere else the catalog backup file program files microsoft sql server dts binn ssisdbbackup bak could not be accessed make sure the database file exist and the sql server service account is able to access itmicrosoft sqlserver integrationservices common objectmodel
77298 im running sqlcmd from batch file and was wondering how to make it return an errorlevel other than when something goes wrong with the backup
77376 we have production sql server server on which we are hosting databases we want to host these databases on our new server on new server sql server is installed and is ready want to know the best and the most convenient way to migrate the whole data includes security and permission users and memberships all of the databases and in one word want to have exactly the same copy of the current sql server on our new instance on the new server will be thankful if you guide me on this case best regards
77402 myself and another dba at our company are tasked with reviewing database design that vendor has developed for us the vendor has said they use kimball as the basis for their design note am not looking for arguments of kimball vs inmon etc they have designed mart with multiple facts and dimensions now in all fairness our company has never designed single mart we have always had the consultants do it and we have never been sent to classes or anything so our knowledge of warehousing marts dimensional modelling etc is based on what little experience we have what we can find on the internet and self reading we have inmons and kimballs books and are trying to make our way through them now that the stage is set for my level of knowledge we come to the design challenge there is fact table called claim loss statistics this is for insurance and they are trying to capture both the payments for claims rolled up to monthly level and then money in the reserves kind of like bank account for claims they wish to see the monthly amounts for payments no biggie but they wish to see the account current balance of the reserves ill give pictorial example say we set up usd in reserves for claim this gets set aside so in some respects it functions kind of like bank account in october we dont pay out anything yet so the business wants to see the payments and the reserve balance at the end of october month year payments reserve balance then november comes along we make out payments of and dollars they wish to see those amounts aggregated and the reserve at the balance as follows month year payments reserve balance and then say we have zero payments in december and then more in january next year month year payments reserve balance here is where struggle my understanding is that the payments part is correct they are all rolled up at monthly level within each record so you can rollup further if you want for the year quarter etc but the reserves amount is different it is balance and the business wants to see how much is in the balance at each month but you cant aggregate on this field if you did you would get some wonky results somehow this strikes me as wrong but cant truthfully say ive modelled enough or know enough all can say is what know and from what know all values in fact should be at the same granularity both numbers are at the same granularity of month but they are not from the standpoint of what they stand for one is aggregate dollars within month the other is just the balance is this correct have been pushing back on this design am wrong to do so is it ok to do this in fact or is my sense of code smell of bad design accurate any help would be appreciated note please dont just say it should be way please explain why it should be that way so can learn from this edit well learned that my initial understanding of the fact is wrong the granularity is not monthly the granularity is transaction level so that means within the month year ie really it is the financial reporting period there will be multiple payment and recovery transactions those will be posted by date or transaction date but because of prior report the business sees and also because of how the data is stored in the legacy system that this comes from they wanted to put both the transactional data one row per and the reserve monthly balance one row per month once learned that realize the problem was not so much additive vs non additive or even semi additive as it was grain which is what was suspecting from the start our dba team discussed this with the project team and reported that they are attempting to put two different grains in the same fact and this was not correct that they should either role up the transactions to monthly level allowing them to then have the payments recoveries and the monthly reserve balance ie semi additive fact because everything would be at monthly grain or they need to find way to break down the reserve balance into transactions to preserve the transaction level grain or they need to break the fact into two facts one can be the monthly level for the reserve balance the other can be at the transaction level for the payments and recoveries there is no reason why they also couldnt put the payments and recoveries at monthly level in the monthly level fact too just depends on the business needs given what have learned will be marking thomas answer as the correct one however feel the discussion started with the original question is still good one for others to learn from so will leave the original portion of my question intact also intend to award bounty to nikadams answer as that taught me lot about additive non additive and semi additive facts and corrected lot of misunderstandings had about dimensional modelling
77489 have only 2gb left so need to remove this history table this table now is empty but the database disk space not released and the database file is 320gb
77560 we run manual vacuum analyze verbose on some of our larger tables after we do major delete insert changes to them this seems to work without issue although sometimes tables vacuum job will run for hours see this post for similar issues and reasoning on doing more research found that we have large tables with large number of dead tuples even after running vacuum for example here are some of the stats produced from the query in this response record relname example last vacuum last autovacuum tup dead tup av threshold expect av record relname example last vacuum last autovacuum tup dead tup av threshold expect av record relname example last vacuum last autovacuum tup dead tup av threshold expect av the last field states that these and most tables would meet the threshold for autovacuum however having just run vacuum analyze vebose on each of those tables shouldnt the dead tuple count be or close to not 125m of 300m the documentation states vacuum reclaims storage occupied by dead tuples does this mean our vacuum is not working update per request in repsonse here are some logs from the verbose jobs info vacuuming public example info scanned index idx example on gp id and dd id to remove row versions detail cpu 83s 42u sec elapsed sec info scanned index index example on id to remove row versions detail cpu 10s 91u sec elapsed sec info example removed row versions in pages detail cpu 09s 05u sec elapsed sec info index idx example on gp id and dd id now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info index index example on id now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info example found removable nonremovable row versions in out of pages detail dead row versions cannot be removed yet there were unused item pointers pages are entirely empty cpu 26s 51u sec elapsed sec info vacuuming pg toast pg toast info index pg toast index now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info pg toast found removable nonremovable row versions in out of pages detail dead row versions cannot be removed yet there were unused item pointers pages are entirely empty cpu 00s 00u sec elapsed sec info analyzing public example info example scanned of pages containing live rows and dead rows rows in sample estimated total rows this table now shows dead tuples in the stats most of the tables are much lower dead tuples this morning so either our vacuum or autovacuum is working we do have handful of tables that output nothing and yet still show dead tuples record relname example last vacuum last autovacuum tup dead tup av threshold expect av couple times have seen in the logs where the indexes will get checked over and over again this seems to correspond to long running vacuum jobs any idea why is this just working around record locking dont think any writes were happening during this jobs run info vacuuming public example info scanned index index example on gsg id and dd id to remove row versions detail cpu 88s 54u sec elapsed sec info scanned index index example on id to remove row versions detail cpu 74s 13u sec elapsed sec info example removed row versions in pages detail cpu 71s 32u sec elapsed sec info scanned index index example on gsg id and dd id to remove row versions detail cpu 84s 11u sec elapsed sec info scanned index index example on id to remove row versions detail cpu 46s 70u sec elapsed sec info example removed row versions in pages detail cpu 67s 38u sec elapsed sec info index index example on gsg id and dd id now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info index index example on id now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info example found removable nonremovable row versions in out of pages detail dead row versions cannot be removed yet there were unused item pointers pages are entirely empty cpu 49s 13u sec elapsed sec info vacuuming pg toast pg toast info index pg toast index now contains row versions in pages detail index row versions were removed index pages have been deleted are currently reusable cpu 00s 00u sec elapsed sec info pg toast found removable nonremovable row versions in out of pages detail dead row versions cannot be removed yet there were unused item pointers pages are entirely empty cpu 00s 00u sec elapsed sec info analyzing public example info example scanned of pages containing live rows and dead rows rows in sample estimated total rows
77639 according to official microsoft bol dense rank is nondeterministic rank but according to ranking functions by itzik ben gan the rank and dense rank functions are always deterministic who is right what have found so far microsofts definition deterministic functions always return the same result any time they are called with specific set of input values and given the same state of the database so in set theory tables employees employee salary sue right robin page phil factor and employees2 employee salary phil factor sue right robin page are the same but ranking functions return different values create table dbo employees id int identity11 not null employee varchar not null salary smallmoney null on primary go create table dbo employees2 id int identity11 not null employee varchar not null salary smallmoney null on primary insert into dbo employees employee salary values sue right robin page phil factor go insert into dbo employees2 employee salary values phil factor sue right robin page go select rank over order by salary as rank dense rank over order by salary as dense rank employee from dbo employees select rank over order by salary as rank dense rank over order by salary as dense rank employee from dbo employees2 select ntile3 over order by salary employee from dbo employees select ntile3 over order by salary employee from dbo employees2
77664 im running sql server select results this one is even mor confusing to me declare int declare decimal3826 declare decimal3826 select power1 power1 select power1 power1 the first select gives me the correct result the second one truncates the result
77894 is there way to force users to only take full backup with copy only so they dont interfere with differential backups our developers need to take backups every now and then but they dont always remember to use the copy only option and then they move the backup away so it causes few issues
77935 when we did rebuild on clustered index on table that have about 15gb data in it and the datasize shrunk to 5gb how can this be what kind of data is removed data size mean the data column of dbcc sp spaceused before rebuild on clustered index name rows reserved data index size unused ledgerjournaltrans kb kb kb kb after rebuild on clustered index name rows reserved data index size unused ledgerjournaltrans kb kb kb kb tsql for rebuild use dax5test go alter index 212recid on dbo ledgerjournaltrans rebuild partition all with pad index off statistics norecompute off allow row locks on allow page locks on online on sort in tempdb off data compression page fillfactor go
78061 if im making single call to sql server database over high latency network will table locks occur due to that latency say query table for some records and sql server has to return that data over slow network will there be read lock on table while the server sends the response over the network or does sql server release the lock before sending the response also would the answer vary based on the size of the response if it just has to return few kb vs several hundred mb would that make difference creating an explicit transaction running queries and closing the transaction would obviously cause the tables to lock since the duration of the transaction is correlated with my latency
78196 have the following set up sql server linked sql server linked sql server the question is can query sql server from sql server via sql server have unfortunately several restrictions cannot use openquery cannot use views on server there would be many of them and theyd have to be maintained cannot create link from server to server due to firewalling issues servers are ms sql servers
78242 will have huge postgresql database with many tables with more than 100m entries per table this database will be basically be read only once fill all the necessary tables and build the indexes no more write operations on the db and single user access run and benchmark multiple queries from localhost since the db will be used only for research purposes queries will always use join on integer db fields will probably buy ssd 512gb for this purpose have not used an ssd for db before so is there anything should be afraid of can put the entire db on the ssd or just the indexes is there any particular advice tutorial required for tuning postgresql for ssds note that have good workstation with an i7 and 32gb of ram so perhaps you can offer some advice there too
78265 with this command can generate random integers between and select generate series 116384random int as id want to generate sets of such integers each set must have an integer identifier something like that end of first set of random numbers end of second set of random numbers is this possible with an sql command or should write function for that
78323 am trying to compress some tables that have nvarcharmax fields unfortunately the row and the page compression do not have the desire impact only mb saved for gb table also am not able to apply column store and column store archival compressions because they do not support compression of nvarcharmax fields can anyone tell if have any alternatives here also guess the row and page compression do not have effect because the content of the nvarcharmax columns is unique
78341 we are currently using ssis is there any way for user to view execution reports under ssis catalog without being ssis admin or sysadmin this is for production environment and we dont want people to manipulate ssis catalog projects thanks
78564 am trying to copy database from sql server to sql server using copy wizard in my local machine where sql server is installed but the wizard is not detecting destination server source db is on network and destination is my local machine itself when select the destination server as my local machine am getting the below error title copy database wizard the destination server cannot be sql server or later express instance does copy database wizard not support if source is earlier than sql server but the error message says only about destination here my destination is sql server and source is sql server
78569 have database on sql azure and want to create database diagram for that but dont want to have to install sql server and copy the database etc is there any way that can do it that anyone knows of
79980 have table t0 in my postgres db with data that looks something like this t1 id t2 id null null null and have query to return my desired results of t1 id t2 id null null my query looks something like this select distinct on t2 id t1 id t2 id from t0 where t2 id is not null union all select distinct on t1 id t1 id t2 id from t0 where t2 id is null is there faster way to do an operation like this its not too bad but im doing it in several places with joins and all these repeated queries seems to slow stuff down bit seems like there must be better way heres the query in fiddle form http sqlfiddle com d41d8
80140 have table with several columns which want to select select his name her name other name from foo bu instead want to combine the results all into single column as an example can do this with union all as select her name as name from foo union all select his name as name from foo union all select other name as name from foo is there more elegant way to do this operation
80264 ive been tasked with discovering all the instances of sql server that are running within our domain in several cases there are multiple instances per server ive seen two different powershell methods of finding these instances but neither seem to find all the instances use wmi srvr new object typename microsoft sqlserver management smo wmi managedcomputer computername instances srvr foreach object serverinstances select name fullname expression computername name return instances use remote registry as with get sqlinstance1 the biggest problem im running into is that not all of the servers that know about are running with the sql server wmi provider nor are all of them allowing remote registry is there third method can use remote desktop to access all the servers but im looking at approximately machines and would like to avoid manual steps if possible this only needs to work for sql server and higher and while it would be nice to know about the other sql server services ssis ssas ssrs my main focus is on sql server itself
80374 have simple script that gets four random numbers through and then joins back to get the matching database id number when run the script with left join get four rows back every time the expected result however when run it with an inner join get varying number of rows sometimes two sometimes eight logically there shouldnt be any difference because know rows with database ids exist in sys databases and because were selecting from the random number table with four rows as opposed to joining to it there should never be any more than four rows returned this happens in both sql server and what is causing the inner join to return varying numbers of rows works as expected always four rows select rando randomnumber database id from select abschecksumnewid as randomnumber from sys databases where database id as rando left join sys databases on rando randomnumber database id returns varying number of rows select rando randomnumber database id from select abschecksumnewid as randomnumber from sys databases where database id as rando inner join sys databases on rando randomnumber database id also returns varying number of rows with rando as select abschecksumnewid as randomnumber from sys databases where database id select randomnumber database id from rando as inner join sys databases on randomnumber database id
80514 technically null null is false by that logic no null is equal to any null and all nulls are distinct shouldnt this imply that all nulls are unique and unique index should allow any number of nulls
80618 started working on an existing sql server database system in which most of the fields are stored as text except some ids all fields are varchar phone numbers zip codes dates addresses monetary values etc this is not how learned to build database when asked collegues said that it is easier this way is it bad practice to keep all fields in varchar how could argue that it should be changed
80695 need to delete all the rows from given table the table contains millions or records the master database is being replicated to several slaves and wish to do that without creating replication lag or impacting the performance after some research tried dropping the table that took quite few long seconds during which time my master db was locked out know can gradually delete in smaller batches just wondering is theres quicker way thanks
80710 optiona tablea tablea id pkint tableb tableb id pkint tablec tablec id pkint tabled tablea id fk int tableb id fk int tablec id fk int nullable composite key on tablea idtableb idtablec id am thinking to design tabled as the following optionb tabled tablea id fk int tableb id fk int tablec id varcharmax nullable and remove the composite key so tablec id column will have something like these my application will need to use tabled as search value in tableb id and or tablec id column by giving the key for value in tablea id am just wondering which option will be better performance
80817 will migrate from database there is one column of type image that would like to export to binary files on the file system one file for each record how can do this with sql server
80951 introduction and relevant information the following example illustrates the problem face animal has race which can be cat or dog cat can be either siamese or persian dog can be german shepherd or labrador retriver animal is strong entity while its race is an attribute that can have one of the two offered values cat or dog both these values are complex have added here only the type of dog cat to illustrate the problem but there can also be the cats dogs name and bunch of other stuff problem dont know how to create relational tables for this example my efforts to solve the problem have tried to draw er diagram using chens notation that represents the problem but being beginner dont know if did it right here is what have got apologize if drew something wrong please correct me if that is the case dont wish to simply get free solution but also to learn how to deal with this problem so can solve it on my own in the future the only thing that comes to my mind is to create two separate tables one for cats and one for dogs also the race attribute in the animal table would only store cat or dog value something like this animal animal id race other attributes cat cat id animal id breed dog dog id animal id breed really have bad feeling about my solution and fear it is wrong hence the below question questions how can transform my example into er diagram how to transform that er diagram into relational tables if further info is required leave comment and will update my post as soon as possible also feel free to add appropriate tags since am fairly new here thank you
81011 what behaviour would postgresql display if for example the script below were called begin select from foo insert into fooname values bar begin the point of interest end would postgresql discard the second begin or would commit be implicitly decided on and then run the begin end block at the end as separate transaction
81035 in java we use joda time to get the seconds of day as an int value despite the date date second of day date second of day date second of day is there way to do the same in postgressql
81065 am wondering if it would be possible to use over partition by clause in order to avoid using sub select in the example below declare table id int code char2 descriptor int insert into select a1 union select a1 union select a1 union select b1 union select b1 union select b1 union select b2 union select b2 union select b2 union select c4 union select c4 union select c4 union select c7 union select c7 union select c7 select from as where code select mincode from where id id want to see only the records with the code equal to a1 b1 and c4 can over partition by assign to all those and to b2 and c7 codes etc so that at the end could say where row number instead of using sub query
81094 im trying to find information about postgresql user defined functions in procedural languages performance for real time tasks how does they compare to builtin functions is there any difference in overhead how postgres call manage plpython vs plpgsql vs pllua functions im interested in the postgres integration context data transfer side not the vm itself is the context big overhead can use it for realtime data mapping lets say queries is there any benefit of writing user defined functions in plpgsql then other pg language on the documentation they enumerate advantages but think they apply to all postgresql procedural languages related findings speed of pl languages for atypical usage postgresql function language performance vs pl pgsql
81245 is there any built in function stored procedure query which is helpful to retrieve information about the size of mytable in the sql server database
81277 am trying to deploy sql clr function using the httputility urldecode method of system web but cant get it to deploy error received net sqlclient data provider msg level state line assembly system web version culture neutral publickeytoken b03f5f7f11d50a3a was not found in the sql catalog the function as part of ssdt project using system using system web using system data using system data sqlclient using system data sqltypes using microsoft sqlserver server public partial class userdefinedfunctions microsoft sqlserver server sqlfunctionisdeterministic true public static sqlstring udf urldecodesqlstring encodedxml string decodedxml decodedxml httputility urldecodeencodedxml tostring return new sqlstringdecodedxml its in relation to this thread am sql server with vs2012 ssdt and database project have tried with other target frameworks eg and have also tried create assembly with system web but then have to add other assemblies eg microsoft build system xaml until they also fail see system web is not on the list of supported libraries so any ideas
81310 can some one please explain to me in simple terms what is overlapping candidate key what is overlapping as the name suggests consider the below relation rlmnop no mn which of the above functional dependency is bringing in overlapping candidate key in the above relation lets restrict the discussion to dependencies im not interested in bcnf at the moment
81311 looking through database came across table that used its primary key as foreign key to itself ive seen that table can have foreign key to itself to build hierarchy structure but it would use another column to reference the primary key since the primary key is unique in this situation wouldnt the row only be able to point back to itself that seems to be tautological link since if already have the row then already have the row is there any reason this would be done am certain the constraint is written that way not just looking at the diagram because the same table and column are used for both halves of the definition
81369 how could select all grams ie substrings of length from string using sql for example the grams of string example are exa xam amp mpl ple im using postgresql to be more precise
81388 know can cycle the current error log easily by running sp cycle errorlog but im wondering if sql server will ever delete the old archived error log files at all cant seem to find an answer to this anywhere
81460 testing table create table public test patient table entity id integer not null site held at integer not null constraint entityid pk primary key entity id create table public test messageq table entity id varchar not null master id integer not null message body varchar not null constraint mq entity id pk primary key entity id create index test patient table siteid idx on public test patient table site held at alter table public test messageq table add constraint test patient table test messageq table fk foreign key master id references public test patient table entity id on delete no action on update no action not deferrable test patient data insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values insert into test patient table values testing message insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values aaa insert into test messageq table values bbb when tried to find all messages from message table in site am interested wrote cte and it works fine lets say am interested in site and with patient msg in branches as select distinct test messageq table master id as patient id test patient table site held at as site id from test messageq table inner join test patient table on test messageq table master id test patient table entity id and site held at in order by patient id messages for patients as select from test messageq table where master id in select patient msg in branches patient id from patient msg in branches select from messages for patients the result is as expected aaa aaa aaa aaa aaa aaa aaa aaa but when wrap the whole thing in function its returning the wrong rows can you help me see why drop function getmessagefromsitestext create or replace function getmessagefromsitesin ids text returns setof test messageq table as declare sites int result test messageq table rowtype begin sites string to arrayids raise info entire array sites with patient msg in branches as select distinct test messageq table master id as patient id test patient table site held at as site id from test messageq table inner join test patient table on test messageq table master id test patient table entity id and site held at anysites order by patient id messages for patients as select from test messageq table where master id in select patient msg in branches patient id from patient msg in branches select into result from messages for patients return query select from result end language plpgsql when using the function select from getmessagefromsites1111122222 select from getmessagefromsites1 select from getmessagefromsites33333 it always returns the same result below of multiple rows but obviously wrong rows why can you help here aaa aaa aaa aaa aaa aaa aaa bbb solution thanks to horse with no name now have two working solutions one with sql one with pl pgsql solution pl pgsql create or replace function getmessagefromsitesin ids text returns setof test messageq table as declare sites int result test messageq table rowtype begin sites string to arrayids raise info entire array sites return query with patient msg in branches as select distinct test messageq table master id as patient id test patient table site held at as site id from test messageq table inner join test patient table on test messageq table master id test patient table entity id and site held at anysites order by patient id messages for patients as select from test messageq table where master id in select patient msg in branches patient id from patient msg in branches select from messages for patients end language plpgsql solution sql create or replace function getmessagefromsites2ids text returns setof test messageq table as with patient msg in branches as select distinct test messageq table master id as patient id test patient table site held at as site id from test messageq table join test patient table on test messageq table master id test patient table entity id and site held at any string to array int messages for patients as select from test messageq table where master id in select patient msg in branches patient id from patient msg in branches select from messages for patients language sql testing of the code select from getmessagefromsites1111144444 select from getmessagefromsites22222 select from getmessagefromsites1 select from getmessagefromsites33333 select from getmessagefromsites211111 select from getmessagefromsites222222 select from getmessagefromsites233333 select from getmessagefromsites4444411111 select from getmessagefromsites1 both pg stored procedure are working as expected solution better simplified solution see from erwins answer below now case closed
81552 why sql server r2 does full installation for just adding another instance only database engine on the same machine thought it will be quick installation but its not what is sql server doing copying registering all the files again for this new instance please clarify me
81595 when looking at the properties of particular login its possible to see list of users mapped to that login profiled sql server management studio ssms and see that ssms connects to every database one at time and retrieves information from sys database permissions is it possible to write single query that retrieves the user mapping information shown above or am forced to use cursor or sp msforeachdb or something like that
81607 hi have been working on script to update statistics of all tables on all my dbs the idea is to parametrise it later on but just as quick fix and not wanting to implement olas scripts today follow the script below have tested it on few servers but before schedule it to run on live server sunday morning would like to get some ideas and share it with you set nocount on declare dbs table int not null identity11 primary key clustered dbname sysname not null declare int int sql varchar1008 int int mydb sysname insert into dbs select name from sys databases inner join sys master files on database id database id and data space id where state online and database id exclude master tempdb and model left msdb and is read only read write and user access multi user and name not in tablebackups troubleshooting order by size desc select rowcount select while begin begin try drop table end try begin catch end catch create table int not null identity11 primary key clustered mysql varchar1008 not null select mydb quotename dbname sql use quotename dbname char13 char13 insert into tmysql select update statistics quotenamename from sys tables where type char13 from dbs where print cast sql as ntext begin try exec sql select rowcount end try begin catch select end catch select while begin select sql use mydb char13 mysql from where begin try exec sql end try begin catch end catch print cast sql as ntext select end select end while
81631 have large 6db trace table it has clustered key datetime which is created through getdate the connection pool for connections to this database table rises as high as on average across cluster of computers so on average we have at concurrent connections attempting to insert the database fits in memory and hardly any io is seen at all am trying to figure out whether under sustained insert load the clustered index gets to point where it rebalances the tree and whether this will cause slowdown in the number of inserts that the system can sustain there is some question in my mind as to whether the rebalancing an index is something sql server does on clustered index and even on non clustered index questions are there any reasons for periodic cyclic slow down of insert performance do rebalance operations automatically trigger on clustered indexes do rebalance operations automatically trigger on non clustered indexes other info sql server really big server 256gb cores 40mbit lan
82000 the lock pages in memory right can be granted to the service account used by sql server this allows sql server to prevent memory being paged to disk ive noticed several of our sql server machines do not have the local policy configured to allow this right for the service account used by sql server since we have large number of servers it will be tedious at best to manually check each one using the local system policy management console is there sql query extended stored procedure or some other method can use to determine if the server in question has the right or not would like to not use exec xp readerrorlog lock memory privilege was not granted since it relies on checking the current sql server error log and that error log may not contain the relevant entry assuming the log has been rolled over since the server was last restarted realize could check older logs by modify the first parameter from to etc however we only retain error logs and that may not be enough want fail safe way of confirming the setting
82094 so know all about the replace function and bug with char0 have column nvarchar128 that has some nchar0x0000 characters from bad import im using sql server r2 the collation for the column is sql latin1 general cp1 ci as have tried all the stuff online that can possibly find and nothing will get the stinking char0 characters out of the column heres my latest attempt with baffling bug in sql server results have function that loops through each char and replaces 0x0000 with specific char alter function dbo replacecharzero teststring nvarcharmax chartoreplacewith nchar1 returns nvarcharmax as begin declare int fixedstring nvarcharmax while len teststring begin if substring teststring char0x00 begin print found cast as varchar set fixedstring fixedstring chartoreplacewith end else begin print not found cast as varchar set fixedstring fixedstring substring teststring end set end return fixedstring end and heres what do to test begin tran declare shortdescription nvarchar128 supplierid int language char2 select top shortdescription shortdescription supplierid supplierid language language from supplier multilingual where shortdescription like char0x00 set shortdescription replacedbo replacecharzero shortdescription update dbo supplier multilingual set shortdescription null where supplierid supplierid and language language update dbo supplier multilingual set shortdescription dbo replacecharzero shortdescription where supplierid supplierid and language language select from supplier multilingual where supplierid supplierid and language language and shortdescription like char0x00 rollback tran in my test grab the column as variable run my function on it to strip out the 0x0000 then update the original column with null then update it to my fixed variable and then run query to see if 0x0000 chars still exist which they do
82161 am trying to run following command sshpass pass ssh pg dump fc foo some db pg restore create dbname new db get failed fatal database new db does not exist
82304 have some users in my database they are member of db datareader they now also need to have execute permissions on stored procedures was just wondering what would happen if one of the stored procedures issue ddl would they be able to make schema changes to tables drop etc through that stored procedure thank you
82414 do you have to run nodetool repair on every node in cluster or do you only need to run it on one node and from there cassandra will take care of the rest
82573 am no database admin nor is my sql foo any good yet but have few tasks to get done in mssql and one is rather time intensive procedure where fetch from cursor and while fetch status have multiple selects and an update on giant tables the procedure works ive checked with colleague now in script or program id have simple print current cursor or print counter in every while just to have some feedback how could do something like this in mssql working on microsoft sql server enterprise edition v9 sql server service pack could do print current cursor or would that be no go in my case edit ive tested the raiseerror with the following statements declare cur id int set cur id declare cur cursor for select id from tablea where id order by id desc open cur fetch next from cur into cur id while fetch status begin raiserror cur id with nowait fetch next from cur into cur id end but this leads to some time of waiting without output and then an endless wall of assume one for every raiserror msg level state line error severity state was raised but no message with that error number was found in sys messages if error is larger than make sure the user defined message is added using sp addmessage
82604 found table that looks like this create table dbo table1 id int primary key identity iduser int not null amount int not null attempts int not null date datetime not null sum amount int not null on primary this table is created and populated with aggregated data for particular period by job particularities this table will hold up to million rows iduser is unique sum amount is running total of previous rows of amount this table will last as is no update or delete or insert operation just this type of queries select top from table1 order by sum amount desc attempts desc select top from table1 where sum amount order by sum amount asc think it will improve preformance to change to clustered index like this create table dbo table2 id int identity iduser int not null amount int not null attempts int not null date datetime not null sum amount int not null constraint pk nueva primary key clustered sum amount desc attempts desc id asc on primary read that using no unique clustered index will add bytes hidden column http msdn microsoft com en us library ms190639v sql aspx so decide to add identity to cluster index not sure if it is the right approach want to ask at the risk of sound ridiculous but need to be sure how could it be improved will have an impact on disk size should rebuild the index once all data have been inserted edit about id think is there just as bad habit ill kept it not sure how previous job calculate running total ive no access to it there are lot of tables like this like hundreds for each daydont ask me why that is why dba team ask me to not create new index because of size issues that is why thinking of rearrange table structure via clustered index also changing data types which exceeds normal ranges
82824 was looking through the sql server logs and come across the following informational message this instance of sql server has been using process id of since utc this is an information message only no user action is required this message has been generated for the last three days cannot see the process id using the activity monitor is this something that may cause highlight problem that need to look into
82921 one of insert into script is written as following insert into tablename column1 column2 values value1 value2 value1 value2 following is the error we are facing on parsing above insert statement msg level state line the number of row value expressions in the insert statement exceeds the maximum allowed number of row values my simple question is that can we change values limit
83125 have this huge gb sql dump that need to import into mysql havent had to import such huge sql dump before did the usual mysql uroot dbname dbname sql it is taking too long there is table with around million rows its gotten to million in around hours so it seems that the whole thing would take hours thats days and is impractical so my question is is there faster way to do this further info findings the tables are all innodb and there are no foreign keys defined there are however many indexes do not have access to the original server and db so cannot make new back up or do hot copy etc setting innodb flush log at trx commit as suggested here seems to make no clearly visible exponential improvement server stats during the import from mysql workbench https imgflip com gif ed0c8 mysql version is community innodb buffer pool size 16m and innodb log buffer size 8m do need to increase these
83164 understand that upon installation postgresql has no password for its db root user postgres postgres select usename passwd is null from pg shadow usename column postgres row and one is advised to set it with alter role postgres password very secret and then update the pg hba conf file accordingly my question is what is the sql to use to revert back to the previous situation when no password was needed for user postgres in general how can remove the password requirement for any role am not asking how to change the password but rather how to remove the password requirement null passwd column in table pg shadow
83421 this is kind of trivial task in my homeworld but dont yet make it in sql and would prefer to solve it set based without cursors resultset should come from query like this select someid mydate dbo udflasthitrecursiveparam1 param2 mydate as qualifying from how should it work send those three params into udf the udf internally use params to fetch related days older rows from view the udf traverse mydate and return if it should be included in total calculation if it should not then it return named here as qualifying what the udf will do list the rows in date order calculate the days between rows first row in resultset defaults to hit if the difference is up to then pass to next row until the sum of gaps is days 90th day must pass when reached set hit to and reset gap to it would also work to instead omit the row from result column by udf which not work yet date calc date maxdiff qualifying in the table above maxdiff column is the gap from date in previous line the problem with my attempts so far is that cant ignore second last row in the sample above edit as per comment add tag and also paste the udf have compiled just now though is just placeholder and wont give useful result with cte someid otherkey mydate cost as select someid otherkey mydate cost from dbo vgetvisits where someid someid and visitcode and otherkey otherkey and convertdatemydate visitdate union all select top someid otherkey mydate cost from dbo vgetvisits as where convertdate mydate between dateadddd 90convertdate visitdate and convertdate visitdate and someid someid and visitcode and otherkey otherkey and convertdatee mydate visitdate order by mydate have another query which define separately which is more close to what need but blocked with the fact cant calculate on windowed columns also tried one similiar which give more or less same output just with lag over mydate surrounded with datediff select mydate visitcode cost someid otherkey maxdiff datediff from select maxdiff last valuediff diff over order by diff mydate asc rows between unbounded preceding and current row from select diff isnulldatediffday last valuer mydate over order by mydate asc rows between preceding and preceding mydate0 datediff isnulllast valuer mydate over order by mydate asc rows between preceding and preceding mydate from dbo vgetvisits as where visitcode and someid someid and otherkey otherkey as diff as where visitcode and someid someid and otherkey otherkey and diff order by mydate asc
83984 hello am trying to run website sent to me but after doing so this error appeared connect to postgresql server fatal no pg hba conf entry for host 4x xxx xx xxx user userxxx database dbxxx ssl off in xampp htdocs xmastool index php on line after googling it it says that just need to add an entry in the pg hba conf file for that particular user this is my pg hba conf file type database user address method ipv4 local connections local dbxxx userxxx md5 host dbxxx userxxx xx xxx xxx xxx md5 host all all md5 ipv6 local connections host all all md5 allow replication connections from localhost by user with the replication privilege host replication postgres md5 host replication postgres md5 but after doing so the error still persist restarted my xampp server several times but no changes appears thanks in advance
84090 im writing on an upcoming blog post of mine on ranking and aggregate window functions specifically the segment and sequence project iterators the way understand it is that segment identifies rows in stream that constitute the end beginning of group so the following query select row number over partition by somegroup order by someorder will use segment to tell when row belongs to different group other than the previous row the sequence project iterator then does the actual row number calculation based on the output of the segment iterators output but the following query using that logic shouldnt have to include segment because theres no partition expression select row number over order by somegroup someorder however when try this hypothesis both these queries use segment operator the only difference is that the second query does not need groupby on the segment doesnt that eliminate the need for segment in the first place example create table dbo sometable somegroup int not null someorder int not null somevalue numeric8 not null primary key clustered somegroup someorder query select row number over partition by somegroup order by someorder from dbo sometable query select row number over order by somegroup someorder from dbo sometable
84204 have one stupid app which rely on which has hardcoded connection string inside to increase security of my sql server id love to make sql users same as one hardcoded inside app but want to allow that user to be able use sql server only from certain host ip address
84297 want to get only rows having value null and some other value than null for particular username column if both rows have null for that particular username or both have some values other than null then it should not appear in output if there are more than two rows for same username with null and some other value then they should appear below is example sample and output how it can be done using sql query username col2 abc ef null null der null output username col2 der null
84333 it is known fact that the dmvs dont hold accurate information regarding number of pages and count of rows however when you have the stats updated cant see why they wouldnt am working on monitoring tool want to know disk size of each index and data etc eventually would like to find the right fill factor and other things etc the space used by my function and the old sp spaceused differs little bit on the space usage but not on record count can you see if there is anything missing in my select this is the sp spaceused then convert the numbers in mb sp spaceused tblborderrelationship go select as reserved as data as index size as unused but when run my select code below picture below get slightly different figures set transaction isolation level read uncommitted select schema namet schema id as schemaname name as tablename type desc is ms shipped is published lob data space id filestream data space id is replicated has replication filter is merge published is sync tran subscribed is filetable name as indexname type desc is unique is primary key is unique constraint fill factor is padded sump rows over partition by object idi index id as rowcounts suma total pages over partition by object idi index id as totalpages suma used pages over partition by object idi index id as usedpages suma data pages over partition by object idi index id as datapages suma total pages over partition by object idi index id as totalspacemb suma used pages over partition by object idi index id as usedspacemb suma data pages over partition by object idi index id as dataspacemb from sys tables inner join sys indexes on object id object id inner join sys partitions on object id object id and index id index id inner join sys allocation units on partition id container id where name not like dt and object id and name tblborderrelationship the figures the bigger picture including the index names now doing some calculations to check the results the figures from sp spaceused select as reserved as data as index size as unused the figures from my select select as reserved as data as index size it is not so far off really apart the fact did not calculate the unused space what can do to make this accurate after changes after replaced by the results are much more accurate noticed records have been inserted into the table in question and obviously the stats are not so up to date but still the results match under mb difference which is all right for me the new result sets are the figures from sp spaceused select as reserved as data as index size as unused go the figures from my select select as reserved as data as index size
84434 is it possible to update primary key column value with cascading the update among all the foreign keys referencing it edit when run followinq query select from sys foreign keys where referenced object id object idmytable see that update referential action is set to thus no action is taken after updating my primary keys columns how can update the foreign keys to make them on cascade update edit in order to script out creation or dropping of all foreign keys in your schema run the following script taken from here declare schema name sysname declare table name sysname declare constraint name sysname declare constraint object id int declare referenced object name sysname declare is disabled bit declare is not for replication bit declare is not trusted bit declare delete referential action tinyint declare update referential action tinyint declare tsql nvarchar4000 declare tsql2 nvarchar4000 declare fkcol sysname declare pkcol sysname declare col1 bit declare action char6 declare referenced schema name sysname declare fkcursor cursor for select object schema nameparent object id object nameparent object id name object namereferenced object id object id is disabled is not for replication is not trusted delete referential action update referential action object schema namereferenced object id from sys foreign keys order by open fkcursor fetch next from fkcursor into schema name table name constraint name referenced object name constraint object id is disabled is not for replication is not trusted delete referential action update referential action referenced schema name while fetch status begin if action create set tsql alter table quotename schema name quotename table name drop constraint quotename constraint name else begin set tsql alter table quotename schema name quotename table name case is not trusted when then with check else with nocheck end add constraint quotename constraint name foreign key set tsql2 declare columncursor cursor for select col namefk parent object id fkc parent column id col namefk referenced object id fkc referenced column id from sys foreign keys fk inner join sys foreign key columns fkc on fk object id fkc constraint object id where fkc constraint object id constraint object id order by fkc constraint column id open columncursor set col1 fetch next from columncursor into fkcol pkcol while fetch status begin if col1 set col1 else begin set tsql tsql set tsql2 tsql2 end set tsql tsql quotename fkcol set tsql2 tsql2 quotename pkcol fetch next from columncursor into fkcol pkcol end close columncursor deallocate columncursor set tsql tsql references quotename referenced schema name quotename referenced object name tsql2 set tsql tsql on update case update referential action when then no action when then cascade when then set null else set default end on delete case delete referential action when then no action when then cascade when then set null else set default end case is not for replication when then not for replication else end end print tsql if action create begin set tsql alter table quotename schema name quotename table name case is disabled when then check else nocheck end constraint quotename constraint name print tsql end fetch next from fkcursor into schema name table name constraint name referenced object name constraint object id is disabled is not for replication is not trusted delete referential action update referential action referenced schema name end close fkcursor deallocate fkcursor to generate the drop foreign keys script modify action value to be equal to drop in the declaration clause declare action char6 drop
84607 have particular table that cannot add key for mysql alter table tasks add key fruitful user count user id is fruitful error hy000 incorrect key file for table tasks try to repair it googling the issue it seems that this problem is often either configuration issue or disk space issue in fact this database is running on an amazon rds instance which means that it is basically managed server dedicated to mysql with very standard configuration also the disk allocated to us is only about full considering that perhaps there disk on the vm powered by xen believe is full and not my allocated disk space which is likely not even in the same room network storage rebooted the rds instance in the hope that would get new instance on another vm however that did not help what should be my next troubleshooting step this is the table mysql show create table tasks table create table tasks create table tasks user id char32 not null module id int11 not null default is successful tinyint1 default null is fruitful tinyint1 default null last run timestamp not null default current timestamp on update current timestamp last pulled timestamp not null default primary key user idmodule id key phone scrapes user user id key phone scrapes module module id key urgency last pulledlast run key successful user count user idis successful key is successful is successful key fruitness is fruitfulis successful constraint tasks ibfk foreign key user id references users id constraint tasks ibfk foreign key module id references modules id engine innodb default charset utf8 row in set sec
84801 got bitten by this feature recently if your hstore column is uninitialized and you start adding entries to it they are all silently swallowed without error is this expected create table test hstoreid int map hstore insert into test hstoreidmap values0 insert select from test hstore id map update test hstore set map map hstorekey1 value1 where id update select from test hstore id map key1 value1 update test hstore set map null where id update select from test hstore id map null update test hstore set map map hstorekey1 value1 where id update select from test hstore id map null if cannot have not null constraint on the column can safeguard myself by doing something like thatthis doesnt actually work update test hstore set map if map is null then hstorekey1 value1 else map hstorekey1 value1 where id
84833 we had query yesterday that was intended to page through results but on the first page of results the query only returned records instead of the expect changing the query in any way resulted in records which meant to me that it was an execution plan issue im not familiar with way to view execution plan in production and running the query in sql studio didnt result in same problem probably due to minor differences read suggestions that such thing could be caused by corrupted index ran checkdb on the database and it found no errors in the end cleared the execution plans and all has been well if it wasnt corruption issue of some kind and only problem for the very specific execution plan then does that mean there was simply an error in the execution plan and we are hitting bug in sql server we are on sql server rtm with no updates so looked through documentation on all fixes in cumulative updates and service packs but none of the issues appeared to related to our own any other ideas or thoughts as to what could cause this p1 varchar8000 p2 bit p3 varchar8000 p4 bit p5 varchar8000 select from select top row number over order by addeddate desc as row id prefix firstname lastname company address city state zip country workphone homephone mobilephone email mailinglists addeddate awaitingoptin optindate processed processeddate deleted source grrecid databaseid select count from tablea left outer join tableb on tablea grrecid tableb grrecid where tablea accountid p1 and tablea email tablec email and tablea deleted p2 as matches from tablec where accountid p3 and processed p4 and deleted and source p5 as where row and row order by row
86263 am writing large multi step cte for performance reasons in one query data must be moved from one table to another but the quantity of rows moved is uncertain and could be zero in subsequent table the origin from the previous query is deleted but must be after the previous query is completed finally rows must be written in place of the deleted rows after the second query above is completed in the first two queries am using returning to enforce execution order in the second query im determining that the first query is completed by this subquery select count from first query in the the third query im determining that the second query is completed by this subquery select exists select from second query is the subquery to determine that the first query has completed correct is the subquery to determine that the second query which must return rows has completed optimal for accuracy precision and performance using the above subqueries to enforce execution order is giving duplicate key value violations query subsection with copy to other table as insert into other table column column select column column from main table where column bigint returning main table deleted as delete from main table where column bigint and select count from copy to other table returning insert into main table column column select column column from another table where column bigint and exists select from main table deleted it is the final query that is violating the unique constraint
86274 ive got big table 9m rows and want to group the rows on field containing the year so far thats pretty easy greatly simplified select count year from dataset group by year order by we defined some irregular time periods spanning multiple years and ive got no clue on how to group these results in the group by clause could make subquerys for every time period select select count from dataset where year and as pre1945 as period2 from dataset but that feels not right and im wondering if it was possible to let postgresql do it especially because the query is strong simplification of the real query it has multiple conditions amongst them st within clause spanning four tables so choosing the subquery approach results in bloated query is there better way to create this result
86312 playing around with db projects visual studio generated change script with the following lines go set ansi nulls ansi padding etc set numeric roundabort off go setvar databasename foo setvar defaultfileprefix foo setvar defaultdatapath setvar defaultlogpath go on error exit go setvar issqlcmdenabled true go if issqlcmdenabled not like ntrue begin print nsqlcmd mode must be enabled to successfully execute this script set noexec on end go use databasename go create nonclustered index someindex on dbo sometable somecolumn asc go what is the significance of the colons at the beginning of the lines this stackoverflow question says that the colon is for bind variables but then what does the colon in on error exit do
86313 am trying to run simple query to get all rows created in november select count from dbo profile where created between and smss returns the conversion of varchar data type to datetime data type resulted in an out of range value do not understand why the data is being converted from varchar to datetime when created is set to datetime do need to tell the server that created is datetime if not why am getting this varchar message edit the value in the database was yyyy mm dd reply from sqlzim below says that need to use convert to tell sql what format the date is in the db and to replace the space character with the letter select count from dbo profile where created between convertdatetime2014 01t00 and convertdatetime2014 30t23
86383 why is this simple query granted so much memory demo table create table dbo test tid integer identity not null filterme integer not null sortme integer not null unused nvarcharmax null constraint pk dbo test tid primary key clustered tid go example rows insert dbo test with tablockx filterme sortme select top checksumnewid checksumnewid from sys all columns as ac1 cross join sys all columns as ac2 go query select tid filterme sortme unused from dbo test as where filterme order by sortme for an estimated rows the optimizer reserves almost mb for the sort
86415 often need to select number of rows from each group in result set for example might want to list the highest or lowest recent order values per customer in more complex cases the number of rows to list might vary per group defined by an attribute of the grouping parent record this part is definitely optional for extra credit and not intended to dissuade people from answering what are the main options for solving these types of problems in sql server and later what are the main advantages and disadvantages of each method adventureworks examples for clarity optional list the five most recent recent transaction dates and ids from the transactionhistory table for each product that starts with letter from to inclusive same again but with history lines per product where is five times the daystomanufacture product attribute same for the special case where exactly one history line per product is required the single most recent entry by transactiondate tie break on transactionid
86432 when using sql server emaintenance plan for backup purposes the backup database task creates unique filename understand the format all the way to the final set of numbers for example full database backup initiated via the backup database task would create filename like adventureworks backup bak what is referencing is than lsn something else
86526 am working on stored procedure that retrieves the objectguid from active directory am storing the result in temp table and then returning the value in an output parameter for use with other processes the sp will be called from different stored procedures as well as web applications php asp classic and asp net read here that regarding temp tables if created inside stored procedure they are destroyed upon completion of the stored procedure furthermore the scope of any particular temporary table is the session in which it is created meaning it is only visible to the current user multiple users could create temp table named tablex and any queries run simultaneously would not affect one another they would remain autonomous transactions and the tables would remain autonomous objects you may notice that my sample temporary table name started with sign sounds like am good to go but wanted to get some advice to make sure there arent any gotchas am unaware of here is the sp thanks in advance create procedure stp adlookup user varchar100 objectguid varbinary256 output as set nocount on declare qry char1000 create table tmp objectguid nvarchar256 set qry select from openqueryadsi select objectguid from ldap mydomaincontroller com where samaccountname user insert into tmp exec qry select objectguid castobjectguid as varbinary256 from tmp drop table tmp set nocount off go
86596 update with the overwhelming response to the main question being no the more interesting responses have focused on part how to solve the performance puzzle with an explicit order by although ive marked an answer already wouldnt be surprised if there were an even better performing solution original this question arose because the only extremely fast solution could find to particular problem only works without an order by clause below is the full sql needed to produce the problem along with my proposed solution am using sql server r2 if that matters create orders table if object idtempdb orders is not null drop table orders create table orders orderid int not null identity11 custid int not null storeid int not null amount float not null create clustered index ix on orders storeid amount desc custid add million rows 100k customers each of whom had orders with cte0 as select as union all select rows cte1 as select as from cte0 as cte0 as rows cte2 as select as from cte1 as cte1 as rows cte3 as select as from cte2 as cte2 as rows cte4 as select as from cte3 as cte3 as rows cte5 as select as from cte4 as cte2 as rows finalcte as select row number over order by as number from cte5 insert into orders custid storeid amount select custid number storeid number amount randnumber from finalcte where number set statistics io on set statistics time on for storeid find the top customers ordered by their most expensive purchase amount solution without order by declare top int select distinct top top custid from orders withforceseek where storeid optionoptimize for top fast logical reads cpu time ms elapsed time ms go solution with order by declare top int select top top custid from orders where storeid group by custid order by maxamount desc optionmaxdop logical reads cpu time ms elapsed time ms uses sort operator go here are the execution plans for solution and respectively solution gives the performance need but couldnt get it to work with the same performance when adding any kind order by clause see solution and it certainly seems like solution would have to deliver its results in order since the table has only one index on it seek is forced thus eliminating the possibility of its using an allocation order scan based on iam pages so my questions are am right that it will guarantee the order in this case without an order by clause if not is there another method to force plan that is as fast as solution preferably one that avoids sorts note that it would have to solve the exact same problem for storeid find the top customers ordered by their most expensive purchase amount it would also have to still use the orders table but different indexing schemes would be ok
86636 mysql innodb allows us to disable doublewrite buffering by setting innodb doublewrite other databases doesnt seem to allow this setting to be tweaked how could innodb still be able to maintain data integrity and acid if we disable doublewrite buffering in what situations will it be safe to turn off innodb doublewrite buffer
86724 am learning postgresql and trying to figure out how to create temporary table or with declaration that can be used in place of regular table for debugging purposes looked at the documentation for create table and it says values can be used as query but gives no example the documentation for the values clause linked therein does not have an example either so wrote simple test as follows drop table if exists lookup create temp table lookup key integer val numeric as values but postgresql is complaining about syntax error at or near as my questions are how can fix the statement above how can adapt it to be used in with block thanks in advance
86779 is it possible to refresh materialized view incrementally in postgresql only for the data that is new or has changed consider this table materialized view create table graph xaxis integer not null value integer not null create materialized view graph avg as select xaxis avgvalue from graph group by xaxis periodically new values are added to graph or an existing value is updated want to refresh the view graph avg every couple of hours only for the values that have updated however in postgresql the whole table is refreshed this is quite time consuming the next version allows concurrent update but it still refreshes the entire view with 100s of millions of rows this takes few minutes whats good way to keep track of updated new values and only refresh the view partially
86802 have mysql user and want it to view only the views want and not any other table in the database ive granted this user permissions only on certain views as following grant show view on mydatabase awesome view to thisuser if do show grants statement can only see this permissions as expected however id like this user to query just the views and not the tables that are related to these views but cant find way to do this it seems to be that if want the user to do select on the view the select must also be granted for the table or am wrong if deny the selectstatement in the rest of the tables and in the command line try to do select got the following select from mydatabase fordibenforyoutable error select command denied to user thisuser localhost for table fordibenforyoutable thats what want indeed but also got denied if select the view data is there way can make available to the user just the views and not the tables
86855 ive recently had discussion with colleague who was pushing to remove order by clauses from production query because the order by column was the same as the primary key after lengthy discussion in which tried to explain that he cant guarantee ordering based on the primary key the final conclusion was that he wasnt going to push for the mssql queries to be changed but he was still going to change the db2 queries couldnt immediately find an article disproving that db2 orders queries by the primary key and am currently wondering whether or not it does so my question is how does db2 order query if there is no order by clause does it use the primary key how can you guarantee data is coming out ordered correctly without an order by clause in parallel system
87122 am newbie in databases read around and found out that its probably not great idea to use email address as primary key because string comparisons are slower which effects performance in complex joins and if an email changes id have to change all the foreign keys which requires lot of effort but if my users table requires every user to have an email address and each of those email address should be unique will adding unique index on the email column suffice because afaik unique fields allow null values whereas require every user to have an email address not allowing null values is there something im missing here or im suppose to make email column unique and make sure during data validation on the server that user does enter an email address so that every user has one
87145 have typical case where parameter sniffing causes bad execution plan to land in the plan cache causing subsequent executions of my stored procedure to be very slow can solve this problem with local variables optimize for unknown and optionrecompile however can also dive into the query and try to optimize it im trying to determine whether should given limited time to fix problems would like to know the cost of not doing it as see it if just stick with optionrecompile the net effect is that query plan is recreated every time the query is run so think need to know how to find out what the cost of creating query plan is to answer my own question ive googled with this query and ive gone through the documentation of columns for the dm exec query stats dmv ive also inspected the output window in ssms for actual query plan to find this info finally ive searched dba se none of those led to an answer can anyone tell me is it possible ot find or measure time needed for plan creation
87300 came across the following sentence in this blog post know your data this helps you make the right decisions in terms of data types nullability and churn helps with long term maintenance goals and initial maintenance plans cant figure out what is meant by churn what is this only find plenty of articles talking about churn without saying what it is
87317 we have data warehouse with fairly large record count million rows and often run queries that count records between certain dates or count records with certain flags select isfoo count as widgetcount from widgets as join flags as on flagid flagid where date startdate group by isfoo the performance isnt awful but can be relatively sluggish perhaps seconds on cold cache recently discovered that can use group by in indexed views and so tried out something similar to the following create view testview with schemabinding as select date flagid count big as widgetcount from widgets group by date flagid go create unique clustered index pk testview on testview date flagid as result the performance of my first query is now 100ms and the resulting view index is 100k although our row count is large the range of dates and flag ids means that this view only contains rows thought that perhaps this would criple the performance of writes to the widget table but no the performance of inserts and updates into this table is pretty much unaffected as far as could tell plus being data warehouse this table is updated infrequently anyway to me this seems way too good to be true is it what do need to be careful with when using indexed views in this way
87330 im trying to better understand conceptually the relationship between statistics execution plans stored procedure execution am correct in saying that statistics are only used when creating the execution plan for stored procedure and they are not used in the actual execution context in other words if this is true once the plan is created and assuming its properly reused how important are up to date statistics was particularly motivated by an article read statistics row estimations and the ascending date column which describes scenario very similar to one face daily with several of our clients databases we have an ascending date time column in one of our largest tables that we query regularly using specific stored procedure how do you prevent execution plans from growing stale when you have hundred thousand rows being added day if were updating statistics frequently to combat this issue would it make sense to use the option recompile hint on this stored procedures query any advice or recommendations would be appreciated update im using sql server sp1
87355 have the following oracle sql and its works and all but its quite ugly with all of the ors is there more concise way of doing this select from foobar where subject stat and term or subject stat and term or subject english and term or subject comm and term or subject comm and term or subject stat and term
87435 know that when varcharmax nvarcharmax columns are used the data is stored out of the row the data row will have pointer to another location where the large value is stored have the following questions is each field stored out of the row or only the max ones if you are using the clustered index of the table to read the whole record are fields that are stored out of the row read too varcharmax or nvarcharmax is considered as large value type large value types are usually stored out of row it means that the
87467 our site has some large but simple int int date tables for stats each table has up to rows and gets bigger every day the hosting provider has suggested that we split or partition the tables and have seen this recommendation elsewhere on numerous occasions however am struggling to reconcile this advice with the stated max capacity for sql server database size of terabytes with table rows limited only by available storage based upon those figures the table described above could easily have centillions of rows to the power of ah ha you might say there is difference between capability and performance but in virtually every question about sql server performance the answer is it depends on table design and query design that is why am asking this question the table design couldnt be much simpler nor could the queries which are simple count operations based on an indexed id field
87692 is there way to flush ib logfile0 and ib logfile1 without dumping the tables to sql file deleting then re inserting somebody entered couple plain text credit card numbers into the customer notes table it came up in my pci scan and removed them from the table but they still exist in the log files
88794 am trying to examine some sql with extended events like used to with sql profiler have the following event session if exists select name from sys dm xe sessions where name pysoup tracing begin drop event session pysoup tracing on server end create event session pysoup tracing on server add event sqlserver rpc completed actionsqlserver client app name sqlserver sql text add event sqlserver sp statement completed actionsqlserver client app name sqlserver sql text add event sqlserver sql batch completed actionsqlserver client app name sqlserver sql text add event sqlserver sql statement completed actionsqlserver client app name sqlserver sql text add target package0 event fileset filename nc program files microsoft sql server mssql11 mssqlserver mssql log pysoup tracing xel add target package0 ring bufferset max events limit go alter event session pysoup tracing on server state start thought that the action clause was supposed to list the columns that were returned by the event however dont see sqlserver client app name column when view the event data in the gui what am doing wrong
88942 dont think theres way to do this but id like to be able to query what value running session currently has for its commit write session parameter havent seen anything in the performance views anyone know of sys view from which such value could be retrieved clarification the need is to pull the parameter setting for another session running independent of current session
89031 im trying to migrate query from oracle to sql server here is my query which works great in oracle select countdistinct over partition by count over as from mytable here is the error got after tried to run this query in sql server use of distinct is not allowed with the over clause anyone know what is the problem is such as kind of query possible in sql server please advise
89670 can someone point me to the ms article or blog post that explains in details how alwayson availability group secondary replica catches up with primary after secondary server long downtime did below tests with aag async manual failover read only configuration killed secondary instance during continuos insert into primary and started secondary instance few minutes after aag dashboard turned into green almost immediately after secondary restart and started to catch up with primary until number of rows became the same in both instances no transaction log backup was done same as but few transaction logs were done from primary during the test questions are what is the size of log cache messaging framework etc that are used to keep tran log blocks which are sent to secondary replica can above structure log cache send queue etc whatever is used as transport for aag replication sizes be configured increased similar to encrease of tran log backup retention period in log shipping for example as backed up truncated tran log in test and secondary replica was syncronised with primary automatically what was used to find row difference between primary and secondary apparently not tran log as it was truncated and then bring then in sync how does this automatic catch up process work and what are its limitations
89676 in our datawarehouse context we have to update some record in fact tables inside our etl one thing we did is to create nonclustered indexes just before the heaviest queries and drop them afterwards this lead us to much less time spent scanning tables in queries and the time spent in building indexes has very low impact minutes is this bad practice note we cannot partition tables right now
89760 on one of my tables have an auto incrementing id field it also has another field richter code with unique constraint but over the seasons this field can change thats why im not using it as the primary key in my programs code have load function first thing it does is check the richter code field for the search parameter if it doesnt return anything it then goes on to search the on the id field for the same parameter the problem is that it seems to truncate the value as soon as it encounters an alpha character so im getting complete garbage from my query see screenshot for example can prevent mysql from changing the query
89815 am trying to find way to figure out when was couple of my sql databases were taken offline checked the the logs but could not find any such info and moreover there is no default trace enabled just have info that earlier there was dba who took that offline but no emails or written communication as such can we find an info on this please suggest thanks
90033 want to drop all default constraints check constraints unique constraints primary keys and foreign keys from all tables in sql server database know how to get all the constraint names from sys objects but how do populate the alter table part
90137 im looking for specific best practice or pattern concerning entities that are shared between different entities having relation to one of many for example one may have the generic entity address which could be used to store the common address fields for customers suppliers employees etc would seasoned dba take that route or would he rather add the fields to the corresponding entities im also thinking about maintainability constraints that may in the future differ depending on the entity things like that would love to get references to any authoritative or established works on the subject
90258 am using pg dump pg restore to backup and restore postgresql database but am getting some error messages and non zero exit status from pg restore tried super simple base case outlined below but still got these errors pg restore archiver db error while processing toc pg restore archiver db error from toc entry schema public postgres pg restore archiver db could not execute query error schema public already exists command was create schema public steps to reproduce install fresh vanilla ubuntu distro im using vagrant with this vagrant box install postgresql configure to allow local connections as postgresql user postgres from any linux user create test database im just doing vagrant vagrant ubuntu trusty psql username postgres postgres psql type help for help postgres create database mydb create database postgres vagrant vagrant ubuntu trusty psql username postgres mydb psql type help for help mydb create table dataentry bigint create table mydb insert into data values1 insert mydb insert into data values2 insert mydb insert into data values3 insert mydb create backup of the database like so pgpassword postgres pg dump dbname mydb username postgres format custom pg backup dump delete some rows out of the data table in mydb so we will be able to tell if we restored the data successfully restore the database with pgpassword postgres pg restore clean create dbname postgres username postgres pg backup dump the data is restored but the pg restore command in step exits with status and shows the following output pg restore archiver db error while processing toc pg restore archiver db error from toc entry schema public postgres pg restore archiver db could not execute query error schema public already exists command was create schema public warning errors ignored on restore cannot just ignore this because am running this command programmatically and need to use the exit status to determine if the restore failed or not initially wondered if this problem was because put my database in public the default schema reasoned that public would be created as result of the create option by pg restore before the data was restored which could conceivably try to create that schema as well since that is where my table is but when tried the above steps with my table in different schema the results were the same and the error messages were identical am doing something wrong why am seeing this error
90354 problem definition our database server needs to be transferred to an other datacenter it runs on microsoft sql server enterprise bit and contains two databases of about 2tb and 1tb having little to no downtime for this would be ideal workload those databases are used for net website and are constantly getting updated having it not available over the weekend would be acceptable though the currently in use db would remain the only one in use until the switch over to the new one that switch would ideally be made just by changing dns entries to point to the new db server while making sure the db is not being updated also time taken by this operation does not really matter as long as the switch from one server to the other downtime is kept low approaches considered backup and restore this has been done in the past but involved high downtime even though it was done through an internal network so more efficiently than through internet log shipping as far as understand this approach would minimize downtime by configuring master slave and transferring an exact copy of the master db to its slave being read only as mentioned above no access to the slave would be necessary and we just need way to have replica of the master db without data corruption it also seems to be quite efficient in terms of resources utilization and wouldnt impact to much the master performance might be wrong about this approach so feel free to correct me database mirroring im not too aware of that approach but it seems like valid option no need to have real time sync and performance of the master is quite important so asynchronous would be the way to go if this approach were to be chosen other options that server runs directly on bare metal hardware so lower level solutions are unfortunately not an option maybe there is better way to get this done constraints as described those databases are quite big to the point they are hard to maintain but thats an other problem the versions of sql server will be the same microsoft sql server enterprise bit it will have to be transferred over network between two datacenters so most probably over internet having disks sent from one site to the other for an initial sync is unfortunately not an option having some sort of security for the transfer would be ideal but we will do the best of this situation that should give quite good overview of our needs for this task and hopefully some of you had to face that situation before
90482 is there way to export postgres table data as json to file need the output to be line by line like id 1name david id 2name james edit postgres version
90858 am running postgresql have table with fields id name addr n1 ad1 n2 ad2 need to move the data to new table with fields like id data name n1 addr ad1 name n2 addr ad2 row to json is not the solution for me as select id row to jsont as data from select id name addr from myt adds id to the result as well is there way to choose the fields need name addr in my data field
91223 we have daily task to overwrite number of development databases using backups of the associated production databases the backups are produced by maintenance plans on the production server then transferred to the dev server by ftp each day we run sql statement similar to this to overwrite each database restore database database1 from disk nd path to database1 backup bak with file nounload replace stats go each time we run this we have to replace the file name with the correct most recent file would like to automate this somehow to minimise the chance of operator error the problem is that we cant control the name of the bak file although the format is consistent database name date time and whatever that seven digit number is and the folder will usually contain several days worth of backups
91247 this query gets list of posts created by people you follow you can follow an unlimited number of people but most people follow others with this style of query the obvious optimization would be to cache the post ids but unfortunately do not have the time for that right now explain analyze select post id post actionid post commentcount from posts as post inner join users as user on post userid user id left outer join activitylogs as activitylog on post activitylogid activitylog id left outer join weightlogs as weightlog on post weightlogid weightlog id left outer join workouts as workout on post workoutid workout id left outer join workoutlogs as workoutlog on post workoutlogid workoutlog id left outer join workouts as workoutlog workout on workoutlog workoutid workoutlog workout id where post userid in many more and post private is null order by post createdat desc limit yields limit cost rows width actual time rows loops nested loop left join cost rows width actual time rows loops nested loop left join cost rows width actual time rows loops nested loop left join cost rows width actual time rows loops nested loop left join cost rows width actual time rows loops nested loop left join cost rows width actual time rows loops nested loop cost rows width actual time rows loops index scan using posts createdat public index on posts post cost rows width actual time rows loops filter userid any many more integer rows removed by filter index scan using users pkey on users user cost rows width actual time rows loops index cond id post userid index scan using activitylogs pkey on activitylogs activitylog cost rows width actual time rows loops index cond post activitylogid id index scan using weightlogs pkey on weightlogs weightlog cost rows width actual time rows loops index cond post weightlogid id index scan using workouts pkey on workouts workout cost rows width actual time rows loops index cond post workoutid id index scan using workoutlogs pkey on workoutlogs workoutlog cost rows width actual time rows loops index cond post workoutlogid id index scan using workouts pkey on workouts workoutlog workout cost rows width actual time rows loops index cond workoutlog workoutid id total runtime ms how can this be optimized for the time being have the following relevant indexes gets used create index posts createdat public index on public posts using btree createdat desc where private is null dont get used create index posts userid fk index on public posts using btree userid create index posts following index on public posts using btree userid createdat desc where private is null perhaps this requires large partial composite index with createdat and userid where private is null
91338 we started an alter table query hours ago and only recently realized via pg stat activity that it is waiting on lock we discovered the other query that is holding lock on the table we want to alter and not letting it go our query is simple query changing column data type but it is running on massive table rather than killing the process that is holding onto the lock weve decided wed rather kill the alter table we did not wrap the alter table in transaction as far as understand the fact that our query is waiting for lock means it has always been waiting for lock and it has never changed anything is this true is it safe for us to outright cancel our alter table query or is it possible that the query has already modified something and cancelling it would leave our database in halfway state of some kind ps the plan is to cancel it using select pg cancel backendpid if this is bad idea please let me know
91388 have inherited database and am looking to clean and speed it up have table that contains rows many of which are junk data inserted due to an error on behalf of our programmer before add any new more optimized indexes converted the table from myisam to innodb and am looking to delete lot of the rows that contain junk data the database is mysql and have root access to the server was first running these commands through adminer and then phpmyadmin both with the same results the command am running is delete from tablename where columnname like essentially delete anything in this column that begins with dash it runs for about minutes and then when view the process list its gone then run select from tablename where columnname like and it returns millions of rows why is my delete statement not completing ps am aware of how out of date mysql is am working on moving the db to mysql innodb maybe mariadb xtradb but until that happens am looking to answer this with the db as is edit removed see my answer
91509 on this msdn page it says if you should reorganize or rebuild based on the amount of fragmentation to alter index reorganize over alter index rebuild with online on however we have noticed that even with really high fragmentation over on large and small tables reorganize works fine the fragmentation goes down to less that why does the msdn page say this is it not supposed to work like it is for me or is there drawback am missing hidden problem if dont rebuild
92117 would like to ask question about best practice described in this document http info mongodb com rs mongodb images mongodb performance best practices pdf use multiple query routers use multiple mongos processes spread across multiple servers common deployment is to co locate the mongos process on application servers which allows for local communication between the application and the mongos process the appropriate number of mongos processes will depend on the nature of the application and deployment just little bit of background about our deployment we have lot of application server nodes each of them runs one jvm based process with stateless restful ws as this best practice suggests every single application server node runs its own mongos process which means that the number of jvm processes always equals the number of mongos processes all mongos processes connect to config servers and several mongo shards with replica sets within each shard even though we are using sharded deployment we are not really sharding our collections in fact we have large number of databases which are spread across all of the shards during their creation time and this is our main use case for sharding at the moment since best practice also suggest that the appropriate number of mongos processes will depend on the nature of the application and deployment started to wonder whether our usage of mongos is actually appropriate or if it would be better for us to have several dedicated mongos nodes and let our app servers connect to them without having mongos running locally what is your opinion on the best approach to decide how many mongos instances are appropriate in relation to the application server instance count or the size of the mongodb cluster recently we started to look into cluster management for our stateless web services by which mean tools like docker apache mesos and kubernetes if we are using docker then it is generally discouraged practice to run more than one process within container considering this fact it becomes really hard to make sure that application server container and mongos container are always co located on the same physical node and have equal amount of processes this makes me wonder whether this best practice still applies for the cluster architecture just described if not can you please suggest what would be the better way to locate and deploy mongos processes in this architecture
93510 have postgres database table foo that among other things has column for score that ranges from want query to return the total number of scores the number of scores between and the number of scores between and and the number of scores between and something like the following select count as total count select from foo where score between and as low count select from foo where score between and as mid count select from foo where score between and as high from foo tried this but got an error with the select in the count statements any ideas how can do this im sure theres super simple way in postgres just cant figure out the correct terms to google for
93588 have question about xtp checkpoint im using sql server have database that is in simple recovery model mode it is also being replicated there are no open transactions ive run dbcc opentran and it returns no active open transactions but keep getting this message whenever try to create or drop table or delete data ive replaced my actual database name with the word database name the transaction log for database database name is full due to xtp checkpoint does anyone know why this might be happening and more importantly how can make it stop and yes the database really is in simple recovery model mode the transaction log should truncate automatically incidentally another database that have in full recovery mode did the same thing started returning the same error the transaction log for database database name is full due to xtp checkpoint tried to change the log growth settings to unlimited growth but it wouldnt let me returning the same error can reproduce the problem without any xtp stuff at all except for just the filegroup heres how http pastebin com jwsieu9u
93823 mysql manual replication scale out solutions spreading the load among multiple slaves to improve performance in this environment all writes and updates must take place on the master server reads however may take place on one or more slaves see how to setup replication which looks relatively simple but havent seen how an application should communicate to slaves assume the application would have to determine which slave to read from the application will also have to know which server is doing writes or is it possible to have the application send all queries to the master and have reads proxied to slaves
94545 hi everyone and thanks for your help have the following situation table called statements that contains fields idint stmnt datedate debitdouble creditdouble and balancedouble want to calculate the balance following these rules the first row balance chronologically debit credit and for the rest of the rows current row balance chronologically previous row balance current row debit current row credit as you can see on the picture above the rows are not arranged by date and thats why used the word chronologically twice to emphasize on the importance of the stmnt date value thank you very much for your help
94552 why doesnt my restore operation work created dumpfile for database using mysqldump root databasename home databasename bkup sql then opened the dumfile and confirmed that it contains create table statements for each of the tables in the database so dropped the database and re created an empty database of the same name before running the following restore command mysqldump root databasename home databasename bkup sql this restore command resulted in the following printing in the terminal mysql dump distrib for linux x86 host localhost database databasename server version set old character set client character set client set old character set results character set results set old collation connection collation connection set names utf8 set old time zone time zone set time zone set old unique checks unique checks unique checks set old foreign key checks foreign key checks foreign key checks set old sql mode sql mode sql mode no auto value on zero set old sql notes sql notes sql notes set time zone old time zone set sql mode old sql mode set foreign key checks old foreign key checks set unique checks old unique checks set character set client old character set client set character set results old character set results set collation connection old collation connection set sql notes old sql notes dump completed on but then when log into mysql to check the contents of the database found out that the database is empty as follows mysql use databasename database changed mysql show tables empty set sec so why is the database not being restored what specific syntax can use to ensure that the database does get restored properly
94717 have database that stores bunch of custom fields using hstore in order to merge it into another database that doesnt support hstore id like to split the keys into extra columns users can add new custom fields and so cant rely on knowledge of the keys ahead of time which makes the answer at attributes from an hstore column as separate columns in view not applicable for my problem where record doesnt have key present in other records it should get the same column with null value how do do this
94887 so ive few debian servers with postgresql on it historically those servers and postgresql are localized with the latin charset and back then it was fine now we have to handle things like polish greek or chinese so changing it become growing issue when tried to create an utf8 database got the message error encoding utf8 does not match locale fr fr detail the chosen lc ctype setting requires encoding latin9 few times made some research on the subject with my old pal google and all could find was some over complicated procedures like updating the debian lang recompile postgresql with the correct charset editing all the lc system variables and other obscure solutions so for the time being we let this issue aside recently it came back again the greeks want the stuff and latin dont want to and while was looking into this issue again one colleague come at me and said nah its easy look he edited nothing didnt do magic tricks he just make this sql query create database my utf8 db with encoding utf8 owner admin template template0 lc collate lc ctype connection limit tablespace pg default and it worked fine actually didnt know about lc ctype and was surprised that using this wasnt on the first solutions on google and even on stack overflow looked around and only found mention on the postgresql documentation when lc ctype is or posix any character set is allowed but for other settings of lc ctype there is only one character set that will work correctly since the lc ctype setting is frozen by initdb the apparent flexibility to use different encodings in different databases of cluster is more theoretical than real except when you select or posix locale thus disabling any real locale awareness so it made me wonder this is too easy too perfect what are the downside and ive hard time finding an answer yet so here come posting here tl dr what are the downside of using lc ctype over specific localization is it bad to do so what should expect to break
95425 dont add any manual locking or any transaction anything just manual running queries from php script and execute below two update queries at the same time update table set status processing where id and status pending update table set status processing where id and status pending and if rely on the updated rows count then is it possible that both return the requirement here is that from my web server if two processes at the same time try to process same row then have to avoid both processing it in solution where first run select query to check if the status for the given id is pending and then do the processing there is chance of both processing it but im not sure what will happen if do just one update query and rely on the rows updated count
95595 probably this has been asked before but cant figure it out have phone clicks table sql fiddle http sqlfiddle com 855e0 create table phone clicks id integer not null date date not null industry id integer not null clicks integer default not null insert into phone clicksid date industry id clicks values this table holds phone click event counts for multiple industry ids and dates is it possible to count these clicks for all available industry ids with multiple date ranges as conditions would like to have this output industry id today yesterday last days ive tried using counting with partition by date but got nowhere is it possible to select this data in one query additional benefit would be to be able to specify previous months as date ranges today yesterday march february january etc update ive updated the fiddle to define current month previous months and pre previous month as the date ranges sql fiddle http sqlfiddle com 855e0 im using postgresql but solutions are welcome because well be migrating to it soon
95600 the application am working on displays some tasks based on status and date in that order however for particular status the sort condition should be inverted for example table which looks like this id status planned date inactive active inactive inactive active active active should be returned as id status planned date active active active active inactive inactive inactive note for this example to be simple only two statuses are used but this is an enumeration of different values for this field new pending active inactive cancelled completed also there are other fields that need to be sorted planned date and priority numeric value from to being normal am mentioning this only as fyi as dont believe it to be much relevant and that the general idea can be understood with the simple example above to sort by the status field an enum simply perform select id status planned date from tbl tasks where order by status asc now how do sort the planned date field asc for active status and desc for inactive status
95646 am using sql server r2 version running on windows server r2 standard my database size is around 207mb as it contained 100s of thousands of records in table decided to keep only the first records and delete the remaining so as to minimise the size of the database deleted the records from the database and also rebuilt the indexes delete from toptrends where handleid not in select top handleid from toptrends order by lastmodifieddatetime desc go alter index all on toptrends rebuild with fillfactor sort in tempdb on statistics norecompute on go and checked the size of the database and database log files the database log file size has increased in size but the database file remained the same thought it should decrease file size before deletion around file size after deletion around 207892same size of database log file 625mb up from some 300mb cant we reduce the size of the database by purging unwanted old records from the table and rebuilding the indexes my database log file has increased dramatically after purging the table and dont want that too going large
95740 some innodb tables in our production database are about to hit the int auto increment limit of and we need to alter them to bigint otherwise writes will start failing the tables are in production mysql 19a database running on amazon rds how can we do an alter like this without disrupting the production reads and inserts that are happening all the time alter table mytable change id id bigint not null auto increment here is ddl for the table create table mytable id int11 not null auto increment siteid int11 not null filter varchar10 not null default all date varchar10 not null cards varchar250 not null apples varchar45 not null carrots varchar45 not null corn varchar45 not null peas varchar45 not null primary key id unique key unique siteidfilterdatecards key date date key cards cards key apples apples key siteid siteid engine innodb auto increment default charset utf8
96000 we have an always on cluster that consists of servers there will be more so one is primary and others isare secondary the idea was to dedicate secondary as readonly replica so it would be somewhat search server do know how to set connectionstring in the so it will use secondary replica just add applicationintent readonly but dont understand how to address the secondary from stored procedure if im using distributed query is there way to set some parameters to the query like with statements or something so query would use only replica and not the primary node the thing is obviously from the start server1 is primary and server2 is secondary but when server1 fails then server2 is primary and server1 will be secondary after it get fixed so cannot just use the static server2s name so far ive managed to get current replicas server name as variable and use it in exec find first available replica declare replicaserver nvarchar50 select top replicaserver rcs replica server name from sys availability groups cluster as agc inner join sys dm hadr availability replica cluster states as rcs on rcs group id agc group id inner join sys dm hadr availability replica states as ars on ars replica id rcs replica id inner join sys availability group listeners as agl on agl group id ars group id where ars role desc secondary and connected state and query it declare cmd nvarcharmax select from replicaserver somebase someschema sometable exec cmd but thats shame guess
96039 have this code declare mytable as table month int salary int insert into mytable values select month salary from mytable output want to concat the salary grouping by month so that it will be nvarchar like this how would do this efficiently
96245 have cron task that needs to extract customers with birthdays in given month from mysql innodb table the birthday field is indexed and of type date filtering for april can query the customers table either by select from customers where birthday like or select from customers where monthbirthday which one would you recommend and why
96331 recently upgraded from sql server to during validation however bug was discovered certain trigger was coded as follows create trigger dbo trigger on dbo foo for update update as update foobar set datetime getdate from bar where foobar id bar id go can safely execute this oddly on sql server however on sql server it throws what would expect syntax error syntax error duplicate specification of the action update in the trigger declaration why does this not throw syntax error on sql server my google fu on this has failed me why does this seemingly work on sql server
96346 was was running the query in this article http sqlity net en why cxpacket waits are not your performance problem to see what my threads were waiting on in regards to suspended query with wait type of cxpacket however for the spid in question threads that are running were showing wait types of null with every other thread in suspended state with wait type of cxpacket was expecting to one of the threads having some kind of wait type other than cxpacket can anyone explain to me what is happening in this situation thanks
96556 im converting an old ms access based system to postgresql in access fields that were made up in selects could be used as parts of equations for later fields like this select samples id samples wet weight samples dry weight as percent water percent water as percent water from samples when do this in postgresql postgres throws an error error column percent water does not exist heres how can work around it by selecting out of sub selection select s1 id s1 percent water s1 percent water as percent water from select samples id samples wet weight samples dry weight as percent water from samples s1 is there any kind of shortcut like in the first code block to get around complicated nesting could also just say samples wet weight samples dry weight as percent water but this is just small example out of what is much larger system of math going on in my code with dozens of more complex bits of math stacked on top of each other id prefer to do as cleanly as possible without repeating myself
96743 in postgres we get the stack trace of exceptions using this code exception when others then get stacked diagnostics error stack pg exception context this works fine for natural exceptions but if we raise an exception using raise exception this is an error then there is no stack trace according to mailing list entry this might be intentional although cant for the life of me figure out why it makes me want to figure out another way to throw an exception other than using raise am just missing something obvious does anyone have trick for this is there an exception can get postgres to throw that would contain string of my choosing so that would get not only my string in the error message but the full stack trace as well heres full example create or replace function error test returns json as declare error stack text begin comment this out to see how normal exception will give you the stack trace raise exception this exception will not get stack trace this will give divide by zero error complete with stack trace select in case of any exception wrap it in error object and send it back as json exception when others then if the exception were catching is one that postgres threw like divide by zero error then this will get the full stack trace of the place where the exception was thrown however since we are catching an exception we raised manually using raise exception there is no context stack trace get stacked diagnostics error stack pg exception context raise warning the stack trace of the error is error stack return to jsonv error stack end language plpgsql
96917 im upgrading to and ran into some issues with the upgrade specifically got an error when trying to start up mongod via ssh it tried to use the default dbpath instead of the one specified in my new yaml config file went ahead and rebooted the machine and now mongod is up and running again im bit paranoid at this point and would like to know if theres way to make sure the storage engine is wiredtiger from the shell
97171 have procedure like this simplified create procedure test username varchar64 select from member inner join order on memberid memberid where username username there is non clustered index on username column of the member table plan cache shows an implicit conversion as such seek keys prefix mydatabase dbo member username scalar operatorconvert implicitvarchar64 username was just wondering what might be causing this implicit conversion as both the parameter and the field data type username is varchar64 sp is called from framework like this exec test username nwebsite com thank you
97470 mysql and states if you use group by for select output rows are sorted according to the group by columns as if you had an order by for the same columns however mysql states relying on implicit group by sorting in mysql is deprecated to achieve specific sort order of grouped results it is preferable to use an explicit order by clause the above info is also stated in and what exactly does deprecated mean in mysql is the behavior of implicit group by still guaranteed to work on mysql and the current latest version just like in every single version before that is order by null still required to stop mysql from doing needless sorting edit by rick james this covers database issues the ordering of group by without order by the meaning and usefulness of order by null the wisdom of using deprecated feature the meaning of deprecated in the mysql manual not just for this case
97738 we have table with 3b rows in it wed like to change column from not null to null the column is contained in one index not the clustered or pk index the data type isnt changing its an int just the nullability the statement is as follows alter table dbo workflow alter column lineid int null the operation takes in excess of before we stop it we havent even let it run to completion yet because its blocking operation and was taking too long well probably copy the table to dev server test how long it actually takes but im curious if anyone knows what sql server is doing under the hood when converting from not null to null also will affected indexes need to get rebuilt the query plan generated doesnt indicate whats happening the table in question is clustered not heap
97773 have query in mysql which serves me very well by getting all the records within the current month select date fieldval from my table where date field curdate interval month the above query works well so if this month we only had two records and days it will bring only two records date field val but want the number of records returned to be exactly the same as the number of days of the current month if the current month has days and only had two records it should bring date field val how can modify my query to achieve the above result
97781 have two update queries that are similar in structure yet the sql server query plan for one shows indexes being used and for the other it shows only regular table scan the following are the queries as per the query plan does not use indexes does update payment metadata set payment metadata commoditycode raw materials payment metadata c1 raw materials payment metadata c2 ingredients payment metadata c3 other payment metadata ruletext payment metadata lastupdatedindex payment metadata lastupdatedindex payment metadata isexcluded payment metadata logtext commodity raw materials ingredients other from payment metadata where payment metadata isprocessed and payment metadata enrichedvendor nfl or payment metadata vendor no nfl the second query uses an index update payment metadata set payment metadata commoditycode raw materials payment metadata c1 raw materials payment metadata c2 ingredients payment metadata c3 other payment metadata ruletext payment metadata lastupdatedindex payment metadata lastupdatedindex payment metadata isexcluded payment metadata logtext commodity raw materials ingredients other from payment metadata where payment metadata isprocessed and payment metadata enrichedvendor or payment metadata vendor no the following is the table definition create table dbo payment metadata id int identity11 not null company code varchar null comp code desc varchar null vendor acct group varchar null vendor no varchar null vendor name varchar null vendor abn varchar null vendor pterm varchar null vendor pterm desc varchar null purchasing group varchar null purchasing group des varchar null po doctype varchar null po doctype desc varchar null purchasing document varchar null po date varchar null po createdby varchar null plant int null item number varchar null material number varchar null material group varchar null material group desc varchar null account assignment varchar null acct assignment desc varchar null gl account varchar null gl account desc varchar null po desc varchar null po quantity decimal null order uom varchar null order price unit varchar null invoice receipt varchar null invoice reference varchar null invoice date datetime not null invoice scan date datetime null invoice item int null invoice amount decimal null gst decimal not null invoice gross amount decimal null currency varchar null document type varchar null document number varchar null document date datetime not null posting date datetime not null payment term varchar null baseline date datetime not null due date datetime not null payment document varchar null clearing date datetime not null commoditycode varchar null ruletext nvarchar max null isprocessed bit not null datasource varchar null iscontracted bit not null ispreferred bit not null vendorriskscore varchar null enrichedvendor varchar null lastupdatedindex int not null logtext nvarchar max null isexcluded bit not null c1 varchar null c2 varchar null c3 varchar null originalvendor varchar null adjustedamount decimal null on primary as you can see the only change in the where clause is the usage of numeric vs non numeric characters which suspect should not impact the query plan there is one non clustered index on each of the columns enrichedvendor and vendor no any help would be appreciated update including the query plans below and noticed that query suggests an index in green that may be the solution to the answer
97898 ive recently come into an environment where lot of databases logins do not have the enforce password policy flag enabled an upcoming audit is necessitating the verification of these logins passwords used the following query to obtain list of logins and whether the flags are on or off select servername as servername name is srvrolemembersysadmin name as sysadmin type desc create date is policy checked is disabled password hash pwdcomparename password hash as usernameaspassword from sys sql logins however this doesnt tell me if the passwords actually adhere to the password policy as the flag is only relevant when creating user is there known way to test existing users for password policy compliance have no access to the old passwords and would prefer method that doesnt require them
98529 was reading upon some mysql internals when going through the mysql user table in the my mysql shell get mysql select from mysql user limit row host localhost user root password 81f5e21e35407d884a6cd4a731aebfb6af209e1b the password is obviously hashed but why does it begin with the star asterisk
98553 given the following mysql database structure for booking system how can retrieve all available days between two user supplied dates id code date arrival date departure apt01 apt01 apt02 apt02 for example the user enters as their start date and as their end date the available days over this period are 16th 21st using apt01 and 26th 27th using apt02 this means there are available days out of those requested by the user how can query the data and get as my result
98575 so heres my scenario im working on localization for project of mine and typically would go about doing this in the code however want to do this in sql bit more since am trying to buff up my sql bit environment sql server standard net note the programming language itself should be irrelevant im only including it for completeness so sort of accomplished what wanted but not to the extent wanted its been while at least year since have done any sql joins except basic ones and this is quite complex join here is diagramme of the relevant tables of the database there are plenty more but not necessary for this portion all relationships described in the image are complete in the database the pk and fk constraints are all setup and operating none of the columns described are nullable all the tables have the schema dbo now have query which almost does what want that is given any id of supportcategories and any id of languages it will return either if there is right proper translation for that language for that string stringkeyid stringkeys id exists and in languagestringtranslations stringkeyid languageid and stringtranslationid combination exists then it loads stringtranslations text for that stringtranslationid if the languagestringtranslations stringkeyid languageid and stringtranslationid combination did not exist then it loads the stringkeys name value the languages id is given integer my query be it mess is as follows select case when is not null then else select case when dbo stringtranslations text is null then dbo stringkeys name else dbo stringtranslations text end as result from dbo supportcategories inner join dbo stringkeys on dbo supportcategories stringkeyid dbo stringkeys id inner join dbo languagestringtranslations on dbo stringkeys id dbo languagestringtranslations stringkeyid inner join dbo stringtranslations on dbo stringtranslations id dbo languagestringtranslations stringtranslationid where dbo languagestringtranslations languageid and dbo supportcategories id end as result from select select case when dbo stringtranslations text is null then dbo stringkeys name else dbo stringtranslations text end as result from dbo supportcategories inner join dbo stringkeys on dbo supportcategories stringkeyid dbo stringkeys id inner join dbo languagestringtranslations on dbo stringkeys id dbo languagestringtranslations stringkeyid inner join dbo stringtranslations on dbo stringtranslations id dbo languagestringtranslations stringtranslationid where dbo languagestringtranslations languageid and dbo supportcategories id as as the problem is that it is not capable of providing me all of the supportcategories and their respective stringtranslations text if it exists or their stringkeys name if it didnt exist it is perfect at providing any one of them but not at all basically its to enforce that if language does not have translation for specific key then the default is to use stringkeys name which is of stringkeys defaultlanguageid translation ideally it would not even do that but instead load the translation for stringkeys defaultlanguageid which can do myself if pointed in the right direction for the rest of the query ive spent lot of time on this and know if were to just write it in like usually do it would be done by now want to do this in sql and im having trouble getting the output like the only caveat is want to limit the number of actual queries applied all columns are indexed and such as like them for now and without real stress testing cannot index them further edit another note im trying to keep the database as normalized as possible so dont want to duplicate things if can avoid it example data source dbo supportcategories entirety id stringkeyid dbo languages records only showing two for examples id abbreviation family name native en indo european english english fr indo european french fran ais langue fran aise dbo languagesstringtranslations entirety stringkeyid languageid stringtranslationid added as example dbo stringkeys entirety id name defaultlanguageid billing api sales open waiting for customer waiting for support work in progress completed dbo stringtranslations entirety id text billing api sales open waiting for customer waiting for support work in progress completed les apis added as example current output given the exact query below it outputs result billing desired output ideally would like to be able to omit the specific supportcategories id and get all of them as so regardless if language english was used or french or any other language at the moment id result billing api sales additional example given were to add localization for french add to languagestringtranslations the output would change to note this is example only obviously would add localized string to stringtranslations updated with french example result les apis additional desired output given the example above the following output would be desired updated with french example id result billing les apis sales yes know technically thats wrong from consistency standpoint but its what would be desired in the situation edit small updated did change the structure of the dbo languages table and drop the id int column from it and replace it with abbreviation which is now renamed to id and all relative foreign keys and and relationships updated from technical standpoint this is more appropriate setup in my opinion due to the fact that the table is limited to iso codes which are unique to begin with tl dr so the question how could modify this query to return everything from supportcategories and then return either stringtranslations text for that stringkeys id languages id combination or the stringkeys name if it did not exist my initial thought is that could somehow cast the current query to another temporary type as another subquery and wrap this query in yet another select statement and select the two fields want supportcategories id and result if dont find anything ill just do the standard method typically use which is to load all the supportcategories into my project and then with it run the query have above manually against each supportcategories id thanks for any and all suggestions comments critique also apologize for it being absurdly long just dont want any ambiguity im often on stackoverflow and see questions that lack substance didnt wish to make that mistake here
98745 need to know all databases in sql server on which user can connect with sys databases get all databases and with sys server principals or sys syslogins get all server logons but cant find table which contains connection between these tables has anyone an idea how to solve this thanks
98949 have user called test user created under mysql under ms windows 2008r2 want to grant this user select privileges on all databases except mysql database note have around database inside this instance edit edit2
98954 postgresql supports retrieving the current running transaction id using query like this select txid current does mysql have any such equivalent
99317 query execution plan does not show the locking details by default is it possible to view the locks along with the type acquired during the execution of query
99326 ive been tasked with sending small monthly report to for one of my customers the report has previously been run manually on the instance the output copied to spreadsheet and send to the customer as an attachment im looking for more permanent solution so intend on using sp send dbmail stored procedure to run the query and send it as an attachment everything works but the formatting of the message initially tried to attach the output as csv file with query result seperator but the results were everywhere when run the report normally the output looks fine in sql but sending it as csv or just in the message body doesnt think it might work better if export the output as html and send that as an attachment or as xml but dont know how to do this does anyone have any suggestions thanks in advance
99334 have come across three old databases sitting on sql server which need to move to believe the standard approach is to restore into or instance update re export and finally restore into fine except we have no or instances available are there any workarounds or other methods that might be worth trying for information the databases only contain tables and few views they appear very simple and the backups are only 200mb in size
100643 just dont get it see this sql query select nchar65217 select nchar65218 select nchar65219 select nchar65220 if nchar65217 nchar65218 print equal if nchar65217 nchar65219 print equal if nchar65217 nchar65220 print equal based on transitive relation it means that sql server considers them all to be the same character however in other environments say for example theyre not the same what im confused about is how string comparison works in sql server why comparison doesnt behave the same on one machine and one platform but different environments these characters represent one human understandable character why they are so abundant in unicode character map this of course results in tremendous problems because im working on text processing application and data comes almost from everywhere and need to normalize text before processing it if know the reason of difference might find solution to handle it thank you
100665 the following query works select from unnestarray as ta integer integer however wasnt able to use different column type such as varchar255 select from unnestarray 1hello 3world as ta integer varchar255 error function return row and query specified return row do not match detail returned type unkown at ordinal position but query expects text it seems that in the second case the column type is inferred as unknown which is not cast to varchar255 automatically how do make the second example work and return columns with the right type if possible without warnings and without modifying the array definition background am trying to improve performance of large bulk insert operations using the psycopg2 python module which does not support using multiple rows in values arguments stumbled onto the above example while trying out some other methods
100749 suppose have table with an id column and and am maintaining unique values manually while inserting and updating records instead of creating unique key on that column by using date time for the id column value such as am not using any index or key in my table so does it affect query performance when execute select query with where clause even if the cardinality is for the id column select from mytable where id mean does unique constraint help to improve query performance or does it just force the user to maintain the record uniquely so that sql will get higher selectivity on the unique constraint column dont know much about unique constraint and unique index the answer given on this question helps in understanding the difference between uniqueindex and uniquekey want to know more about unique key and how it can improve my query performance
100899 have table with below structure create table dbo audit schema version schema ver major int not null schema ver minor int not null schema ver sub int not null schema ver date datetime not null schema ver remark varchar null some sample data seems problem with sqlfiddle so putting some sample data insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values1613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values1613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values1713cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11013cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11213cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11213cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11613cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values250cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values260cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values270cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values280cast20141209 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values290cast20141209 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values2100cast20141209 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values2130cast20150323 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11013cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11614cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11615cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values220cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values230cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values240cast20140417 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11313cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values11613cast20130405 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values2110cast20141209 as datetimestored procedure build insert into audit schema version schema ver major schema ver minor schema ver sub schema ver date schema ver remark values2120cast20141209 as datetimestored procedure build here is the sqlfiddle with some sample data can someone with sql expertise guide me on how to achieve the final result know that pivot with dynamic columns will be the right approach but cant figure it out expected results so far have below select row number over partition by convertvarchar10 schema ver date order by schema ver date as rownum convertvarchar10 schema ver date as upg date convertvarchar1 schema ver major convertvarchar2 schema ver minor convertvarchar2 schema ver sub as schema ver from audit schema version where schema ver remark like stored procedure build order by upgrade date
100965 im trying to combine multiple date ranges my load is about max most cases that may or may not overlap into the largest possible contiguous date ranges for example data create table test id serial primary key not null range daterange insert into test range values daterange2015 daterange2015 daterange2015 daterange2015 daterange2015 daterange2015 null daterange2015 daterange2015 table looks like id range rows desired results combined visual representation
101211 poorly managed database table has grown to be enormous gigs of orphan records am trying to clean it up and put my dangerously full hard drive back to normal state will be deleting approx million records from this table this is running as type notice that am not seeing any drop in my hard drive space but am seeing the memory drop on system table queries am running to get table size the database is using simple recovery model there are many questions similar to this with responses saying you need to shrink the database but they go on to explain how bad scary this is to do because of fragmented data etc since the database should not be this size is it still bad for me to shrink it this is production database if shrink it will it cause downtime or lock the database in sql server management studio you have two options for shrink database or files given my situation what would be the best option is there rule on the percentage of free space db should have even reading the tag description of shrink makes me not want to do it is there another way
101917 first command use master go second command alter database mydb set single user with rollback immediate go third command restore database mydb from disk nd restore backup restore acctdb bak with file move acctdb to nd program files microsoft sql server mssql10 mssqlserver mssql data wfstageacct mdf move acctdb log to nd program files microsoft sql server mssql10 mssqlserver mssql data wfstageacct log ldf nounload replace stats go forth command alter database mydb set multi user go run the above commands one by one to restore database in different server but sometime cannot restore the database after changed to single user mode because it is accessed by other connection if run the whole script except the last part together would that block other connection so will be the only user
102066 have provisioned my server with ansible playbook ive used the root bedrock ansible playbook one of the tasks was to set up mysql server together with mysql root user password now urgently need to change this password the steps took updated variables for ansible roles executed the command ansible playbook hosts staging server yml in order to reprovision the server all tasks were executed as expected no changes but the script failed at mariadb set root user password with this message msg unable to connect to database check login user and login password are correct or my cnf has the credentials my guess is that once mysql root password has been set reprovisioning the server cannot change this password is it possible at all to change mysql root password by reprovisioning the server with ansible what are my options
102208 ive heard am pretty sure that developer edition is free of charge and for downloading it registration is not required but when started looking for it today could not find where to get it from is my understanding wrong or right sorry didnt find similar question here any ideas
102292 this may fall under the category of opinion but im curious if people are using trace flag as startup parameter for sql server for those that have used it under what circumstances did you experience query regression it certainly seems like potential performance benefit across the board im considering enabling it globally in our non production environment and letting it sit for couple months to ferret out any issues are the fixes in rolled into the optimizer by default in or although understand the case for not introducing unexpected plan changes it seems odd to keep all these fixes hidden between versions were using 2008r2 and mostly
102370 im creating accounting software need to enforce double entry bookkeeping have the classical problem of one row per transaction versus two rows lets take an example and see how it would be implemented in both scenarios consider account cash and account rent when pay my monthly rent transfer from my cash account to me rent account one row per transaction in one row system such transaction would be stored as transactions tx id posting date transaction records id tx id credit account debit account amount cash rent two rows per transaction in two row system id have to mirror the same transaction record to create an opposite record that once sum up both id get zero balance transactions tx id posting date transaction records id tx id type account amount credit cash debit rent the problem first of all id like to note the reason have both transactions and transaction records tables instead of one table is to be able to handle split transactions case where transfer from cash account to two or more different accounts at first tried to implement this with one row per transaction but its pain to calculate the account balance and to actually retrieve the data im leaning towards the second scenario however it also has some issues how do update single record assuming ive made mistake and instead of recording for my rent ive recorded now have transaction records one for credit and one for debit both with amount now do my reconciliation and want to fix this typo how would fix this in the database dont know the connection between records and in case of split one transaction can have more than records the only solution came up with is to add some ref id for each records pair that will uniquely identify those records as being the opposite sides of each other inside context of specific tx id which approach is better simpler to simplify my question want to represent movement of funds from account to account the two scenarios gave are both valid designs to store such transaction as also pointed out they both have cons pros first one easier to save harder to retrieve second one the opposite they might have other pros cons that do not spot right now hence ask an opinion from more experienced people
102463 understand that trigger on table defined with for each statement will run once when execute an update now when is defined with foreign key references on update cascade and update rows in will that cause the trigger to be called once or times put differently are changes to table cascaded by an fk constraint more like single update or more like series of updates
102492 is there way to calculate today and yesterday of last month and last year in sql assuming today is may then how to find today of last year may today of last month april yesterday of last year may yesterday of last month april
102605 wanted to try out the contained database users feature on azure sql database v12 but im having problem authenticating that seems odd to me created database called classifier added my ip to the firewall rules so could connect to the azure db server from ssms on my workstation once was able to get connected via ssms for administration tried adding user with password to the database like this create user classifier with password thepassword also added this user to the data writer and reader roles exec sp addrolemember db datawriter classifier exec sp addrolemember db datareader classifier after this im able to connect to the database with these credentials from ssms but this is where things go awry ive tried several different connection string incantations and cant seem to get connected in web app im working on it didnt work in the azure environment so im running on localhost with connection string to the azure database and it just wont connect heres the connection string im using at the moment add name classifier connectionstring data source xxxxxxx database secure windows net initial catalog classifier user id classifier password xxxxxxxxxxxxx encrypt true trustservercertificate false connection timeout providername system data sqlclient ive tried resetting the password via ssms for the user and then updating the connection string also double checked the password by copying it right out of this connection string and into the connect dialog in ssms to make sure didnt have typo of some kind there enabled auditing in the azure db server hoping to get some details as to why its failing but all get is this and this is where im stuck most of what ive been able to find by way of documentation or blogs indicate that the thing to do is look at sql server logs to see what the real error state is which would indicate more narrowly the nature of the failure but since im dealing with azure theres no way to do that as far as know what could cause the application to fail where ssms and linqpad and visual studio server explorer incidentally succeeds
102620 im wondering how to query the definition of materialized view in postgres for reference what hoped to do is very similar to what you can do for regular view select from information schema views where table name some view which gives you the following columns table catalog table schema table name view definition check option is updatable is insertable into is trigger updatable is trigger deletable is trigger insertable into is this possible for materialized views from my research so far it appears that materialized views are deliberately excluded from information schema because the information schema can only show objects that exist in the sql standard http www postgresql org message id sss pgh pa us since they appear to being entirely excluded from information schema im not sure how to go about this but what id like to do is twofold query whether particular materialized view exists so far the only way ive found to do this is try creating mat view with the same name and see if it blows up and then query the definition of the materialized view similar to the view definition column on information schema views
102677 supposing we have table with four columns abcd of the same data type is it possible to select all distinct values within the data in the columns and return them as single column or do have to create function to achieve this
102745 want to take backup of particular tables available in my database in bak file and all these should be done using sql script
102903 is it acceptable to have circular reference between two tables on the foreign key field if not how can these situations be avoided if so how can data be inserted below is an example of where in my opinion circular reference would be acceptable create table account id int primary key identity name varchar50 create table contact id int primary key identity name varchar50 accountid int foreign key references accountid alter table account add primarycontactid int foreign key references contactid
103273 have two example queries shown below both drop and create database and partitioned table in it the table has several partitions with data in each of them including the last unbounded one the partitioned data goes from to then each script adds new partition the new partition starts at in the first script the transaction log shows that every record in the last partition partition is deleted and re inserted the second script shows that no activity for the rows took place no deletes and inserts the only difference between the two scripts is the range left vs range right on the partition function range right causes no deletes or inserts range left causes all the rows in the last partition to be deleted and reinserted thought range left or range right just controlled if the border value went with the left or right partition but it clearly does something else too is there more to range left and range right that dont understand also like the idea of adding partitions without impacting my system and am willing to use range right if it gets me that however am worried that this may be bug of some kind and that should not rely on it as it may be fixed in later version is this feature that can be relied on scripts range left does deletes and inserts use master go comment this next line out for the first run drop database partitiontest go create database partitiontest go use partitiontest go add filegroups alter database partitiontest add filegroup datepartitiontest go alter database partitiontest set recovery simple go add files alter database partitiontest add file name npartitiontest filename nd partitiontest ndf to filegroup datepartitiontest go this is the only part that is different between the two scripts left vs right create partition function orders id function bigint as range left for values go create partition scheme create partition scheme orders scheme as partition orders id function to datepartitiontest datepartitiontest datepartitiontest datepartitiontest create table create table dbo orders orddate datetime not null id bigint identity11 not null addr varchar100 not null partition the table create unique clustered index ix orders on orders orddate ascid asc on orders scheme id go insert rows into partitions partition in this case use partitiontest set nocount on go declare int set declare date datetime while begin set date dateaddmi i2012 01t10 insert into testtable values date insert into orders values date denzil insert into orders values dateaddmonth3 date denzil insert into orders values dateaddmonth6 date denzil insert into orders values dateaddmonth9 date denzil set end check the rowcount in each partition select partition orders id function id as partionnumcount as countrows from orders group by partition orders id function id alter partition scheme orders scheme next used datepartitiontest go set check point for the log file checkpoint go add the new partition alter partition function orders id function split range select operationcount as numlogrecords from fn dblognullnull where allocunitname dbo orders ix orders group by operation order by count desc select name as tablenamei name as indexname partition id as partitionid partition numberrows fg name from sys tables as inner join sys indexes as on object id object id inner join sys partitions as on object id object id and index id index id inner join sys destination data spaces dds on partition number dds destination id inner join sys filegroups as fg on dds data space id fg data space id where name orders and index id in range right does not do inserts or deletes use master go comment this next line out for the first run drop database partitiontest go create database partitiontest go use partitiontest go add filegroups alter database partitiontest add filegroup datepartitiontest go alter database partitiontest set recovery simple go add files alter database partitiontest add file name npartitiontest filename nd partitiontest ndf to filegroup datepartitiontest go this is the only part that is different between the two scripts left vs right create partition function orders id function bigint as range right for values go create partition scheme create partition scheme orders scheme as partition orders id function to datepartitiontest datepartitiontest datepartitiontest datepartitiontest create table create table dbo orders orddate datetime not null id bigint identity11 not null addr varchar100 not null partition the table create unique clustered index ix orders on orders orddate ascid asc on orders scheme id go insert rows into partitions partition in this case use partitiontest set nocount on go declare int set declare date datetime while begin set date dateaddmi i2012 01t10 insert into testtable values date insert into orders values date denzil insert into orders values dateaddmonth3 date denzil insert into orders values dateaddmonth6 date denzil insert into orders values dateaddmonth9 date denzil set end check the rowcount in each partition select partition orders id function id as partionnumcount as countrows from orders group by partition orders id function id alter partition scheme orders scheme next used datepartitiontest go set check point for the log file checkpoint go add the new partition alter partition function orders id function split range select operationcount as numlogrecords from fn dblognullnull where allocunitname dbo orders ix orders group by operation order by count desc select name as tablenamei name as indexname partition id as partitionid partition numberrows fg name from sys tables as inner join sys indexes as on object id object id inner join sys partitions as on object id object id and index id index id inner join sys destination data spaces dds on partition number dds destination id inner join sys filegroups as fg on dds data space id fg data space id where name orders and index id in
103402 have triggers on one table one works for inserts create trigger get user name after insert on field data for each row execute procedure add info this updates some values in the table and one for updates to fill history table create trigger set history before update on field data for each row execute procedure gener history the problem is that when insert new row in the table the procedure add info makes an update and therefore fires the second trigger which ends with an error error record new has no field field1 how can avoid this
103625 due to mishap deleted the entire var lib mysql directory since the database did not contain anything important do not want to go through the hassle of restoring it from an old backup but instead create the directory structure from scratch how do do it without reinstalling mysql
103688 have been asked to identify permissions issue with stored procedure this stored procedure behaves in two possible ways depending on what values are used for its parameters exec ps my stored procedure is handled very differently from exec ps my stored procedure you could say that ps my stored procedure is divided logically into two completely separate processes using dm exec procedure stats and dm exec query stats can find the execution plan which shows the sql of the stored procedure used have not been able however to recover how the parameters were defined and with what values is it possible using dm exec procedure stats and dm exec query stats and any other management views to reconstruct the execution of the stored procedure that shows the values used for its parameters what id really like is to find in cache is the actual execution of the stored procedure so that can execute it as is using execute as login someone to resolve the permissions issues
103815 this is related to this question joining multiple tables results in duplicate rows have two tables that am joining they share key the person table has one name per primary key but the email table has multiple emails per personid want to only show the first email per person presently get multiple rows per person because they have multiple emails am running sql server edit this is sql first email is literally the first email row per person edit first email as see it would be the first email row that shows up in the join as sql works through the query does not matter which email shows up only that no more than one email shows up hope that makes it clearer table1 person table2 email select person personname email email from person left join on person id email personid
104172 have more than tables named public test how can easily grant all access to the user test to all that tables at once tried with grant all on table public test to test but it is not working
104192 have problem understanding why sql server decides to call user defined function for every value in the table even though only one row should be fetched the actual sql is lot more complex but was able to reduce the problem down to this select groupcode ordercategory from orderline join orderhdr on orderid orderid join product on product product cross apply dbo getgroupcode factory where ordernumber xxx yyy and rmphase and orderline for this query sql server decides to call getgroupcode function for every single value that exists in product table even though the estimate and actual number of rows returned from orderline is its the primary key same plan in plan explorer showing the row counts tables orderline 5m rows primary key ordernumber orderline rmphase clustered orderhdr 900k rows primary key orderid clustered product rows primary key product clustered the index being used for the scan is create unique nonclustered index product factory on product product factory the function is actually slightly more complex but the same thing happens with dummy multi statement function like this create function getgroupcode factory varchar4 returns table type varchar8 groupcode varchar30 as begin insert into type groupcode values xx yy return end was able to fix the performance by forcing sql server to fetch the top product although is max that can ever be found select groupcode ordercat from orderline join orderhdr on orderid orderid cross apply select top factory from product where product product cross apply dbo getgroupcode factory where ordernumber xxx yyy and rmphase and orderline then the plan shape also changes to be something expected it to be originally also though that the index product factory being smaller than the clustered index product pk would have an affect but even with forcing the query to use product pk the plan is still the same as original with calls to the function if leave out orderhdr completely then the plan starts with nested loop between orderline and product first and the function is called only once would like to understand what could be the reason for this since all the operations are done using primary keys and how to fix it if it happens in more complex query that cant be solved this easily edit create table statements create table dbo orderhdr orderid varchar8 not null ordercategory varchar2 null constraint orderhdr pk primary key clustered orderid create table dbo orderline ordernumber varchar16 not null rmphase char1 not null orderline char2 not null orderid varchar8 not null product varchar8 not null constraint orderline pk primary key clustered ordernumberorderlinermphase create table dbo product product varchar8 not null factory varchar4 null constraint product pk primary key clustered product
104378 im getting extremely long delays seconds in sql server management studio when attempting to connect to sql server instance over tcp using windows authentication this happens when connecting object explorer or new blank query window once connected running queries is fast the problem does not happen when connect using sql server authentication environment windows logged in as domain user tcp connection via ip address not hostname the server is at remote location connected via vpn no encryption when logged into co workers windows computer with my domain account and connected to the same sql server through the same vpn there was no delay when the same co worker logged into my pc with his own domain account he experienced the delay these tests show that the problem is unique to my pc also the problem only appears when connecting to this specific sql server and vpn can connect to other sql servers on the local network via windows authentication without any delay things ive tried with no success disabled anti virus and firewall renamed the folder under userprofile appdata roaming microsoft sql server management studio to to force ssms to recreate my user settings force network protocol to tcp rather than default also tried named pipes but my server isnt setup for that installed ssms and tried that instead of disabled ipv6 blackholed crl microsoft com to in my etc hosts file disabled the customer experience improvement program in ssms visual studio and windows uninstalled all sql server related apps from my pc and reinstalled just tcpview clues using tcpview noticed that when make new connection its state becomes established right away but then one or two more connections with the sql server are continually attempted and closed with time wait on my co workers computer these connections are established and solid so im pretty sure this is the source of the timeouts but what are the connections for and why do they fail dont have any addons in my ssms any ideas update intellisense autocomplete clue noticed that once finally do connect intellisense autocomplete doesnt work do those require separate connections from ssms tried disabling them and it didnt seem to resolve the long connection delay
104421 lately ive been refactoring the sql server indexes of the products that have developed for my employer one of these products is an online dashboard containing multiple user levels each user on admin level can create projects that they can then link to users of the user level below each project has its own name field and also the field administratoruserid to keep track of the project admin created combined non clustered unique index on these two fields meaning that project names have to be unique per administrator now heres the catch am noticing that often do select on administratoruserid only and not on name according the indexing guidelines and please correct me if im wrong am to create non clustered index on the field in order to optimize my queries my question being does my existing index on name administratoruserid already improve my queries or is it advisable to create separate index besides it would there be any downside to administratoruserid being present in two separate indexes
104460 for mysql know the database is backed up table by table in sql statements this results in locking and if you update columns while backing up you might end up in integrity problems to my understanding this does not apply for the microsoft sql server but how does the sql server handles this is there some internal freeze to keep the db consistent also heard that backing up is single threaded meaning it uses only one core assuming you backup to single file also assuming you have multicore machine for example cores or at least significant larger number than one from my personal experience never had issues when taking backups neither locking nor overhead issues but my experience is limited thats why always recommend turning backup compression on in the server properties so what happens when backup job is running and also are there significant differences for the different versions for example and not the licenses
104565 it didnt happen to me yet but was thinking about this was messing around with my training environment and by mistake clicked in the database name and then touched the letter lets assume that hit enter now the database is called and dont remember the original name ctrl doesnt work what to do in case like this in production environment know it could not happen because the database isnt set as single user but if it happens what to do for arguments sake lets say its database that nobody is using right now
104573 have years worth of inventory data and want to extract the first and last day of each months data what is the best way to do this
104596 consider the following table id group id order val reset val val null null null null null null null null null null for each row need to compute the cumulative sum of val for all previous rows ordered by order val and grouped by group id but each time non null reset val is encountered need to use that value for the sum the rows that follow also need to build on top of the reset val instead of using the actual sum note that each group can have multiple reset values this is the result expect for the above table id group id order val reset val val cumsum null null null null null null null null null null if not for the reset values could use window query select temp coalescesumval over partition by group id order by order val rows between unbounded preceding and preceding as cumsum from temp sqlfiddle of the above was originally mistakenly thinking that could just put reset val at the beginning of the coalesce but this wont work since it wont reset the value for following rows also tried this solution but it only resets back to zero not to value in the column adapting it to do so is proving non trivial because that value must propagate to all following rows recursive query seems like natural fit but havent been able to figure out how to do that as of yet should probably mention that the table actually have to deal with is much larger hundreds of thousands to couple million rows than the example above so please mention if there are any performance pitfalls with any answer
104898 am building config table and want one row to be the default value for example have list table of all my databases and table called msg which has all my return values in the msg table have one row that is alldbs and want that value returned if there isnt another row for the joined database so the output should be list table dbname createdate master model msdb tempdb dummy msg table dbname msgval alldbs message dummy message desired output dbname msgval dummy message master message model message msdb message tempdb message now can get the result but the query is clunky and it seems like there should be better way select dbname msgval from list join msg on dbname dbname union all select dbname msgval from list msg where dbname not in select dbname from msg and dbname alldbs is there more efficient way to write this query sqlfiddle link using sql
104987 is possible to create atomic transaction in postgresql consider have table category with these rows id name tablets phones and column name has unique constraint if try begin update category set name phones where id update category set name tablets where id commit im getting error duplicate key value violates unique constraint category name key detail key name tablets already exists
105023 is there any point at which you become so familiar with your language database system that there is no need to test new feature configuration query etc by contained simulated testing before implementing it in your system especially concerning feature that modifies data or is it always essential to test new query by simulation in test environment to specify further it is clear that it is always safest to test however is there way to determine when the risk is so minimal that testing is not worth the effort another way of phrasing that when or is it ever professional practice to take measured risk to implement feature also lets assume that everything is backed up so worst case scenario the data could with some effort be restored can someone cite specific expert experience to address this please include references where appropriate possible
105195 im creating conceptual diagram yes know ive included attributes and keys but this is just for me to consolidate what im doing whilst learning so please treat it as conceptual with the focus on relationships and tables and not how to diagram my mind hurdle is im trying to ascertain the best way to model the profile location and organization relationships first of all rules one or more profiles can be member friend of one or more organizations and vice versa one or more profiles can be member friend of other profiles one or more organizations can be member friend of other organizations friend and member differ in that friends are like read only and members depending on level have full access to amend things to complicate things further locations have their own set of further refinable rules an organization owns two locations but depending on location rules member profile of that organization may have full access at one location but restricted access at the other sorry youll most likely have to open the image in another window for better viewing size so as you can see the concept of profiles and organizations are much the same as well as this yet to be modelled concept of friends and members which imagine will be handled much like the current intermediary tables with setting owner admin member friend etc in the record hence why thinking of the following concept see option in the above image which would remove the current organization and organization locations tables and their relationships replacing it with the option organization table as somewhat recursive relationship with profile suppose the crux of the matter is whether or not im being too programmatically minded with polymorphism to the detriment of simplicity and flexibility confusing myself entirely in the process thanks for your thoughts in advance much appreciated revised diagram in response to mdccls questions yes profile is made up of one person and has the same meaning though where your rationale is headed believe youre correct organization and person could be subtypes of profile therefore profile is either made up of one person or one organization one email address per profile yes as above organizations should at least have an email address correct one fixed address its possibility but rarity though from what im learning one should therefore model such for future longevity etc and just to confirm location could therefore be owned by more than one person location is definitely the integral entity between most others perhaps will clarify what can be done succinctly here then let you read though my other answers which will hopefully pertain beneficial additions to this question first then see my answer to at the end re the role owner an organization can be an owner of zero or more locations person can be an owner of zero of more locations therefore as you previously surmised simply put profile can be an owner of zero or more location yes profile that is an owner of location assumes all role permissions super user profile that is an admin can amend certain details of the location but mainly helps edits the details data supplied via all other profile this will primarily be supplied by basic member of said location which leaves basic member who can read only all related location details and supply data that must be scrutineered by an admin owner beyond this any profile organization person is much like basic member read only lets term them guest but only if the location is set as public and not private though they cant supply input like basic member can correct your intuition is amazing yes it is foreseen that single location could contain one to many locationtypes to complicate things further it is anticipated that those individual locationtypes could have varying permissions for profiles associated with the parent location of which permissions would filter down from the location to the locationtype much like os folder security permissions note via your diagram you might be referring to type more as description yes see correct the ability for profile1 person or organization to act upon profile2 person or organization owned locations if theyre friend member with correct permissions is paramount very reasonable agree agree yes hmm perhaps it should be much the same as profile person to profile person friends whatever the description it would revolve around location as organization will act upon other organization location though semantically doubt any organization would want to appear subservient as member of that locations organization to be able to so no matter how good the cause this is still tad grey for me but here goes to possibly my detriment the similarity between member friend relations are so close that thought to combine them in hindsight with your diagram and interpreting it appears you may be right to keep them separate was going to differentiate the single relationship via enum property owner admin member friend your concept as location that is owned by an organization will have zero to many profile person or organization act upon it though there should be clear difference between how the profiles act upon the location via its relation member or friend denoted through roles so perhaps the default relation between any profile is friend much like guest at answer enabling them to view the read only location data and msg email the location owner admin but not allow them to receive location updates news etc as member would
105386 im trying to perform insert update by selecting data from another db and this is what have so far insert into pdone reps veeva rep iddisplay nameusernamefirstlastemail select id concatucaseleftfirstname 1ucaseleftlastname 1username firstname lastname email from veeva user where id 00580000003ub5vaaw first problem got this error err you have an error in your sql syntax check the manual that corresponds to your mariadb server version for the right syntax to use near from veeva user where id 00580000003ub5vaaw at line and im not sure what is wrong on the query any advice firstname could be john or john or john or any and want to normalize as john is ucaseleftfirstname fine for this the same apply to lastname if firstname is john and lastname is doe then username should be john doe with space between them is my concatenate right should insert some statics fields like one url or just veeva how can do that values are not present on the query shown here but is just add two more columns to the insert avatar url and rep type am planning to add on duplicate key update but can be possible to add restriction based on column lets said update only if now lastsyncdate
105431 would like to use oracles automatic memory management with limit of around 4gb past experience has shown this to be plenty for my dev pc and that it is easier to get an idea about any performance issues with less than that believe these are the instructions to follow oracle automatic memory management it says in short to enable automatic memory management set memory target and optionally memory max target the former parameter should be dynamic value and the latter harder limit that can only be changed when stopping starting the database set these as follows as sysdba sql alter system set memory target 4g scope spfile system altered sql alter system set memory max target 8g scope spfile system altered and check the values with sql show parameter target scope can be just memory for the current uptime spfile or both pick the spfile as will be restarting sql shutdown immediate database closed database dismounted oracle instance shut down sql startup that however was too simplistic ora specified value of memory target is too small needs to be at least 13104m it is not hard to revert but in my opinion also not immediately obvious how to get the memory adjustment through so will post what did below and the senior posters can correct shred me in either case think it is nice to have it in one place here
105443 im running sql server on my laptop along with ton of other installed applications from visual studio to video games this is obviously not production server but for development and debugging only ive scripted out fairly small and simple database with few dozen tables and views and ive inserted less than rows of dummy data for testing at one point accidentally created 100gb index using some badly formed cross joins the query creating this index was consuming of the ram in my laptop 16gb which slowed everything to crawl so aborted the query rather than waiting for it to finish then promptly dropped and re created the entire database with this index removed from the creation scripts however sql server kept consuming all 16gb of my ram from that point forward even with the query aborted and the database dropped so rebooted my laptop next and the ram usage for sql server dropped down to 7gb where it has remained ever since regardless whether or not drop all of the 5mb of existing databases etc id like to get sql server back to reasonable memory consumption level but dont know where to begin looking for possible causes know this is pretty open ended question but im willing to do the legwork with little guidance just dont know where to begin troubleshooting since im not dba myself and apparently google doesnt know either
105461 have table of people who are doing piece work every entry in the table has user id name and date for each report they enter need to make weekly report of what these people have done per day starting with sunday how do you group by week starting sunday how do you make column for each count per day t0001 tod t0001 tod t0001 tod t0001 tod t0001 tod t0001 tod t0001 tod b0002 ben b0002 ben b0002 ben this is what im looking for name total tod ben
105525 have two tables t1 table id int date datetime t2 table id int date datetime these tables have non clustered index on id date and join these tables select from t1 as t1 inner join t2 as t2 on t1 id t2 id where t1 date getdate and t2 date getdate this can also be written as select from t1 as t1 inner join t2 as t2 on t1 id t2 id and t1 date getdate and t2 date getdate my question is which of these two queries gives the better performance and why or are they equal
105537 have table with about million rows in it and an index on date field when try and extract the unique values of the indexed field postgres runs sequential scan even though the result set has only items why is the optimiser picking this plan and what can do avoid it from other answers suspect this is as much related to the query as to the index explain select labeldate from pages group by labeldate query plan hashaggregate cost rows width group key labeldate seq scan on pages cost rows width rows table structure http pages table public pages column type modifiers pageid integer not null default nextval createdate integer not null archive character varying16 not null label character varying32 not null wptid character varying64 not null wptrun integer not null url text urlshort character varying255 starteddatetime integer renderstart integer oncontentloaded integer onload integer pagespeed integer rank integer reqtotal integer not null reqhtml integer not null reqjs integer not null reqcss integer not null reqimg integer not null reqflash integer not null reqjson integer not null reqother integer not null bytestotal integer not null byteshtml integer not null bytesjs integer not null bytescss integer not null byteshtml integer not null bytesjs integer not null bytescss integer not null bytesimg integer not null bytesflash integer not null bytesjson integer not null bytesother integer not null numdomains integer not null labeldate date ttfb integer reqgif smallint not null reqjpg smallint not null reqpng smallint not null reqfont smallint not null bytesgif integer not null bytesjpg integer not null bytespng integer not null bytesfont integer not null maxagemore smallint not null maxage365 smallint not null maxage30 smallint not null maxage1 smallint not null maxage0 smallint not null maxagenull smallint not null numdomelements integer not null numcompressed smallint not null numhttps smallint not null numglibs smallint not null numerrors smallint not null numredirects smallint not null maxdomainreqs smallint not null byteshtmldoc integer not null maxage365 smallint not null maxage30 smallint not null maxage1 smallint not null maxage0 smallint not null maxagenull smallint not null numdomelements integer not null numcompressed smallint not null numhttps smallint not null numglibs smallint not null numerrors smallint not null numredirects smallint not null maxdomainreqs smallint not null byteshtmldoc integer not null fullyloaded integer cdn character varying64 speedindex integer visualcomplete integer gziptotal integer not null gzipsavings integer not null siteid numeric indexes pages pkey primary key btree pageid pages date url unique constraint btree urlshort labeldate idx pages cdn btree cdn idx pages labeldate btree labeldate cluster idx pages urlshort btree urlshort triggers pages label date before insert or update on pages for each row execute procedure fix label date
105965 have two sql server databases one is client windows application and the second is on the server want to sync these two databases every so often every minutes have read about different ways of syncing like replication time stamp log tables using triggers microsoft sync framework and so on actually dont like to use syncing method which might be black box like replication because dont want the sql server specific tables to be blocked while im updating them and syncing them with the server which method do you think that should use in such circumstance remember that every several minutes must send several table changes from client to the server and fetch also two table changes from server have found method which is strange but new is that possible that log all executed for specific preferred stored procedures in client and send them with their parameters in sql file to the server and execute them there the same will happen on the server and sent to the client do you think that this is simple yet useful method or not please suggest me any useful approach if you can thank you so much edit remember that this is real time synchronization and this makes it special it means when the client user is using the table the synchronization process with server must happen every several minutes so none of the tables must be locked
105999 am new to sql administration and have been tasked with creating some nightly jobs that send an email with certain details contained within spreadsheet and so far have the following exec msdb dbo sp send dbmail profile name support recipients test mail co uk subject post code analysis query nset ansi warnings off set no count on select substringpostofficebox14 as postcode countcase when new accounttype then new accounttype else null end as new connections countcase when new accounttype then new accounttype else null end as domestic metered countcase when new accounttype then new accounttype else null end as commercial metered low countcase when new accounttype then new accounttype else null end as commerical metered high countcase when new accounttype then new accounttype else null end as domestic keypad countcase when new accounttype then new accounttype else null end as generator countcase when new accounttype then new accounttype else null end as commercial keypad from be crm4 mscrm dbo accountextensionbase as inner join be crm4 mscrm dbo customeraddressbase as on accountid parentid where new accountstage and addresstypecode and substringpostofficebox12 bt group by substringpostofficebox12substringpostofficebox14 order by substringpostofficebox12substringpostofficebox14 attach query result as file query attachment filename pca test csv query result header query result separator in the query attachment filename would like to append the date and have tried using select convertvarchar10 getdate as dd mm yyyy and appending this to the file name as follows query attachment filename pca test select convertvarchar10 getdate as dd mm yyyy csv any advice would be greatly appreciated
106001 say have large table that holds the users info and another table that holds several locations then use another table that holds the user id and the location id in order to retrieve the data have to use left join query doesnt that make the whole process longer to retrieve rather than having it all in one table eg could have the location as text on the same table edit here is an example create table user id int11 not null name varchar45 default null gender enummf default null create table user location user id int11 not null location id int11 not null create table location id int11 not null location varchar45 parent id varchar45 note please assume that all related fields are properly indexed between them edit currently have large database with users that retrieve their location via junction table as described above was asked to optimize the database because the search results are slow ive added memcache and it improved significantly but now am just wondering about left joins for example the current query is something like that select from users left join user location on user location user id user id left join location on location id user location location id and that is just to get the location they have several other fields that are retrieved through junctions and they are all needed to view users profile we have phone numbers addresses passwords and many others all in different tables in order for me to create page for the user profile have to send the server large query now after the first time it gets cached and its fine but was just wondering why would someone build their database like that
106014 would like to partition table with 1m rows by date range how is this commonly done without requiring much downtime or risking losing data here are the strategies am considering but open to suggestions the existing table is the master and children inherit from it over time move data from master to child but there will be period of time where some of the data is in the master table and some in the children create new master and children tables create copy of data in existing table in child tables so data will reside in two places once child tables have most recent data change all inserts going forward to point to new master table and delete existing table
106037 have to select values that were less that the given id and greater than the given id have tried this query but is there any better way to do it fiddle sql begin declare rootvalue int declare repid int set repid set rootvalue select id from tbllookups where id repid declare rootminustwo int declare rootplustwo int set rootminustwo select count from tbllookups where id rootvalue set rootplustwo select count from tbllookups where id rootvalue if rootminustwo and rootplustwo begin select from tbllookups where id between rootvalue and rootvalue end else if rootminustwo and rootplustwo select from tbllookups where id repid union select from tbllookups where id rootvalue union select top rootminustwo from tbllookups where id rootvalue else if rootminustwo and rootplustwo select from tbllookups where id repid union select from tbllookups where id rootvalue union select top4 rootplustwo from tbllookups where id rootvalue else if rootminustwo and rootplustwo select from tbllookups where id repid union select from tbllookups where id rootvalue union select from tbllookups where id rootvalue end forgot to add something at any point there should be records if at all there are that many records satisfying the condition example the ids are if supply then also it should return
106126 were running postgres on centos and have select query that has worked for years but stopped working and hangs after we upgraded from it took while to notice it so dont know if it was immediately after we upgraded or not select id group number as uniq id from table one where id group number not in select id group number from table two and id not in select id from table three where timestamp now interval days and client id in all tables id is an integer but is stored as character varying legacy system group number is stored as smallint the sub query for table two returns about million records the sub query for table three returns about records both return in about second if run separately but adding in either query or both as sub queries causes the query to just hang indefinitely for days if we let it run ive seen others online with the same problem query not returning when using not in not in seems like such straight forward sub query we have plenty of hardware gb ram xeon cores disk 15k rpm raid why is this happening ie is this major ongoing bug in postgres how can fix debug it in the meantime here are the results of explain query plan index only scan using table one id pk on table one cost rows width filter not hashed subplan and not subplan subplan bitmap heap scan on table three cost rows width recheck cond timestamp now days interval and client id bitmapand cost rows width bitmap index scan on table one timestamp idx cost rows width index cond timestamp now days interval bitmap index scan on fki table three client id cost rows width index cond client id subplan materialize cost rows width seq scan on table two cost rows width my settings from postgresql conf max connections shared buffers 24gb temp buffers 8mb work mem 96mb maintenance work mem 1gb cpu tuple cost cpu index tuple cost cpu operator cost effective cache size 128gb from collapse limit join collapse limit update used the following method to adjust the work mem just for this query begin set work mem 256mb query set work mem default commit using not in returned in seconds vs never with work mem 96mb using left join returned in seconds vs seconds with work mem 96mb so it looks like the problem was with work mem and using left join was just workaround however the real problem is postgres going for days with work mem 96mb with 15k sas drives in raid we have very fast so even going to disc the query should have returned just bit slower update here are the results for explain analyze on the left join approach query plan nested loop anti join cost rows width actual time rows loops hash anti join cost rows width actual time rows loops hash cond t1 id text t3 id text seq scan on table one t1 cost rows width actual time rows loops hash cost rows width actual time rows loops buckets batches memory usage 51kb bitmap heap scan on table three t3 cost rows width actual time rows loops recheck cond client id filter timestamp now days interval rows removed by filter heap blocks exact bitmap index scan on fki table three client id cost rows width actual time rows loops index cond client id index only scan using table two id2 idx on table two t2 cost rows width actual time rows loops index cond id t1 id text and group number t1 group number heap fetches planning time ms execution time ms rows time ms and here they are for the not exists approach query plan nested loop anti join cost rows width actual time rows loops hash anti join cost rows width actual time rows loops hash cond t1 id text t3 id text seq scan on table one t1 cost rows width actual time rows loops hash cost rows width actual time rows loops buckets batches memory usage 51kb bitmap heap scan on table three t3 cost rows width actual time rows loops recheck cond client id filter timestamp now days interval rows removed by filter heap blocks exact bitmap index scan on fki table one client id cost rows width actual time rows loops index cond client id index only scan using table two id2 idx on table two t2 cost rows width actual time rows loops index cond id t1 id text and group number t1 group number heap fetches planning time ms execution time ms rows time ms
106264 this is spin off from comments to the previous question postgres query takes forever using postgresql there always seems to be recheck cond line after bitmap index scans in query plans output by explain like in the explain output of the referenced question bitmap heap scan on table three cost rows width recheck cond timestamp now days interval and client id bitmapand cost rows width bitmap index scan on table one timestamp idx cost rows width index cond timestamp now days interval bitmap index scan on fki table three client id cost rows width index cond client id or in the output of explain analyze for simple huge table with very little work mem explain analyze select from aa where between and bitmap heap scan on aa cost rows width actual time rows loops recheck cond and rows removed by index recheck heap blocks exact lossy bitmap index scan on aai cost rows width actual time rows loops index cond and does that mean index conditions have to be checked second time after bitmap index scan what else can we learn from the explain output
106539 am very new to the subject of databases so this may sound ignorant but am curious why key should be made explicit within table is this primarily to tell the user that the given column value is hopefully guaranteed to be unique within each row the uniqueness should still be there even if it isnt mentioned
106632 am working with table that has all character types set to nvarchar some of them are nvarcharmax we are converting all these to varchar and specifying character width based upon the actual usage in production the production data uses range of characters up to characters of actual used width for any given column we are going to add padding of when applicable insert statements for procedure here update listings with rowlock set subtype where idsettings idsettings and idretsclass or idretsclass idretsclass and idretssetting or idretssetting idretssetting and isnew and subtype like single family home or subtype like modular or subtype like mobile home or subtype like story or subtype or subtype residential or subtype house on lot or subtype houses on lot or subtype detached or subtype like single family or subtype ranch or subtype semi detached or subtype single or subtype one family or subtype residential or subtype ranch type or subtype or more stories or subtype cape cod or subtype split level or subtype bi level or subtype detached single or subtype single family homes or subtype house or subtype detached housing or subtype det large overhaul of this table that consists literally nvarchar columns being max am dropping indexes and recreating them afterwards my question is in what situations is varcharmax preferred only when you expect to have 4k or more characters what should learn and prepare for when doing this will this improve performance when clustered index update that affects the clustered key has to update the all non clustered indexes we are having update procedures timing out that are using to of the query execution plan displayed plan for an clustered index update link to the actual execution plan
106762 have deadlock report that tells me that there was conflict involving waitresource key 543066506c7c and can see this keylock hobtid dbid objectname mydatabase myschema mytable indexname myprimarykeyindex id locka8c6f4100 mode associatedobjectid within resource list want to be able to find the actual value for the key id for example what sql statement would need to use to obtain that information
106795 am using postgres have tables agent create table agent agent id bigserial not null agent total users integer not null agent dim time id integer not null other unnecessary columns dim time create table dim time dim time id bigserial not null dim time date date not null dim time month start date date not null dim time week start date date not null dim time quarter start date date not null dim time year start date date not null dim time guid uuid not null default uuid generate v4 now agent dim time id field in agent is fk referencing dim time id of dim time my dim time table has date and its related week start date month start date etc for example if dim time date is then dim time week start date will be monday is start of week and start of month is always the first have to find agent total users from agent given date range and dimension week month quarter what ive tried select dim time week start date agent total users from agent join dim time on agent dim time id dim time id where dim time week start date in select generate series2015 day interval the output dim time week start date agent total users but only need the first value in case week start dates are same the output am expecting here is dim time week start date agent total users how can do this
106898 how can convert dateb set dateb dateaddmonth datediffmonth getdate that returns as date to an integer of thanks
107078 our database is sql server r2 we have some tables that have some varchar500 columns that want to switch to datetime2 or bigint can guarantee all the data in the columns to be switched are valid for the proper type the column changes do affect indexes but not keys while discussing with colleagues we have come to two ways to approach the problem both these would be done through sql scripts create temp table via select into drop the old table and recreate the table with the proper datatypes recreate the indexes alter the current table data types via alter table alter column datetime2 and then rebuild or recreate the indexes because am confident the data will convert cleanly am leaning towards my colleague and dba friend prefer but my colleague cant remember why they trained him that way the dba friend is on vacation so didnt ask him why can someone provide insight on which option they think is better and why ultimately it is my decision and am wondering why would be preferred over
107096 my team uses oracle and sql developer ive been relying heavily on explain plans lately to try and determine the most efficient way to solve various problems recently coworker pointed out that explain plan is not always accurate to what actually happens in the database and that an autotrace is better indication since the query is actually run against the data testing query ive gotten the following results method cost query explain query autotrace query explain query autotrace when using the autotrace query had cost increase of and query of nearly obviously should be using query in both cases but dont understand what causes them to differ
107238 have big query if necessary will post it here and im getting this error msg level state line for xml could not serialize the data for node noname because it contains character 0x0000 which is not allowed in xml to retrieve this data using for xml convert it to binary varbinary or image data type and use the binary base64 directive the only part use for xml is here where codfuncionario results codfuncionario for xml path type value text varcharmax as experiencia but what is node noname and how can look for this value 0x0000 this is one of the subqueries the only part have for xml select codfuncionario stuff select cast descfuncao desctempoexperiencia as varcharmax from linked server db dbo tblfuncionarioexperiencia t0 inner join linked server db dbo tblfuncao t1 on t0 codfuncao t1 codfuncao inner join linked server db dbo tbltempoexperiencia t2 on t0 codtempoexperiencia t2 codtempoexperiencia where codfuncionario results codfuncionario for xml path type value text varcharmax as experiencia from linked server db dbo tblfuncionarioexperiencia results group by codfuncionario as t2 on t0 codfuncionario t2 codfuncionario left join
107475 im working on postgresql db design and am wondering how best to store timestamps assumptions users in different timezones will use the database for all crud functions have looked at options timestamp not null default now at time zone utc bigint not null default for timestamp would send string that would represent the exact utc timestamp for the insert moment for bigint would store the exact same thing but in number format time zone issues are handled before millis is handed over to the server so always millis in utc one main advantage with storing bigint could be that it would be easier to store and to retrieve as passing correctly formatted timestamp is more complex than simple number millis since unix epoc my question is which one would allow for the most flexible design and what could be the pitfalls of each approach
107549 for production backup mongodb recommends mongodump instead of mongoexport for accuracy of data however would need to scrub data off mongodb database before backing it up am not aware of any server side data scrubbing options other than mongoexport two questions does mongoexport access mongodb cache in ram would it alter the working set in ram like mongodump does mongodump command has this query option provides json document as query that optionally limits the documents included in the output of mongodump does it take query to exclude certain fields in document
107556 just imported an existing sql server 2008r2 production database into vs database project now get number of errors along the lines of error sql71501 user mydbuser has an unresolved reference to login mydbuser dont really need my vs db project to manage users but im concerned that it would try to remove them upon deploy if they werent there the files themselves are generated as create user mydbuser for login mydbuser or create user mydomainuser for login mydomain mydomainuser the error marker shows that its specifically for the login as thats system level object can understand it being outside the scope of the db project is it preferred that change them all to create user mydbuser without login or add the create login clause to the beginning of each file removing the login reference seems to be simpler and removing the users altogether would be the simplest want to make sure that im using the tool the way it was intended will there be any issues in re publishing any of those back to production what is the proper procedure for adding user login via project
107597 we using nagios for our servers monitoring each web environment have cluster as backend cassanra each cluster have nodes question is want write plugin for nagios for cassandras nodes monitoring unfortunately im not so good familiar with cassandra and dont sure what parameters need exactly to be checked im planing use nodetool utility to grab data from nodes but it have lot of commands and each provides lot of information cfstats info status etc so for monitoring need get some data about memory each nodes usage used disk space may be something else
107669 last week something strange happened on our database all of sudden the application blocked for our users who were not able to save new entities etc after looking at the activity monitor of the sql server with compatibility mode saw the following three entries after some time the users got connection timeout when killed the process they could save normally again the problem is that the entities they tried to save during the block were inserted into the db more than once up to times even though there is code which should prevent this from happening number column which has to be unique but without constraint the check happens in the code we use entity framework does anyone of you know why and when these async network io wait types occur and how to avoid them and what exactly do they mean
107744 have trigger and stored procedure in it so sp runs when trigger runs need function which finds the next saturday to put in sp so lets say it is wednesday today if my trigger runs today the sp in it must find the next saturday plus even if it is saturday but the time is earlier of pm it must find the current day also after pm it must return the next saturday would like to the put my whole trigger and sp on here but dont want to get here crowded just need ideas thanks edit thanks to onaye coded that create definer root localhost procedure newguess in muserid int in numm1 int in numm2 int in numm3 int in numm4 int in numm5 int in numm6 int begin set today select weekdaycurdate monday is the first day in here if today or today then it is not saturday set nextsaturday select date addnowinterval ifweekdaynow weekdaynow5 weekdaynow day end if if today then it is saturday set current time select curtime set nextsaturday select date addnowinterval ifweekdaynow weekdaynow5 weekdaynow day if current time cast21 as time then set nextsaturday curdate end if if current time cast21 as time then set nextsaturday curdate interval week end if end if insert into guessestbl useridnum1num2num3num4num5num6current datetimedraw date values museridnumm1numm2numm3numm4numm5numm6now nextsaturday end it worked now solved
107820 the major browsers are moving beyond ssl3 and tls1 the pci security council has declared an end of life date for these protocols to be considered sufficiently strong encryption we need to move away from these protocols to use newer and stronger ones on windows servers you can very easily disable these old protocols and instead offer only tls1 or greater however as noted elsewhere microsoft sql server r2 and sql server standard at least both will not start if those lower protocols are disabled however there are growing number of versions of ms sql server there are sql server standard business intelligence enterprise express web and compact editions and of course there is sql server and in pre release which of these editions support or will support the use of only tls1 or greater protocols
107947 have broad question regarding sql server that was given to me on quiz and am not sure what the answer is as do not have access to sql server at home my question is how can utilise sql server tools to identify query optimisations missing indexes etc can somebody give me very brief answer to this thank you
108129 am using postgresql have query like this select count from ab where and are the primary keys of tables and so there are indexes on by default postgresql will use seq scan on ab and use hash join force it to do the index scan and index only scan the result showed that seq scan is much faster than the other two it takes more time to do the full scan on ab for index scan and index only scan explain analyze select count from journalpaper where journal paper id paper paper id can someone explain it thank you so much
108210 have table representing movies the fields are id pk title genre runtime released in tags origin downloads my database cannot be polluted by duplicated rows so want to enforce uniqueness the problem is that different movies could have the same title or even the same fields except tags and downloads how to enforce uniqueness thought of two ways make all the fields except downloads primary key im keeping downloads out since its json and it will probably impact the performance keep only id as primary key but add unique constraint with all the other columns except again downloads read this question which is very similar but didnt quite understand what should do currently this table is not related to any other tables but in the future could be at the moment have slightly less than records but expect the number to grow dont know if this is somewhat relevant to the issue edit modified the schema and here is how would create the table create table movies id serial primary key title text not null runtime smallint not null check runtime released in smallint not null check released in genres text not null default array text tags text not null default array text origin text not null default array text downloads json not null inserted at timestamp not null default current timestamp constraint must be unique uniquetitleruntimereleased ingenrestagsorigin also added the timestamp column but that is not problem as wont touch it so it will always be automatic and unique
108266 have installed instances of sql server plus ssis on the following server note the amount of ram is nearly gb and this is the max and min memory settings that have applied to my instances assume that both instances will use equal amount of resources if that would really be the case mb which is gb be good starting number to set up my memory how much memory would allocate to ssis
108287 select from where posted date and posted date but the result contains record that has posted date today my database server is not in my country what is the problem
108290 my function new customer is called several times per second but only once per session by web application the very first thing it does is lock the customer table to do an insert if not exists simple variant of an upsert my understanding of the docs is that other calls to new customer should simply queue until all previous calls have finished lock table obtains table level lock waiting if necessary for any conflicting locks to be released why is it sometimes deadlocking instead definition create function new customersecret bytea returns integer language sql security definer set search path postgrespg temp as lock customer in exclusive mode with as insert into customercustomer secretcustomer read secret select secretdecodemd5encodesecret hexhex where not existsselect from customer where customer secret secret returning customer id insert into collectioncustomer id select customer id from select customer id from customer where customer secret secret error from log bst detail process waits for exclusivelock on relation of database blocked by process process waits for exclusivelock on relation of database blocked by process process select new customerdecode text hex process select new customerdecode text hex bst hint see server log for query details bst context sql function new customer statement bst statement select new customerdecode text hex relation postgres select relname from pg class where oid relname customer edit ive managed to get simple ish reproducible test case to me this looks like bug due to some sort of race condition schema create table test id serial primary key val text create function testv text returns integer language sql security definer set search path postgrespg temp as lock test in exclusive mode insert into testval select where not existsselect from test where val select id from test where val bash script run simultaneously in two bash sessions for in do psql postgres postgres select testblah done error log usually handful of deadlocks over the calls bst error deadlock detected bst detail process waits for exclusivelock on relation of database blocked by process process waits for exclusivelock on relation of database blocked by process process select testblah process select testblah bst hint see server log for query details bst context sql function test statement bst statement select testblah edit ypercube suggested variant with the lock table outside the function for in do psql postgres postgres begin lock test in exclusive mode select testblah end done interestingly this eliminates the deadlocks
108710 suppose there is postgresql server running and it has ssl enabled using standard linux and postgresql tools how can examine its ssl certificate im hoping for output similar to what you would get from running openssl x509 text and im hoping for one or two liner command line answer so dont have to resort to running packet sniffer do not have access to the postgresql server so cannot look at its configuration files directly do not have superuser login so cant get the value of the ssl cert file setting and then pg read file on it using openssl client connect doesnt work because postgresql doesnt seem to want to do the ssl handshake right away from quick look at the psql documentation could not find command line parameter that makes it show that information on startup though it does show me certain cipher information
108952 this is part of table definition from here create table dbo jobitems itemid uniqueidentifier not null lots of other columns constraint primarykey guid here primary key nonclustered itemid asc create unique clustered index jobitemsindex on dbo jobitems itemid asc this is some legacy design so please dont ask why anyway when look up the list of indexes see that therere two indexes one of them is jobitemsindex and the other is pk guid here and they are both for jobitems table my question is why is there need for separate index to maintan the pk when already have jobitemsindex which is unique and includes the very same column and so suitable for maintaining the pk constraint
108975 was looking at information schema role table grants when saw public in grantee column then ive checked at information schema enabled roles but this role name does not exist who or what is the public role
109078 trying to get info about why mysql does not using my index when create inner join and trying to order by on the end have my sql query here select from product inner join productstore ps on productuuid ps productuuid order by ps storetitle limit when im using order by this select take over sec when remove order by its taking like 16ms to run the same sql my explain sql is follow with order by id select type table type possible keys key key len ref rows extra simple ps all primary null null null using filesort simple eq ref primary primary foeniks core ps productuuid null without order by id select type table type possible keys key key len ref rows extra simple ps all primary null null null null simple eq ref primary primary foeniks core ps productuuid null the field there not are indexing right is varchar on length my table design is here create table productstore productuuid binary16 not null storeuuid binary16 not null distributorlastused binary16 default null storetitle varchar282 default null storeurl varchar282 default null storedescription text storedescriptiondemo text storeprice int11 not null default storepricenext int11 not null default storepricecost int11 not null default overwrites int11 not null default updated datetime not null default added datetime not null default allowdisplay tinyint1 not null default activated tinyint1 not null default primary key productuuidstoreuuid key productstorelanguagetostore idx storeuuid key productstoretodistributor idx distributorlastused key storeurl storeurl180 using btree key teststoretitle storetitle182 constraint productstoretodistributor foreign key distributorlastused references distributor distributoruuid on delete set null on update cascade constraint productstoretoproduct foreign key productuuid references product productuuid on delete cascade on update cascade constraint productstoretostore foreign key storeuuid references store storeuuid on delete cascade on update cascade engine innodb default charset utf8 product table create table product productuuid binary16 not null productmanufactureruuid binary16 not null productmanufacturersku varchar40 default null productean varchar40 default null cnetid varchar10 default null edbid int10 default null overwrites int10 not null default updated datetime not null default added datetime not null default activated tinyint1 not null default primary key productuuid key manufacturersku productmanufacturersku16 key producttomanufacturer idx productmanufactureruuid key cnetid cnetid key productean productean constraint producttomanufacturer foreign key productmanufactureruuid references manufacturer manufactureruuid on delete no action on update cascade engine innodb default charset utf8
109210 have table with columns the type of both columns is set to varchar38 if create row with an empty value for one of the columns will it take same storage space as if the value was not empty in other words will mysql reserve storage space for the column depending on its type when row is created
109377 microsoft buying revolution results amongst other things in the integration of in sql server looking through the features supported by the sql server cannot find support does anyone know to what feature integration belongs to or what versions integrate this feature
109392 say am running query begin tran update users set name jimmy where name john if dont rollback the transaction will these changes still be made will it throw an error or will it act as rollback anyway
110460 with postgres im doing the following query quite often select distinct onrecipient from messages left join identities on messages recipient identities name where timestamp between timea and timeb order by recipient timestamp desc so decided to create view create view myview as select distinct onrecipient from messages left join identities on messages recipient identities name order by recipient timestamp desc just realized if query my view like select from myview where timestamp between timea and timeb get significantly worse performance doing explain analyze on both queries found out the reason is that the database in the second case brings up all the records does the left join and then applies the where clause in other words the where clause is not pushed down into the views query also tried to remove the order by from the view but still the database performs the left join on full data rather on the filtered set what is the reason of this behavior is there way can get comparable performance when using view
110774 we are thinking about buying an sql standard server for implementing an etl via ssis since it is very expensive for us would like to test developing ssis packages on free version since the express version does not integrate the ssis want to try it on the evaluation expires version of the sql server but cannot find anything about is it possible are there limitations could anyone help me here
110781 have table called example create table if not exists example id int11 not null auto increment int11 not null int11 not null int11 not null primary key id engine innodb default charset latin1 want to insert values if not exists and if the value exists then update so am using following statement insert into example values on duplicate key update valuesa valuesb valuesc after the above queries executed the table look like this again execute the above statement the result looks like this what is wrong with my statement
110880 have an application which uses postgresql table the table is very big billions of rows and has column which is an integer the integer can be up to digits no negatives thought about changing it to be numeric60 would this be good idea would numeric60 take fewer bytes how about the performance this table is being queried lot
110911 have set the database collation to latin1 general bin to make string comparisons case sensitive will this have an impact on performance will it have any impact on dml or ddl operations in the database the database already exists with tables in it
110916 have table with the following structure create table rings id ringtype char2 number mediumint unsigned id user int11 and with data insert into rings values aa11 aa21 aa31 aa111 aa121 aa131 aa141 aa151 ab161 ab171 ab181 ab191 ab202 ab212 ab222 wish to group the data based on id user and id ringtype and for each contiguous range of numbers list the min and max the results should look like id user id ringtype min max aa aa ab ab went through several posts on this topic but was not able to tweak them to fit my data any help would be appreciated
110949 if have this tadd is the address table concattadd street number tadd street name tadd apt number tadd city tadd postal code tadd country as address is there way to exclude the apt number if it doesnt exist was thinking of where tadd apt number is not null but it will return only those rows with apt number and even if something works how do then deal with that extra comma if its duplicate please post link in comments
111013 the best way think can ask my question is by starting with an example imagine these two queries select car serialnumber tire serialnumber from dbo tcar as car inner join dbo ttire as tire on tire carid car id where car brand jaguar and select car serialnumber tire serialnumber from dbo tcar as car cross apply select tire serialnumber from dbo ttire as tire where tire carid car id as tire where car brand jaguar note ttire carid is indexed and tcar brand is indexed these two would return the same output know the second query is just foolish as it stand but again it just serve as an example for my question now guess even if dont know the details sql server will perform clever and limited lock on the data read by the first query join but im wondering how it will perform lock for the data read by the second query apply mainly and obviously imo on the ttire data read in case of apply how do sql server perfom lock edit this example is so simple it seems sql server manage to create the same execution plan but my apply question should be though in terms where rbar effect occurs
111095 am wondering why for scalar valued function that have to grant the user to execute rather than just select meanwhile table valued functions works just fine with only select permission or db datareader membership to be more clear here is my example need user that has read only permission to the database so created user called testuser and give it db datareader membership then created table valued function called fn inlinetable and all is great testuser runs this sql all day long select from dbo fn inlinetable then need scalar function so created scalar function called fn scalartest testuser cannot run this sql select dbo fn scalartest1 well understandably it is because have not given testuser permission to execute fn scalartest my question is based on this link https stackoverflow com questions insert update delete with function in sql server that says function cannot be used to perform actions that modify the database state so why not let scalar function to be used with the same select permission rather than execute permission hope my question makes sense thank you
111187 ive imported data into new database about 600m rows of timestamp integer double then created some indexes and tried to alter some columns got some out of space issues the database is vacuumed now pgadmin iii tells me that the size of temporary files is 50g what are these temporary files are these like sql server transaction log how can get rid of them it seems the database is much bigger than it should the total size of the database is gb using posgres on windows server screenshot of the database statistics tab
111223 have table in sql server that looks like this id version name date fielda fieldb fieldz foo foo null null bar null bee null am working on stored procedure to diff that takes input data and version number the input data has columns from name uptil fieldz most of the field columns are expected to be null each row usually has data for only the first few fields the rest are null the name date and version form unique constraint on the table need to diff the data that is input with respect to this table for given version each row needs to be diffed row is identified by the name date and version and any change in any of the values in the field columns will need to show in the diff update all the fields need not be of type decimal some of them may be nvarchars would prefer the diff to happen without converting the type although the diff output could convert everything to nvarchar since it is to be used only for display purposed suppose the input is the following and the requested version is name date fielda fieldb fieldz foo null null foo null bar null baz null null the diff needs to be in the following format name date field oldvalue newvalue foo fielda foo fielda null foo fieldb null bar fieldb baz fielda null my solution so far is to first generate diff using except and union then convert the diff to the desired output format using join and cross apply although this seems to be working am wondering if there is cleaner and more efficient way to do this the number of fields is close to and each place in the code that has is actually large number of lines both the input table and existing table are expected to be quite large over time am new to sql and am still trying to learn performance tuning here is the sql for it create table diff change nvarchar not null name nvarchar not null date int not null fielda decimal null fieldb decimal null fieldz decimal null generate the diff in temporary table insert into diff select from select old as change name date fielda fieldb fieldz from mytable mt where version version and mt name castmt date as varchar in select name castdate as varchar from diffinput except select old as change from diffinput union select new as change from diffinput except select new as change name date fielda fieldb fieldz from mytable mt where version version and mt name castmt date as varchar in select name castdate as varchar from diffinput as mydiff select d3 name d3 date crossapplied field crossapplied oldvalue crossapplied newvalue from select d2 name d2 date d1 fielda as oldfielda d2 fielda as newfielda d1 fieldb as oldfieldb d2 fieldb as newfieldb d1 fieldz as oldfieldz d2 fieldz as newfieldz from diff as d1 right outer join diff as d2 on d1 name d2 name and d1 date d2 date and d1 change old where d2 change new as d3 cross apply values fielda oldfielda newfielda fieldb oldfieldb newfieldb fieldz oldfieldz newfieldz crossapplied field oldvalue newvalue where crossapplied oldvalue crossapplied newvalue or crossapplied oldvalue is null and crossapplied newvalue is not null or crossapplied oldvalue is not null and crossapplied newvalue is null thank you
111258 in order to get an overview and compareable data my current task is to create performance baseline to get some figures about the different productive sql server instances my thoughts are want to use several dmvs want to include profiler trace incl exec plans want to include perfmon data so what try to achieve is general performance monitoring startable and stoppable also scheduable that returns all information required to identify the success of ongoing performance optimization tasks couple of aggregated simple figures that help to visualize the long term progress esp for management re executable execution plans within profiler trace to compare individual queue changes and improvements by index optimization tasks found couple of information describing the creation of performance baselines most of them are either very complicated or focus only on one of the desired performance indicators mostly perfmon data the most matching sample description was the following creating performance baseline for sql server the question is does anyone have experience creating this kind of performance monitor in quickly doable manner
111334 consider the following product table which is highly trimmed down id int auto increment category id int subcategory id int vendor id int price decimal62 inserted at timestamp for given category id am attempting to retrieve list containing the vendor with the lowest latest price for each subcategory with latest mean that vendors may have multiple prices for given category id subcategory id combination so only the most recently inserted price for that category id subcategory id vendor id should be used if theres tie between or more vendors prices the lowest id should be used as the tie breaker for example with this data id category id subcategory id vendor id price inserted at so first find the most recent prices for every subcategory vendor combination row with price would be removed because its not the most recent for that vendor in that subcategory then for subcategory the lowest price would be so vendor id and for subcategory the lowest price is two vendors tie ids and so we choose the one with lowest vendor id would expect the following results for category id subcategory id vendor id price heres what have so far feel like its already starting to get out of hand and this doesnt even account for ties between or more vendors prices select subcategory id vendor id price from products as join select mina price as min price subcategory id from products as join select maxinserted at as latest price time vendor id subcategory id from products where category id group by vendor id subcategory id as on inserted at latest price time and vendor id vendor id and subcategory id subcategory id where category id group by subcategory id as on price min price and subcategory id subcategory id where category id before go any further wanted to see if there was an easier way when it comes to grouping aggregating results of additional groupings aggregations is there method that will give me the best performance most important and or be easier to read less important
111335 am junior dba with years of experience our job is to fine tune queries or advise developers that particular code should be re written or indexes are needed one simple question the dev team asks frequently is yesterday it ran fine what changed suddenly and we will be asked to check the infrastructure side the first reaction on any problem always appears to be to put maximum blame on the infrastructure which is always the first thing being validated how should we answer to what changed questions by the development team did you guys ever faced the same situation if so please share your experience
111347 on one of my servers win r2 sql server is constantly under attack with the sa account being hit like times per second with different passwords dont have an sa account but still would like to stop this probing as its probably consuming resources keep checking the logs and blocking the offending ips at firewall level manually still would like better solution sql server runs locally with iis to serve its websites only need to connect to sql server remotely for database development with ssms so at first thought setting up vpn but not sure if this is going to play well with ssms and other services like ftp as in interim solution where can stop sql server from being visible to the outside world at the firewall by disabling port or elsewhere can then enable disable this when developing and perhaps just for my ip thanks
111419 sql server ran dbcc sqlperflogspace then took two log backups and ran dbcc sqlperflogspace again no change from used even though dbcc opentran foo says that there are no active open transactions does that mean that there are inactive open transactions that are preventing the log free space from changing after backups thanks
111462 scenario lets assume have sql server with sockets with each numa node each socket has physical cores there is gb of memory total so each numa node has gb of ram key table is loaded into the first numa node question lets assume we have lot of traffic reading from that table if all physical cores of the socket that owns the numa node have percent cpu utilization does that negatively influence the cost of non local numa access coming from other sockets or on the other hand is the cost of non local numa access is irrespective of how busy that socket is hope my question makes sense please let me know if it doesnt will try to clarify background we had database issue in our production server last week and some of our business processed appeared more impacted than others we had queries with few logical reads taking more than minute we looked at overall cpu utilization which was around percent we did not look at socket specific cpu metrics metrics were average
111477 want to download jsonbx to rds instance rds postgresql features supported shows only built in features does this mean there is no way we can install an extension to rds which is not in the feature matrix is there work around for this
111487 learning how to formulate query like expressions in relational algebra is traditional part of many perhaps most introduction to databases courses this is usually justified by the assertion that relational algebra is the mathematical foundation of relational databases in general and sql in particular with the implication that it is important to know it however it seems to me that formulating expressions in relational algebra is basically the same as formulating queries in sql and that much the same thought processes underly both tasks in particular cant really see that knowing relational algebra makes it easier to write sql queries or vice versa this makes me wonder if the teaching of relational algebra is just some sort of historical hangover or if there actually specific benefits to knowing it so my question is are there specific practical benefits to knowing relational algebra of sufficient importance to make it worthwhile teaching as database administrators do you feel that relational algebra is or was important to your career trajectory sort of sub question is whether the time spent learning relational algebra could more effectively be used by learning more sql
111897 am using mysql ver distrib for debian linux gnu how can show my tables page wise in terminal
112090 need to install sql server msde sp4 on windows have other machines that run windows and and sql server msde works fine with merge replication configured too now testing the same scenario with windows but setup is closing unexpectedly during installation know that this version is very old but is there workaround to install it on windows
112094 need to add column into table that contains columns groupkey shows the number of an item status the number of the status that the group was according the insert date the status number can be changed back and forward several times insert date the time date that the status changed this columns never change its like log table rows can be only added and not deleted or changend the fourth column calculated column that need is flag column that will show if the status was the last time the status has been changed based on the insert date if it is the last status then show else show hope explained this well enough here is an example
112167 have sql server r2 database being used by several deployed programs question is there an easy way to display how much space each table consumes for all of the tables in the database and distinguish logical space from disk space if use ssms management studio the storage properties shown for the database reads mb with mb available about the right size but im concerned about the mb available is this limit to be concerned about assuming know have enough disk space can drill into each table but that takes forever to do know can write my own queries and test around but id like to know if theres already an easy built in way to do this
112179 am setting cron job to collect results from mongodb database profiler id like to collect results within hrs period plan to run mongo command with javascript question is in mongo shell how do write query to find date range from hrs ago such as db system profile find timestamp lte current date time gt date time hrs ago
112408 have table coursemaster like courseid coursename abc def ghi jkl mno pqr stu and have another table studentmaster for student details like rollno name address course ram ram address hari hari address jeff jeff address daisy daisy address here want to fetch the student details with coursenamenot courseid if the values in course is not comma separated than it would be very simple query to fetch the details with join as of my knowledge can run two queries for the same result what want one query for fetching the details of student from studentmaster to the front end and other one for only fetching the coursename from coursemaster by corresponding courseid through loop but the fact want the result by only one query rather than write two queries for this small task guess it is possible and my expected result will look like rollno name address course ram ram address abcdefpqr hari hari address defpqr jeff jeff address ghimnopqrstu daisy daisy address ghi thank you and any valuable suggestion will be highly appreciate
112424 is there way in sql server to have data being written to one table routed to another sql server on another host to clarify we are using hibernate envars to write audit logs hibernate envars does not allow this data to be sent to another server instead of the server where the audit log was created by hibernate id like for these audit logs to be sent to another server is there way to configure sql server to write specific tables to another server
112576 have tables related like this its an example company id name cnpj department id name code id company classification id name code id company workers id name code id classification id department suppose that have classification with id id company and department that has id company that represents another company this will allow the creation of worker that is from two companies because the classification and department are linked to the company separately dont want that to happen so think have problem with my relationships and dont know how to solve it
112810 im getting extremely slow query on an indexed column given the query select from orders where shop id order by updated at desc limit explain analyze returned query plan limit cost rows width actual time rows loops index scan backward using index orders on updated at on orders cost rows width actual time rows loops filter shop id rows removed by filter planning time ms execution time ms rows the table description is table public orders column type modifiers id integer not null default nextvalorders id seq regclass sent boolean default false created at timestamp without time zone updated at timestamp without time zone name character varying255 shop id integer recovered at timestamp without time zone total price double precision indexes orders pkey primary key btree id index orders on recovered at btree recovered at index orders on shop id btree shop id index orders on updated at btree updated at its postgres server running on an aws rds t2 micro instance the table has around million rows
112876 given some table with primary key create table customers customerid int not null primary key firstname nvarchar50 lastname nvarchar50 address nvarchar200 email nvarchar260 we have unique primary key on customerid traditionally might then need some additional covering indexes for example to quickly find user by either customerid or email create index ix customers customeridemail on customers customerid email and these are the kinds of indexes ive created for decades its not required to be unique but it actually is the index itself exists to avoid table scan it is covering index in order to aid performance the index is not there as constraint to enforce uniqueness today remembered tid bit of information sql server can use the fact that column has foreign key constraint column has unique index constraint is trusted in order to help it optimize its query execution in fact from sql server index design guide if the data is unique and you want uniqueness enforced creating unique index instead of nonunique index on the same combination of columns provides additional information for the query optimizer that can produce more efficient execution plans creating unique index preferably by creating unique constraint is recommended in this case given that my multi column index contains the primary key this composite index will de facto be unique its not constraint that particularly need sql server to enforce during every insert or update but the fact is that this non clustered index is unique is there any advantage in marking this de facto unique index as actually unique create unique index ix customers customeridemail on customers customerid email it seems to me that sql server could be smart enough to realize that my index already is unique by virtue of the fact that it contains the primary key but perhaps it doesnt know this and theres an advantage for the optimizer if declare the index as unique anyway except perhaps that might now lead to slowdowns during inserts and updates where it must perform uniqueness checks where before it never had to before unless it knows the index is guaranteed to already be unique because it contains the primary key cannot find any guidance from microsoft about what to do when composite index contains the primary key the benefits of unique indexes include the following data integrity of the defined columns is ensured additional information helpful to the query optimizer is provided should mark composite index as unique if it already contains the primary key or can sql server figure out this for itself
112992 have several identical databases with the same schema table structure within instance need to find the size of one table that they all have for example have databases on instance and all the databases have personal information table need to find way to query the size of personal information table that they all have instead of going individually to each database is there way to do this similar to sp spaceused for size data column
113122 currently have foreign key between two entities and would like to make that relation conditional to the entitytype of one of the tables heres the hierachy of tables this is done via fk refrences from child to parent store employees transactionalstores kiosks brickmortars onlines currently have fk relation from employee to store alter table employees add constraint employee store foreign key transstoreid references transactionalstoresstoreid would like to add the conditional where transactionalstores storetype online type is this possible or must subclass transactionalstores into two new subtypes physicalstores and virtualstores
114260 have table create table dbo realty id int identity11 not null rankingbonus int not null ranking as id rankingbonus persisted not null and view create view dbo filteredrealty as select realty id as realtyid coalescerealty wgs84x ruian cobce wgs84x ruian obec wgs84x as wgs84x coalescerealty wgs84y ruian cobce wgs84y ruian obec wgs84y as wgs84y realty ranking from realty join category on realty categoryid category id left join ruian cobce on realty cobceid ruian cobce cobce kod left join ruian obec on realty obecid ruian obec obec kod left join okres on realty okresid okres okres kod left join externfile on realty id externfile foreignid and externfile ismain and externfile foreigntable inner join person on realty ownerid person id where person confirmstatus have dbml model in linqtosql with the filteredrealty view in it the ranking field is recognized as nullable int and so have to fix the type in the generated code every time when change anything in the database this is very frustrating for me and lot of manual work there are no aggregates used in filteredrealty regarding this related question why is the ranking column of the view considered as nullable if realty ranking is non nullable
114332 have worked couple of times with post deployment scripts and always intuitively used the build action postdeploy because that is what it is now for the first time try to follow the built in instruction from the scripts template to use the somescript sql syntax immediately this line is getting marked as wrong sql80001 wrong syntax next to found suggestions to set the pds to build action none this does not help the error stays what am missing here
114337 have relatively simple query on table with 5m rows select mtid from publication where mtid in or last modifier limit explain analyze output limit cost rows width actual time rows loops bitmap heap scan on publication cost rows width actual time rows loops recheck cond mtid or last modifier bitmapor cost rows width actual time rows loops bitmap index scan on publication pkey cost rows width actual time rows loops index cond mtid bitmap index scan on publication last modifier btree cost rows width actual time rows loops index cond last modifier total runtime ms so far so good fast and uses the available indexes now if modify query just bit the result will be select mtid from publication where mtid in select or last modifier limit the explain analyze output is limit cost rows width actual time rows loops seq scan on publication cost rows width actual time rows loops filter hashed subplan or last modifier subplan result cost rows width actual time rows loops total runtime ms not so fast and using seq scan of course the original query run by the application is bit more complex and even slower and of course the hibernate generated original is not select but the slowness is there even for that select the query is generated by hibernate so it is quite challenge to change them and some features are not available union is not available which would be fast the questions why cannot the index be used in the second case how could they be used can improve query performance some other way additional thoughts it seems that we could use the first case by manually doing select and then putting the resulting list into the query even with numbers in the in list it is four times faster than the second solution however it just seems wrong also it could be times faster it is completely incomprehensible why the query planner uses completely different method for these two queries so would like to find nicer solution to this problem
114360 if have an update statement that does not actually change any data because the data is already in the updated state is there any performance benefit in putting check in the where clause to prevent the update for example would there be any difference in execution speed between update and update in the following create table mytable id int primary key value int insert into mytable id value values update update mytable set value where id and value select rowcount update update mytable set value where id select rowcount drop table mytable the reason ask is that need the row count to include the unchanged row so know whether to do an insert if the id does not exist as such used the update form if there is performance benefit to using the update form is it possible to get the row count that need somehow
114376 have server have just restarted and verified which trace flag are active using dbcc tracestatus trace flag function removes messages to errorlog about traces started and stopped here you can see what each trace flag does flag the start parameters are as follows question how can find what the startup parameters of the sql server services are through sql
114403 need to calculate rolling sum over date range to illustrate using the adventureworks sample database the following hypothetical syntax would do exactly what need select th productid th transactiondate th actualcost rollingsum45 sumth actualcost over partition by th productid order by th transactiondate range between interval day preceding and current row from production transactionhistory as th order by th productid th transactiondate th referenceorderid sadly the range window frame extent does not currently allow an interval in sql server know can write solution using subquery and regular non window aggregate select th productid th transactiondate th actualcost rollingsum45 select sumth2 actualcost from production transactionhistory as th2 where th2 productid th productid and th2 transactiondate th transactiondate and th2 transactiondate dateaddday th transactiondate from production transactionhistory as th order by th productid th transactiondate th referenceorderid given the following index create unique index on production transactionhistory productid transactiondate referenceorderid include actualcost the execution plan is while not horribly inefficient it seems like it should be possible to express this query using only window aggregate and analytic functions supported in sql server or so far for clarity am looking for solution that performs single pass over the data in sql this is likely to mean that the over clause will do the work and the execution plan will feature window spools and window aggregates all language elements that use the over clause are fair game sqlclr solution is acceptable provided it is guaranteed to produce correct results for sql solutions the fewer hashes sorts and window spools aggregates in the execution plan the better feel free to add indexes but separate structures are not allowed so no pre computed tables kept in sync with triggers for example reference tables are allowed tables of numbers dates etc ideally solutions will produce exactly the same results in the same order as the subquery version above but anything arguably correct is also acceptable performance is always consideration so solutions should be at least reasonably efficient dedicated chat room have created public chat room for discussions related to this question and its answers any user with at least reputation points can take part directly please ping me in comment below if you have less than rep and would like to take part discussion for date range rolling sum using window functions
114503 my understanding is that one of the big benefits of using vm is that you can share resources between the virtual machines on host so you can have host with cpus and put vms on it with cpus each the extra cpus are shared between the vms with the host assigning cpus dynamically based on need same for memory also understood that this is big no no for vms that house sql servers but my vm admins disagree does anyone have any evidence or documentation one way or the other dont know if it matters but we are using vmware
114555 if write query that includes compound where clause select from mytable where bitfield and varcharfield asdf and the inclusion of that bit comparison simply excludes the same fields that the varchar comparison will exclude will the presence of that bit field comparison render me performance improvement
114580 need to setup history feature on project to keep track of prior changes lets say have two tables right now notes table id userid submissionid message submissions table id name userid filepath example have row in notes and the user wants to change the message want to keep track of its state before the change and after the change what would be the best approach to setting up column in each of these tables which will say if an item is old if active or if deleted invisible also want to create history audit trail table which holds the id of the prior state the id of the new state which table these ids relate to
114665 ive written this query select bd utilizadores utilizador bd utilizadores palavra passe bd reas rea from bd utilizadores inner join bd permiss es on bd utilizadores id bd permiss es user id join bd reas on bd permiss es area id bd reas id want to join data from table bd utilizadores with bd reas since they have no direct link up had to use man in the middle bd permiss es here is the diagram my main question is is there any other way to do this and get the same result
114774 have very simple table create table content id serial not null text text fullfilename text constraint pk id primary key id with oids false alter table content owner to postgres create index content idx on content using gin to tsvectordanish regconfig text where text is rather long aggregated text to perform full text searching when run the following query it resorts to seq scan explain analyze select id from content where to tsvectordanish text to tsquerydanishfelter seq scan on content cost rows width actual time rows loops filter to tsvectordanish regconfig text felt tsquery rows removed by filter planning time ms execution time ms that is absolutely not okay but when disabling seq scans with set enable seqscan to off get the following result bitmap heap scan on content cost rows width actual time rows loops recheck cond to tsvectordanish regconfig text felt tsquery heap blocks exact bitmap index scan on content idx cost rows width actual time rows loops index cond to tsvectordanish regconfig text felt tsquery planning time ms execution time ms what exactly is going on and what parameters needs tuning for it to run better dont like the idea of removing tool from the query planners toolbox as it stands the setup is running stock postgresql
114850 my interest is to store the time with fsp of as ive read this cannot be achieved with timestamp or datetime data types so have double field to store the output of the microtime function is there anyway can set or even write some code to create default value for such field want to use something like now6 and get for example
114856 have table of about million rows with the following definition and indexes create table digiroad liikenne elementti ogc fid serial not null wkb geometry geometrygeometry4258 tiee tila numeric90 vaylatyypp numeric90 toiminnall numeric90 eurooppati character varying254 kansalline numeric90 tyyppi numeric90 liikennevi numeric90 ens talo numeric90 talonumero numeric90 ens talo numeric90 oik puol character varying254 tieosan ta numeric90 viim talo numeric90 viim tal numeric90 vas puol character varying254 laut tyypp numeric90 lautta lii numeric90 inv paalu numeric1911 inv paal numeric1911 liitalue numeric90 ketju oid numeric90 tietojoukk numeric90 ajoratanum numeric40 viite guid character varying254 timestamp date tiee kunta numeric90 toissij ti character varying254 viite oid numeric90 elem id numeric90 region character varying40 default region character varying constraint digiroad liikenne elementti pkey primary key ogc fid create index digiroad liikenne elementti wkb geometry geom idx on digiroad liikenne elementti using gist wkb geometry create index dle elem id idx on digiroad liikenne elementti using btree elem id create index dle ogc fid idx on digiroad liikenne elementti using btree ogc fid create index dle region idx on digiroad liikenne elementti using btree region collate pg catalog default another table with million rows contains attributes for the rows of the first table the tables can be joined with elem id and region create table digiroad segmentti ogc fid serial not null wkb geometry geometrygeometry4258 segm tila numeric90 tyyppi numeric90 loppupiste numeric1911 alkupiste numeric1911 vaikutuska numeric90 vaikutussu numeric90 vaikutusai character varying254 tieosanume numeric1911 tienumero numeric90 dyn arvo numeric90 dyn tyyppi numeric90 omistaja numeric90 pysakki va numeric90 pysakki ty numeric90 pysakki su numeric90 pysakki ka numeric90 pysakki yl character varying254 palvelu pa numeric90 toissijain numeric90 siltataitu numeric90 rdtc tyypp numeric90 rdtc alaty numeric90 rdtc paikk numeric1911 rdtc luokk numeric90 rdtc liitt character varying254 palvelu ob numeric90 ketju oid numeric90 tietojoukk numeric90 ajoratanum numeric40 viite guid character varying254 timestamp date sivusiirty numeric1911 toissij ti character varying254 viite oid numeric90 elem id numeric90 region character varying40 default region character varying constraint digiroad segmentti pkey primary key ogc fid create index digiroad segmentti wkb geometry geom idx on digiroad segmentti using gist wkb geometry create index ds dyn arvo idx on digiroad segmentti using btree dyn arvo create index ds dyn tyyppi idx on digiroad segmentti using btree dyn tyyppi create index ds elem id idx on digiroad segmentti using btree elem id create index ds ogc fid idx on digiroad segmentti using btree ogc fid create index ds region idx on digiroad segmentti using btree region collate pg catalog default create index ds tyyppi idx on digiroad segmentti using btree tyyppi am trying to insert the rows of the first table with some modification into new table create table edge table id serial not null geom geometry source integer target integer km double precision kmh double precision default kmh winter double precision default cost double precision cost winter double precision reverse cost double precision reverse cost winter double precision x1 double precision y1 double precision x2 double precision y2 double precision elem id integer region character varying40 constraint edge table pkey primary key id since running single insert statement would take long time and would not be able to see if the statement is stuck or something have decided to do it in smaller chunks inside loop in function the function looks like this drop function if exists insert function create or replace function insert function returns void as declare const type constant int const type constant int int row count int begin create table if not exists edge table id serial primary key geom geometry source integer target integer km double precision kmh double precision default kmh winter double precision default cost double precision cost winter double precision reverse cost double precision reverse cost winter double precision x1 double precision y1 double precision x2 double precision y2 double precision elem id integer region varchar40 batch size select count from digiroad liikenne elementti into row count while batch size row count loop raise notice insert batch size row count insert into edge table kmh kmh winter elem id region select case when ds dyn arvo is null then else ds dyn arvo end case when ds dyn arvo is null then else ds dyn arvo end dr elem id dr region from select dle elem id dle region from digiroad liikenne elementti dle where dle ogc fid batch size and dle ogc fid batch size batch size as dr left join digiroad segmentti ds on ds elem id dr elem id and ds region dr region and ds tyyppi const type and ds dyn tyyppi const type end loop end language plpgsql volatile strict the problem is that it starts off going through the loops quite fast but then at some point slows down to crawl when it slows down at the same time the disk usage in my windows task manager rises up to so suspect this is related to the problem somehow running the insert statement on its own with some random value of executes very quickly so the problem seems to only arise when running it in the loop inside function here is the explain analyzebuffers from one such single execution insert on edge table cost rows width actual time rows loops buffers shared hit read dirtied nested loop left join cost rows width actual time rows loops buffers shared hit read index scan using dle ogc fid idx on digiroad liikenne elementti dle cost rows width actual time rows loops index cond ogc fid and ogc fid buffers shared hit read index scan using ds elem id idx on digiroad segmentti ds cost rows width actual time rows loops index cond elem id dle elem id filter tyyppi numeric and dyn tyyppi numeric and vaikutussu numeric and region text dle region text rows removed by filter buffers shared hit read total runtime ms my system is running postgresql on windows with 8gb of ram have experimented with different batch sizes doing the query in different ways and increasing the memory variables in postgres configuration but nothing seems to have really solved the issue configuration variables that have been changed from their default values shared buffers 2048mb work mem 64mb effective cache size 6000mb id like to find out what is causing this to happen and what could be done about it
114902 given table employees employee id salary department id only using sql find all the variants of employee transfers from one department to another so that average salary in both departure and arrival department grew ps was asked the question on interview which never gave an answer and google is of little help
115136 have production database that is experiencing wildly fluctuating page life expectancy ple issues it crashes to zero at random times have been researching the ple issue and have found something that seems to point to vmware issue but am not sure am using the data right it seems like am losing buffer cache pages am using this query select count as cached pages count case database id when then resourcedb else db namedatabase id end as database name from sys dm os buffer descriptors group by db namedatabase id database id order by cached pages count desc found here am totaling the results the count before and after my ple crashes an example is before and after so seem to lose pages my guess is that the hardware for all the virtual machines is under stress so it will randomly swap out some memory from the server for while this is just guess when that happens all the pages are lost so the ple plummets so am using the sys dm os buffer descriptors view correctly from what read it always shows used buffer cached pages so if it is empty or significantly reduced either dont have the memory anymore or it is empty would love way to confirm this conclusion or is there another explanation as to why the count drops so much information below the line was added from the ops comments our system admins manage the vms am hoping to understand my query before go to them with this data the timing of the ple crashes seems random from the database point of view no re indexing or other high performance stuff happening during the ple crashes have done ton of work to see if it was work load related and while there is one poorly performing query it is not enough to use up all the cache there is no rebuilding or other non routine user activity on the server when the buffer counts go down and even if it was would not see that being used in my query above meaning if it was sql server action wouldnt the counts stay the same just with different stuff dont have access to the vmware settings was hoping to understand my findings better before involving those that do the point of this question was to ensure was using the view correctly first at the end of the comment chain was trying to say that the ple issue lead me to the loss of buffer pages issue the query was using to get ple would show low ple because the pages were being lost so what was in them was gone it was false reading because the amount of memory was reduced here is my version microsoft sql server sp1 x64 dec copyright microsoft corporation enterprise edition bit on windows nt x64 build hypervisor
115175 we have very large 100million row table and we need to update couple of fields on it for log shipping etc we also obviously want to keep it to bite size transactions will the below do the trick and how can we get it to print some output so we can see progress we tried adding print statement in there but nothing was output during while loop the code is declare chunk size int set chunk size update top chunk size huge table set deleted deleteddate where deleted is null or deleteddate is null while rowcount begin update top chunk size huge table set deleted deleteddate where deleted is null or deleteddate is null end
115201 this seems like basic question but cant find any answers out there need to be able to get the server name instance etc from linked server ive tried couple of things select linked server servername select linked server serverpropertyservername but no joy any ideas this is sql r2 2008r2 is the linked server edit errors are msg level state line incorrect syntax near servername
115270 on my dev box have my sql server configured with high maximum amount of memory 8gb of 16gb this is required for many development tasks sometimes want my memory back though dont want to leave 8gb consumed permanently whats good way to get sql server to suddenly release all its memory to the operating system dbcc dropcleanbuffers does not do that it merely marks clean buffers as available restarting sql server is problem because this causes minute database recovery to be run due to this bug feature stopping the sql server service causes databases to not shut down cleanly please vote for that connect item seems like an egregious bug
115338 recently restored database to the same instance it was backed up from sql server r2 enterprise and found that couldnt access the database properties have done the following checked the database owner was set correctly using sp helpdb changed the database owner to sa not fix changed the database owner back to my sysadmin user not fix issued dbcc updateusage against the affected database not fix run dbcc checkdb on restored copy to another instance no corruption found the restored copy from the same backup file did not throw any errors when accessing the database properties window can anyone help the error message get when trying to view properties is cannot show requested dialog sqlmgmt property size is not available for database dbname this property may not exist for this object or may not be retrievable due to insufficient access rights microsoft sqlserver smo am sysadmin on this instance update as suggested created new user made it sysadmin and changed the database owner to it not fix unfortunately will see if profiler trace yields anything useful update aaron the original database was renamed and taken offline but is still on that instance the backup of that database was then restored using the original name the filenames of the new database files are different from the original as they live in the same folder as the original mdf ldf the restored db is currently driving our critical apps as normal
115339 we have an instance of sql server installed on customer server the customers it department is in charge of the various backups of the machine the sql server log shows there is full backup every day at pm but we cant find evidence of any plan scheduled in sql server the technicians we contacted couldnt tell us if there was some sort of automatic backup all they told is that the entire machine is being backup up using script found in this forum thread found out that the physical device name is guid and that this means that this is an external backup process use msdb go select bs database name bs backup start date bs backup finish date bs server name bs user name as backupcreator bmf physical device name from msdb backupset bs inner join msdb backupmediafamily bmf on bs media set id bmf media set id order by bs backup start date desc this is row from the above query db name 000server name nt authority system 424f084a f35d 4a66 8fc7 072268a89a77 moreover the backup start and finish date spans only for seconds so guess its clear its not job of sql server this is line from the log backup database backed up database db name creation datetime pages dumped first lsn last lsn number of dump devices device information file type virtual device 95380b0a d50b 408f b95f 1ab8975ba7f8 this is an informational message only no user action is required so since they cant help us what can do to track down the process responsible of the backups we need this because we want to coordinate the backup in an ordered manner and do the transaction log backups too in order to keep the log at reasonable size now we do shrink the log every week and this is not the good way to go
115391 have simple timeseries table movement history data id serial item id character varying event time timestamp without timezone location id character varying area id character varying my frontend developer is telling me that the cost is too high if he wants to know where an item is at given timestamp because he has to sort the table he wants me to add another timestamp field for the next event so he doesnt have to sort yet that is going to more than double the cost of my code to insert new movement as will need to query for the previous entry for the item update that and then insert the new data my inserts of course far outnumber his queries in frequency and have never seen timeseries table which included an entry for the time of the next event hes telling me my table is broken because his infrequent query requires sort any suggestions dont know what query he is using but would be doing this select from movement history where event time timestamp and item id h665ayg3 order by event time desc limit we currently have about 15k items they are at most entered into the database once day however we will soon have 50k of items with sensor data that is updated every to minutes do not see his query being performed very often but another query to get the current status of the pallets will be select distinct on item id from movement history order by item id event time desc this server is currently running but it could be running on if it needs to
115461 im working on database that stores address information related to this question posted have tables create table dbo country code char not null code3 char not null codenumeric char not null name varchar not null continentcode char not null currencycode char not null primary key clustered code asc constraint countrycontinentfk foreign key continentcode references dbo continent code constraint countrycurrencyfk foreign key currencycode references dbo currency code and create table dbo city id int identity not null code varchar not null name nvarchar not null countrycode char not null primary key clustered id asc constraint citycountryfk foreign key countrycode references dbo country code now want to add capital to my country table using fk from the city table know that this is impossible because of the constraints there has to be city with the capitals id but there also has to be country with the countrys code for the city if this makes any sense approach im using this database together with and was thinking to write method that would first add country with capital that is nullable then add the city to its table and update the countries capital with the city that was added approach another approach consider is not adding capital to the country table but instead creating new table countrycapital and link both the country and the city capital together which one of these is the better approach what think is that approach is good solution if the database is maintained through but would be hard to keep up with if data is added manually approach on the other hand looks easier to maintain if manual edits have to be made add city add country and add record in the countrycapital table with the citys id and the countrys code
115621 ive created the filtered index below however when run the queries further down this index only gets used for seek in the first example that has the end dttm in the join rather than the where clause thats the only difference in the queries can anybody explain why this happens index creation create nonclustered index ix patient list bespoke list id includes on dbo patient list bespoke list id asc end dttm asc where end dttm is null queries declare list id int this one seeks on the index select patient lists list id from dbo patient lists left join dbo patient list bespoke on patient lists list id patient list bespoke list id and patient list bespoke end dttm is null where patient lists list id list id this one scans on the index select patient lists list id from dbo patient lists left join dbo patient list bespoke on patient lists list id patient list bespoke list id where patient lists list id list id and patient list bespoke end dttm is null
115765 have some processes that require different steps to be taken before they start for example disable check constraint on foreign key select alter table fk table schema fk table name nocheck constraint fk constraint name from saproduct information schema table constraints fk join saproduct information schema tables on table schema fk table schema and table name fk table name where constraint type foreign key this will produce big list of statements am copying only use saproduct go alter table dbo tblprodclassificationcode nocheck constraint fk tblprodclassificationcode tblprodclassification alter table dbo tblprodclassificationcodedescr nocheck constraint fk prodclassificationcodedescr prodclassificationcode alter table dbo tblprodclassificationcodegenerate nocheck constraint fk prodclassificationcodegenerate prodclassification if was to run that on one go how would find out where am at which table am dealing with and which one is the next the same for the delete select delete from table schema table name from saproduct information schema tables where table name like tbl the script above would produce the set of statements below use saproduct go delete from dbo tblblanguage delete from dbo tblblgcategoryxref delete from dbo tblblgsegmentxref delete from dbo tblprodclassification delete from dbo tblprodclassificationcode delete from dbo tblprodclassificationcodedescr delete from dbo tblprodclassificationcodegenerate delete from dbo tblprodclassificationdescr delete from dbo tblprodclassificationmarket delete from dbo tblprodclassificationtier delete from dbo tblprodclassificationvalue delete from dbo tblproddata delete from dbo tblproddatadescr delete from dbo tblproddatamarket delete from dbo tblproddatatemptier1descr delete from dbo tblproddatatemptier1tier2descr delete from dbo tblproddatatier delete from dbo tblproddatavalue delete from dbo tblproddatavaluedate delete from dbo tblproddatavaluenumber delete from dbo tblproddatavaluestring delete from dbo tblprodname delete from dbo tblprodnamesizealias delete from dbo tblprodnamesizerulealias delete from dbo tblprodnamestructure delete from dbo tblprodnamestructuredescr delete from dbo tblproduct which table is the next one for me to delete am trying to access tables in certain order to minimise deadlocks specially when having to put any type of exclusive locks it is not here but try to avoid lock escalation too so it can get complex like to see what exactly is going on am using sql server microsoft sql server x64 feb copyright microsoft corporation developer edition bit on windows nt x64 build hypervisor
115913 have to update table2 from another table1 located in another database the table1 located in another database is updated daily with transactions from the last day just want to update table2 with the new values added into table1 the databases are located in the same server rows are not going to be deleted from table1 its historical record of transactions which want to replay into table2 because table2 in the other database in the same server will be used for ssas for bi think it requires procedure this is what tried use bdid bi go set ansi nulls on go set quoted identifier on go create procedure transactionupdate as insert bdid bi dbo transaction select from bdid dbo hl transaction as where select from bdid bi dbo transaction where date date go it will be daily procedure so dont have to insert the values previously inserted have just to update the new values added into table1
115943 im not dba but im responsible for database that currently has hundreds of tables and 5tb of data recently ran the following query in hopes of determining index fragmentation declare databaseid int db idods select object namet object id as tablename t2 name as indexname index id as indexid index type desc as indextype index level as indexlevel avg fragmentation in percent as averagefragmentationpercent avg page space used in percent as averagepagespaceusedpercent page count as pagecount from sys dm db index physical stats databaseid null null null detailed inner join sys indexes t2 on index id t2 index id and object id t2 object id order by avg fragmentation in percent desc the first rows of the result set looks as follows after importing into excel this was very startling to me am reading this correctly that have all these indexes actually lot more that are fragmented is my query correct
116054 want to prevent explicit inserts into serial columns have come up to the following trigger drop table test table create table test table id bigserial primary key foobar text create or replace function serial id check returns trigger as begin if new id currvaltg table name id seq then raise exception explicit insert into serial id currval tried to insert currvaltg table name id seq new id end if return new end language plpgsql create trigger test table serial id check before insert on test table for each row execute procedure serial id check maybe there is better approach maybe this approach is broken and this cannot be achieved at all also think about not granting rights for the insert and update but only for pgplsql procedure for inserting updating but this approach is not possible for me right now
116142 im working with postgresql have table that contains the following entries id postcode date created al2 2qp al2 2qp sp2 8ag se4 e2 fk20 8ru fk20 8ru se2 fk20 8ru fk20 8ru se1 fk20 8ru se1 8ga se1 hp27 9rz hp27 9rz se1 tn21 8qb tn21 8qb tn21 8qb n4 1ny what want to achieve is query that returns the most recent unique postcode records for each id id postcode n4 1ny tn21 8qb se4 sp2 8ag al2 2qp se1 hp27 9rz se1 8ga fk20 8ru se2 what would be the best way of achieving this ive been playing around with subqueries but keep hitting walls when it comes to ordering them whilst doing distinct and group by
116240 am creating temporary table mytable and using cursor does this create problem when concurrent users are accessing the cursor through my application does it allow me to create separate temp tables with the same name following is the sample code open cursor fetch next from cursor into variable temp table name create table mytablepk int while fetch status begin fetch next from cursor into variable temp table name end
116280 is it possible to have multiple fields as range key say have table where each row is uniquely identified by abc where is the primary hash key and want and to be the primary range keys how can have more than fields as primary key in dynamodb
116330 using sql plus cannot use the backspace button to erase content during line command when hit backspace instead of erasing it writes is there way to configure backspace so it works as intended
116347 in previous question of mine is it good idea to disable lock escalation while adding new calculated columns to table am creating computed column alter table dbo tblbgiftvoucheritem add isusgift as cast isnull case when sintmarketid and strtype card and strtier1 like gg then else end as bit persisted the calculated column is persisted and according to computed column definition transact sql persisted specifies that the database engine will physically store the computed values in the table and update the values when any other columns on which the computed column depends are updated marking computed column as persisted allows an index to be created on computed column that is deterministic but not precise for more information see indexes on computed columns any computed columns used as partitioning columns of partitioned table must be explicitly marked persisted computed column expression must be deterministic when persisted is specified but when try to create an index on my column get the following error create index fix tblbgiftvoucheritem incl on dbo tblbgiftvoucheritem stritemno include strtier3 where isusgift filtered index fix tblbgiftvoucheritem incl cannot be created on table dbo tblbgiftvoucheritem because the column isusgift in the filter expression is computed column rewrite the filter expression so that it does not include this column how can create filtered index on computed column or is there an alternative solution
116361 want to migrate my database instance from aws rds mysql to aurora but have doubt about replication and how aurora management the write read operations have my application and want to separate the write operations from the reads want to create master instance only for write operations and other instance read replica only for read operations the problem is here read on aws documentation that need to do this separation on my application and think or hope to find way to do that and be transparent for my application draw simple schema do have to do to get with aurora from aws what aws says do have to do application writes reads master read replica replication what do need need the master always keep with the write operations and it redirects the read to the read replica instance application write read reads master read replica replication the replication is always running but want to separate the write and read process so my summary the master instance detecte the different between writes reads operations and all read operation will be manage by read replica need this solution because aurora offer good features to improve my rds but the only problem is that need to create balance between write and read operations the write operation be processed in the master and it send the read operation to the read replica dont want to define this process and make selection between them in my application code as amazon propose
116365 in my database design course we are learning both relational algebra and relational calculus can see where relational algebra could be useful since it is closely tied to sql our professor said that relational calculus was used as an alternative to sql in some rdmbs most of which are not around any more is there still practical use for relational calculus or is much of this theoretical
116368 why is the second insert statement 5x slower than the first from the amount of log data generated think that the second is not qualifying for minimal logging however the documentation in the data loading performance guide indicates that both inserts should be able to be minimally logged so if minimal logging is the key performance difference why is it that the second query does not qualify for minimal logging what can be done to improve the situation query inserting 5mm rows using insert with tablock consider the following query which inserts 5mm rows into heap this query executes in second and generates 64mb of transaction log data as reported by sys dm tran database transactions create table dbo minimalloggingtest int not null go insert into dbo minimalloggingtest with tablock select any table view sub query that correctly estimates that it will generate 5mm rows from dbo fivemillionnumbers provides greater consistency on my laptop where other processes are running option maxdop go query inserting the same data but sql underestimates the of rows now consider this very similar query which operates on exactly the same data but happens to draw from table or complex select statement with many joins in my actual production case where the cardinality estimate is too low this query executes in seconds and generates 461mb of transaction log data create table dbo minimalloggingtest int not null go insert into dbo minimalloggingtest with tablock select any table view sub query that produces 5mm rows but sql estimates just rows from dbo fivemillionnumbersbadestimate provides greater consistency on my laptop where other processes are running option maxdop go full script see this pastebin for full set of scripts to generate the test data and execute either of these scenarios note that you must use database that is in the simple recovery model business context we are semi frequently moving around millions of rows of data and its important to have these operations be as efficient as possible both in terms of the execution time and the disk load we had initially been under the impression that creating heap table and using insert with tablock was good way to do this but have now become less confident given that we observed the situation demonstrated above in an actual production scenario albeit with more complex queries not the simplified version here
116494 this is my slow query select products counts cid from products counts products counts left outer join products products on products counts product id products id left outer join trademarks trademark on products trademark id trademark id left outer join suppliers supplier on products counts supplier id supplier id where products counts product id in order by products counts inflow asc supplier delivery period asc trademark sort desc trademark name asc limit average query time is 5s on my dataset and this is unacceptable solutions see add all columns from order clause to products counts table but have order types in application so should create lot of columns and indexes plus products counts have very intensively updates inserts deletes so need to perform immediately update all order related columns using triggers is there other solution explain id select type table type possible keys key key len ref rows extra simple products counts range product id supplier idproduct idpid count product id supplier id null using where using temporary using filesort simple products eq ref primary primary uaot products counts product id simple trademark eq ref primary primary uaot products trademark id simple supplier eq ref primary primary uaot products counts supplier id tables structure create table products counts id int11 not null auto increment product id int11 unsigned not null supplier id int11 unsigned not null count int11 unsigned not null cid varchar64 not null inflow varchar10 not null for delete tinyint1 unsigned not null default primary key id unique key cid cid unique key product id supplier id product idsupplier id key product id product id key count count key pid count product idcount engine innodb default charset utf8 create table products id int11 not null auto increment external id varchar36 not null name varchar255 not null category id int11 unsigned not null trademark id int11 unsigned not null photo varchar255 not null sort int11 unsigned not null otech tinyint1 unsigned not null not liquid tinyint1 unsigned not null default applicable varchar255 not null code main varchar64 not null code searchable varchar128 not null total int11 unsigned not null slider int11 unsigned not null slider title varchar255 not null primary key id unique key external id external id key category id category id key trademark id trademark id engine innodb default charset utf8 create table trademarks id int11 not null auto increment external id varchar36 not null name varchar255 not null country id int11 not null sort int11 unsigned not null default sort list int10 unsigned not null default is featured tinyint1 unsigned not null is direct tinyint1 unsigned not null default primary key id unique key external id external id engine innodb default charset utf8 create table suppliers id int11 not null auto increment external id varchar36 not null code varchar64 not null name varchar255 not null delivery period tinyint1 unsigned not null is default tinyint1 unsigned not null primary key id key external id external id engine innodb default charset utf8 mysql server information mysqld ver deb sury org trusty for debian linux gnu on i686 ubuntu
116521 we are in the planning stages for an upgrade from sql server r2 to sql server our system executes stored procedures against different databases on different servers if we upgrade one server to sql server but leave the remaining servers on sql server r2 would we lose that ability
116552 first words you can safely ignore the sections below and including joins starting off if you just want to take crack of the code the background and results just serve as context please look at the edit history before if you want to see what the code looked like initially objective ultimately want to calculate interpolated gps coordinates for the transmitter or xmit based on the datetime stamps of available gps data in table secondtable that directly flank the observation in table firsttable my immediate objective to accomplish the ultimate objective is to figure out how to best join firsttable to secondtable to get those flanking time points later can use that information can calculate intermediate gps coordinates assuming linear fitting along an equirectangular coordinate system fancy words to say dont care that the earth is sphere at this scale questions is there more efficient way to generate the closest before and after time stamps fixed by myself by just grabbing the after and then getting the before only as it related to the after is there more intuitive way that doesnt involve the or structure byrdzeye provided the basic alternatives however my real world experience didnt line up with all of his join strategies performing the same but full credit to him for addressing the alternative join styles any other thoughts tricks and advice you may have thusfar both byrdzeye and phrancis have been quite helpful in this regard found that phrancis advice was excellently laid out and provided help at critical stage so ill give him the edge here still would appreciate any additional help that can receive with regard to question bulletpoints reflect who believe helped me most on the individual question table definitions semi visual representation firsttable fields rectstamp datetime can contain milliseconds via vba code see ref receivid long xmitid text25 keys and indices pk dt primary unique no null compound xmitid asc rectstamp asc receivid asc uk drx unique no null compound rectstamp asc receivid asc xmitid asc secondtable fields id long autonumber seeded after main table has been created and already sorted on the primary key xtstamp datetime will not contain partial seconds latitude double these are in decimal degrees not degrees minutes seconds longitude double this way straight decimal math can be performed keys and indices pk primary unique no null simple xtstamp asc uidx id unique no null simple id asc receiverdetails table fields receivid long receiver location description text null ok beginning datetime no partial seconds ending datetime no partial seconds lat double lon double keys and indicies pk rid primary unique no null simple receivid asc validxmitters table field and primary key xmitid text25 primary unique no null simple sql fiddle so that you can play with the table definitions and code this question is for msaccess but as phrancis pointed out there is no sql fiddle style for access so you should be able to go here to see my table definitions and code based on phrancis answer http sqlfiddle com e9942 external link joins starting off my current inner guts join strategy first create firsttable rekeyed with column order and compound primary key rectstamp receivid xmitid all indexed sorted asc also created indexes on each column individually then fill it like so insert into firsttable rekeyed rectstamp receivid xmitid select distinct row rectstamp receivid xmitid from firsttable where xmitid in select xmitid from validxmitters order by rectstamp receivid xmitid the above query fills the new table with records and returns within matter of seconds or so the following completes within second or two when this whole method is wrapped in select count from when the top subquery method is used select receiverrecord rectstamp receiverrecord receivid receiverrecord xmitid select top xmitgps id from secondtable as xmitgps where receiverrecord rectstamp xmitgps xtstamp order by xmitgps id as afterxmit id from firsttable rekeyed as receiverrecord inner join secondtable as xmitgps on receiverrecord rectstamp xmitgps xtstamp group by rectstamp receivid xmitid no separate join needed for the top method but it would be required for the other methods additionally no restriction of the returned set is needed if create the rekeyed table may not need group by either could try order by the three afterxmit id alternatives below take longer than minutes to complete or do not ever complete firstxmitgps id minxmitgps id minswitchxmitgps xtstamp receiverrecord rectstamp xmitgps id null previous inner guts join query first fastish but not good enough select rectstamp receivid xmitid maxiifb xtstamp rectstampb xtstampnull as beforextstamp miniifb xtstamp rectstampb xtstampnull as afterxtstamp from firsttable as inner join secondtable as on rectstamp xtstamp or rectstamp xtstamp group by rectstamp receivid xmitid alternative for beforextstamp max xtstamp rectstamp xtstamp alternatives for afterxtstamp see aside note below max1 xtstamp rectstamp xtstamp min1 xtstamp rectstamp xtstamp second slower select rectstamp abyb1 xtstamp as beforextstamp abyb2 xtstamp as afterxtstamp from firsttable as inner join select top b1 xtstamp a1 rectstamp from secondtable as b1 firsttable as a1 where b1 xtstamp a1 rectstamp order by b1 xtstamp desc as abyb1 max time points before on rectstamp abyb1 rectstamp inner join select top b2 xtstamp a2 rectstamp from secondtable as b2 firsttable as a2 where b2 xtstamp a2 rectstamp order by b2 xtstamp asc as abyb2 min time points after on rectstamp abyb2 rectstamp background have telemetry table aliased as of just under million entries with compound primary key based on datetime stamp transmitter id and recording device id due to circumstances beyond my control my sql language is the standard jet db in microsoft access users will use and later versions only about of these entries are relevant to the query because of the transmitter id there is second telemetry table alias that involves approximately entries with single datetime primary key for the first step focused on finding the closest timestamps to the stamps in the first table from the second table join results quirks that ive discovered along the way during debugging it feels really odd to be writing the join logic as from firsttable as inner join secondtable as on rectstamp xtstamp or rectstamp xtstamp which as byrdzeye pointed out in comment that has since disappeared is form of cross join note that substituting left outer join for inner join in the code above appears to make no impact in the quantity or identity of the lines returned also cant seem to leave off the on clause or say on just using comma to join rather than inner or left outer join results in countselect from countselect from rows returned in this query rather than just one line per table as the or explicit join returns this is clearly not suitable first doesnt seem to be available to use given compound primary key type the second join style although arguably more legible suffers from being slower this may be because an additional two inner joins are required against the larger table as well as the two cross joins found in both options aside replacing the iif clause with min max appears to return the same number of entries max xtstamp rectstamp xtstamp works for the before max timestamp but doesnt work directly for the after min as follows min xtstamp rectstamp xtstamp because the minimum is always for the false condition this is less than any post epoch double which datetime field is subset of in access and that this calculation transforms the field into the iif and min max methods the alternates proposed for the afterxtstamp value work because division by zero false generates null values which the aggregate functions min and max skip over next steps taking this further wish to find the timestamps in the second table that directly flank the timestamps in the first table and perform linear interpolation of the data values from the second table based on the time distance to those points if the timestamp from the first table is of the way between the before and after would like of the calculated value to come from 2nd table value data associated with the after point and from the before using the revised join type as part of the inner guts and after the suggested answers below produce select avggps xmitid strdateiso8601msecavggps rectstamp as rectstamp ms strdateiso8601msec is vba function returning text string in yyyy mm dd hh nn ss lll format avggps receivid rd receiver location description rd lat as receiver lat rd lon as receiver lon avggps before lat avggps afterweight avggps after lat avggps afterweight as xmit lat avggps before lon avggps afterweight avggps after lon avggps afterweight as xmit lon avggps rectstamp as rectstamp basic from select aftertimestampid rectstamp aftertimestampid xmitid aftertimestampid receivid gpsbefore beforextstamp gpsbefore latitude as before lat gpsbefore longitude as before lon gpsafter afterxtstamp gpsafter latitude as after lat gpsafter longitude as after lon aftertimestampid rectstamp gpsbefore xtstamp gpsafter xtstamp gpsbefore xtstamp as afterweight from select receiverrecord rectstamp receiverrecord receivid receiverrecord xmitid select top xmitgps id from secondtable as xmitgps where receiverrecord rectstamp xmitgps xtstamp order by xmitgps id as afterxmit id from firsttable as receiverrecord where receiverrecord xmitid in select xmitid from validxmitters group by rectstamp receivid xmitid as aftertimestampid inner join secondtable as gpsafter on aftertimestampid afterxmit id gpsafter id inner join secondtable as gpsbefore on aftertimestampid afterxmit id gpsbefore id as avggps inner join receiverdetails as rd on avggps receivid rd receivid and avggps rectstamp between rd beginning and rd ending order by avggps rectstamp avggps receivid which returns records conforming at least approximately to the final number of expected records run time is probably minutes on my i7 16gb ram no ssd win pro system reference ms access can handle millisecond time values really and accompanying source file txt
116615 want to merge one table into another need to apply conditional logic in my when matched clause which would ideally be done like this merge into atable as using btable as on id id when not matched then do insert when matched and needsadjustment then update set col1 col1 adjustment col2 col2 adjustment col3 col3 adjustment when matched then default case needsadjustment update set col1 col1 col2 col2 col3 col3 this is not valid sql according to the msdn documenation if there are two when matched clauses then one must specify an update action and one must specify delete action this leads me to the following query merge into atable as using btable as on id id when not matched then insert happens here when matched then update set col1 case when needsadjustment then col1 else col1 adjustment end col2 case when needsadjustment then col2 else col2 adjustment end col3 case when needsadjustment then col3 else col3 adjustment end the conditional logic is moved inside of the update to get around the fact that merges can only have one when matched then update clause now instead of one check per row have one check per row per column and there are many more columns than the three in the example can avoid repeating this condition for every column that needs to be updated is there better way to do conditional updates that perhaps dont involve merges
116731 this is piece of code on msdn page for over clause use adventureworks2012 go select businessentityid territoryid datepartyymodifieddate as salesyear convertvarchar20salesytd1 as salesytd convertvarchar20avgsalesytd over partition by territoryid order by datepartyymodifieddate as movingavg convertvarchar20sumsalesytd over partition by territoryid order by datepartyymodifieddate as cumulativetotal from sales salesperson where territoryid is null or territoryid order by territoryidsalesyear so am having issues understanding why convert function had to be used there assume it is to do with return types of one of the fields in the expression part of the convert function can cast be used instead second question have is how exactly does this part work sumsalesytd over partition by territoryid order by datepartyymodifieddate what exactly is that saying is it calculating sum for every year is that it
116747 have to add trigger which should update column using the following format strings current date per day incremental id ids must be incremental and gaps are allowed my approach is rather naive make table with current date and current sequence value and maintain single record in it create table dailysequence date date sequence int insert into dailysequence values getdate create trigger makehumanreadableid on dbo auditmeasures for insert as declare ret int declare tempdate date declare nowdate date set nowdate getdate select ret sequence tempdate date from dailysequence as if nowdate tempdate begin set ret ret update dailysequence set sequence ret end else begin set ret update dailysequence set sequence ret date nowdate end update auditmeasures set humanreadableid cast nowdate as varchar10 cast ret as varchar10 from inserted inner join auditmeasures on inserted id auditmeasures id go questions are there any pitfalls for my solution code inside the trigger wont run inside transaction thus giving incorrect values am missing better solution
116965 is there sql server equivalent of the using index clause in oracle specifically for the construct create table cc1 int c2 int create index ci on c1 c2 alter table add constraint cpk primary key c1 using index ci in the sql server documentation on unique indexes it states emphasis added unique indexes are implemented in the following ways primary key or unique constraint when you create primary key constraint unique clustered index on the column or columns is automatically created if clustered index on the table does not already exist and you do not specify unique nonclustered index the primary key column cannot allow null values which seems to imply that there is way of specifying what index should be used for primary key
117036 in the interest of keeping the database as secure as possible id like to lock the sys and system accounts so that no one can login with them assuming that there are no os scripts cron jobs logging in as sys or system there arent any applications or outside utilities using either of these accounts can always login as sysdba with the proper os account will locking these two accounts have any adverse effects has anyone done this before who can comment on whether or not its good idea
117146 were currently running into some performance problems since our database is getting too big there are data stored from the last years and dont see reason why the data older than years have to be stored in the same tables as the new data now since dont have very profound experience in administrating databases im looking for the best ways to archive old data info there are about records in the database in total the database needs gb on the hard disk the server version is sql server with compatibility level sql server but were planning on upgrading to sql server soon ive thought about two possibilities new database create database similar to the one on the production server and insert all the old data in the new database disadvantage since linked servers are not allowed in our environment it would be difficult to join the old data if needed history schema create new schema hist with the same tables as in the production database insert all old data in these new tables in the new schema advantage easy joining if old data would be needed in the future do you prefere one of the solutions over the other why are there any better possibilities are there existing tools with which this task is easily possible any other thoughts thanks in advance edit additional question would the newly created archive table also need primary foreign keys or should they just have the columns but without keys constraints
117193 have table with 170m rows that looks as follows create table dbo panel subid varchar not null lineageid int null buck varchar null lot varchar null glasstype varchar null eta varchar null constraint pk panel primary key clustered subid asc of the queries against this table reference subid either in the where clause or join one of our dbas told me he could make all those queries and joins perform better by creating the following index create unique nonclustered index ix panel subid lineageid on dbo panel subid asc include lineageid when he told me this thought he was nuts but just checked index usage since this index was created and found the following pk panel seeks scans ix panel subid lineageid seeks scans was bit shocked to see this under what circumstances would this new index every get used why would sql server ever select it or maybe better question would be why would sql server select the new index to do seek instead of the clustered index approximately 25k times it thought seeking on the new index was better choice in case this helps the lineageid essentially indicates where panel was created and there are distinct values it could contain
117306 have query that runs in milliseconds in sql server and takes about seconds in sql server think that ive narrowed this down to poor cardinality estimate for the row count spool operator ive read bit about spool operators here and here but am still having trouble understanding few things why does this query need row count spool operator dont think its necessary for correctness so what specific optimization is it trying to provide why does sql server estimate that the join to the row count spool operator removes all rows is this bug in sql server if so ill file in connect but id like deeper understanding first note can re write the query as left join or add indexes to the tables in order to achieve acceptable performance in both sql server and sql server so this question is more about understanding this specific query and plan in depth and less about how to phrase the query differently the slow query see this pastebin for full test script here is the specific test query im looking at prune any existing customers from the set of potential new customers this query is much slower than expected in sql server select from potentialnewcustomers 10k rows where cust nbr not in select cust nbr from existingcustomers 1mm rows sql server the estimated query plan sql server believes that the left anti semi join to the row count spool will filter the rows down to row for this reason it selects loop join for the subsequent join to existingcustomers sql server the actual query plan as expected by everyone but sql server the row count spool did not remove any rows so we are looping times when sql server expected to loop just once sql server the estimated query plan when using sql server or option querytraceon in sql server the row count spool does not reduce the estimated of rows and hash join is chosen resulting in far better plan the left join re write for reference here is way that may re write the query in order to achieve good performance in all sql server and however im still interested in the specific behavior of the query above and whether it is bug in the new sql server cardinality estimator re writing with left join yields much better performance in select from potentialnewcustomers left join select as test cust nbr from existingcustomers on cust nbr cust nbr where test is null
117391 am running sql r2 and the db was working fine and fast for last years untill about months ago we added ntext field on very active and used table now we are starting to get out of server space because of the huge expanding size of this table read that shrinking we do not want to loose the indexing of db because it was working fast for years and we do not want to get fragmentation expending we decided to delete that field and all its values is there way to delete the ntext field and all its values and release space without removing indexing without shrinking without loosing db performance am attaching the db size query output to show you size expanding of last months
117469 need to create some test data that involves hierarchy could make it easy and do couple of cross joins but that would give me structure that is completely uniform without any variation that not only seems dull but lack of variation in test data sometimes masks problems that would otherwise be found so am wanting to generate non uniform hierarchy that follows these rules levels deep level is randomly nodes level is nodes random per each node of level level is nodes random per each node of level all branches will be levels deep uniformity in depth is ok at this point there can be overlap in names of child nodes on any given level names of child nodes do not need to be unique across all nodes on the same level the term random is defined here as being pseudo random not uniquely random this needs to be mentioned since the term random is often used to mean random ordering of given set that does not produce duplicates accept that random random and if the number of children per each node of level is only and even across nodes on level that has potential spread of children per each of those nodes then that is fine because that is what random is even though this can be done quite easily with nested while loops the preference is to find set based approach generally speaking generating test data does not have the requirements for efficiency that production code would have but shooting for set based approach will likely be more educational and help in the future with finding set based approaches to problems so while loops are not ruled out but can only be used if no set based approach is possible set based ideally single query regardless of ctes applys etc so using an existing or inline numbers table is fine using while cursor procedural approach will not work suppose staging portions of the data into temp tables or table variables fine just so long as the operations are all set based no loops however that being said single query approach will probably be favored over multiple queries unless it can be shown that the multi query approach is actually better please also keep in mind that what constitutes better is typically subjective please also keep in mind that the usage of typically in the prior sentence is also subjective any version and edition of sql server and newer suppose will do only pure sql none of that silly sqlclr stuff at least in terms of generating the data creating the directories and files will be done using sqlclr but here am just focusing on generating the values of what to create sql multi statement tvf are considered procedural not set based even though on the outside they mask the procedural approach in set there are times when that is absolutely appropriate this is not one of those time along those same lines sql scalar functions are also not allowed not only because they are also procedural but the query optimizer sometimes caches their value and repeats it such that the output is not as expected sql inline tvfs itvfs are okey dokey as they are set based and effectively the same as using cross outer apply which was stated above as being ok repeated executions of the queryies should produce mostly different result from the prior run clarification update the final result set should be expressed as having one row for each distinct node of level3 having the full path starting at level1 this means that the level1 and level2 values will necessarily repeat across one or more rows except in cases of there being only single level2 node containing only single level3 node clarification update there is very strong preference for each node having name or label and not just number this will allow for the resulting test data being more meaningful and realistic am not sure if this additional info matters but just in case it helps to have some context the test data relates to my answer on this question import xml files into sql server while not relevant at this point the end goal of generating this hierarchy is to create directory structure to test recursive file system methods levels and will be directories and level will end up being the file name have searched around both here and via the googles and have only found one reference to generating random hierarchy linux create random directory file hierarchy that question on stackoverflow is actually quite close in terms of desired result since that also seeks to create directory structure for testing but that question and the answers are focused on linux unix shell scripting and not so much the set based world that we live in now know how to generate random data and am doing so already to create the contents of the files so that they can also show variations the tricky part here is that the number of elements within each set is random not particular field and the number of elements within each node needs to be random from other nodes on that same levels example hierarchy level level iii vi vii ix aaa ddd asdf qwerty beft roygbp poi moi soy joy roy example result set describing the hierarchy above level level level iii vi vii ix aaa ddd asdf asdf asdf qwerty beft roygbp poi roygbp moi roygbp soy roygbp joy roygbp roy
117484 sql server sample query at the bottom of this post im trying to create simple report for when given database was last backed up when executing the sample query with output to text in ssms the db name column is formatted to be the max possible size for data same issue exists in db2 btw so ive got column that contains data that is never more than say characters but its stored in varchar128 get characters of data no matter what rtrim has no effect on the output is there an elegant way that you know of to make the formatted column length be the max size of actual data there rather than the max potential size of data guess there exists an xp sprintf function but im not familiar with it and it doesnt look terribly robust ive tried casting it like this declare servername length int select servername length len cast serverpropertyservername as varcharmax select convertchar servername length serverpropertyservername as server but then sql server wont let me use the variable database name length in my varchar definition when casting sql server apparently demands literal number when declaring the char or varchar variable im down to building the statement in string and using something like sp executesql or building temp table with the actual column lengths need both of which are really bit more trouble than was hoping to go to just to not get spaces in my output on character column have searched the interwebs and found bupkus maybe im searching for the wrong thing or google is cross with me it seems that ssms will format the column to be the maximum size allowed even if the actual data is much smaller was hoping for an elegant way to fix this without jumping through hoops im using ssms if go to results to grid and then to excel or something similar the trailing space is eliminated was hoping to basically create report that email though sample query query select convertchar32 serverpropertyservername as server msdb dbo backupset database name maxmsdb dbo backupset backup finish date as last db backup date from msdb dbo backupmediafamily inner join msdb dbo backupset on msdb dbo backupmediafamily media set id msdb dbo backupset media set id where msdb backupset type group by msdb dbo backupset database name order by msdb dbo backupset database name
117567 as part of the process to add articles to my publication use the stored procedure sp addarticle use returns exec sp addarticle publication nusreturns article ntblreturnscontainertypedescr source owner ndbo source object ntblreturnscontainertypedescr type nlogbased description creation script pre creation cmd ndrop schema option 0x000000000803509f identityrangemanagementoption nnone destination table ntblreturnscontainertypedescr destination owner ndbo status vertical partition nfalse ins cmd ncall sp msins dbotblreturnscontainertypedescr del cmd ncall sp msdel dbotblreturnscontainertypedescr upd cmd nscall sp msupd dbotblreturnscontainertypedescr go have publications with over hundreds of articles save the scripts but would like them to be properly formatted how can format my scripts in ssms wanted to see my scripts in this way use returns exec sp addarticle publication nusreturns article ntblreturnscontainertypedescr source owner ndbo source object ntblreturnscontainertypedescr type nlogbased description creation script pre creation cmd ndrop schema option 0x000000000803509f identityrangemanagementoption nnone destination table ntblreturnscontainertypedescr destination owner ndbo status vertical partition nfalse ins cmd ncall sp msins dbotblreturnscontainertypedescr del cmd ncall sp msdel dbotblreturnscontainertypedescr upd cmd nscall sp msupd dbotblreturnscontainertypedescr go what is working for me at the moment is copying and pasting the script into word then following this link special characters you can use with find and replace in word can put all the in new line and other similar tricks is there any way of doing this level of formatting using ssms
117734 need to backup up sql server r2 databases with sizes between gb while they are online and used simultaneously by single enterprise app also need to restore them to state that is largely synchronized across all databases can afford up to few seconds of desync between databases the purpose is to capture production data for qa dev environments would strongly like to not demand databases run in full recovery and to come up with backup method that is dedicated to capturing data for qa environments and remains independent of main backup process which is not under my control for my customers it will take hours to capture full backups at gb each this makes taking full backups sequentially unacceptable as the databases would be too desynchronized when running in simple recovery im looking for an idea better than these idea san level snapshot of vm disks xcopy mdfs ldfs from snapshot once the copied files are attached to different server instance its recovery process should produce consistent databases that are snapshot pretty much simultaneously googling around convinced me this is bad idea at least because may get desync vs master msdb etc idea orchestrate complex backup sync restore across all databases this requires me demanding databases run in full recovery which dont want start parallel backups for all databases well before the deadline t0 once t0 is reached backup all logs should take at most few minutes take the resulting myriad of backups and try to restore them roll logs forward back to obtain somewhat consistent state across databases relative to t0 this requires lot of planning scripting to have it used reliably so would go to great lengths to avoid it am missing some other solution wouldve loved being able to use db snapshots the idea was to initiate snapshot on each db which should be over in seconds then fully backup each one sequentially over the following minutes hours then restore all of them on different server and revert each one to the snapshot afaik this scenario is not possible because snapshots cant be backed up along with the database they can only be rolled back in place on the server where they were created in addition they require enterprise edition which dont have for all customers if you know of 3rd party solution capable of producing cross db synchronized backups please mention it
117740 is there any way to have postgres like query on array field currently want something like that select from list where lowerarray field like currently lower is not needed that much however it should find one matching field inside the array is that even possible currently use materialized view to generate the list table with join and array agg since join table where more values could be on the right table which would duplicate fields on the left table which is not what want edit this is how create the view really sluggish and ugly create materialized view article list new as select id oa nr date deleted lock sds nr kd art nr kd art index kd art extend surface execution surface area cu thickness endintensity drilling array aggo id text as offer list from article list left join task offer on article oa nr group by also need to return the ids of the task offer table
118057 have an sql table of varchar columns which contain greek formatted numbers as thousand separator and comma as decimal separator the classic conversion convertnumeric102replace value does not work because the thousand separator kills the conversion try convertnumeric102replace7 want to convert such values to numeric102 any suggestions of how to handle it
118158 im attempting to run an unpivot on various columns contained in sys databases across various versions of sql server ranging from to the unpivot is failing with the following error message msg level state line the type of column compatibilitylevel conflicts with the type of other columns specified in the unpivot list the sql declare dbname sysname set dbname db name select database unpvt databasename configuration item unpvt optionname configuration value unpvt optionvalue from select databasename name recoverymodel convertvarchar50 recovery model desc compatibilitylevel convertvarchar50 case compatibility level when then sql server when then sql server when then sql server when then sql server when then sql server when then sql server else unknown end autoclose convertvarchar50 case is auto close on when then false else true end autocreatestatistics convertvarchar50 case is auto create stats on when then false else true end autoshrink convertvarchar50 case is auto shrink on when then false else true end autoupdatestatistics convertvarchar50 case is auto update stats on when then false else true end autoupdatestatisticsasynch convertvarchar50 case is auto update stats async on when then false else true end closecursoroncommit convertvarchar50 case is cursor close on commit on when then false else true end defaultcursor convertvarchar50 case is local cursor default when then local else global end ansinull default convertvarchar50 case is ansi null default on when then false else true end ansinulls enabled convertvarchar50 case is ansi nulls on when then false else true end ansipadding enabled convertvarchar50 case is ansi padding on when then false else true end ansiwarnings enabled convertvarchar50 case is ansi warnings on when then false else true end arithmeticabort enabled convertvarchar50 case is arithabort on when then false else true end concatnullyieldsnull convertvarchar50 case is concat null yields null on when then false else true end crossdbownerchain convertvarchar50 case is db chaining on when then false else true end datecorrelationoptimized convertvarchar50 case is date correlation on when then false else true end numericroundabort convertvarchar50 case is numeric roundabort on when then false else true end parameterization convertvarchar50 case is parameterization forced when then simple else forced end quotedidentifiers enabled convertvarchar50 case is quoted identifier on when then false else true end recursivetriggers enabled convertvarchar50 case is recursive triggers on when then false else true end trustworthy convertvarchar50 case is trustworthy on when then false else true end vardecimal storage convertvarchar50 true pageverify convertvarchar50 page verify option desc brokerenabled convertvarchar50 case is broker enabled when then false else true end databasereadonly convertvarchar50 case is read only when then false else true end encryptionenabled convertvarchar50 case is encrypted when then false else true end restrictedaccess convertvarchar50 user access desc collation convertvarchar50 collation name from sys databases where name dbname or dbname is null src unpivot optionvalue for optionname in recoverymodel compatibilitylevel autoclose autocreatestatistics autoshrink autoupdatestatistics autoupdatestatisticsasynch closecursoroncommit defaultcursor ansinull default ansinulls enabled ansipadding enabled ansiwarnings enabled arithmeticabort enabled concatnullyieldsnull crossdbownerchain datecorrelationoptimized numericroundabort parameterization quotedidentifiers enabled recursivetriggers enabled trustworthy vardecimal storage pageverify brokerenabled databasereadonly encryptionenabled restrictedaccess collation as unpvt this is designed to provide nicely formatted list of database options for the given database similar to database configuration item value in use master recoverymodel simple master compatibilitylevel sql server master autoclose false master autocreatestatistics true master autoshrink false master autoupdatestatistics true master autoupdatestatisticsasynch false master closecursoroncommit false master defaultcursor global master ansinull default false master ansinulls enabled false master ansipadding enabled false master ansiwarnings enabled false master arithmeticabort enabled false master concatnullyieldsnull false master crossdbownerchain true master datecorrelationoptimized false master numericroundabort false master parameterization simple master quotedidentifiers enabled false master recursivetriggers enabled false master trustworthy true master vardecimal storage true master pageverify checksum master brokerenabled false master databasereadonly false master encryptionenabled false master restrictedaccess multi user master collation latin1 general ci as ks ws when run this in server with latin1 general ci as ks ws collation the statement succeeds if modify the sql so that certain fields have collate clause it will run on servers that have other collations the code that works on servers with collations other than latin1 general ci as ks ws is declare dbname sysname set dbname db name select database unpvt databasename configuration item unpvt optionname configuration value unpvt optionvalue from select databasename name recoverymodel convertvarchar50 recovery model desc collate sql latin1 general cp1 ci as compatibilitylevel convertvarchar50 case compatibility level when then sql server when then sql server when then sql server when then sql server when then sql server when then sql server else unknown end autoclose convertvarchar50 case is auto close on when then false else true end autocreatestatistics convertvarchar50 case is auto create stats on when then false else true end autoshrink convertvarchar50 case is auto shrink on when then false else true end autoupdatestatistics convertvarchar50 case is auto update stats on when then false else true end autoupdatestatisticsasynch convertvarchar50 case is auto update stats async on when then false else true end closecursoroncommit convertvarchar50 case is cursor close on commit on when then false else true end defaultcursor convertvarchar50 case is local cursor default when then local else global end ansinull default convertvarchar50 case is ansi null default on when then false else true end ansinulls enabled convertvarchar50 case is ansi nulls on when then false else true end ansipadding enabled convertvarchar50 case is ansi padding on when then false else true end ansiwarnings enabled convertvarchar50 case is ansi warnings on when then false else true end arithmeticabort enabled convertvarchar50 case is arithabort on when then false else true end concatnullyieldsnull convertvarchar50 case is concat null yields null on when then false else true end crossdbownerchain convertvarchar50 case is db chaining on when then false else true end datecorrelationoptimized convertvarchar50 case is date correlation on when then false else true end numericroundabort convertvarchar50 case is numeric roundabort on when then false else true end parameterization convertvarchar50 case is parameterization forced when then simple else forced end quotedidentifiers enabled convertvarchar50 case is quoted identifier on when then false else true end recursivetriggers enabled convertvarchar50 case is recursive triggers on when then false else true end trustworthy convertvarchar50 case is trustworthy on when then false else true end vardecimal storage convertvarchar50 true pageverify convertvarchar50 page verify option desc collate sql latin1 general cp1 ci as brokerenabled convertvarchar50 case is broker enabled when then false else true end databasereadonly convertvarchar50 case is read only when then false else true end encryptionenabled convertvarchar50 case is encrypted when then false else true end restrictedaccess convertvarchar50 user access desc collate sql latin1 general cp1 ci as collation convertvarchar50 collation name from sys databases where name dbname or dbname is null src unpivot optionvalue for optionname in recoverymodel compatibilitylevel autoclose autocreatestatistics autoshrink autoupdatestatistics autoupdatestatisticsasynch closecursoroncommit defaultcursor ansinull default ansinulls enabled ansipadding enabled ansiwarnings enabled arithmeticabort enabled concatnullyieldsnull crossdbownerchain datecorrelationoptimized numericroundabort parameterization quotedidentifiers enabled recursivetriggers enabled trustworthy vardecimal storage pageverify brokerenabled databasereadonly encryptionenabled restrictedaccess collation as unpvt the observed behavior is that the following fields do not observe either the server collation or the database collation they are always presented in latin1 general ci as ks ws collation on sql server we can use sys sp describe first result set to easily obtain metadata about the columns returned from particular query used the following to determine the collation mismatch declare cmd nvarcharmax set cmd select databasename convertvarchar50 name recoverymodel convertvarchar50 recovery model desc collation convertvarchar50 collation name from sys databases where name db name exec sp describe first result set command cmd the results why is the collation of these columns statically set
118178 have performance related question lets say have user with first name michael take the following query update users set first name michael where users id will the query actually execute the update even though it is being updated to the same value if so how do prevent it from happening
118331 understand how single indexed column works in sql server and how it is implemented using balanced trees there are plenty of interesting videos on youtube on this topic however dont understand how it works if the index is based on multiple columns for example create nonclustered index idxitemscatstate on items categoryofferstate include id ranking and how it can speedup query like select id ranking from items where category and offerstate it is still implemented as tree how it can evaluate the combination of values what are the restrictions for such feature
118452 when right clicking on table and selecting select top rows get this error is there any workaround other than the obvious upgrade to sql server
118458 am trying to use what appears to be rather badly written application which connects to an sql server database which specify cannot seem to connect to the sql server instance wish to use without specifying the port with comma dbserver dbinstance port number now the problem that this causes and why think the application is badly written is that for some reason the application then thinks that the port number is the name of the database and the server instance name is dbserver dbinstance believe it is not expecting to have to deal with port number like this now have used sql server instances before which did not require the port number to be specified know this must be some setting under the tcp ip settings in sql server configuration manager and have tried setting the port to the default as well as turning on dynamic ports but neither of these have worked am still fairly new to sql server if anybody could point me in the right direction it would be greatly appreciated edit having solved the problem as per ste bovs answer also now know why the application could not handle the port being specified internally the application was storing its data in temporary csv file and didnt bother to check input for commas or enclose it in quotes so when it came to the port in the csv it thought it was the next field which was supposed to be the database name so yes badly written application indeed
118492 im attempting to look at the estimated execution plan for the following sql statement exec msdb dbo sp delete job job id 3a015189 f4eb 439b 9ca0 27afb74719d8 originating server local delete history delete unused schedule when do that sql server throws the following error msg level state procedure sp delete all msx jobs line column name or number of supplied values does not match table definition the definition of sp delete all msx jobs contains the following pertinent lines starting at line wrapped for readability insert into temp jobs to delete select sjv job id case sjs server id when then else end sjv owner sid from msdb dbo sysjobs view sjv left outer join msdb dbo sysjobservers sjs on sjv job id sjs job id where isnullsjs server id and sjv originating server msx server line of that stored proc shows the definition of temp jobs to delete is create table temp jobs to delete job id uniqueidentifier not null job is cached int not null owner sid varbinary85 not null that looks valid to me why is display estimated execution plan returning an error can run this on sql server 2008r2 and both return the error im using ssms
118649 is there some way to ignore both cached query plans and any data pages in memory for single query or batch something like table hint or other option which can be turned on just for the current connection want to force this query to hit the disk because im trying to tune it and the execution times are all over the place trying to reduce variables in play
118729 have process which involves executing various commands between multiple databases however when use dynamic sql to change db with use var then it doesnt actually change the database executing this in test db declare currentdb varcharmax declare sql varcharmax set currentdb db name set sql use currentdb use master exec sql select db name returns master as the current database name if put use test db as command rather than dynamically then it returns the correct name is there way to do this which will correctly switch between databases
118737 have seen this question ssis how to query currently running packages in sql it gives me the following script select execution id folder name project name package name reference id reference type environment folder name environment name project lsn executed as sid executed as name use32bitruntime operation type created time object type object id status start time end time caller sid caller name process id stopped by sid stopped by name dump id server name machine name total physical memory kb available physical memory kb total page file kb available page file kb cpu count folder id name description created by sid created by name created time project id folder id name description project format version deployed by sid deployed by name last deployed time created time object version lsn validation status last validation time pkg package id pkg name pkg package guid pkg description pkg package format version pkg version major pkg version minor pkg version build pkg version comments pkg version guid pkg project id pkg entry point pkg validation status pkg last validation time from ssisdb catalog executions as inner join ssisdb catalog folders as on name folder name inner join ssisdb catalog projects as on folder id folder id and name project name inner join ssisdb catalog packages as pkg on pkg project id project id and pkg name package name but it does not answer my quest am investigating the reasons why packages fail and need to get hold of the error messages where can find it would like to use sql to query for the error message have also this script below that takes me near but not quite select from select em from ssisdb catalog event messages em where em operation id select maxexecution id from ssisdb catalog executions and event name not like validate put in whatever where predicates you might like where event name onerror where package name infogroup feed dtsx where execution path like some executable order by message time desc this is the email would like to tackle how did they get to that error message any information as how to troubleshoot ssis errors is welcome
118823 currently ive only got one backup file imtdb bak and its on the same hdd as the database itself want to increase the redundancy of this db backup by essentially copying it to another disk but get an error backup failed for server the media is formatted to support media families think this means that when created the backup its only meant to put the backup on one drive and cant retroactively add more want to migrate the backup to this other drive really just copy it over but dont want to have to delete the current backup to do it what do do am safe in just copying imtdb bak to folder on the other drive
118880 have problem because have been using this query without problem until now update t1 set ordinal datediffday t2 opening date t1 date from facttransactions t1 inner join dimstore t2 on t1 cod store t2 cod storekey but now it gave me an error conversion failed when converting date and or time from character string have no idea whats going on here are the columns ordinalnumericnull opening datevarchar not null datevarchar not null cod storeintnot null cod storekeypkint not null
119621 using mysql with innodb storage engine for most of the tables innodb buffer pool size is gb and innodb db indexes are around gb server has 32gb ram and is running cent os x64 have one big table which contains around millions records get an updated dump file from remote server every hours the file is in csv format dont have control over that format the file is mb tried inserting data to myisam table row by row and it took minutes need to take only values per line out of from the file and update it in the database whats the best way to achieve something like this need to do this daily currently flow is like this mysqli begin transaction read dump file line by line update each record line by line mysqli commit above operations takes around minutes to complete and while doing this there are other updates going on which gives me lock wait timeout exceeded try restarting transaction update data loading in new table using load data local infile in myisam it took sec while in innodb it took min sec then did update table1 t1 table2 t2 set t1 field1 t2 field1 t1 field2 t2 field2 t1 field3 t2 field3 where t1 field10 t2 field10 query ok rows affected hours min sec update same update with join query update table1 join table2 on field1 field1 set field2 field2 field3 field3 field4 field4 hours min sec clarifications from questions in comments about of the rows in the table will be updated by the file but sometimes it can be as much as there are indexes on the fields being updated there are indexes on the table and indexes include the update fields it is not necessary to do the update in one transaction it can take time but not more than hours am looking to get it done in hour without locking the whole table as later have to update the sphinx index which is dependent on this table it does not matter if the steps take longer duration as long as the database is available for other tasks could modify the csv format in preprocess step the only thing that matters is quick update and without locking table is myisam it is the newly created table from csv file using load data infile myi file size is mb table is indexed on the field1 column myd of the myisam table is 663mb update here are more details about both table create table content hash char40 character set ascii not null default title varchar255 collate utf8 unicode ci not null default og name varchar255 collate utf8 unicode ci not null default keywords varchar255 collate utf8 unicode ci not null default files count smallint5 unsigned not null default more files smallint5 unsigned not null default files varchar255 collate utf8 unicode ci not null default category smallint3 unsigned not null default size bigint19 unsigned not null default downloaders int11 not null default completed int11 not null default uploaders int11 not null default creation date datetime not null default upload date datetime not null default last updated datetime not null default vote up int11 unsigned not null default vote down int11 unsigned not null default comments count int11 not null default imdb int8 unsigned not null default video sample tinyint1 not null default video quality tinyint2 not null default audio lang varchar127 character set ascii not null default subtitle lang varchar127 character set ascii not null default verified tinyint1 unsigned not null default uploader int11 unsigned not null default anonymous tinyint1 not null default enabled tinyint1 unsigned not null default tfile size int11 unsigned not null default scrape source tinyint1 unsigned not null default record num int11 unsigned not null auto increment primary key record num unique key hash hash key uploaders uploaders key tfile size tfile size key enabled category upload date verified enabledcategoryupload dateverified key enabled upload date verified enabledupload dateverified key enabled category verified enabledcategoryverified key enabled verified enabledverified key enabled uploader enableduploader key anonymous uploader anonymousuploader key enabled uploaders upload date enableduploadersupload date key enabled verified category enabledverifiedcategory key verified enabled category verifiedenabledcategory engine innodb auto increment default charset utf8 collate utf8 unicode ci row format fixed create table content csv dump temp hash char40 character set ascii not null default title varchar255 collate utf8 unicode ci not null category id int11 unsigned not null default uploaders int11 unsigned not null default downloaders int11 unsigned not null default verified tinyint1 unsigned not null default primary key hash engine myisam default charset utf8 collate utf8 unicode ci and here is the update query which updates content table using data from content csv dump temp update content join content csv dump temp on hash hash set uploaders uploaders downloaders downloaders verified verified update all above testing was done on test machine but now did same tests on the production machine and queries are very fast mysql update content test join content csv dump temp on hash hash set uploaders uploaders downloaders downloaders verified verified query ok rows affected min sec rows matched changed warnings apologize for my mistake its better to use join instead of each record update now am trying to improve mpre using the index suggested by rick james will update once bench marking is done
119865 when transaction is opened in sql server what are all of the ways that it can be rolled back we are currently working with 3rd party application that is frequently showing open transactions while the query is in sleeping status sometimes for days showing via sp whoisactive this leads me to believe that there is an error somewhere in the application that is not allowing the transactions to commit what are all the ways that these transactions could possibly rollback client side timeout if specified client closes or restarts application manual kill of the spid anything else should be doing anything with transactions that have been open for hours or days if they are going to rollback or timeout at some point anyway is there any harm in killing the process
120060 have an xml column that contains data with similar structure root elements element code value aaa element element code value bbb element element code value ccc element elements root how can modify the data using sql server to change each value attribute into an element root elements element code value aaa value element element code value bbb value element element code value ccc value element elements root update my xml looks more like this root attr1 val1 attr2 val2 elements element code value aaa extradata extra element code value bbb extradata extra element code value ccc extradata extra element code value extradata extra element code extradata extra elements extradata some xml is here extradata root would like only to move value attribute and preserve all other attributes and elements
120064 have table with string column and predicate that checks for rows with certain length in sql server am seeing an estimate of row regardless of the length am checking for this is yielding very poor plans because there are actually thousands or even millions of rows and sql server is choosing to put this table on the outer side of nested loop is there an explanation for the cardinality estimate of for sql server while sql server estimates rows is there good workaround here is short reproduction of the issue create table with 1mm rows of dummy data create table customers cust nbr varchar10 not null go insert into customers with tablock cust nbr select top convertvarchar10 row number over order by select null as cust nbr from master spt values v1 cross join master spt values v2 go looking for string of certain length while both ces yield fairly poor estimates the ce is much more conservative higher estimate and therefore much more likely to yield an okay plan rather than drastically understimated loop join rows estimated 900k rows actual row estimated 900k rows actual select count from customers where lencust nbr option querytraceon optionally use ce go here is more complete script showing additional tests have also read the whitepaper on the sql server cardinality estimator but didnt find anything there that clarified the situation
120488 need to determine the dates which are the 3rd friday of each month for date range of in sql server expect should use combination of dense rank and partition by to set rank however am new to sql and unable to find the correct code
120635 with postgresql im using the version is it possible to do mass update with single query column value with the value of another column but if the other column value is null to use the value of third column and if the third one is absent to use the current datetime all the column have type timestamp need to change columna columnb columnc null foo bar null null baz null null null to columna columnb columnc foo foo bar baz null baz quz null null where quz is the current datetime
120756 according to this blog parameters to function or stored procedure are essentially pass by value if they arent output parameters and essentially treated as safer version of pass by reference if they are output parameters at first thought the goal of forcing tvp to be declared readonly was to clearly signal to developers that the tvp cant be used as an output parameter but there must be more going on because we cant declare non tvp as readonly for example the following fails create procedure dbo test int readonly as select msg level state procedure test the parameter can not be declared readonly since it is not table valued parameter since statistics arent stored on tvp what is the rationale behind preventing dml operations is it related to not wanting tvp to be output parameters for some reason
120945 when run the following command get an error however one of my scripts requires it set time zone utc error hy000 unknown or incorrect time zone utc
121034 is there best practice between using left join or not exists format what is benefit to using one over the other if none which should be preferred select from tablea left join tableb on idx idx where idx is null select from tablea where not exists select idx from tableb where idx idx am using queries within access against sql server database
121160 in mongodb3 appeared new storage engine wiredtiger yet mmapv1 is still the default choice in mongo one might not be better than the other its often matter of use case and choosing the right tool for the job but which engine is right for what job in fact while mmapv1 is the default engine wiredtiger seems better in almost every field it has the same features as mmapv1 plus better write performance document level concurrency compression snapshots and checkpoints system found comparing table on mongodbs blog so except if you are on solaris is there reason not to choose wiredtiger edit here are two videos that explain in details the internals of wiredtiger and mmapv1
121208 im taking over project that involves removing and limiting permissions of all database users across our server farm fun times one of the permissions currently being limited is db owner permissions this permission is being reviewed on case by case basis but common change is to replace the db owner permissions with the following db datareader db datawriter db ddladmin db executor would like to define the exact difference between the two to inform clients however as far as can tell the difference between the two should be db accessadmin permissions db backupoperator permissions db securityadmin permissions so in effect they would lose alter any user create schema backup database backup log checkpoint alter any application role alter any role drop database is there anything else that user would loose once db owner is replaced by the four roles above does this actually serve much of purpose security wise
121236 seeing this note in mysqld log note innodb page cleaner 1000ms intended loop took 15888ms the settings might not be optimal flushed and evicted during the time there seems to be mention of something like this here mysql instance stalling doing sync index my question is what action should be taken if any when this note is seen in the logs mysql and os versions mysql community server el7 x86 centos release el7 centos x86 running show variables like innodb as suggested shows innodb page cleaners
121370 order by datetimecolumn orders datetime column like this datetimecolumn how can order the data so the the values with are last but still order the values ascending by time like so datetimecolumn dumb business rule know know basically if user does not spec time the time does not matter so it goes to the bottom of the list
121804 im trying to write munin plugin to graph db sizes alongside using pg database size want to graph the components thereof as well so far ive come up with the following select sumpg relation sizeoid main as main size sumpg relation sizeoid vm as vm size sumpg relation sizeoid fsm as fsm size sum case reltoastrelid when then else pg total relation sizereltoastrelid end as toast size sumpg indexes sizeoid as indexes size from pg class where reltype indices covered by pg indexes size however summing up those values returns me something which is not the same as the result of pg database size the difference seems to be less significant for larger dbs example on larger db main vm fsm toast indexes sum of values pg database size diff mb mb kb row example on smaller db main vm fsm toast indexes sum of values pg database size diff kb kb kb row what am missing maybe related maybe not im shocked to see the index size they are huge is something in my query wrong here is script used to inspect the different values select sumpg relation sizeoid main as main sumpg relation sizeoid vm as vm sumpg relation sizeoid fsm as fsm sum case reltoastrelid when then else pg total relation sizereltoastrelid end as toast sumpg indexes sizeoid as indexes pg size pretty sumpg relation sizeoid main bigint sumpg relation sizeoid vm bigint sumpg relation sizeoid fsm bigint sumpg indexes sizeoid bigint sum case reltoastrelid when then else pg total relation sizereltoastrelid end bigint as sum of values pg size prettypg database sizecurrent database as pg database size pg size pretty sumpg relation sizeoid main bigint sumpg relation sizeoid vm bigint sumpg relation sizeoid fsm bigint sumpg indexes sizeoid bigint sum case reltoastrelid when then else pg total relation sizereltoastrelid end bigint pg database sizecurrent database bigint as diff from pg class where reltype
121877 currently am transitioning server from company to another they want to keep sql server installed on the server so we are wiping clean all the databases maintenance plans and jobs im trying to delete bunch of sql server agent jobs but even though they are disabled they are still on idle mode so it raises me the following error when trying to delete them drop failed for job job name subplan microsoft sqlserver smo an exception occurred while executing transact sql statement or batch microsoft sqlserver connectioninfo the delete statement conflicted with the reference constraint fk subplan job id the conflict occurred in database msdb table dbo sysmaintplan subplans column job id the statement has been terminated microsoft sql server error how can remove the idle status from the job so the delete gets through
121982 please if this is not place to post questions like these let me know and will delete it inside the glenn berrys diagnostic queries there is query to show how much cpu database is using this is the query get cpu utilization by database query cpu usage by database with db cpu stats as select databaseid db namedatabaseid as database name sumtotal worker time as cpu time ms from sys dm exec query stats as qs cross apply select convertint value as databaseid from sys dm exec plan attributesqs plan handle where attribute ndbid as db group by databaseid select row number overorder by cpu time ms desc as cpu rank database name cpu time ms as cpu time ms cast cpu time ms sum cpu time ms over as decimal5 as cpu percent from db cpu stats where databaseid resourcedb order by cpu rank option recompile and would like to know if this is query to see databases that are now using more cpu or is this based with past information im trying to know what is causing my server to have high cpu use well in this picture the server is pretty good but almost always we have of cpu usage and using sp whoisactive cant find nothing obviously got lot of queries but none of them seems to be hammering the server and read write is pretty low everytime and thats what im having problems to understand how can low read write server be using so much cpu does have nothing in common with cpu im trying to know what database is the heaviest one to migrate it
122262 am trying to run sp askbrent from powershell using invoke sqlcmd and capture its output in variable query exec saidba monitoring sp askbrent seconds check invoke sqlcmd serverinstance serverinstance query query erroraction stop connectiontimeout when running with expertmode no problem but when running with expertmode we notice three things it outputs every data to the shell check is null it ends with the following error invoke sqlcmd duplicate column names are not permitted in sql powershell to repeat column use column alias for the duplicate column in the format column name as new name at line char check invoke sqlcmd serverinstance serverinstance query query er categoryinfo syntaxerror invoke sqlcmd sqlpowershellsqlexecutionexception fullyqualifiederrorid duplicatecolumnnameerrormessage microsoft sqlserver management powershell getscriptcommand think there is workaround of providing parameters for sp askbrent to store the expertmode data into tables and select from those tables afterwards but want to make sure there is no way to get everything back at once in powershell
122582 have several hundred currently but ever growing tables have to copy from one server to another have never had to do this before so im not sure at all on how to approach it all the tables are in the same format cart eight character customer number this is part of larger project of which am merging all these cart number tables to one carts table but thats whole different question altogether does anyone have best practice method can use to copy all these tables over the database names on both servers are the same if that helps and as said earlier have the sa account so can do whatever is necessary to get the data from to both servers are in the same server farm as well
122623 in postgres for column of type uuid how do specify uuid to be generated automatically as default value for any row insert
122666 we are using transactional replication in sql server with one master publisher distributor dedicated server and slaves subscribers all writes are made to master and reading is done from one of the subscribers my issue is that if you make insert update delete and the page is refreshing the update isnt there yet there is sec delay until subscribers are updated which will confuse the user as the row inserted deleted updated but isnt reflected on the subscribers yet we are considering going for peer to peer replication but it seems to be an overhead with identity which goes back to write to one but it will also take too much time to replicate what could should we do
122707 im trying to understand the differences between the different installers of sql server express if you are able to tell me whatever you can or find documentation on them id appreciate it https www microsoft com en us download confirmation aspxid express 32bit wow64 sqlexpr32 x86 enu exe express 32bit sqlexpr x86 enu exe express 64bit sqlexpr x64 enu exe expressadv 32bit sqlexpradv x86 enu exe expressadv 64bit sqlexpradv x64 enu exe expressandtools 32bit sqlexprwt x86 enu exe expressandtools 64bit sqlexprwt x64 enu exe localdb 32bit sqllocaldb msi localdb 64bit sqllocaldb msi mgmtstudio 32bit sqlmanagementstudio x86 enu exe mgmtstudio 64bit sqlmanagementstudio x64 enu exe
122756 have table of million record to join another table with records however there are potential keys lets assume account number email address membership number alternative email and id number in table and columns in table that must be used as joining keys so my code would be something like below select from tbl1 t1 join tbl2 t2 on t1 col1 t2 col1 or t1 col1 t2 col2 or t1 col1 t2 col3 or t1 col5 t2 col1 and so forth the combination of and is huge and it kills the server it also does not sound logical was thinking of putting columns as row and increase the number of records for reducing the number of column however not yet sure if this is the best solution any solution that does not kill server is highly appreciated note kindly note that each column from tb1 is to be matched against columns in tbl2 for instance column col1 that contains account number is joined with col1 which hold all potential account number col1 in no way is joined with email address or some other columns
122823 for reasons out of my control must find solution to this issue simply reinstalling the instance is not an option the server name that appears as server id in sys servers is still showing the old servername im getting an error when running the below command sp dropserver old instance go sp addserver new instance local go error message msg level state procedure sp dropserver line there are still remote logins or linked logins for the server old instance msg level state procedure sp addserver line the server new instance already exists the strangest thing is the remote login is null login exec sp dropremotelogin remoteserver old instance go error message msg level state procedure sp dropremotelogin line there is no remote user null mapped to local user null from the remote server old instance no logins exist for the old instance sp helpremotelogin old instance msg level state procedure sp helpremotelogin line there are no remote logins for the remote server old instance how do rename this instance if cant drop the non existing login is there way to flush the logins
122926 would like to add check constraint to very large table something like alter table accounts add constraint positive balance check balance unfortunately postgresql blocks reads or writes until the constraint check has been completed verified this by starting transaction running the alter table then opening second transaction and checking that couldnt read or write from the table until the first transaction completed is there any way can add this check constraint without locking the table
123327 have table with value column want to calculate the last row minus the first row as shown here id value want to obtain ive tried to use the below command in sql server however last and first dont work select lastvalue firstvalue from counter what is the syntax for this command in sql server
123664 whats the difference between countcase when column then end and countcase when column then else end ive been using the former and havent seen the difference thus far what is the reason for adding the else are there situations where sql server will incorrectly count
123863 want to have fast lookup based on if two columns are equal tried to use computed column with an index but sql server doesnt seem to use it if just use statically populated bit column with an index get the expected index seek seems there is some other questions like this out there but none focused on why an index wouldnt be used test table create table dbo diffs id int not null identity dataa int null datab int null diffpersisted as isnullconvertbit case when dataa is null and datab is not null then when dataa datab then else end persisted diffcomp as isnullconvertbit case when dataa is null and datab is not null then when dataa datab then else end diffstatic bit not null primary key id create index ix diffpersisted on diffs diffpersisted create index ix diffcomp on diffs diffcomp create index ix diffstatic on diffs diffstatic and the query select id from diffs where diffpersisted select id from diffs where diffcomp select id from diffs where diffstatic and the resulting execution plans
123969 when monitor my backups with the following query select command percent complete elapsed total elapsed time remaining estimated completion time from sys dm exec requests where command like backup or command like restore notice that before the backup sql server perform restore headeronly then the backup was wondering what was the use for it and if its execution time could be reduced somehow it appears to take longer than the actual backup
123978 am looking at an application that uses highly dynamic sql queries against sql server looking at the queries that are constructed in very weird and complicated ways but thats different story tell it to give good reason for me being not able too stupid to find things out myself cant see any code where the queries are wrapped with sp executesql but when trace can see lot of queries coming in wrapped with sp executesql the whole application solution does not even contain the command sp executesql at all so wondered if there is kind of configuration do not know yet that forces the software to wrap queries with sp executesql by default what could cause this behaviour
124082 so the title sums it up ive got sql server database with tables and stored procedures that must be reverse engineered im pretty sure that some tables are never used and that not all procs are used as well the biggest problem is that all the windows services that were created to be used with this db and all the software and database documentation is lost and the person that designed the whole system is nowhere to be found ive already managed to create an er diagram to help me understand the relationships but as am not experienced in database administration have no idea where should start also im sorry if this kind of question is not meant to be asked here
124204 we have sql database that stores application usage logs for about pcs these pcs send their usage data to the sql server around times per day we used to store only the most recent days of application usage but the customer asked us to no longer purge data now that we have about years worth of data about rows the sql database is suffering from some performance issues not significant mind you but far more than any other database we have there are significant number of records added each hour application open records and within few hours at most that record will be updated just once with the associated application close it is these updates that you can see via sql activity monitor that are taking considerable time to complete that update query is simple select top id from tb applicationusage where application xxxxxxx and computername xxxxxxxxx and endtime is null order by starttime desc effectively it finds the most recent matching application start for specific machine that doesnt yet have an associated application close cant think of more efficient way to run the query so im considering the following alternative move to two databases working database with only the most recent hours worth of records final database with all other records im no sql guru so im probably missing some drawbacks of this method the goal would be to just have sql agent job move the completed records over to the final database every night then when the customer wants to run their monthly reports can just have that report query only the final database and not the working database with only maybe records to query in the working database instead of it would seem logical that it would work faster but again it seems so simple im probably missing something obvious version microsoft sql server r2
124785 have table custpassmaster with columns in it one of which is custnum varchar8 and created an index ix dbo custpassmaster custnum when run my select statement select from dbo custpassmaster where custnum it ignores the index completely this confuses me as have another table custdatamaster with way more columns one of which is custnum varchar8 created an index on this column ix dbo custdatamaster custnum in this table and use practically the same query select from dbo custdatamaster where custnum and it uses the index created is there any specific reasoning behind this why would it use the index from custdatamaster but not the one from custpassmaster is it due to the low column count the first query returns rows for the second row is returned also additional note custpassmaster has records and custdatamaster has records could this be the reasoning behind ignoring the index custpassmaster also has duplicate records that have the same custnum values as well is this another factor am basing this claim on the actual execution plan results of both queries here is the ddl for custpassmaster the one with the unused index create table dbo custpassmaster custnum varchar not null username char not null password char not null more columns here vbterminator varchar not null on primary create nonclustered index ix dbo custpassmaster custnum on dbo custpassmaster custnum asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary and the ddl for custdatamaster ive omitted lot of irrelevant fields create table dbo custdatamaster custnum varchar not null more columns here vbterminator varchar not null on primary create nonclustered index ix dbo custdatamaster custnum on dbo custdatamaster custnum asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary dont have clustered index on either of those tables only one nonclustered index ignore the fact that the datatypes dont entirely match the type of data being stored these fields are backup from an ibm as db2 database and these are the compatible datatypes for it have to be able to query this backup database with the exact same queries and get the exact same results this data is only used for select statements dont do any insert update delete statements on it except for when the backup application is copying data from the as
124847 which one is the best option to get the identity value just generated via an insert what is the impact of these statements in terms of performance scope identity aggregate function max select top identitycolumn from tablename order by identitycolumn desc
124896 have query which uses where clause and happen to use the exact same where clause in many queries on this table et al the query is select datenamedw atdatetime as day of week count as number of searches castcastcount as decimal10 countdistinct convertdate atdatetime as decimal10 as average searches per day sumcase when numfound then else end as number of searches with no results castcastsumcase when numfound then else end as decimal10 count as decimal10 as percent of searches with no results from db dbo searchhistory where customernumber and customernumber group by datenamedw atdatetime datepartdw atdatetime order by datepartdw atdatetime the part wish to change is the where clause to instead allow me to use table so that if have to add customer number to be ignored dont have to update all my queries and there are quite few queries that have this same where clause
124947 im looking at alwayson availability groups the more look at it the more it appears that the availability listener group is single point of failure where exactly does the listener actually run separate server the primary sql server all of them say have whole application stack at my second data center how do configure the listeners such that they will be running at both sites and that the applications will point to their own local copies im sure im missing something here but dont know what
124949 im trying to update target table which has one row of size 5k to row of size 5k also since its one row its easy to know the row actual size select from sys dm db index physical statsdb idrods hsd es object idntbl bm hsd subject an repro null null detailed reproduce table wasnt altered since creation dont see any reason why it should fail ideas
125069 have table data in which there are two columns startdate and enddate startdate is the date when an employee joined working in company and enddate is the date when he left need to find the working days of employee per month if the startdate is and the enddate is then the working days will be june 10days july 31days august 21days have the query for the total days for the work select datediffddstartdateenddate from tablename but need to take the days as above per month update actually have three columns startdate enddate and salary the full query needs to divide the salary over the calendar months if salary is then for the above dates sample the result is june for days july august
125171 you find that an error in system has been incorrectly naming men as women and vice versa in the database the columns only allows for one character without using any temp tables write one update query to resolve this this question was asked at recent interview had and im going into more interviews that may have similar questions so wanted to get an idea of how to handle this
125190 have to provide reports on file system usage im collecting statistics on file server usage down to individual file level so we can see who is using what files folders how much storage theyre using how many files they have when they were created and last used to do this have powershell scripts the first reads through the file system and captures the attributes want and saves them to file dir rec select lastwritetime directory name extension length name owner expression get acl fullname select owner export csv fileinfo csv the 2nd script reads the csv file and inserts the data into table once the data is in sql can parse the text and split it into various columns and and then produce variety of reports and analyse the data in different ways my approach works but its cumbersome is there better way to collect ntfs information and save it into sql server what are the alternatives ssis edit could this all be combined to operate together in single process
125198 today we were discussing the following brazil has states and each one has its own abbreviation just like usa so weve got rj for rio de janeiro sp for paulo mg for minas gerais and so on one of our programmers proposed that we should use those abbreviations rj sp mg etc as pk on the states table that we are planning to add to new project extrapolating the use of our database countered his argument saying that if we were to someday expand our services to other countries we would have problem with repeated abbreviations for example in the usa there is sc for south carolina and on brazil weve got sc for santa catarina the same happens for mt pa and ma counting on this weve agreed that there should be id column as pk identity now supposing that we do not expand our services to other countries and stay only in brazil started considering the idea of using varchar2 column as pk in this scenario it doesnt sound like totally bad idea is it why in which cases this could be applied should memory be considered in order to choose from one to another
125251 have query in which do some calculation and then give that column an alias in the next column would like to use the result of that calculation in an if statement mysql did not recognise the alias when used it as condition but instead required me to rewrite the whole query put some brackets around it and then carry out the condition checking here is the sql query select studentid subjectid countstudentid select countsubjectid from lectureattendancein where subjectid mis4 as percentage ifcountstudentid select countsubjectid from lectureattendancein where subjectid mis4 allowed not allowed as examadmit from attendancein group by studentid subjectid having subjectid mis4 order by studentid asc would like to use the column alias called percentage in the if statement
125272 we recently deployed new environment with newer version of oracle 12c instead of 11r2 only recently noticed my data returned is sorted differently instead of having small letters capital letters and numbers on oracle 11r2 get my data sorted the opposite way numbers capital letters small letters oracle 12c this is realy confusing the end user of various applications querying the database what settings do need to check compare to fix this problem in oracle 12c the queries refer to have an order by clause edit the settings on for these parameters are the same on both evironments nls sort type string value nls language type string value dutch so there is no value for nls sort what is the default then phil select from nls instance parameters parameter oracle11r2 oracle12c nls language dutch dutch nls territory the netherlands the netherlands nls sort nls date language nls date format nls currency nls numeric characters nls iso currency nls calendar nls time format nls timestamp format nls time tz format nls timestamp tz format nls dual currency nls comp binary binary nls length semantics byte char nls nchar conv excp false false select from nls session parameters these are the same as my nls settings in oracle sql developer so not very relevant guess select from nls database parameters these are the same only nls rdbms version differs so only nls length semantics differs is there way to check the values for other sessions
125279 in sql server standard edition know that the max number of user connections is what should do as dba if am heading towards this number currently there are user connections and this number is expected to increase
125300 constantly see people say that indexes slow down update delete and insert this is used as blanket statement as if it is an absolute while tuning my database to improve performance keep coming across this situation that seems to contradict that rule logically for me and nowhere can find anyone say or explain in any way otherwise in sql server and believe presume most other dbms your indexes are created based on specific columns you specify inserts and deletes will always affect an entire row so there is no way they will not affect the index but updates seem bit more unique they can specifically affect only certain columns if have columns that are not included on any index and update them are they slowed down just because have an index on other columns in that table as an example say in my user table have one or two indexes the primary key which is an identity auto increment column and possibly another on some foreign key column if update column without the index directly on it like say their phone number or address is this update slowed down because have indexes on this table on other columns in either situation the columns am updating are not in indexes so logically the indexes should not be updated shouldnt they if anything would think they are sped up if use the indexes in the where clause
125308 im testing different architectures for large tables and one suggestion that ive seen is to use partitioned view whereby large table is broken into series of smaller partitioned tables in testing this approach ive discovered something that doesnt make whole lot of sense to me when filter on partitioning column on the fact view the optimizer only seeks on the relevant tables additionally if filter on that column on the dimension table the optimizer eliminates unnecessary tables however if filter on some other aspect of the dimension the optimizer seeks on the pk ci of each base table here are the queries in question select od year avgvalue avgobservationvalue from dbo observation join dbo observationdates od on observationdatekey od datekey where observationdatekey and observationdatekey group by od year select od year avgvalue avgobservationvalue from dbo observation join dbo observationdates od on observationdatekey od datekey where od datekey and od datekey group by od year select od year avgvalue avgobservationvalue from dbo observation join dbo observationdates od on observationdatekey od datekey where od year and od year group by od year heres link to the sql sentry plan explorer session im working on actually partitioning the larger table to see if get partition elimination to respond in similar fashion do get partition elimination for the simple query that filters on an aspect of the dimension in the meantime heres stats only copy of the database https gist github com swasheck 9a22bf8a580995d3b2aa the old cardinality estimator gets less expensive plan but thats because of the lower cardinality estimates on each of the unnecessary index seeks id like to know if theres way to get the optimizer to use the key column when filtering by another aspect of the dimension so that it can eliminate seeks on irrelevant tables sql server version microsoft sql server x64 feb copyright microsoft corporation developer edition bit on windows nt x64 build hypervisor
125399 im very green when it comes to the world of database permissions management in sql sever lets keep an example simple say account admin is the owner of schemas and there another account minion that you want to have full rights update delete insert select alter on any object table view created under schemas and is this possible or do you have to execute grant statement each and every time you add table view under these schemas seems bit silly to me
125422 the normal join on syntax is well known but it is also possible to position the on clause separately from the join that it corresponds to this is something that is rarely seen in practice not found in tutorials and have not found any web resource that even mentions that this is possible here is script to play around with select into widgets1 from values xwidgetid select into widgets2 from values somevalue1 somevalue2 somevalue3 xwidgetid somevalue select into widgetproperties from values xwidgetid propertyname q1 select w1 widgetid w2 somevalue wp propertyname from widgets1 w1 left join widgets2 w2 on w2 widgetid w1 widgetid left join widgetproperties wp on w2 widgetid wp widgetid and wp propertyname order by w1 widgetid q2 select w1 widgetid w2 somevalue wp propertyname from widgets1 w1 left join widgets2 w2 no on clause here join widgetproperties wp on w2 widgetid wp widgetid and wp propertyname on w2 widgetid w1 widgetid order by w1 widgetid q3 select w1 widgetid w2 somevalue wp propertyname from widgets1 w1 left join widgets2 w2 no select or from here join widgetproperties wp on w2 widgetid wp widgetid and wp propertyname on w2 widgetid w1 widgetid order by w1 widgetid q1 looks normal q2 and q3 have these unusual positionings of the on clause this script does not necessarily make much sense it was hard for me to contrive meaningful scenario so what do these unusual syntax patterns mean how is this defined noticed that not all positions and orderings for the two on clauses are allowed what are the rules governing this also is it ever good idea to write queries like this
125494 have to get selection for given string with comma separated values passed to the cursor there can be multiple values that could be passed via single variable my code is somewhat like cursor my cursor vsstr1 is select some field from some table where txtfield1 in vsstr1 this field is varchar2 type vsstr1 varchar2100 how to pass that variable correclty any help would be appreciated
125504 am not sure if have chosen the right title for this question what am really after is given individual windows ad user would like to find out the list of the windows ad groups logins that have access to an specific database in this server when run the following query select name principal id type type desc default schema name create date modify date owning principal id sid is fixed role from sys database principals in my server microsoft sql server r2 sp1 x64 jun copyright microsoft corporation standard edition bit on windows nt build service pack get the following results partial list need to know all the permissions particular login has this login has access to my server databases through ad groups what ad groups from the list above does my login belong to have been doing this below but would really like to find out the list of the ad groups that have access to this server according to the above picture that this user belongs to first execute as the user in question execute as login mycompany hthorne declare user varchar20 select user substringsuser sname charindex suser sname lensuser sname make sure that have the right credentials select user suser sname system user user name current user original login user session user go to the specific database and use the fn my permissions run as the user in question use webdataimportstage go select from fn my permissions null database go revert and that is giving me the result below
125617 have database with triggers is there way to delete all the triggers with single command from single database called system db audits
125771 how to concatenate multiple columns in to single row for example id name car sam dodge ram maserati john benz null mazda kirk lexus jim rolls gmc the expected result set is id name car samramjohn dodgemaseratibenzmazdagmc kirkjim lexusrolls using solution found on stack overflow select from select idstuff query name value varcharmax11 as somefield combined1 stuff query car value varcharmax11 as somefield combined2 from dbo test outer applyselect select id name as name car as car from test where test id id for xml path type as as group by idsomefield combined1somefield combined2 are there any better solutions the inner select comes from an expensive multi table join not the single table test shown above the query is in an in line tvf so cannot use temporary table also if there is blank column the results will yield extra commas like id name car samramjohn dodgemaseratibenzmazdagmc kirkjim lexusrolls is there any way to prevent this
125850 how can insert an id into an identity column in sql server ill make special item into my table and will quickly detect that item by the id so because all other ids are bigger or equal to one will give that special item the id zero the problem is now when ive created that table ive place the identity of the column id on true if insert now the special item with code below ive got next error insert into mytableid name description values special title special item error cannot insert explicit value for identity column in table mytable when identity insert is set to off how can change this table column or property so can create record with id zero
125885 long story short we have database here that manages some employee data such as email first name last name etc our company bought into this sap based expense report system that needs an export of our employee based data in very strange format without getting into too many details the export of this data needs total of columns with many of these columns having an empty value simple put together query that basically pulled this information from our database and set some constants to what was needed its not relevant what this query is in this question its simply select statement that pulls some data then needed to export this out on daily basis with specific file name and with pipe delimited format something to this effect employee export declare filename varchar500 set filename select somefileserver public somefolder employee p06010603ace replacereplacereplaceconvertvarchar19 convertdatetime getdate txt declare sql varchar8000 declare header varchar8000 set sql bcp exec mydbserver mydbname dbo concuremployeeexport queryout filename exec master xp cmdshell sql perfect get rows like this jon doe steve smith with about more columns didnt want to display all of this since it is irrelevant thought was done until the implementation coordinator said yes looks good except the first row needs to start with this strange row with some values meaning the resultant query that have should have one additional row with some values in it simple thought would union what they wanted and so did except the union all had to put in some additional empty values to match the number of columns in my query the issue is when run the export end up with bunch of empty columns that are pipe delimited end up with this sso update en imagine that going on for about more columns submitted this to them and they said well everything looks good except your first row we just need the first values that is they only want this sso update en but cannot do that with my union all as my union all requires the same number of columns from both queries so thought could somehow after generating this file just replace the in the first line after the last from above is this possible without having to write another app interface to do this want to keep what have but only modify that first row such that the result get is fixed currently here is sample of three rows sso update en jon doe jon doe company com en us usa tk symbolic usd usa is cc usa0000202105 eol steve smith steve smith company com en us usa tk symbolic usd usa is cc usa0000202105 eol need to get it to this sso update en jon doe jon doe company com en us usa tk symbolic usd usa is cc usa0000202105 eol steve smith steve smith company com en us usa tk symbolic usd usa is cc usa0000202105 eol notice the first row how removed all those ideally would like to do this in maybe the query that exports the file something to the effect of removing all of this on the first row after the export the first row is static generated using select as transaction type as error threshold sso as password generation update as existing record handling en as language code as validate expense group as validate payment group unfortunately my brain keeps telling me this is not good idea is not possible know can easily do this in net but id hate to have yet another program sitting out there doing these sorts of things
125886 im working with sql server want to check if user exists before adding it to database this is what have tested use mydatabase go if not exists select name from sys server principals where name niis apppool mywebapi apppool begin create user iis apppool mywebapi apppool for login iis apppool mywebapi apppool with default schema dbo end alter role db owner add member iis apppool mywebapi apppool go but this code select name from sys server principals doesnt return if that user exists in mydatabase how can check if an user exists in mydatabase
125915 was reviewing our company dr procedures and when looked online for solutions to an always on cluster losing quorum to compare to was three pages into google results before finding the first se post on the subject clustering vs transactional replication vs availability groups which only lightly touches on the subject of lost quorum while everyone agrees the losing quorum is bad and there are some suggestions for decreasing the potential it can still happen am looking for good peer reviewed answer to the best path to recovery from an always on cluster loss of quorum
125985 have pretty large query in view lets call it sql that is really fast unless use order by in an outer select with small limit select customs id as custom id customs custom name as custom name customs slug as slug customs use case as custom use case sumcase when designers id orders user id and orders bulk then order rows quantity else end as sale bulk sumcase when designers id orders user id and orders bulk then order rows quantity else end as sale not bulk sumcase when designers id orders user id then order rows quantity else end as sale total sumcase when designers id orders user id and orders bulk then order rows quantity else end as buy bulk sumcase when designers id orders user id and orders bulk then order rows quantity else end as buy not bulk sumcase when designers id orders user id then order rows quantity else end as buy total sumcase orders bulk when then order rows quantity else end as total bulk sumcase orders bulk when then order rows quantity else end as total not bulk coalescesumorder rows quantity as total minshoes id as shoe id minshoe models id as shoe model id minshoe models name as shoe model name minshoe models title as shoe model title minmodel categories id as model category id minmodel categories name as model category name minbusiness orders id as business order id minbusiness orders state as business order state minbusiness orders published at as business order published at mindesigners id as designer id mindesigners email as designer email mindesigner details first name as designer first name mindesigner details last name as designer last name from business orders rows left join users designers on designers id business orders user id rows business orders has or users users has business orders left join user details designer details on designers id designer details user id rows users has or user details user details has users inner join customs on business orders id customs business order id rows business orders has customs customs has business order left join shoes on shoes product id customs id and shoes product type custom rows customs has shoes shoes has customs left join shoe models on shoe models id shoes shoe model id rows shoes has shoe models shoe models has shoes left join model categories on shoe models model category id model categories id rows shoe models has model categories model categories has models inner join sizes on shoes id sizes shoe id rows sizes has shoes shoes has sizes left join order rows on order rows article id sizes id and order rows article type text size text rows sizes has order rows order rows has or size left join orders on orders id order rows order id rows order rows has orders orders has order rows where orders state in funded confirmed paid delivered production produced ready to ship shipped or orders id is null group by business orders id returns around rows query of the following type is executed in ms select from sql limit the related explain output limit cost rows width actual time rows loops buffers shared hit subquery scan on cost rows width actual time rows loops buffers shared hit groupaggregate cost rows width actual time rows loops group key business orders id buffers shared hit nested loop left join cost rows width actual time rows loops filter orders state text any fundedconfirmedpaiddeliveredproductionproducedready to shipshipped text or orders id is null rows removed by filter buffers shared hit nested loop left join cost rows width actual time rows loops buffers shared hit nested loop cost rows width actual time rows loops buffers shared hit nested loop left join cost rows width actual time rows loops buffers shared hit nested loop left join cost rows width actual time rows loops join filter shoe models model category id model categories id rows removed by join filter buffers shared hit nested loop left join cost rows width actual time rows loops buffers shared hit nested loop cost rows width actual time rows loops buffers shared hit nested loop left join cost rows width actual time rows loops buffers shared hit merge join cost rows width actual time rows loops merge cond business orders id customs business order id buffers shared hit index scan using business orders pkey on business orders cost rows width actual time rows loops buffers shared hit index scan using index customs on business order id on customs cost rows width actual time rows loops buffers shared hit index scan using users pkey on users designers cost rows width actual time rows loops index cond id business orders user id buffers shared hit index scan using index shoes on product id and product type on shoes cost rows width actual time rows loops index cond product id customs id and product type text custom text buffers shared hit index scan using shoe models pkey on shoe models cost rows width actual time rows loops index cond id shoes shoe model id buffers shared hit materialize cost rows width actual time rows loops buffers shared hit seq scan on model categories cost rows width actual time rows loops buffers shared hit index scan using index user details on user id on user details designer details cost rows width actual time rows loops index cond designers id user id buffers shared hit index scan using index sizes on shoe id on sizes cost rows width actual time rows loops index cond shoe id shoes id buffers shared hit index scan using index order rows on article id on order rows cost rows width actual time rows loops index cond article id sizes id filter article type text size text rows removed by filter buffers shared hit index scan using orders pkey on orders cost rows width actual time rows loops index cond id order rows order id buffers shared hit planning time ms execution time ms query of the following type instead is executed in 141ms select from sql order by custom id limit the related explain output limit cost rows width actual time rows loops buffers shared hit read temp read written sort cost rows width actual time rows loops sort key business order id sort method top heapsort memory 27kb buffers shared hit read temp read written subquery scan on cost rows width actual time rows loops buffers shared hit read temp read written groupaggregate cost rows width actual time rows loops group key business orders id buffers shared hit read temp read written sort cost rows width actual time rows loops sort key business orders id sort method external merge disk 56936kb buffers shared hit read temp read written hash right join cost rows width actual time rows loops hash cond order rows article id sizes id filter orders state text any fundedconfirmedpaiddeliveredproductionproducedready to shipshipped text or orders id is null rows removed by filter buffers shared hit read temp read written hash left join cost rows width actual time rows loops hash cond order rows order id orders id buffers shared hit seq scan on order rows cost rows width actual time rows loops filter article type text size text rows removed by filter buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 470kb buffers shared hit seq scan on orders cost rows width actual time rows loops buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 567kb buffers shared hit read temp read written hash left join cost rows width actual time rows loops hash cond shoes shoe model id shoe models id buffers shared hit read temp read written hash join cost rows width actual time rows loops hash cond sizes shoe id shoes id buffers shared hit read temp read written seq scan on sizes cost rows width actual time rows loops buffers shared hit read hash cost rows width actual time rows loops buckets batches memory usage 2679kb buffers shared hit read temp written hash left join cost rows width actual time rows loops hash cond designers id designer details user id buffers shared hit read hash join cost rows width actual time rows loops hash cond customs id shoes product id buffers shared hit read hash left join cost rows width actual time rows loops hash cond business orders user id designers id buffers shared hit read hash join cost rows width actual time rows loops hash cond customs business order id business orders id buffers shared hit read seq scan on customs cost rows width actual time rows loops buffers shared hit read hash cost rows width actual time rows loops buckets batches memory usage 2513kb buffers shared hit seq scan on business orders cost rows width actual time rows loops buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 3679kb buffers shared hit read seq scan on users designers cost rows width actual time rows loops buffers shared hit read hash cost rows width actual time rows loops buckets batches memory usage 2154kb buffers shared hit seq scan on shoes cost rows width actual time rows loops filter product type text custom text buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 2748kb buffers shared hit seq scan on user details designer details cost rows width actual time rows loops buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 4kb buffers shared hit hash left join cost rows width actual time rows loops hash cond shoe models model category id model categories id buffers shared hit seq scan on shoe models cost rows width actual time rows loops buffers shared hit hash cost rows width actual time rows loops buckets batches memory usage 1kb buffers shared hit seq scan on model categories cost rows width actual time rows loops buffers shared hit planning time ms execution time ms table definitions are the following no integrity constraints are defined on the database using orm create table business orders id integer not null user id integer published at timestamp without time zone constraint business orders pkey primary key id create index index business orders on user id on business orders using btree user id create table users id serial not null email character varying255 not null default character varying constraint users pkey primary key id create unique index index users on email on users using btree email collate pg catalog default create table user details id serial not null user id integer first name character varying255 last name character varying255 constraint user details pkey primary key id create index index user details on user id on user details using btree user id create table customs id serial not null shoes assortment id integer business order id integer constraint customs pkey primary key id create index index customs on business order id on customs using btree business order id create table shoes id serial not null product id integer product type character varying255 constraint shoes pkey primary key id create index index shoes on product id and product type on shoes using btree product id product type collate pg catalog default create index index shoes on shoe model id on shoes using btree shoe model id create table shoe models id serial not null name character varying255 not null title character varying255 model category id integer constraint shoe models pkey primary key id create index index shoe models on model category id on shoe models using btree model category id create unique index index shoe models on name on shoe models using btree name collate pg catalog default create table model categories id serial not null name character varying255 not null sort order integer created at timestamp without time zone not null updated at timestamp without time zone not null access level integer constraint model categories pkey primary key id create unique index index model categories on name on model categories using btree name collate pg catalog default create table sizes id serial not null shoe id integer constraint sizes pkey primary key id create index index sizes on shoe id on sizes using btree shoe id create table order rows id serial not null order id integer quantity integer article id integer article type character varying255 article name character varying255 unit taxed cents integer constraint order rows pkey primary key id create index index order rows on article id on order rows using btree article id create index index order rows on article type on order rows using btree article type collate pg catalog default create index index order rows on order id on order rows using btree order id create index index order rows on quantity on order rows using btree quantity create index index order rows on unit taxed cents on order rows using btree unit taxed cents create table orders id serial not null user id integer state character varying255 bulk boolean default false constraint orders pkey primary key id create index index orders on user id on orders using btree user id because the sql is view cant insert the order by clause inside the view will need to call it as black box the use cases for this query are with limit of ordered by custom id with limit of ordered by total to filter all rows that have business order user id orders id and business orders id usually not more than rows as result the graphical explain of pg admin even if dont understand much seems to be telling me that if run the query with no ordering then the query is using indexes and doing nested loop joins while if do it with the ordering then it doesnt it uses hash joins are there any ways to increase performance
126003 in this answer https stackoverflow com questions strings as primary keys in sql database single remark caught my eye also keep in mind that theres often very big difference between char and varchar when doing index comparisons does this apply still apply for postgres found pages on oracle claiming that char is more or less an alias for varchar and so index performance is the same but found nothing definitive on postgres
126030 during our last weekly meeting person that has no background experience in database administration brought up this question would there be scenario that justifies storing data in line string instead of several lines let us assume table called countrystates where we want to store the states of country ill use usa for this example and will not list all the states for the sake of laziness there we would have two columns one called country and the other called states as discussed here and proposed by srutzkys answer the pk will be the code defined by iso alpha our table would look like this country states statename usa al ca floh ny wy alabama california florida ohio new york wyoming when asking this same question to friend developer he said that from the data traffic size point of view this might be useful but not if we need to manipulate this data in this case there would have to be an intelligence on the application code which could transform this string in list lets say that the software that has access to this table needs to create combo box we concluded that this model is not very useful but got suspicious that there might be way to make this useful what id like to ask is if any of you already saw heard or done something like this in way that really works
126095 problem we have been experiencing high levels of user disruption due to sql timeouts accross our systems since the beginning of the year the sql server instance in question has very high cpu usage higher than on all cores all the time during business hours we have also noticed very high wait times the combination of cxpacket latch ex accounts for about of all waits this is split about between cxpacket latch ex the non buffered latch wait accounting for the vast majority of latch ex is access methods dataset parent this suggests the problem is to do with parallelism an example of the scale of wait times is cxpacket ms latch ex ms pageiolatch sh ms this was for the period between on jan 11th options under consideration change maxdop from to something between and modify the cost threshold of parallelism from to higher number suggestions most welcome on how to ease the very high cpu load we are seeing and reduce timeouts in particular whether the proposed course of action is wise and which numbers to change maxdop and cost threshold of parallellism to background information sql server r2 running on amd opteron se of which cores are given to this instance of sql server type of workload something of the order of connections at the same time during business hours majority oltp type workload with some olap mixed in microsoft sql server r2 sp1 x64 enterprise edition bit on windows nt x64 build service pack memory is appx gigs between cores of the cores are available to this instance
126127 im trying to write query which replaces the special characters with space below code helps to identify the rows alpha numeric characters comma and space is valid select columna from tablea where columna like z0 how can integrate the replace function into the select statement so that all characters other than alphanumeric comma and space in the result set are replaced by space this one wont work select replacecolumna z0 from tablea where columna like z0
126154 our sysadmins keep pushing us to use data domain and dd boost for taking our sql backups has anyone successfully used this solution know what brent ozar says about data domain and sql backups but apparently the dd boost makes it better solution has anyone used dd boost and also used sql backup by redgate just wanted to get thoughts on what you would say to your sysadmins and managers apart from this article from brent ozar
126235 when run the following code it takes minutes and does 106million reads however if run just the inner select statement by itself it only takes seconds and does 264k reads as side note the select query returns no records any idea why the if exists would make it run so much longer and do so many more reads also changed the select statement to do select top dlc id and killed it after minutes as temporary fix have changed it to do count and assign that value to variable cnt then it does an if cnt statement but thought exists would be better because if there were records returned in the select statement it would stop performing the scan seeks once it found at least one record whereas the count will complete the full query what am missing if exists select dlc id from tabledlc dlc join tabled on id dlc id join tablec on id id2 where name dlc name begin do something end
126246 example query select from table where id in how do adjust this query so that the order of the returned entries follows the input of the ids ie first second third edit to be clear the ids dynamically generated list that is in the right order
126446 ive recently inherited sql server database that uses binary16 instead of uniqueidentifier to store guids it does this for everything including primary keys should be concerned
126495 have piece of sql that creates table form select statement in postgresql want to add constraints to the table so that for example column cannot be null cannot find valid sql for this in the postgres documentation so it seems it is not possible what would be the best approach to achieve this kind of functionality would like creation of the table to fail if the constraint is violated cannot create the table with the constraint beforehand because there might be some variation in some of the columns as they are the result of dynamic process
126604 have database table that currently looks like this pagefield id int pk fieldtype string can be text decimal integer or bit value this stores the value regardless of the fieldtype so it is not strongly typed should remove the value column and replace it with separate columns textvalue decimalvalue integervalue and bitvalue the data type would be set to what is relevant this would mean in every row of these columns would be null
126632 have table with name columns create table test testid int identity primary key clustered name eng nvarchar50 name nat nvarchar50 now need query to get this name column separated by like this declare namecolumns nvarchar1024 set namecolumns stuff select test name as text from select name from sys columns inner join sys tables on object id object id where name test and name like name as for xml path type value varcharmax select namecolumns but this query has warning in the execution plan is there any way to remove this warning
126846 following code is used for write number in the text file but instead it is writing echo is on into the text file set cmd echo ver vercheck print cmd exec xp cmdshell cmd no output print cmd gives echo pharmsuite vercheck txt here instead of the query is writing echo is on
126852 could somebody tell what is wrong with obvious query below db2 select next value for schema name sequence name as result im getting sql0104n an unexpected token end of statement was found following schema name sequence name expected tokens may include table expr sqlstate
126871 if theres something cant learn is pivot people are trying to teach me this im reading blogs about this but just cant understand the logic how can turn this query into pivot the query select case datenameweekdayvit dataaberturareal when monday then segunda feira when tuesday then ter feira when wednesday then quarta feira when thursday then quinta feira when friday then sexta feira when saturday then bado when sunday then domingo end as dia da semana case dateparthourvit dataaberturareal when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then end as hora countvit dataaberturareal as count reps abertas from tb vitima where vit dataaberturareal between and group by datenameweekdayvit dataaberturarealdateparthourvit dataaberturareal order by this is the result and this is what want
126969 ran the following query select session idcase transaction isolation level when then unspecified when then readuncommitted when then readcommitted when then repeatable when then serializable when then snapshot end as transaction isolation level from sys dm exec sessions where transaction isolation level and then dbcc inputbuffer157 where was one of the prior session ids to see the statement of one of the results from query no it showed the following id uniqueidentifier select from ps with nolock where id id how can the statement using with nolock run with isolation level serializable is there anything overriding the with nolock
126996 have restored the northwind databasehttps northwinddatabase codeplex com from backup file using sql server trying to execute the following query results in error invalid column name products product is an alias select orderid countproductid products from northwnd dbo orderdetails group by orderid having products what is the problem
127083 am still new to query optimisation and have stored procedure which uses cursor to go through each row within the table and performs the following operations calculate the time difference between each row calculate distance between each row if distance and timedifference minutes then add to temp table tried converting this cursor to while loop but performance decreased so need help converting this into set based approach instead of procedural based approach so the cursor performs this logic read current row into cursor variables fetch next from crassetignitiononoff into current ivehiclemonitoringid current iassetid current dtutcdatetime current sptgeolocationpoint current flatitude current flongitude current fangle current fspeedkph current signitionstatus current eeventcode current seventcode if current iassetid prev iassetid begin calculate time difference from previous point declare diffinseconds int set diffinseconds datediffsecond prev dtutcdatetime current dtutcdatetime declare diffinminutes int set diffinminutes diffinseconds calcualte the distance from previous position declare tempdistance float select tempdistance current sptgeolocaitonpoint stdistance prev sptgeolocaitonpoint check if distance travelled less than and time difference between points greater than user selected idle minutes iidleminutes and prev ignition status on if diffinseconds iidleminutes and tempdistance and prev signitionstatus on begin declare stime varchar30 select stime dbo xpt converttimetoddhhmmss diffinsecondss insert into tblexcessiveidletime assetid previousdate currentdate timestring timeinseconds values current iassetid prev dtutcdatetime current dtutcdatetime stime diffinseconds end end set previous values end of loop set prev ivehiclemonitoringid current ivehiclemonitoringid set prev iassetid current iassetid set prev dtutcdatetime current dtutcdatetime set prev sptgeolocationpoint current sptgeolocationpoint set prev flatitude current flatitude set prev flongitude current flongitude set prev fangle current fangle set prev fspeedkph current fspeedkph set prev signitionstatus current signitionstatus set prev eeventcode current eeventcode set prev seventcode current seventcode end now this takes minutes to execute in some cases so tried converting it to while loop http www sqlbook com sql avoiding using sql cursors aspx which was not good idea as its performance with number of logical reads was times that of the cursor and it took longer to process while rowcount numberrecords begin check for first row if rowcount begin set first row as previous select previous iassetid iassetid previous sptgeolocaitonpoint sptgeolocaitonpoint previous dtutcdatetime dtutcdatetime previous signitionstatus signitionstatus from tblvehiclemonitoringlog where rowid rowcount end else begin select current row select current iassetid iassetid current sptgeolocaitonpoint sptgeolocaitonpoint current dtutcdatetime dtutcdatetime current signitionstatus signitionstatus from tblvehiclemonitoringlog where rowid rowcount implement report logic if current iassetid previous iassetid begin calculate time difference from previous point declare diffinseconds int set diffinseconds datediffsecond previous dtutcdatetime current dtutcdatetime declare diffinminutes int set diffinminutes diffinseconds calcualte the distance from previous position declare tempdistance float select tempdistance current sptgeolocaitonpoint stdistance previous sptgeolocaitonpoint check if distance travelled less than and time difference between points greater than user selected idle minutes iidleminutes and prev ignition status on if diffinseconds iidleminutes and tempdistance and previous signitionstatus on begin declare stime varchar30 select stime dbo xpt converttimetoddhhmmss diffinsecondss insert into tblexcessiveidletime iassetid dtignitionon dtnextperiodic stime itimedurationinseconds values current iassetid dateaddhour fgmtoffset previous dtutcdatetime dateaddhour fgmtoffset current dtutcdatetime stime diffinseconds end end set previous values end of loop set previous iassetid current iassetid set previous sptgeolocaitonpoint current sptgeolocaitonpoint set previous dtutcdatetime current dtutcdatetime set previous signitionstatus current signitionstatus end increment row number set rowcount rowcount end end of while loop so after looking online again found we can calculate time difference between two rows https stackoverflow com questions calculate time difference between two rows this is what the raw data looks like need to calculate the time difference and the distance between each row where current iassetid previous iassetid previous sdigitalinputvalue this is the query came up with with rows as select row number over order by dtutcdatetime as rn from vehiclemonitoringlog where dtutcdatetime getdate order by iassetid dtutcdatetime select mc ivehiclemonitoringid as currentid mp ivehiclemonitoringid as previousid mc iassetid as currentasset mp iassetid as previousasset mc dtutcdatetime as currenttime mp dtutcdatetime as previoustime datediffsecond mc dtutcdatetime mp dtutcdatetime as datediffseconds from rows mc join rows mp on mc rn mp rn edit my query which is working now please let me know if you see any performance issues with this select dt currentasset dt distance dt datediffseconds dt currentignition dt previousignition ta sreference ta scategoryname ta ssitename dbo xpt converttimetoddhhmmssdatediffsecondss from select ivehiclemonitoringid as currentid leadivehiclemonitoringid over partition by iassetid order by dtutcdatetime as previousid iassetid as currentasset leadiassetid over partition by iassetid order by dtutcdatetime as previousasset sdigitalinputvalue as currentignition leadsdigitalinputvalue over partition by iassetid order by dtutcdatetime as previousignition dtutcdatetime as currenttime leaddtutcdatetime over partition by iassetid order by dtutcdatetime as previoustime datediffsecond dtutcdatetime leaddtutcdatetime over partition by iassetid order by dtutcdatetime as datediffseconds sptgeolocaitonpoint stdistanceleadsptgeolocaitonpoint over partition by iassetid order by dtutcdatetime as distance from vehiclemonitoringlog where dtutcdatetime utcstartdate and dtutcdatetime utcenddate as dt inner join tblassets ta on ta iassetid dt currentasset where currentignition and distance and datediffseconds iidleminutes
127158 very much newbie on db work so appreciate your patience with basic question im running sql server on my local machine and have small table and basic client application to test different approaches with im getting what appears to be table lock during both insert into and update statements the client is an asp net application with the following code oledbconnection cn new oledbconnection provider sqlncli11 server localhost sqlexpress database my db user id my uid password my pwd cn open oledbtransaction tn cn begintransaction oledbcommand cmd new oledbcommand insert into layoutsv2 layouts name layouts enabled layouts data values name data cn tn cmd executenonquery cmd commandtext select scope identity int newkey decimal toint32decimalcmd executescalar console writeline created index newkey thread sleep15000 tn commit tn cn begintransaction cmd commandtext udpate layoutsv2 set layouts enabled where layouts key newkey cmd transaction tn cmd executenonquery console writeline updated row thread sleep15000 tn rollback cn close run this code then from the management studio run select from layoutsv2 during both cases when the client thread is paused prior to commit rollback the select query hangs until the commit rollback occurs the table has field layouts key assigned as the primary key in the properties window it shows that it is unique and clustered with page locks and row locks both allowed the lock escalation setting for the table is disable ive tried both the other available settings of table and auto with no changes ive tried select with nolock and that returns result immediately but as is well cautioned here and other places its not what should be doing ive tried putting the rowlock hint on both the insert and update statements but nothing has changed the behavior im looking for is this prior to commit of an insert queries from other threads read all rows except the one thats being inserted prior to commit of an update queries from other threads read the starting version of the row being updateed is there any way can do this if need to provide other information to clarify my use case please let me know thanks
127221 have two queries which are doing basically the same thing but have different grouping the first query query is used to populate chart and the second to populate table query select key id sumsalary sumbonus created at from table where emp id group by key id created at query select key id sumsalary sumbonus count over as full count from table where emp id group by key id have created function that returns those two queries in json format chart table the problem is that need to query the same table twice because of my grouping is there any way of dealing with this situations
127303 im calculating median as declare temp table id int select select top id from select top percent id from temp where id is not null order by id as order by id desc select top id from select top percent id from temp where id is not null order by id desc as order by id asc above query want to use but in my case there are so many columns for those want to calculate median but think it would be bad to repeat above block of code for each column so im trying to define separate function which would accept column values process and return median should have to define table value funtion for that or there is another optimized way to do so this question is related to the following questions how to write function in sql which accept table as input and return result back as table getting error must declare the scalar variable in sql function even though it is declared why unable to execute table valued function https dba stackexchange com questions how to use udf function in insert into select from clause calculating multiple medians
127316 am working on project of classified ads site have main categories and their items are like vehicles have cars scooters bikes etc mobiles phones have mobiles tablets accessories now should use few tables and use json or should create separate tables for each item tables currently have decided tables to save myself from relations and joins and heavy coding which way is correct or suggest another one what are the pros and cons have search lot but didnt got anything that answers my question
127405 was under the impression that if were to sum the datalength of all fields for all records in table that would get the total size of the table am mistaken select sumdatalengthfield1 sumdatalengthfield2 sumdatalengthfield3 totalsizeinbytes from sometable where and are true used this query below that got from online to get table sizes clustered indexes only so it doesnt include nc indexes to get the size of particular table in my database for billing purposes we charge our departments by the amount of space they use need to figure out how much space each department used in this table have query that identifies each group within the table just need to figure out how much space each group is taking up space per row may swing wildly due to varcharmax fields in the table so cant just take an average size the ratio of rows for department when use the datalength approach described above only get of the total space used in the query below thoughts select name as schemaname name as tablename rows as rowcounts suma total pages as totalspacemb suma used pages as usedspacemb suma total pages suma used pages as unusedspacemb from sys tables with nolock inner join sys schemas with nolock on schema id schema id inner join sys indexes with nolock on object id object id inner join sys partitions with nolock on object id object id and index id index id inner join sys allocation units with nolock on partition id container id where is ms shipped and object id and type desc clustered group by name name rows order by totalspacemb desc it has been suggested that create filtered index for each department or partition the table so can directly query the space used per index filtered indexes could be created programmatically and dropped again during maintenance window or when need to perform the periodic billing instead of using the space all the time partitions would be better in this respect like that suggestion and would typically do that but to be honest use the each dept as an example to explain why need this but to be honest that is not really why due to confidentiality reasons cant explain the exact reason why need this data but its analogous to different departments regarding the nonclustered indexes on this table if can get the sizes of the nc indexes that would be great however the nc indexes account for the size of the clustered index so we are ok not including those however how would we include the nc indexes anyway cant even get an accurate size for the clustered index
127537 am trying to install mysql in serving having centos linux release take look to the process installation sudo yum install mysql server output dependencies resolved package arch version repository size removing mysql community client x86 el7 mysql57 community mysql community server x86 el7 mysql57 community transaction summary ran the mysql damon sudo service mysqld start checking the service ps ef grep mysql mysql usr sbin mysqld daemonize pid file var run mysqld mysqld pid here comes the problem driving me crazy want to set root password for the very first time so did sudo mysql secure installation when password is required just type enter key but the output securing the mysql server deployment enter password for user root error access denied for user root localhost using password no googling the error in of cases the solution is to call mysqld safe skip grant tables command service mysqld stop mysqld safe skip grant tables mysql user root mysql update user set password passwordnew password where user root flush privileges exit but mysqld safe prompts an command not found error also tested with sudo mysqld skip grant tables but it does not do anything will appreciate if you guide me to the right direction in order to set root password thank you in advance
127613 have servers one of which has linked server configured that points to the other ill call this server server has over user databases for various purposes server is running sql which were trying to eliminate server has copies of some of the databases from server and were migrating applications from server bs database copies to server cs when im on server can see connections from server to certain database but dont know how to tell what procedure task or job from server is using that linked server connection to server in order to retire the database on server need to re point server as connections to database on server but in order to do that need to know what procedures tasks or jobs on server are using that connection so they can be updated is there way to see the dependencies on linked server without disabling the linked server to see what starts failing
127617 want to sum few values using case statements my problem is compiler complains if do not add group by to the end of the query which then that negates my distinct hah for example in my sample below this returns entries for red12 even though entry does not fit any of the criteria above but the group by messes me up how can run this query and only return applicable results what the case statements show create table bobsled event varchar100 time decimal184 employeeid varchar25 name varchar500 insert into bobsled values walk 32red12 red arrow eat red12 red arrow run 13pink01 pink pig walk bl81 blue fire sleep gr99 green pony select distinct employeeid case when event walk then sum time else end as walktime case when event run then sum time else end as runtime case when event sleep then sum time else end as sleeptime from bobsled only executes sucesfully with this group by employeeid event
127639 have fairly simple query select top dc document id dc copies dc requestor dc id cj file number from document queue dc join correspondence journal cj on dc document id cj document id where dc queue date getdate and dc print location order by cj file number that is giving me horrible performance like never bothered to wait for it to finish the query plan looks like this however if remove the top get plan that looks like this and it runs in seconds correct pk indexing below the fact that the top changed the query plan doesnt surprise me im just bit surprised that it makes it so much worse note ive read the results from this post and understand the concept of row goal etc what im curious about is how can go about changing the query so that it uses the better plan currently im dumping the data into temp table then pulling the first row off of it im wondering if there is better method edit for people reading this after the fact here are few extra pieces of information document queue pk ci is id and it has 5k rows correspondence journal pk ci is file number correspondence id and it has mil rows when started there were no other indexes ended up with one on correspondence journal document id file number
127737 we have quite number of sql servers that need to be upgraded from version to r2 work is planned before the middle of the year as microsoft is ending its support for the same the sql servers are all sp3 and sp4 running on windows server whose support has already ended but we have exception of extension for one more year but where required we might go with server os upgrade as well these servers include replication transactional log shipping reporting services and an integration server running ssis packages my question here is not how rather would like to know the risks involved or any pre checks that can be done before planning this upgrade also will an in place upgrade be better plan than side by side for this migration upgrade
128314 made this query to list tables its columns data type and etc select name as table name column ty name data type max length as max lenght is nullable as null is identity as identity from sys tables join sys columns on object id object id join sys types ty on system type id ty system type id order by have questions ques how can add column is primary key to it could not find table that help me took look at sys indexes sys foreign keys information schema table constraints ques in sys columns int values have the max lenght as and some other fields too what is this 4kb per data datetime and decimal too thanks
128348 in stored procedure am finding that if it successfully inserts record it returns if it fails on primary key violation which is caught and ignored it returns where is the coming from here is the stored procedure alter procedure dbo psetuseritemlike userid int itemid int as begin set nocount on for exception handling declare errormessage nvarcharmax declare errorseverity int declare errorstate int begin try insert dbo useritemlikes userid itemid values userid itemid end try begin catch if error number ignore primary key violation select errormessage error message line casterror line as nvarchar5 errorseverity error severity errorstate error state raiserror errormessage errorseverity errorstate end catch end note am not actually using the return value just noticed it as ssms execute procedure automatically captures the return value and puts it in variable and couldnt figure out why this was happening
128424 we happen to be using sql server standard edition also happen to use ola hallengrens scripts to provide an easy more flexible framework for doing backups and maintenance this question isnt so much about olas scripts as they are about best practice realize the ultimate answer is it depends on your companys requirements but am trying to seek the communitys advice on how best to fulfill what understand of our companys requirements wish to set up transaction log backups for every minutes this way we hopefully lose no more than minutes of data should set up one job that uses all databases or is it better to set up one job for each database and kick them all off in parallel ask because have the feeling based on how see olas script functioning that the backups are kicked off in serial the downside of serial would be that each successive backup waits until the other completes this could potentially increase the amount of time between backups ie greater than minutes plus my concern would be that failure in one backup stops the others from happening and wouldnt want that to be the case would want the others to continue backing up so is it true that olas scripts execute in serial and also failure stops successive backups and is it better to have job for each database or single job that does all my inclination is toward separate jobs but wish to understand what sql server dbas in general tend to do
128433 created table in db that already exists in another db it was initially populated with the old db data the tables pk had to receive the values that already exist on those records so it couldnt be autoincrement now need the new table to have its pk as autoincrement but how can do that after the pk already exists and have data
128474 user mrdenny wrote that after you set sql server maximum memory now this isnt going to limit all aspects of sql server to that amount of memory this only controls the buffer pool and the execution plan cache things like clr full text the actual memory used by the sql server exe files sql agent extended stored procedures etc arent controlled by this setting my problem is that my server is going over the set gb memory and if things like clr full text and server exe files taking additional memory how do configure sql server so it does not go over the set max limit was told there was way to do it but cannot find solution after searching on google
128535 ive been reading msdn about try catch and xact state it has the following example that uses xact state in the catch block of try catch construct to determine whether to commit or roll back transaction use adventureworks2012 go set xact abort on will render the transaction uncommittable when the constraint violation occurs set xact abort on begin try begin transaction foreign key constraint exists on this table this statement will generate constraint violation error delete from production product where productid if the delete operation succeeds commit the transaction the catch block will not execute commit transaction end try begin catch test xact state for or if the transaction is committable if the transaction is uncommittable and should be rolled back xact state means there is no transaction and commit or rollback operation would generate an error test whether the transaction is uncommittable if xact state begin print the transaction is in an uncommittable state rolling back transaction rollback transaction end test whether the transaction is active and valid if xact state begin print the transaction is committable committing transaction commit transaction end end catch go what dont understand is why should care and check what xact state returns please note that the flag xact abort is set to on in the example if there is severe enough error inside the try block the control will pass into catch so if im inside the catch know that transaction has had problem and really the only sensible thing to do in this case is to roll it back isnt it but this example from msdn implies that there can be cases when control is passed into catch and still it makes sense to commit the transaction could somebody provide some practical example when it can happen when it makes sense dont see in what cases the control can be passed inside catch with transaction that can be committed when xact abort is set to on msdn article about set xact abort has an example when some statements inside transaction execute successfully and some fail when xact abort is set to off understand that but with set xact abort on how can it happen that xact state returns inside the catch block initially would have written this code like this use adventureworks2012 go set xact abort on will render the transaction uncommittable when the constraint violation occurs set xact abort on begin try begin transaction foreign key constraint exists on this table this statement will generate constraint violation error delete from production product where productid if the delete operation succeeds commit the transaction the catch block will not execute commit transaction end try begin catch some severe problem with the transaction print rolling back transaction rollback transaction end catch go taking into account an answer by max vernon would write the code like this he showed that it makes sense to check whether there is an active transaction before attempting to rollback still with set xact abort on the catch block can have either doomed transaction or no transaction at all so in any case there is nothing to commit am wrong use adventureworks2012 go set xact abort on will render the transaction uncommittable when the constraint violation occurs set xact abort on begin try begin transaction foreign key constraint exists on this table this statement will generate constraint violation error delete from production product where productid if the delete operation succeeds commit the transaction the catch block will not execute commit transaction end try begin catch some severe problem with the transaction if xact state begin there is still an active transaction that should be rolled back print rolling back transaction rollback transaction end end catch go
128914 am trying to write spec for data warehouse server for our planned data warehouse upgrade as we run virtual servers on vmware hosts we have the ability to add or remove resources as necessary in the past weve incrementally added ram and cpu as required as our demands have increased weve lobbied for more resources primarily disk ram we ask for more they give us as little as possible however recently whenever we talk about resources we are now criticized for not specing the machine right in the first place and am now being told the dev hosts are maxed out there is no more ram available were small local government organisation with regular users of the dw in normal daily use it runs fine we get good mdx query performance and our reports and dashboards are fast users are happy however our etl processes run throughout the night and were starting to see evidence of memory pressure when processing datamarts simultaneously last night ssis failed with warnings about an out of memory error our existing dw server is win r2 with cpus and 16gb of ram running sql std have max server memory set to 12gb leaving 4gb for os and services etc our existing dw has datamarts olap cubes and we are developing more datamart files gb fact rows fact mb etl process olap cube time hours pbi fbi rbi abi ebi planned estimated our new server is planned to be win running sql enterprise it will run sql ssis ssrs ssas storage isnt an issue but im not sure about ram cpu according to the fast track data warehouse reference guide for sql server the minimum should have is 128gb for socket machine which seems bit excessive the hardware and software requirements for installing sql server recommends minimum of 4gb of ram for sql thats quite difference so what is good starting point 32gb 64gb how do justify my starting position spec to it are there any good guides about how to calculate server resources are there any good rules of thumb what are the key ingredient metrics for ram sizing in dw context the volume of data the number of cubes the time it takes to do etl or process cube peak processing load overnight or performance as viewed by end users during the day
129090 our etl flow has long running select into statement thats creating table on the fly and populating it with several hundred million records the statement looks something like select into desttable from srctable for monitoring purposes we would like to get rough idea of the progress of this statement while it is executing approx rowcount written number of bytes or similar we tried the following to no avail is blocked by the select into statement select count from desttable with nolock returns select rows rowmodctr from sysindexes with nolock where id object iddesttable returns select rows from sys partitions where object id object iddesttable furthermore we can see the transaction in sys dm tran active transactions but was not able to find way to get the count of affected rows on given transaction id something similar to rowcount perhaps but with the transaction id as argument understand that on sql server the select into statement is both ddl and dml statement in one and as such the implicit table creation will be locking operation still think there must be some clever way to obtain some kind of progress information while the statement is running
129358 need to return or dependent on previous records example table declare tableproductid int failed bit sampledate date levelcode int insert values the only thing we care about on the last record is the levelcode last record of each productid whether the last record passed failed doesnt matter we then look at all other records for that productid so all records before the last record and if there was failure with the same levelcode as the last record we set islastrunsamelevelaspreviousrun to else productid islastrunsamelevelaspreviousrun if there are no failures for productid the islastrunsamelevelaspreviousrun should return any help or tips are very much appreciate
129504 dug couple of hours regarding my question and didnt get satisfactory answer still have doubt have found the following about clustered index data is stored in the order of the clustered index only one clustered index per table when primary key is created cluster index is automatically created as well got these points but my questions are is cluster index exist in oracle database since read in some blogs oracle does not have concept of clustered index if yes please let me know the sql statement to create cluster index as said above cluster index automatically gets created when primary key is defined on column of table how can check the index type if it is created or not please find my table architecture let me know if anything else is required to get answers for these questions
129522 have table tag with columns id uuid and name text now want to insert new tag into the table but if the tag already exists want to simply get the id of the existing record assumed could just use on conflict do nothing in combination with returning id insert into tag name values foo on conflict do nothing returning id but this returns an empty result set if the tag with the name foo already exists then changed the query to use noop do update clause insert into tag name values foo on conflict name do update set name foo returning id this works as intended but it is somewhat confusing because im just setting the name to the already existing value is this the way to go about this problem or is there simpler approach im missing
129659 came across developer code where sqlcommand prepare see msdn method is extensively used in advance of execution of sql queries and wonder what is the benefit of this sample command prepare command executenonquery command parameters value command executenonquery have played around little bit and traced the execution of the command after calling the prepare method makes sql server execute the following statement declare p1 int set p1 exec sp prepexec p1 outputn id int desc textninsert into dbo testtable id values id id select p1 after that when the parameter gets its value and sqlcommand executenonquery is called the following gets executed on sql server exec sp execute id to me this looks like the statement gets kind of compiled as soon prepare is executed wonder what is the benefit of this does this mean that it is put into the plan cache and can be re used as soon the final query is executed with the desired parameter values figured out and documented it in another question that sqlcommands that are executed with sqlparameters always are wrapped in sp executesql procedure calls this makes sql server able to store and reuse the plans independant of the parameter values regarding this wonder if the prepare method is kind of useless or obsolete or if am missing something here
129694 am having trouble with mysql not null columns it seems my mysql installation is accepting null values for not null columns my mysql version is dotdeb 1debian take this table for instance create table cities id int10 unsigned not null auto increment name varchar255 not null state id int10 unsigned not null primary key id engine innodb auto increment when insert value like this insert into citiesstate id values mysql spills warning but commits the value anyway here is the warning insert into citiesstate id values rows affected warnings field name doesnt have default value sec if remove the unique key unique city in state get the same behaviour also tried creating the name column with default null like so name varchar255 not null default null this spills error that would roughly translate to default value invalid for name tried on different mysql installation debian and have the same behavior now if do this insert into cities name state id values null get the error meaning the column name cannot be empty any help is welcome
129747 have question about joining two tables schema create table dbo dcstring id bigint identity11 not null dcdistributionboxid bigint not null currentmpp decimal null constraint primarykey3 primary key clustered id asc alter table dbo dcstring add constraint fk dcstring dcdistributionbox foreign key dcdistributionboxid references dbo dcdistributionbox id create table dbo stringdata dcstringid bigint not null timestamp datetime not null dccurrent decimal null constraint primarykey4 primary key clustered timestamp desc dcstringid asc the stringdata table has the following storage stats data space mb row count partitioned true partition count usage now want to join the data in the stringdata table with the data from the dcstring table something like declare begin datetime declare end datetime declare dcstringid bigint select dcstring id stringdata timestamp from stringdata right outer join stringdata on stringdata dcstringid dcstring id where stringdata id dcstringid and stringdata timestamp begin and stringdata timestamp end what expect on searched date range where matching data in the stringdata table exist is this id timestamp and on searched date range where no matching data in the stringdata table exist is this id timestamp null question what get on searched date range where no matching data in the stringdata table exist is rows result why simply want always to get all dcstring id whats wrong with my join or did mess up completely update related to the answer of aaron bertrand tried your way already but have to cancel testing the query because after 10min it is still running it looked like this declare begin datetime declare end datetime declare dcstringid bigint select dcstring id stringdata timestamp from stringdata left join dcstring on stringdata dcstringid dcstring id and stringdata timestamp begin and stringdata timestamp end where stringdata id dcstringid
129751 in table that stores events the date and time are two separate columns create table events pk int serial detail text ev date date ev time without time zone if filter for dates ev date between start date date and end date date and ev time between start date time and end date time am missing all events within the two dates that occurred outside the time range of every day thus the start time is only relevant for the start date and the same for the end date anyone with an advice on how to do it efficiently for millions of events
129785 am making small program where users makes posts or write blogs on those posts other users can like or dislike the post as in facebook or upvote or downvote the post as in stackoverflow would like to know good database structure which is commonly used the program works efficiently with that structure have two options first post id head message datepost likes dislikes ab anchdg date in the above way id is the postid in the likes column is the users id who liked or upvoted the post or blog is the id of the users who disliked or downvoted the post or blog second post id head message datepost ab anchdg date likes id postid userid dislikes id postid userid in this way have to create two separate tables for likes dislikes to get posts likes in this way the tables likes dislikes will get heavily filled this might make table heavy processing slow so would like to know which is the better standard way to achieve this task
129827 have built an application as part of my ug thesis that uses mysql database community edition now my professor wants me to provide fault tolerance to the database by parity argued that we wouldnt need that anyway searched and found here that mysql has built in replication engine mechanism to provide fault tolerance right along with many other techniques to provide reliability but what learned from there is that there is master server and some slave servers to provide the fault tolerance using replication now my questions are what if have only one database server does mysql have any fault tolerance for single standalone database servers no master slave formation no cluster etc do need to try to provide any kind of fault tolerance whatsoever for the data stored in mysql database what kind of differences are there in terms of fault tolerance only between community edition and enterprise edition of mysql servers somehow get the feeling that we dont need to do anything for providing fault tolerance to mysql db and it is fine by itself but need some solid info on the matter bounty edit the second question from above again do need to try and provide any kind of fault tolerance whatsoever for the data stored in mysql database what is the sensibleness of trying to provide fault tolerance to mysql database by using parity bit per byte little details about the data in the database the data to be specific is collection of about unicode strings less than mb in size is initialization data that will never change over time the only transaction will be to read the data from the database no update no delete my application requires full text search on those strings and this is the only reason am using mysql since it provides full text search am aware that could avoid mysqls ft search by using something like elasticsearch instead
129854 have varchar column has data like which have imported from csv file to sql table need to convert this column to datatime as alter table track date alter column start time datetime getting error as msg level state line conversion failed when converting date and or time from character string while converting using convert function getting same error select convertdatetimestart time5 from track date msg level state line conversion failed when converting date and or time from character string can you please help how to convert this varchar data to datetime format
130018 is it possible to change the password for login with an expired password using sql server management objects smo programming many times during working hours got this message from production server have sa level privilege in sql server working environment microsoft sql server express windows server r2 vmwareinc vmware virtual platform
130028 have piece of code that performs inserts into highly denormalized tables the tables have numbers of columns ranging from to this is sql server r2 running on windows server each insert consists of inserting to number of tables under the same transaction some inserts are batched by nhibernate but some cannot be but they are all under the same transaction nonetheless when perform inserts for say times by repeatedly calling piece of code that performs the insert get an average of ms the weird bit is when run the test code simultaneously using processes the same exe run from different command prompts under windows server the insertion performance per call gets much better see bursts that go as fast as ms almost x4 faster im measuring the insertion time from the code since the processes know nothing about each other im assuming that this has something to do with sql server but have absolutely no clue why id like to know why this is happening and if there is any configuration that would allow me to get the same performance when the inserts are not that frequent suggestions regarding sql server monitoring methods to understand what is going on at the db level are equally welcome
130141 am trying to write query where have to calculate number of visits for customer by taking care of overlapping days suppose for itemid start date is 23rd and end date is 26th therefore item is between these days we will not add this purchase date to our total count example scenario item id start date end date number of days number of days candidate for visit count output should be visitdays input table create table items custid int itemid int startdate datetime enddate datetime insert into items select union all select union all select union all select union all select union all select have tried so far create table visitstable startdate datetime enddate datetime insert into visitstable select distinct startdate enddate from items items where custid order by startdate asc if exists select top from visitstable begin select isnullsumvisitdays1 from select distinct abc startdate abc enddate datediffdd abc startdate abc enddate visitdays from visitstable abc inner join visitstable bc on bc startdate not between abc startdate and abc enddate visits end drop table items drop table visitstable
130321 according to this post in single replica set you cannot distribute writes they all must go to the primary you can distribute reads to the secondaries already via read preferences as you deem appropriate the driver keeps track of what is primary and what is secondary and routes queries appropriately according to the mongo docs you may also deploy group of mongos instances and use proxy load balancer between the application and the mongos in these deployments you must configure the load balancer for client affinity so that every connection from single client reaches the same mongos so basically it seems like if youve got single replica set of nodes you cant really use proxy load balancer since all writes need to go to the primary and you need client affinity so all reads also need to go to the primary what im thinking though is that it might be possible to have applications connect to load balancer the load balancer would route all requests to the primary not very balanced but whatever until unless the primary went down at which point the load balancer would start routing requests to new primary im not sure if this is possible however since how would the load balancer know which mongo server had been elected the new primary and thus where it should route new requests assuming it was possible this would achieve degree of redundancy in case the primary ever goes down im also hoping it would also have the side effect of avoiding stale writes when network partition occurs since the load balancer and thus all db clients would only ever connect to single primary or is this stupid question
130392 description try to insert million rows into empty table on mssql express here my script set statistics time off drop table t1 create table t1 id int text text go 30s 45s with idnumber as select as number union all select number from id where number insert into t1 select number cast number as varchar cast number as varchar from id optionmaxrecursion million rows rows 120s have to cancel query declare count int set count while count begin set count count insert into t1 values count cast count as varchar cast count as varchar end rows 18s 20s with temp as select row number overorder by object id as tcount from sys all columns sys all columns where object id object id insert into t1 select tcount cast tcount as varchar cast tcount as varchar from temp go declare count int set count while count begin with temp as select maxid as max id from t1 insert into t1 select max id cast max id as varchar cast max id as varchar from t1 temp set count count end 3s 4s have to drop t1 first with ak as select as union all select from where t2 as select row number overorder by as from select as id cast as varchar as cast as varchar as into t1 from t2 question after researching found solutions are there any better solution not using copy data from files
130399 need to pull out the list of stored procedures which are available in my instance used the following sql statement to get the stored procedures in given database select from mydatabase information schema routines where routine type procedure is there is any script to obtain the all stored procedures or to check the database name of the stored procedure by using the stored procedure name
130441 have been using oracle up to now where instance process memory database physical datafiles normally one instance maps to one database inside database there can be many tablespaces system users etc however am not so sure about sql server it seems to me that for sql server one instance multiple databases or one instance multiple schemas in oracle one database contains the system sysaux temp tablespaces and the user tablespace in short both system and user are in the same database in sql server it seems like there is one system database and that can serve multiple user databases
130606 need to determine from program what version of oracle is installed in each of the oracle homes on server as there may not be any databases created in the home yet need to be able to do this outside of the database without connecting to the database also it would be highly preferable to be able to do this from remote program this is from windows program running net if that matters am currently reading remote registry keys using this technique https stackoverflow com questions how to read remote registry keys to find all of the oracle homes according to this method this works fine however have looked around those keys and do not see any information on the exact version release the name of an oracle home itself is of course not reliable indicator and does not have the exact version release for instance basically am looking for way to figure out what the oracle universal installer tells you in the installed products button should clarify all of the servers will be running windows
130659 getting an error report of error report sql error ora no matching unique or primary key for this column list no matching unique or primary key for this column list cause references clause in create alter table statement gives column list for which there is no matching unique or primary key constraint in the referenced table action find the correct column names using the all cons columns catalog view can know why parent table create table studentinfo student id varchar2 primary key full name varchar2 not null contact number number 15not null address varchar2 not null nationality varchar2 not null ic passportno varchar2 not null programme varchar not null email address varchar2 not null references usernamepasswordusername parents number number 15not null fingerprint template clob child table create table bit sep cit4114 fyp student id varchar2 primary key references studentinfostudent id full name varchar2 not null references studentinfofull name nationality varchar2 not null references studentinfonationality fingerprint template clob not null references studentinfofingerprint template varchar2 not null keep nationality and fingerprint as duplicate because it will be time consuming in verifying based on all information stored in studentinfo therefore by breaking into individual class will be easier and faster like for the table studentinfo consist million record in table bit sep cit4114 fyp will only have records keep the nationality column because have more column in my table structure which is visa renewal which calculated based on the value of nationality in the table
131204 need to show the average weight of product over its last production runs im not sure how best to describe it other than an example lets imagine have the following table that lists product by date it was created and the average weight of the product for that day product date weight jan march july july aug june june the end result im looking for is to add column that includes the average weight for the last dates the product was run for so something like this product date weight average weight jan null march null july null july null aug jan1 mar3 july6 july7 aug mar3 july6 july7 aug6 june null june null june null june null june etc the nulls are just there since in this sample you cant calculate the average over the last runs because the data isnt there could anyone point me in the direction need to be looking to do something like this
131310 im moving up to the next level of my mystery query it looks like theres subselect inside of an exists but on the same table think this could probably be simplified with an inner join higher up using postgresql table definitions https gist github com neezer 879f5d3649ca1903c6f3 cardinalities billing pricequote rows billing pricequotestatus rows billing lineitem rows heres the original query without modifications suggested for the subquery inside exists select quote id acct id as account id sumi delta amount as amt from billing lineitem inner join billing pricequote pq on quote id pq id where pq date applied at time zone pst between 02t00 timestamp and 03t22 timestamptz and exists select s1 quote id from billing pricequotestatus s1 inner join select distinct on quote id quote id maxcreated at as max created at from billing pricequotestatus where quote id quote id group by quote id created at order by quote id created at desc as s2 on s1 quote id s2 quote id and s1 created at s2 max created at where s1 name in adjustmentpaymentbillable group by quote id acct id the part noticed looking weird is the select on billing pricequotestatus and then another subselect on the same table inside the inner join tried changing this with the modification from my other so post select quote id acct id as account id sumi delta amount as amt from billing lineitem inner join billing pricequote pq on quote id pq id where pq date applied at time zone pst between 02t00 timestamp and 03t22 timestamptz and exists select quote id maxcreated at as max created at from billing pricequotestatus where quote id quote id and name in adjustmentpaymentbillable group by quote id group by quote id acct id that cut my execution time in half seconds to seconds but yielded slightly different results the original query returned rows but my new query returns rows its not immediately clear to me why my modification didnt produce equivalent output which it needs to explain analyze for both queries explain analysis for original query depesz com would really appreciate any help guidance on this tried updating ypercube answer with lateral join and the performance seems about the same each has an equal number of wins by less than second select quote id acct id as account id sumi delta amount as amt from billing lineitem inner join billing pricequote pq on quote id pq id left join lateral select name from billing pricequotestatus where quote id quote id order by created at desc limit pqs on true where pq date applied at time zone pst between 02t00 timestamp and 03t22 timestamptz and pqs name in adjustment payment billable group by quote id acct id explain analysis any other suggestions to get this below seconds
131350 am running correlated subquery to find out the listing of vendors by vendor name that are in different cities states we want to know the vendors that do not have common city and state with other vendors it seemed like self join was the thing to do only hints if possible please the vendors table is vendorsvendorid vendorcity vendorstate vendorname this is what have select vendorname vendorcity vendorstate from vendors as v1 where vendorcity vendorstate not in select vendorcity vendorstate from vendors as v2 where v2 vendorid v1 vendorid this is the error message get msg level state line an expression of non boolean type specified in context where condition is expected near dont see why there is reference to boolean types since this is not an exists or other related query
131400 proposed schema first and foremost here is an example of my proposed schema to reference throughout my post clothes clothesid pk int not null name varchar50 not null color varchar50 not null price decimal52 not null brandid int not null brand clothesid fk pk int not null viewingurl varchar50 not null someotherbrand1specificattr varchar50 not null brand clothesid fk pk int not null photourl varchar50 not null someotherbrand2specificattr varchar50 not null brand clothesid fk pk int not null someotherbrandxspecificattr varchar50 not null problem statement have clothes table which has columns like name color price brandid and so on to describe the attributes for particular item of clothing heres my problem different brands of clothing require differing information what is the best practice for dealing with problem like this note that for my purposes it is necessary to find brand specific information starting from clothes entry this is because first display the information from clothes entry to the user after which must use its brand specific information to purchase the item in summary there has to be directional relationship between clothes from and the brand tables proposed current solution to cope with this have thought of the following design scheme the clothes table will have brand column which may have id values ranging from to where particular id corresponds to brand specific table for example id value will correspond to table brand which might have url column id will correspond to brand which might have supplier column etc thus to associate particular clothes entry with its brand specific information imagine the logic at the application level will look something like this clothesid some value brand query select brand from clothes where id clothesid if brand get brand attributes for given clothesid else if brand get brand attributes for given clothesid etc other comments thoughts im attempting to normalize my entire database in bcnf and although this is what came up with the resulting application code makes me feel very anxious there is no way to enforce relations except at the application level and thus the design feels very hacky and anticipate very error prone research made sure to look through previous entries before making post heres post with near identical problem that managed to find made this post anyway because it seems like the only answer provided does not have sql or design based solution it mentions oop inheritance and interfaces im also kind of novice when it comes to database design and so id appreciate any insights it appears there are more helpful responses on stack overflow here and here aaaand here key concept being class table inheritance have referred to the solutions there and suggest others finding my question do so as well despite the above provided links am still on the lookout for responses here and would appreciate any solutions provided am using postgresql
131634 id just like to know if there is any way to query when the ag group failed over eg this is the primary replica now but im pretty sure it was the secondary yesterday how can find when the failover took place is there something specific in the logs should be looking for or is there tsql script to use
131759 is there sql server implementation of the longest common substring problem solution that checks with all rows of column in sql server have seen solutions that take two strings as input but no sql server solution that looks at all rows of column in table did try few things but to be honest think solution goes over my head at the moment so any suggestions are welcome there is no real world problem here im just looking at programming problems and how they could be solved with sql server
131801 have result set of rows but this are for invoice need only the top of the last invoices found something like similar how to select top records from each category sample data invnr detailline what hope to get is for example select distinct top invnr detailline from tbl invoice with this result invnr detailline update so need the last or first in the example above invoices they have created but each invoice can have amount of detail lines and want all their detail lines of these last in the results
132029 have the following tables create table users id int primary key already exists with data create table message how do alter messages table such that new column called sender is added to it where sender is foreign key referencing the users table this didnt work alter table message add foreign key sender references users error column sender referenced in foreign key constraint does not exist does this statement not create the column as well
132155 am completely new to pl sql have written the following pl sql script but it doesnt execute and gives compilation error set serveroutput on size if exists select from my table begin dbms output put linehas rows end else begin dbms output put lineno rows end can anyone tell me what is wrong with this how can do this
132170 previous question sql server changes execution plan we are using sql server developer edition sql server changes execution plan on identical query and the same database and sql server checked several times if connect with management studio from my dev computer with ad account it takes seconds to finish the query most of the time if go with remote connection to server and execute query in management studio it takes seconds to finish on my colleagues machine it takes second with management studio and if he connects with mvc application ado net things we did restart sql server dbcc freeproccache sp updatestats try with different users ad and sql users slow execution plan fast execution plan query set ansi nulls on go set ansi padding on go set ansi warnings on go set arithabort on go set concat null yields null on go set numeric roundabort off go set quoted identifier on go declare ccode varchar500 declare pagesize int declare pagenumber int set ccode skd set pagesize set pagenumber select distinct classifications abbrev text classifications title text classifications cfn uid from klasje dbo classifications inner join klasje dbo cfn versions on classifications cfn uid cfn versions cfn uid inner join klasje dbo version category xrefs on cfn versions cvn uid version category xrefs cvn uid left outer join klasje dbo veljavnost on cfn versions life cycle code veljavnost rv low value where category code like ccode or descriptor text like ccode or definition text like ccode order by abbrev text offset pagesize pagenumber rows fetch next pagesize rows only both execution plans on gist select options returns in in both cases slow and fast
132293 we have several databases in which large number of tables are created and dropped from what we can tell sql server does not conduct any internal maintenance on the system base tables meaning that they can become very fragmented over time and bloated in size this puts unnecessary pressure on the buffer pool and also negatively impacts the performance of operations such as computing the size of all tables in database does anyone have suggestions for minimizing fragmentation on these core internal tables one obvious solution could to avoid creating so many tables or to create all transient tables in tempdb but for the purpose of this question lets say that the application does not have that flexibility edit further research shows this unanswered question which looks closely related and indicates that some form of manual maintenance via alter index reorganize may be an option initial research metadata about these tables can be viewed in sys dm db partition stats the system base table that contains one row for every column in the system select row count reserved page count row count as bytes per row reserved page count as space mb from sys dm db partition stats where object id object idsys syscolpars and index id row count bytes per row space mb however sys dm db index physical stats does not appear to support viewing the fragmentation of these tables no fragmentation data is returned by sys dm db index physical stats select from sys dm db index physical stats db id object idsys syscolpars null null detailed ola hallengrens scripts also contain parameter to consider defragmentation for is ms shipped objects but the procedure silently ignores system base tables even with this parameter enabled ola clarified that this is the expected behavior only user tables not system tables that are ms shipped msdb dbo backupset are considered returns code successful but does not do any work for system base tables instead of the expected commands to update statistics and reorganize indexes no commands are generated the script seems to assume the target tables will appear in sys tables but this does not appear to be valid assumption for system tables like sys sysrowsets or sys syscolpars declare result int exec result indexoptimize databases test fragmentationlow index reorganize fragmentationmedium index reorganize fragmentationhigh index reorganize pagecountlevel updatestatistics all indexes test sys sysrowsets proc works properly if targeting non system table instead indexes test dbo numbers msshippedobjects execute print result additional requested info used an adaptation of aarons query below the inspect system table buffer pool usage and this found that there are tens of gb of system tables in the buffer pool for just one database with of that space being free space in some cases compute buffer pool usage by system table select object namep object id countb page id pages sumb free space in bytes free pages from sys dm os buffer descriptors join sys allocation units on allocation unit id allocation unit id join sys partitions on partition id container id and object id loose proxy for system tables where database id db id group by object id order by pages desc
132437 have what is to me an interesting question on sargability in this case its about using predicate on the difference between two date columns heres the setup use tempdb set nocount on if object idtempdb sargme is not null begin drop table sargme end select top identity bigint as id castdateaddday severity getdate as date as datecol1 castdateaddday severity getdate as date as datecol2 into sargme from sys messages as alter table sargme add constraint pk whatever primary key clustered id create nonclustered index ix dates on sargme datecol1 datecol2 what ill see pretty frequently is something like this definitely not sargable select datediffday datecol1 datecol2 from sargme as where datediffday datecol1 datecol2 which definitely isnt sargable it results in an index scan reads all rows no good estimated rows stink youd never put this in production it would be nice if we could materialize ctes because that would help us make this well more sargable er technically speaking but no we get the same execution plan as up top would be nice if it were sargable with as select datediffday datecol1 datecol2 as ddif from sargme as select from where ddif and of course since we are not using constants this code changes nothing and is not even half sargable no fun same execution plan not even half sargable select datediffday datecol1 datecol2 from sargme as where datecol2 dateaddday datecol1 if youre feeling lucky and youre obeying all the ansi set options in your connection strings you could add computed column and search on it alter table sargme add ddiff as datediffday datecol1 datecol2 persisted create nonclustered index ix dates2 on sargme ddiff datecol1 datecol2 select id datecol1 datecol2 from sargme as where ddiff this will get you an index seek with three queries the odd man out is where we add days to datecol1 the query with datediff in the where clause the cte and the final query with predicate on the computed column all give you much nicer plan with much nicer estimates and all that which brings me to the question in single query is there sargable way to perform this search no temp tables no table variables no altering the table structure and no views im fine with self joins ctes subqueries or multiple passes over the data can work with any version of sql server avoiding the computed column is an artificial limitation because im more interested in query solution than anything else
132673 need to return partial result as simple select from stored procedure before it is finished is it possible to do that if yes how to do that if not any workaround edit have several parts of the procedure in the first part calculate several string use them later in the procedure to make addtional operations the problem is that string is needed by the caller as soon as possible so need to calculate that string and pass it back somehow from select for example and then continue to work the caller gets its valuable string much more quickly caller is web service
132689 am working on optimizing some queries for the query below set statistics io on declare orderstartdate datetime2 feb declare orderenddate datetime2 feb select strbxorderno sintorderstatusid sintorderchannelid sintordertypeid sdtmordcreated sintmarketid strorderkey stroffercode strcurrencycode decbcshipfullprice decbcshipfinal decbcshiptax decbctotalamount decwrittentotalamount decbcwrittentotalamount decbcshipofferdisc decbcshipoverride dectotalamount decshiptax decshipfinal decshipoverride decshipofferdisc decshipfullprice lngaccountparticipantid convertdate sdtmordcreated as ordercreateddateconverted from tablebackups dbo tblborder where sdtmordcreated orderstartdate and sdtmordcreated orderenddate and exists select from tablebackups dbo tblborderitem oi where oi strbxorderno strbxorderno and oi deccatitemprice option recompile have created the following filtered index table dbo tblborderitem create nonclustered index ix tblborderitem deccatitemprice incl on dbo tblborderitem strbxorderno asc sintorderseqno asc deccatitemprice include blnchargeshipping decbccatitemprice decbccostprice decbcfinalprice decbcofferdiscount decbcoverridediscount decbctaxamount deccostprice decfinalprice decofferdiscount decoverridediscount dectaxamount decwasprice dtmorditemcreated sintorderitemstatusid sintorderitemtype sintquantity stritemno where deccatitemprice with drop existing on fillfactor this index is not used only for this query in particular there are other queries that use this same index therefore the included columns for this query in particular just want to check exists if an order has any item where deccatitemprice sql server is doing an index scan as you can see in the pictures below statistics have just been updated the item table has rows in test please note dont select any columns from the items table this item table has in live would like to avoid scan there questions why is sql server not doing an index seek are there other factors things should consider in order to improve this query rows affected table tblborder scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table tblborderitem scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected this is the definition and indexes on table tblborderitem if object id dbo tblborderitem is not null drop table dbo tblborderitem go create table dbo tblborderitem strbxorderno varchar20 not null sintorderseqno smallint not null sintorderitemstatusid smallint not null sintnamestructureid smallint not null stritemno varchar20 not null sintquantity smallint not null strcurrencycode varchar3 not null deccostprice decimal184 not null deccatitemprice decimal182 not null decofferdiscount decimal182 not null decoverridediscount decimal182 not null decfinalprice decimal182 not null dectaxamount decimal182 not null strbccurrencycode varchar3 not null decbccostprice decimal184 not null decbccatitemprice decimal184 not null decbcofferdiscount decimal184 not null decbcoverridediscount decimal184 not null decbcfinalprice decimal184 not null decbctaxamount decimal184 not null dtmorditemcreated datetime not null blnchargeshipping bit not null lngtimeoforderqtyonhand int null sdtmtimeoforderduedate smalldatetime null lngprodsetseqno int null lngprodrelationid int null lngprodrelationmemberid int null decwasprice decimal182 null sintorderitemtype smallint null tsrowversion timestamp null sdtmorderitemstatusupdated smalldatetime null constraint pk tblborderitem primary key clustered strbxorderno asc sintorderseqno asc with fillfactor go create nonclustered index ix tblborderitem dtmorditemcreated on dbo tblborderitem dtmorditemcreated asc with fillfactor create nonclustered index ix tblborderitem sintorderitemstatusid on dbo tblborderitem sintorderitemstatusid asc include sdtmorderitemstatusupdated sintorderseqno strbxorderno stritemno with fillfactor create nonclustered index ix tblborderitem sintorderitemstatusid decfinalprice sdtmorderitemstatusupdated include strbxorderno on dbo tblborderitem sintorderitemstatusid asc decfinalprice asc sdtmorderitemstatusupdated asc include strbxorderno with fillfactor create nonclustered index ix tblborderitem strbxorderno on dbo tblborderitem strbxorderno asc with fillfactor create nonclustered index ix tblborderitem stritemno on dbo tblborderitem stritemno asc with fillfactor create nonclustered index ix tblborderitem deccatitemprice incl on dbo tblborderitem strbxorderno asc sintorderseqno asc deccatitemprice asc include blnchargeshipping decbccatitemprice decbccostprice decbcfinalprice decbcofferdiscount decbcoverridediscount decbctaxamount deccostprice decfinalprice decofferdiscount decoverridediscount dectaxamount decwasprice dtmorditemcreated sintorderitemstatusid sintorderitemtype sintquantity stritemno where deccatitemprice with fillfactor this is the definition and indexes on table tblborder if object id dbo tblborder is not null drop table dbo tblborder go create table dbo tblborder strbxorderno varchar20 not null uidorderuniqueid uniqueidentifier not null sintorderstatusid smallint not null sintorderchannelid smallint not null sintordertypeid smallint not null blnisbasket bit not null sdtmordcreated smalldatetime not null sintmarketid smallint not null strorderkey varchar20 not null stroffercode varchar20 not null lngshippedtoparticipantid int not null lngorderedbyparticipantid int not null lngshiptoaddressid int not null lngaccountaddressid int not null lngaccountparticipantid int not null lngorderedbyaddressid int not null lngordertakenby int not null strcurrencycode varchar3 not null decshipfullprice decimal182 not null decshipofferdisc decimal182 not null decshipoverride decimal182 not null decshipfinal decimal182 not null decshiptax decimal182 not null strbccurrencycode varchar3 not null decbcshipfullprice decimal184 not null decbcshipofferdisc decimal184 not null decbcshipoverride decimal184 not null decbcshipfinal decimal184 not null decbcshiptax decimal184 not null dectotalamount decimal182 not null decbctotalamount decimal184 not null decwrittentotalamount decimal182 null decbcwrittentotalamount decimal184 null blnproratashipping bit not null blnchargewithfirstshipment bit not null sintshippingservicelevelid smallint not null sintshippingmethodid smallint not null sdtmdonotshipuntil smalldatetime null blnholduntilcomplete bit not null tsrowversion timestamp null constraint pk tblborder primary key clustered strbxorderno asc with fillfactor go create nonclustered index ix tblborder lngaccountaddressid on dbo tblborder lngaccountaddressid asc sintorderstatusid asc with fillfactor create nonclustered index ix tblborder lngaccountparticipantid on dbo tblborder lngaccountparticipantid asc with fillfactor create nonclustered index ix tblborder lngorderedbyaddressid on dbo tblborder lngorderedbyaddressid asc sintorderstatusid asc with fillfactor create nonclustered index ix tblborder lngorderedbyparticipantid on dbo tblborder lngorderedbyparticipantid asc with fillfactor create nonclustered index ix tblborder lngshippedtoparticipantid on dbo tblborder lngshippedtoparticipantid asc with fillfactor create nonclustered index ix tblborder lngshiptoaddressid on dbo tblborder lngshiptoaddressid asc sintorderstatusid asc with fillfactor create nonclustered index ix tblborder sdtmordcreated sintmarketid include strbxorderno on dbo tblborder sdtmordcreated asc sintmarketid asc include strbxorderno with fillfactor create nonclustered index ix tblborder sdtmordcreated incl on dbo tblborder sdtmordcreated asc include decbcshipfinal decbcshipfullprice decbcshipofferdisc decbcshipoverride decbcshiptax decbctotalamount decbcwrittentotalamount decshipfinal decshipfullprice decshipofferdisc decshipoverride decshiptax dectotalamount decwrittentotalamount lngaccountparticipantid lngorderedbyparticipantid sintmarketid sintorderchannelid sintorderstatusid sintordertypeid strbxorderno strcurrencycode stroffercode strorderkey with fillfactor create nonclustered index ix tblborder sintmarketid sdtmordcreated on dbo tblborder sintmarketid asc sdtmordcreated asc include sintorderchannelid strbxorderno with fillfactor create nonclustered index ix tblborder sintorderchannelid sdtmordcreated incl on dbo tblborder sintorderchannelid asc sdtmordcreated asc include decbcshipfinal decbcshipfullprice decbcshiptax decshipfinal decshipfullprice decshiptax lngaccountparticipantid sintmarketid sintordertypeid strbxorderno strcurrencycode strorderkey with fillfactor create nonclustered index ix tblborder strbxorderno sdtmordcreated incl on dbo tblborder strbxorderno asc sdtmordcreated asc include sintorderchannelid sintordertypeid sintmarketid strorderkey lngaccountparticipantid strcurrencycode decshipfullprice decshipfinal decshiptax decbcshipfullprice decbcshipfinal decbcshiptax conclusion applied my index on the live system and updated my stored procedure to use smalldatetime in order to match the data types in the database for the columns involved after that when looking at the query plan see the picture below it was exactly how wanted it to be think the query optimizer in this case did good work to get the best query plan on both environments and am glad did not add any query hints learned with the answers posted thanks to max vernon paul white and daniel hutmacher for their answers
132851 our production environment just froze this morning for while when altering table adding column actually offending sql alter table cliente add column topicos character varying20 login into our system requires select from that very same table so no one could login during the alter table we actually had to kill the process to allow the system resume normal operations table structure create table cliente rut character varying30 not null nombre character varying150 not null razon social character varying150 not null direccion character varying200 not null comuna character varying100 not null ciudad character varying100 not null codigo pais character varying3 not null activo boolean default true id serial not null stock boolean default false vigente boolean default true clase integer default plan integer default plantilla character varying15 default waypoint character varying facturable integer default toolkit integer default propietario integer default creacion timestamp without time zone default now codelco boolean not null default false familia integer default enabled machines boolean default false enabled canbus boolean default false enabled horometro boolean default false enabled comap boolean default false enabled frio boolean default false enabled panico boolean default false enabled puerta boolean default false enabled rpm boolean default false enabled supervisor integer default demo boolean interno boolean mqtt enable boolean not null default false topicos character varying20 constraint pk cliente primary key rut constraint fk cliente familiaid foreign key familia references cliente familia id match simple on update no action on delete no action constraint pk pais foreign key codigo pais references pais codigo match simple on update no action on delete no action constraint unique id cliente unique id with oids false alter table cliente owner to waypoint grant all on table cliente to waypoint grant all on table cliente to waypointtx grant select update insert delete on table cliente to waypointtomcat grant select on table cliente to waypointphp grant select on table cliente to waypointpphppublic grant all on table cliente to waypointsoporte grant select insert on table cliente to waypointsalesforce grant select on table cliente to waypointadminuser grant select on table cliente to waypointagenda grant select on table cliente to waypointmachines grant select on table cliente to waypointreports grant select on table cliente to readonly create index index cliente on cliente using btree rut collate pg catalog default create index index cliente activo on cliente using btree activo create index index cliente id activo on cliente using btree id activo create index index cliente rut activo on cliente using btree rut collate pg catalog default activo create trigger trigger default admin after insert on cliente for each row execute procedure crea default admin create trigger trigger default grupo after insert on cliente for each row execute procedure crea default clientegrupo should disable constraints triggers or something else perhaps any db tuning what else should provide for further analysis version postgresql on x86 unknown linux gnu compiled by gcc debian bit
132951 am not able to set current timestamp as default value my mysql version is query is alter table downloads add date datetime not null default current timestamp while it is working fine on my local db with mysql v5
132966 im on sql server and need to clean whitespace from start and end of columns content where whitespace could be simple spaces tabs or newlines both and this content should become this content this content should become this content and so on was able to only achieve the first case with update table set column ltrimrtrimt column but for the other cases it doesnt work
133208 im having an issue in production environment and was trying to simulate it in my local sql server express instance used ssms to change the maximum number of concurrent connections from max to after restarting the service im able to connect to it with ssms but when try to open server properties or new query window to change the setting back to ssms crashes is there any other place can change this setting without ssms registry
133384 simple would like to count the number of rows from the sub query note that status is whether the host is online or not bad code select countip address from ports select distinct ip address from ports where status is true explained the first query when run on its own returns this select distinct ip address from ports where status is true ip address the second query run on its own returns this select countip address from ports question would like to know how to count that list of ip addresses have been looking online at possible solutions to this simple problem and just getting frustrated so thought id ask the experts
133546 we have team who designs the tables and relations for software developers in our organization they are pretty strict about enforcing 3nf normalization which to be honest agree with given the size of our organization and how the needs or our clients change over time there is only one area im not clear about the reasons behind their design decision addresses while this mostly focuses on addresses in the united states think this could apply to any country that does this each piece of an address gets its own column in the addresses table for instance take this gnarly address attn jane doe smith st sw apt 300b chicago il it would get split up in the database like this street number street fraction street pre directional north street name smith street type st street street post directional sw southwest city chicago state il illinois zip code zip4 code country assumed to be attention jane doe box null dwelling type apt apartment dwelling number 300b and there would be few other columns related to rural routes and contract routes furthermore our specific application will likely have few international addresses in it the data modelers said they would add columns specific for international addresses which would be the normal line line fields at first thought this was way overboard researching online repeatedly refers to using address line and possibly then splitting out city region and postal code we do have one use case for our new application where this granularity is beneficial we have to validate that the user is not creating duplicate business and checking the address is one of the validations we can get it to work with address line and but it would be more difficult as for our specific application we need to store multiple kinds of addresses for businesses and people physical mailing shipping etc we might need to generate printable form letters but that requirement hasnt been discussed so far some other things applications in our organization need to support auditing with full history tables printing mailing labels generating printed forms reporting for national and regional governments while our application might not be doing everything that every other application is doing splitting addresses into multiple components is an enterprise standard where work regardless of whether our application would benefit from it we are forced to do this semi related stackoverflow question where is good address parser which was closed but illustrates how difficult parsing addresses can be in order for me to better understand their design decision and to sell our client on the idea what problems are solved by splitting the street address into individual columns bonus points for anyone who has implemented system like this because they ran into problems
133556 tl dr the question below boils down to when inserting row is there window of opportunity between the generation of new identity value and the locking of the corresponding row key in the clustered index where an external observer could see newer identity value inserted by concurrent transaction in sql server detailed version have sql server table with an identity column called checkpointsequence which is the key of the tables clustered index which also has number of additional nonclustered indexes rows are inserted into the table by several concurrent processes and threads at isolation level read committed and without identity insert at the same time there are processes periodically reading rows from the clustered index ordered by that checkpointsequence column also at isolation level read committed with the read committed snapshot option being turned off currently rely on the fact that the reading processes can never skip checkpoint my question is can rely on this property and if not what could do to make it true example when rows with identity values and are inserted the reader must not see the row with value prior to seeing the one with value tests show that the query which contains an order by checkpointsequence clause and where checkpointsequence clause reliably blocks whenever row is to be read but not yet committed even if row has already been committed believe that at least in theory there may be race condition here that might cause this assumption to break unfortunately documentation on identity doesnt say lot about how identity works in the context of multiple concurrent transactions it only says each new value is generated based on the current seed increment and each new value for particular transaction is different from other concurrent transactions on the table msdn my reasoning is it must work somehow like this transaction is started either explicitly or implicitly an identity value is generated the corresponding row lock is taken on the clustered index based on the identity value unless lock escalation kicks in in which case the whole table is locked the row is inserted the transaction is committed possibly quite lot of time later so the lock is removed again think that between step and there is very tiny window where concurrent session could generate the next identity value and execute all the remaining steps thus allowing reader coming exactly at that point of time to read the value missing the value of of course the probability of this seems extremely low but still it could happen or could it if youre interested in the context this is the implementation of neventstores sql persistence engine neventstore implements an append only event store where every event gets new ascending checkpoint sequence number clients read events from the event store ordered by checkpoint in order to perform computations of all sorts once an event with checkpoint has been processed clients only consider newer events events with checkpoint and above therefore it is vital that events can never be skipped as theyd never be considered again im currently trying to determine if the identity based checkpoint implementation meets this requirement these are the exact sql statements used schema writers query readers query if im right and the situation described above could arise can see only two options of dealing with them both of which are unsatisfactory when seeing checkpoint sequence value before having seen dismiss and try again later however because identity can of course produce gaps when the transaction is rolled back might never come so same approach but accept the gap after milliseconds however what value of should assume any better ideas
133651 im trying to migrate mysql database to sql server using the sql server migration assistant for mysql converting the schema and synchronizing with sql server works fine however after clicking the migrate data button ssma crashes after few seconds ssma has stopped working what might be the cause of this problem
133712 this is simple question that cant seem to find the answer for in terms of performance if have where clause such as and and would gain any performance if replaced that condition with in other words is there any performance gain by replacing the following select from mytable where and and and with select from mytable where know it can depend on indexes but for this purpose lets just say no indexes exist does the arithmetic operator perform better than an or or and logical operator im under the impression that the addition performs better than multiple conditions with ands or ors test results on table of million rows returning rows where and rows the addition took seconds while the logical conditions and and took seconds on the other hand returning rows where or rows seconds returning rows where f65 f67 f64 rows seconds for the or it seems that there is no significant difference agree with gbn if is and is but and is false and with amtwo absa absb absc absd even if you expect only positive values if the column accepts negative values you should assume that you might encounter one the results are very impressive as thought it seems that the addition is much quicker than the logical operators float money and float the query used is as shown in my case all are positive numbers no indexes it is just logical in my mind that addition would be quicker than logical conditions
134016 scenario is this prod is two two node fci with mirroring primary disaster dc this is working just fine new hardware is coming and version upgrade to sql server from r2 in the form of two two node fci in primary with async ag to get the data over to the second fci dr is all san mirroring with identical boxes in dr site can take the r2 primary and log ship all my databases to the sql server fci primary while it is in an ag if not can leave the ag off and just log ship to do the migration go live and setup the ag after the fact thats not the end of the world just trying to see if can skip the this setup
134027 the following code was added by one of our developers to delete duplicate records from the table delete subquery from select id fk1 fk2 createddatetime row number overpartition by fk1 fk2 order by createddatetime as rownumber from table as subquery where rownumber when reviewing the code assumed that it wouldnt work however testing it in our test environment sql shows that it does how does sql know to resolve the sub query and delete the the records from table
134129 need to perform an update and an insert in single transaction that code works fine on its own but id like to be able to call it easily and pass in the required parameters when try to nest this transaction in stored procedure run into lots of syntax errors how can encapsulate the following code so it can be easily called begin transaction assignusertoticket go declare updateauthor varchar100 declare assigneduser varchar100 declare ticketid bigint set updateauthor user1 set assigneduser user2 set ticketid update tblticket set ticketassignedusersamaccountname assigneduser where ticketid ticketid insert into dbo tblticketupdate ticketid updatedetail updatedatetime usersamaccountname activity values ticketid assigned ticket to assigneduser getdate updateauthor assign go commit transaction assignusertoticket
134296 have sql server instance and an active directory group which can add active directory users to have my personal ad account myaccount and service account serviceaccount in that group need to grant admin access to this ad group for the whole instance this means that wont need to execute grant commands to every new table and database created any user on that group will automatically be able to create drop insert select etc even trickier must be able to use windows authentication to login with myaccount login on windows and then login to sql server without needing to type username password again and login using serviceaccount by typing username and password cant be forced to login on windows with serviceaccount or run ssms ssdt etc with that account to be able to login on mssql whats the best practice for configuring that
134525 as shown in using common table expressions on msdn you can define cte as with expression name column name as cte query definition and use it like select column list from expression name lets say have following ctes with cte1 as select name from table1 with cte2name as select name from table1 query outputs the same results for both ctes as the inner query is same the only difference between these two is that cte2 has column namename defined in its declaration when execute both ctes dont see any difference in the execution plan am just curious to know what difference does it make if dont specify any column names in cte definition why should should not specify column names while creating cte does it affect query execution plan by any chance as far as have seen it doesnt make any difference
134555 have table that is several hundred columns wide is there way to convert each row into single concatenated string with the column title included without having to list each single column in the query im doing this because the columns represents fields in an event report im putting them back together so person can read the report in logical manner ive done some of this with query but it is laborious to do for each column and seems error prone here is brief snippet showing three columns concatenated in the format need done in the column by column approach select concat iifid is null null concatid id iifstandardclientid is null null concatstandardclientid standardclientid iifclientname is null null concatclientname clientname as reportline from dbo datadecoded thanks
134685 have table that has an clustered unique index and non clustered primary key with the same structure as the index if object id dbo tblbaccountholder is not null drop table dbo tblbaccountholder go create table dbo tblbaccountholder lngparticipantid int not null sdtmcreated smalldatetime not null strusername varchar20 null strpassword varchar20 null tsrowversion timestamp not null constraint pk tblaccountholder primary key nonclustered lngparticipantid asc constraint ix tblbaccountholder lngparticipantid unique clustered lngparticipantid asc with fillfactor only one column as you can see on the definition create unique clustered index ix tblbaccountholder lngparticipantid on dbo tblbaccountholder lngparticipantid asc would like to drop the unique index and alter the primary key so that it is clustered will keep the same primary key just change it from non clustered to clustered this table is part of transaction replication would get this done on the subscriber database only not in the publisher it is table with over rows will mess up the replication the problem is that have to drop the primary key constraint and re create it as clustered this is what would like to get done in the subscriber database drop index ix tblbaccountholder lngparticipantid on dbo tblbaccountholder go alter table dbo tblbaccountholder drop constraint pk tblaccountholder go alter table dbo tblbaccountholder add constraint pk tblaccountholder primary key clustered lngparticipantid asc with pad index off fillfactor sort in tempdb off ignore dup key off statistics norecompute off online on allow row locks on allow page locks on on primary go
134704 am running sql server and am trying to put some queries together for monitoring using the dmvs however when looking at the total elapsed time field in the sys dm exec requests dmv the numbers look way off heres an example select session id runtime current timestamp start time total elapsed time from sys dm exec requests where session id session id runtime start time total elapsed time by my calculations the elapsed time should be around around not thats off by over factor of from the difference in milliseconds between the current time and the start time select datediffmillisecond 07t16 07t16 heres the server info select version microsoft sql server x64 apr copyright microsoft corporation enterprise edition core based licensing bit on windows nt x64 build service pack any ideas what could be causing this discrepancy
134741 as start this is for an etl stored procedure that is serialized so paralellism is not of concern need to assign custom id numbers for markers during loads can not use an identity field because the id numbers are to be unique by bucket number essentially another numbered field currently use the following code declare idrunner smallint select idrunner isnullmax id from sim variable where bucketref simbucketno declare variable cursor cursor for select distinct variable from simstg parameter left outer join sim variable on variable code where bucketref stgbucketno and bucketref simbucketno and code is null open variable cursor declare variable varchar64 fetch next from variable cursor into variable while fetch status begin set idrunner idrunner insert into sim variable bucketref variableno code values simbucketno idrunner variable fetch next from variable cursor into variable end close variable cursor deallocate variable cursor and do not like it it uses cursors which prefer to avoid sidenote this code is untested is there way to do this more efficient without cursor ir numbers must increase from the highest used when new elements are added multiple buckets exist by bucket no and have their own numbering am always only going to process data for one bucket at time one bucket in sim one in simstg
134828 using microsoft sql server sp3 kb3072779 x64 given table and index create table user session sessionid int identity1 not null primary key createdutc datetime27 not null default sysutcdatetime create nonclustered index ix user session createdutc on user session createdutc include sessionid actual rows for each of the following queries is 1m the estimated rows are shown as comments when these queries feed another query in view the optimizer chooses loop join because of the row estimates how to improve the estimate at this ground level to avoid overriding the parent query join hint or resorting to an sp using hardcoded date works great select distinct sessionid from user session 9m great where createdutc but hardcoded these equivalent queries are view compatible but all estimate row select distinct sessionid from user session where createdutc dateaddday sysutcdatetime select distinct sessionid from user session where dateaddday createdutc sysutcdatetime select distinct sessionid from user session inner loop join select dateaddday sysutcdatetime as mincreatedutc on mincreatedutc createdutc also tried reversing join order not shown no change select distinct sessionid from user session cross apply select dateaddday sysutcdatetime as mincreatedutc where mincreatedutc createdutc also tried reversing join order not shown no change try some hints but to view select distinct sessionid from user session where createdutc dateaddday sysutcdatetime option recompile select distinct sessionid from user session where createdutc select dateaddday sysutcdatetime option recompile optimize for unknown select distinct sessionid from select dateaddday sysutcdatetime as mincreatedutc inner loop join user session on createdutc mincreatedutc option recompile try using parameter hints but to view declare mindate datetime27 dateaddday sysutcdatetime select distinct sessionid from user session 2m adequate where createdutc mindate select distinct sessionid from user session 96m great where createdutc mindate option recompile select distinct sessionid from user session 2m adequate where createdutc mindate option optimize for unknown the statistics are up to date dbcc show statisticsuser session ix user session createdutc with histogram the last few rows of the histogram rows total are shown
134898 so ive never used mongodb just read lot about it and think its going to be good for my project also do not have lots of experience with mysql and to be even more honest have no clue of what im about to ask scenario mysql table profile id pk auto increment smallint user id pk fk varchar category id pk fk smallint role id pk fk tinyint country id pk fk smallint state id pk fk smallint legal document pk varchar unique name pk varchar unique type pk boolean last activity pk date all the fk you see there are mysql tables of course then im thinking to use mongodb to store the profile info profile info collection should contain documents like id profile id address some street in some state of some country phone email example example com etc im planning to use mongodb because my project needs to be public asap and we may add several new profile info and other things and do not want to do alter table and migrate from tools with lots of rows with that said we may have to add few more pks to the mysql table so was thinking of migrating all the project to mongodb and do not know if that is good move questions its better to keep the pks in mysql and trivial info in mongodb or would it be fine if move all to mongodb may mongodb be faster if just migrate the whole project but keep the structure like that mean like having profile and profile info collections instead of just profile im worried about how many resources mongodb could use for table collection with that many indexes want to keep disk space and ram at minimum use is there critical difference between mysql and mongodb ps ssds are going to be used in the system ps ii all tables are just planned nothing is written yet im bit of plan very ahead person so please be patient with me
135032 have business requirement that each record in the invoice table has an id which looks like yyyynnnnnn the nnnnnn part needs to restart at the beginning of each year so the first row entered in would look like and the second like etc lets say the last record for was the next row of should look like dont need this id to be the primary key and store the creation date as well the idea is that this display id is unique so can query by it and human group able by year it is unlikely that any records would be deleted however would be inclined to code defensively against something like that is there any way could create this id without having to query for the max id this year every time insert new row ideas createnewinvoicesp which gets the max value for that year yucky some magical built in feature for doing exactly this can dream right being able to specify some udf or something in the identity or default declaration view which uses partition over row deleted would be problematic trigger on insert would still need to run some max query an annual background job updated table with the max for each year inserted which then something all of which are bit non ideal any ideas or variations welcome though
135142 successfully granted require ssl to single user by doing mysql targetmysqluser targetmysqlpass grant usage on dbname to dbusername require ssl but im failing on removing or revoking this flag from the user using revoke guess im fighting with the syntax is there proper way to remove it with the revoke command without revoking the whole permission mysql manual this site and the interwebs didnt helped me yet finding proper counter way this sql statement will work update mysql user set ssl type where ssl type any flush privileges but believe where is grant require ssl there must me revoke require ssl isnt there
135150 we have process of archiving wherein we take the backup of current database and restore it as xxx archive database now this database contains previous data and will not have any insert update delete as this database will only be used for reporting we are thinking of shrinking the database however this increases the index fragmentation we are thinking of following below steps shrink the database with re organize re organize indexes will this hamper the database performance what other options can we consider for recovering the space update in the process we change the recovery model to simple backup the database to move the database out of pseudo simple state and then planning to shrink the database
135365 im trying to get whatever stored procedure returns with the column names and their types can do this with tables but couldnt figure it out for stored procedures tried the sp columns but only managed to make it work for tables have also tried something like this but im not sure what am supposed to match with what select from sys procedures nolock as aa inner join sys schemas nolock as bb on aa schema id bb schema id inner join sys columns nolock as cc on aa object id cc object id for example user id varchar200 only need the names of the column names stored procedure returns and their data types sql server version is if it matters any ideas thanks
135420 started to use sql server recently and still dont know the best way to do some things created all the tables with column id as the primary key now when try to insert values get the following error cannot insert the value null into column id table project dbo table column does not allow nulls insert fails the statement has been terminated what is good simple solution to this problem
135455 have done some digging on what the option fast xxx query hint does inside select statement and am still confused on it according to msdn specifies that the query is optimized for fast retrieval of the first number rows this is nonnegative integer after the first number rows are returned the query continues execution and produces its full result set for me that does not make much sense but basically the query can get the first xxx rows really fast then the rest at normal speed the microsoft dynamics query that got me thinking on this is select pjproj projectpjproj project descpjproj customerpjproj cpnyid from pjproj with nolock where project like order by project optionfast can anyone explain exactly what this query hint is doing and its advantage over not using it
135478 have fairly large 240gb mongo database that need to transfer across sluggish network and onto new server traditionally for these situations ive found that mongodump followed by mongorestore has been much faster than db clonecollection method however realized today that doing full mongodump followed by mongorestore is bit wasteful think since do all of the data transfer then do all of the insertions would prefer to transfer data from the old mongo the mongodump step while simultaneously inserting available data into the new database the mongorestore step does anyone know how to parallelize the dumping and inserting process in mongodb and would this actually be faster
135645 was reading this article on bbc it tells story of person named jenifer null and how she faces day to day problems while using online databases like booking plane tickets net banking etc am not well versed in databases and do not use it very often when made website for learning the server side form validation used regular expressions from what remember it would happily accept the name null have not tried it though could someone explain the technicalities when this situation would occur is the form validation just doing string null or something even so do not think null is same as null
135717 the title is pretty self explanatory but if youre curious as to why want this want this because have an archive log table that stores past values of an active table and due to this dont want the data at risk of being compromised in any way the only thing that should ever insert on the table is the trigger created on the active table to log its changes in the rare case we may need to manually edit the log table will turn off if it exists the insert lock am using sql server enterprise with sql management studio
135720 am trying to assign select privilege to group in redshift so created group and user in that group create group data viewers create user user password password in group data viewers now would like to allow this group to be able to read data from any table grant select on all tables in schema public to group data viewers the command returns grant now when connect to redshift as my newly created user and issue select from something something get permission denied for schema something tried granting permissions to something grant select on all tables in schema something to group data viewers but this has not changed anything how can allow users from my group to select data from any table in the schema
135745 restored database from backup the database uses replication to publish to different server assuming the database restore would break the replication tried to delete the replication and re create it we have script to re create it from scratch im not sure exactly what did but now it is in completely messed up state and cant fix it first try to get rid of the subscription on the publisher server exec sp dropsubscription publication publicationname article nall subscriber subscriberservername this seems to work select from syssubscriptions shows no results looking on the subscriber server ssms subscriberserver replication local subscriptions the subscription is not there so then try to delete the publication ssms server replication local publications publicationname delete this gives the following error message could not delete publication publicationname could not drop article subscription exists on it changed database context to databasename microsoft sql server error ok so try to drop the articles exec sp droparticle publication publicationname article nall and get this error invalidated the existing snapshot of the publication run the snapshot agent again to generate new snapshot msg level state procedure sp msdrop article line could not drop article subscription exists on it ok so try starting the snapshot agent and get this internal sql exception the sql command sp msactivate auto sub had returned fewer rows than expected by the replication agent so tried an alternative method of deleting the article delete from sysarticles this seems to have worked have now got rid of the articles but still get the same cannot drop the publication because at least one subscription exists for this publication error when try to delete the publication have also restarted sql server didnt help dont know what is going on here and how do fix it btw this is what happens when you give software developer who knows just enough to be dangerous the keys to the database fortunately this isnt production environment
135814 how to view the result between dates and time my current code select from db dbo table where user id and date between and and time for date and time for date order by date asc time asc this code does not work and do not know how to fix it why doesnt this request work
135887 yesterday made serious mistake restoring the wrong database background you may skip this section right clicked the test database and proceeded to the restore database section as usual then when selecting the media file for the source browsed for my latest production database backup performed at am this was not mistake did need to have up to date information on the test database in order to test however as soon as hit ok on the add media dialog ssms detected that my backup was made from the production db and silently changed the destination field to reflect that didnt realize this until it was too late all database changes from 01am to 30pm were lost after few hours struggling on how could approach my manager and deliver these unhappy news decided would try my best to repair the damage in any way possible and for certain extent luckily succeed now the steps taken first and foremost made sure that todays backup had been performed since it contained db entries between 30pm to 00pm which could not be lost went to the restore dialog for the production db and then clicked the timeline option hoping to perform point in time restore at precisely 29pm unzipped the previous day backup same one accidentally restored previously to my default backup location and made sure suitable log backup was present on my system set up the desired point in time clicked verify backup media and proceeded to backup now im relieved that it worked but as expected all of my tables contains records up to 29pm and everything else is missing its large database with more than tables and hand checking and inserting everything would take days question is it possible to restore only the missing records which are present on my latest backup but not in my current state db if so what are the steps necessary not as serious as yours fear bane edit in response to james anderson concerns about the backup chain actually do have backup file with the changes from 30pm till 10pm it was created by job ive set up in fact even restored this backup to new temporary database just to be sure about getting more people involved to help my manager is travelling for next couple of weeks my team is really small people and im the one in charge right now which is to say ill inform them but im responsible for handling this alone even if manually if there are any tools that could help please suggest thank you everyone
136225 im new to db2 and am connecting to db2 for v7r1 database using unixodbc and the ibm access odbc driver for linux when query the database the results only include the first letter of the column names for example typical query run with the isql utility will give me something that looks like this sql select column1 column2 from schema table where column1 lorem ipsum dolar sit sqlrowcount returns rows fetched this is problem for me because when try to query the database in applications using the pyodbc library for python cant access the results by column name both columns are named in the result set my questions are why is this happening is it possible to change this behavior how edit this happens even if try to give the columns aliases sql select column1 as foo column2 as bar from schema table where column1 lorem ipsum dolar sit sqlrowcount returns rows fetched
136235 need to convert data between two systems first system stores schedules as plain list of dates each date that is included in the schedule is one row there can be various gaps in the sequence of dates weekends public holidays and longer pauses some days of the week may be excluded from the schedule there can be no gaps at all even weekends can be included the schedule can be up to years long usually it is few weeks long here is simple example of schedule that spans two weeks excluding weekends there are more complicated examples in the script below id contractid dt dowchar dowint mon tue wed thu fri mon tue wed thu fri id is unique but it is not necessarily sequential it is primary key dates are unique within each contract there is unique index on contractid dt second system stores schedules as intervals with the list of week days that are part of the schedule each interval is defined by its start and end dates inclusive and list of week days that are included in the schedule in this format you can efficiently define repetitive weekly patterns such as mon wed but it becomes pain when pattern is disrupted for example by public holiday here is how the simple example above will look like contractid startdt enddt daycount weekdays montuewedthufri startdt enddt intervals that belong to the same contract should not overlap need to convert data from the first system into the format used by the second system at the moment im solving this on the client side in for the single given contract but id like to do it in sql on the server side for bulk processing and export import between servers most likely it could be done using clr udf but at this stage cant use sqlclr the challenge here is to make the list of intervals as short and human friendly as possible for example this schedule id contractid dt dowchar dowint thu fri mon tue wed thu fri mon tue should become this contractid startdt enddt daycount weekdays montuewedthufri not this contractid startdt enddt daycount weekdays thufri montuewedthufri montue tried to apply gaps and islands approach to this problem tried to do it in two passes in the first pass find islands of simple consecutive days the end of the island is any gap in the sequence of days be it weekend public holiday or something else for each such found island build comma separated list of distinct weekdays in the second pass group found islands further by looking at the gap in the sequence of week numbers or change in the weekdays with this approach each partial week ends up as an extra interval as shown above because even though week numbers are consecutive the weekdays change besides there can be regular gaps within week see contractid in sample data which has data only for monwedfri and this approach would generate separate intervals for each day in such schedule on the bright side it generates one interval if the schedule doesnt have any gaps at all see contractid in the sample data that includes weekends and in that case it doesnt matter if the start or end week is partial please see other examples in the script below to get better idea of what im after you can see that quite often weekends are excluded but any other days of the week could also be excluded in example only mon wed and fri are part of the schedule besides weekends can be included as in example the solution should treat all days of the week equally any day of the week can be included or excluded from the schedule to verify that the generated list of intervals describes the given schedule correctly you can use the following pseudo code loop through all intervals for each interval loop through all calendar dates between start and end dates inclusive for each date check if its day of the week is listed in the weekdays if yes then this date is included in the schedule hopefully this clarifies in what cases new interval should be created in examples and one monday is removed from the middle of the schedule and such schedule cant be represented by single interval in example there is long gap in the schedule so two intervals are needed intervals represent weekly patterns in the schedule and when pattern is disrupted changed the new interval has to be added in example first three weeks have pattern tue then this pattern changes to thu as result we need two intervals to describe such schedule im using sql server at the moment so solution should work in this version if solution for sql server can be simplified improved using features from later versions thats bonus please show it as well have calendar table list of dates and numbers table list of integer numbers starting from so it is ok to use them if needed it is also ok to create temporary tables and have several queries that process data in several stages the number of stages in an algorithm has to be fixed though cursors and explicit while loops are not ok script for sample data and expected results src is sample data dst is expected result declare src table id int primary key contractid int dt date dowchar char3 dowint int insert into src id contractid dt dowchar dowint values simple two weeks without weekend mon tue wed thu fri mon tue wed thu fri partial end of the week the whole week partial start of the week without weekends thu fri mon tue wed thu fri mon tue only mon wed fri are included across two weeks plus partial third week mon wed fri mon wed fri mon whole week without weekend in the second week mon is not included mon tue wed thu fri tue wed thu fri three weeks but without mon in the second week no weekends mon tue wed thu fri tue wed thu fri mon tue wed thu fri long gap between two intervals thu fri mon tue wed thu fri mon tue mon tue wed thu fri mon tue wed thu fri two weeks no gaps between days at all even weekends are included mon tue wed thu fri sat sun mon tue wed thu fri no gaps between days at all even weekends are included with partial weeks sat sun mon tue wed thu fri sat sun mon tue wed thu fri sat only mon wed included two weeks plus partial third week mon tue wed mon tue wed mon tue only thu sun included three weeks thu fri sat sun thu fri sat sun thu fri sat sun only tue for first three weeks then only thu for the next three weeks tue tue tue thu thu thu one week then one week gap then one week mon tue wed thu fri mon tue wed thu fri select id contractid dt dowchar dowint from src order by contractid dt declare dst table contractid int startdt date enddt date daycount int weekdays varchar255 insert into dst contractid startdt enddt daycount weekdays values montuewedthufri montuewedthufri monwedfri montuewedthufri tuewedthufri montuewedthufri montuewedthufri montuewedthufri montuewedthufri sunmontuewedthufrisat sunmontuewedthufrisat montuewed sunthufrisat tue thu montuewedthufri montuewedthufri select contractid startdt enddt daycount weekdays from dst order by contractid startdt comparison of answers the real table src has rows with distinct contractids all answers produce correct results at least for my data and all of them are reasonably fast but they differ in optimality the less intervals generated the better included run times just for curiosity the main focus is the correct and optimal result not the speed unless it takes too long stopped the non recursive query by ziggy crueltyfree zeitgeister after minutes answer intervals seconds ziggy crueltyfree zeitgeister while loop ziggy crueltyfree zeitgeister recursive michael green recursive geoff patterson weekly gaps and islands with merging of partial weeks vladimir baranov daily then weekly gaps and islands mikael eriksson weekly gaps and islands vladimir baranov cursor
136239 how to grant select on all tables starting with vvc grant select on vvc to user1
136268 the built in hierarchyid is clr that stores paths in an efficient binary form and provides other useful functionality unfortunately there is limit to how deep the represented paths can be and its for binary tree would like to increase that limit for complex existing application that is bound to hit this limit dont wish to change the interface of the type am not confident that could pull off changing the interface of the type without introducing subtle bugs into all the code that would have to be changed as result could in theory create binhierarchyid clr udt that implements the same interface as hierarchyid but only supports binary trees that should get me depth of while still remaining inside the byte limit not sure how big of an undertaking that would be is the source of this hierarchyid clr available somewhere so that could create my own based on it that supports deeper structures
136674 have rather big table with one of the columns being an xml data with an average size of xml entry being kilobytes all other columns are regular ints bigints guids etc to have some concrete numbers lets say the table has million rows and is gb in size what noticed is that this table is really slow to select data from if want to select all the columns when do select top from table it takes around seconds to read the data from disk even though dont impose any ordering on the result run the query with the cold cache after dbcc dropcleanbuffers heres io statistics results scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads it grabs mb of data execution plan shows clustered index scan as id expect theres no io going on on the disk besides my queries ive also checked that clustered index fragmentation is close to this is consumer grade sata drive however id still think sql server would be able to scan the table faster than mb min presence of xml field causes most of the table data to be located on lob data pages in fact of table pages are lob data guess my question is am correct in thinking that lob data pages can cause slow scans not only because of their size but also because sql server cant scan the clustered index effectively when theres lot of lob data pages in the table even more broadly is it considered reasonable to have such table structure data pattern recommendations for using filestream usually state much bigger field sizes so dont really wanna go that route ive not really found any good info about this particular scenario ive been thinking towards xml compression but it needs to be done on the client or with sqlclr and would require quite some work to implement in the system tried the compression and since xmls are highly redundant can in app compress xml from 20kb to 5kb and store it in varbinary column preventing usage of lob data pages this speeds selects 20x times in my tests
136733 when are the values for computed columns determined when the value is retrieved when the value is changed some other time im guessing this is novice question since im not finding anything in my searches
136894 know that the last run date and last run time are stored as int but when coverted over to datetime of which have examples of is the time gmt or what when converted it doesnt match the same time as what the gui log history displays can anyone provide guidance on this here is the sql code have that converts the value to datetime value last run literal dateaddmillisecond sjs last run time convertdatetimecastnullifsjs last run date0 as nvarchar10
137045 am trying to query two tables and get results like the following section names shoes accountname1 accountname2 accountname3 books accountname1 the tables are create table dbo tableaid int section varchar64 accountid varchar64 insert dbo tableaid section accountid values shoesa1 shoesa2 shoesa3 booksa1 create table dbo tablebaccountid varchar20 name varchar64 insert dbo tablebaccountid name values a1accountname1 a2accountname2 a3accountname3 saw few questions answered saying to use xml path and stuff to query the data to get the results am looking for but think there is something missing have tried the below query and get the error message column accountid is invalid in the select list because it is not contained in either an aggregate function or the group by clause dont have it in the select clause of either query but assume the error is because accountid is not unique in tablea here is the query am currently trying to get working correctly select section names stuff select name from tableb as where accountid accountid for xml path from tablea as group by section
137097 specifically want programmable way to change the recovery model of all the databases on server and know this solution can be applied to all alter database commands while know smo in powershell can solve this problem very easily import module sqlps cd sql servername instancename databases foreach database in ls force database recoverymodel full database update im looking for the most efficient sql solution it has been ingrained in my head to avoid cursors at all costs however cant think of set based solution to perform alter statements here is the cursor based solution declare sql varcharmax name varchar50 declare cur cursor for select name from sys databases where name tempdb open cur fetch next from cur into name while fetch status begin set sql alter database name set recovery full exec sql fetch next from cur into name end close cur deallocate cur this can also be done with while loop but that solution still goes through looping process do while loops offer significantly better performance am being paranoid about performance say that because administrative tasks such as this arent done very often and there generally isnt massive amount of objects you need to loop through however always think what if worked in large environment with thousands of objects and then begin to feel guilty that may not be using the most efficient solution
137200 am working with sql server and am looking for function like ltrim and rtrim which will also remove leading and trailing tabs double spaces carriage returns line feeds etc there are number of functions out there but the ones found all have limitations truncate the string to characters according to some of the comments sql server enhenced trim function remove trailing spaces leading spaces white space tabs carriage returns line feeds one of the comments proposed better solution but the causes an incorrect syntax error and am not sure why create function dbo supertrimleft str varcharmax returns varcharmax as begin if asciileft str begin set str stuff str patindex char0 char32 str end return str end so my question is what is the best approach to accomplish the task above
137391 have sql server database noticed value of reason for early termination of statement optimization for some queries and all gave good enough plan found now my questions are what are all the possible types of reason for early termination of statement optimization did searching for this in msdn but didn get complete list of values is there dmv or extended event to list all queries for which optimization was terminated due to reasons other than good enough plan found referred following two articles which does not list the complete list of possibilities also they give me different result in my database finding query compilation timeout identifying query plans that are not good enough
137472 ive just inherited about instances of sql server as part of wider acquisition project im in the process of assessing performance and dont like the way maintenance plans have been implemented im seeing daily blanket index rebuilds can deal with this one and also daily manual updating of statistics around half of the databases have been set to auto update statistics false for reasons which are not clear other than am told it is to reduce performance issues always thought and worked to best practice of setting this to true and felt the manual update was not necessary if this setting was true am wrong can anyone explain what the benefit would be in having this set as false but doing daily manual update instead should mention that some of the databases are highly transactional millions of inserts deletes updates per day others are low in terms of transaction rates and some are all but read only there is no rhyme or reason though as to which have the auto update setting set to false it appears to be lottery
137704 was reading the examples on msdn for try catch blocks the code listing for example is as follows begin transaction begin try generate constraint violation error delete from production product where productid end try begin catch select error number as errornumber error severity as errorseverity error state as errorstate error procedure as errorprocedure error line as errorline error message as errormessage if trancount rollback transaction end catch if trancount commit transaction go what dont understand is why they would bother to check the trancount in the catch block is it really possible for the transaction count to be in this code snippet or is the guard clause just an example of the principle that examples should be exemplary
137791 was reading this bbc news article and the following excerpt caught my attention it sounds like always on availability groups or high availability mirroring maybe with security automatically included is blockchain potentially viable database solution for modern high transaction volume applications it is pretty easy to see its value for low volume transactions like personal medical records but what about high volume databases what is blockchain blockchains rely on cryptography to allow set of computers to make changes to global record without needing central actor removing the middleman cuts costs in almost every sector the blockchain is ledger that records everything that happens to collection of data known as block in chronological order or chain as currency this is an important feature because it allows users to be sure their digital money is one of kind the same way each note in your wallet is unique blockchain tech will be the way we create assets because it allows you to transfer digital information without copying says adam ludwin chief executive of chain com which builds blockchain networks blockchain can be used to track the history of all sorts of information and maintain its value so for example doctors could use it to update medical records since each change to blockchain is made simultaneously across the whole network no information is lost and because changes cannot be undone the system maintains its transparency special key is needed to make changes to each block so individuals can keep their records safe by protecting that key
137911 on database hosted on sql server instance have enabled allow snapshot isolation and verified the state as on using select snapshot isolation state descname from sys databases however in separate sessions if run long running select with tablock in the 1st and an update in the 2nd or vice versa whichever query starts first blocks the second query as per sp who2 looking at select from sys dm exec requests both queries have transaction isolation level of read committed as per my understanding with snapshot isolation on tempdb usage should increase however blocking should not occur in this situation am missing some configuration steps to achieve this behaviour
138029 after clicking on take database offline in management studio this message stays hang and wont close if you click on close whats good way to deal with stuck jobs like these in management studio can you kill them via the activity monitor should seek what process is stopping this job from going through and terminate it
138080 requirement in recent project was to report when resource would be fully consumed as well as the exhaustion calendar date was asked to show the remaining time in english like format something like year months to go the built in datediff function returns the count of the specified datepart boundaries crossed between the specified startdate and enddate if used as is this could produce misleading or confusing results for example using an interval of year would show yyyy mm dd and to be one year apart whereas common sense would say these dates are separated by only day conversely using an interval of day and are separated by days while most people would see years as better description starting from the number of days and calculating months and years from there would be prone to leap year and size of month errors got to wondering how this could be implemented in the various sql dialects example output includes create table testdata fromdate date not null todate date not null expectedresult varchar100 not null exact formatting is unimportant insert testdata fromdate todate expectedresult values days day month month month length not important month day leap years to be accounted for months days days day not leap year year years day catch overflow in date calculations years months days mindate to maxdate happen to be using sql server 2008r2 but am interested to learn how other dialects would handle this
138114 have run server side profile trace for over an hour to produce trc file with all activities in one of my databases have then passed this trc trace file as parameter to the database engine tuning advisor after running the dta get the recommendations how do script out the recommendations am using sql server and dont seem to find any other way than scripting them individually which is overly time consuming
138311 can anybody show me good example of mdxs advantages over regular sql when doing analytical queries would like to compare an mdx query with an sql query that gives similar results wikipedia says while it is possible to translate some of these into traditional sql it would frequently require the synthesis of clumsy sql expressions even for very simple mdx expressions but there is neither citation nor example am fully aware the underlying data must be organized differently and olap will require more processing and storage per insert my proposal is to move from an oracle rdbms to apache kylin hadoop context am trying to convince my company that we should be querying an olap database instead of an oltp database most siem queries make heavy use of group by sort and aggregation besides the performance boost think olap mdx queries would be more concise and easier to read write than the equivalent oltp sql concrete example would drive the point home but am not an expert at sql much less mdx if it helps here is sample siem related sql query for firewall events that happened in the past week select seoul average as term substrto charidate hh24 mi as event time roundavgtot accept as cnt from select from st event yyyymm 1m where idate between truncsysdate iw and truncsysdate iw stat monitor group query union all select from st event yyyymm where idate between truncsysdate iw and truncsysdate iw stat monitor group query pm group by substrto charidate hh24 mi union all select today as term substrto charidate hh24 mi as event time roundavgtot accept as cnt from st event yyyymm cm where idate truncsysdate stat monitor group query group by substrto charidate hh24 mi order by term desc event time asc
138315 have timestamp stored as bigint format cannot turn into human readable format it points to date in may 11th however function to timestamp turns this into id like to create view where could display these values dont know what function to use the field is written by java program eclipselink presume dont have the source code of the program
138357 am attempting to resolve some difficulties in getting zip codes to display properly the original spreadsheet has zip codes of mixed and digit formats after the import process these digit zip codes are reporting length of digits now when try to add hyphen to the digit zip codes im getting abnormal results and errors because of the wrong length and various data type conversion issues the import was performed using an openrowset method for importing the data from spreadsheet when query the newly imported data see the zip codes showing the same as they were in the spreadsheet but the length is wrong select zip lenltrimrtrimzip as ziplength from xls import zip ziplength if select the left characters of the data everything gets converted to float and the zip codes are now unreadable select leftzip9 from xls import where lenltrimrtrimzip zip 32013e 42034e 56637e 41153e 36045e 41133e how can get these zip codes back to the correct digits or how can add hyphen to those digit zip codes which are reporting length of my end goal is to simply get the digit zip code to have hyphen in the middle the datatype of the zip column is float just discovered that some of my spreadsheets like nj and ny have an apostrophe before the leading in the zip code will need to investigate how to handle the 0xxxx zip codes to get this to work on some of my spreadsheet imports
138522 what are the differences between the daily tasks duties of mongo dba compared to rdbms dba for example some sites claim that mongodb dba would not require to do data modelling or designing the database as that would be done by developer or application designer this would mean some tasks are no longer needed to be done wrt mongodb administrations which was earlier being done by the rdbms dbas what other tasks would be required that are not normally in the schedule of rdbms dba and also the task which the rdbms dbas used to do but is no longer in the schedule of mongodb dbas im new in mongodb administration so im trying to identify these task so that may not commit the mistake of doing things in my daily task that are not needed or miss something that need to can any experienced mongodb dbas help me out so that dont do any foolish mistakes in my work
138720 was running alter column from int to bigint my table size is gb and after hour got the version store error because tempdb was full as far as know it only takes your memory pages and log files but am not sure about tempdb can someone please explain this internals am using read committed isolation select name snapshot isolation state desc is read committed snapshot on from sys databases for this database returns name databasename snapshot isolation state desc off is read committed snapshot on
138724 have to maintain and extend an old legacy system which contains webservice methods and database tables that are no longer used since not entirely sure that the tables are really redundant afraid to drop them is there any other way to achieve the same effect tables cannot be used any more without dropping them my idea was to transfer them to different schema deleted from the current default dbo if not exists select from sys schemas where name deleted begin execcreate schema deleted end alter schema deleted transfer dbo tablename is there any other option or are there any drawbacks to the schema approach
138800 in sql server why does this return the row even when add whitespace or two or more to the end of the where clause shouldnt zero records be found in the following example with src as select cast12345 as varchar demo select from src where demo what if one needs to query to find but not
139021 is there function or keyword that will allow me to get the current line number in stored procedure know theres an undocumented lineno function that allows you to set the line number and affect the output of system error messages https stackoverflow com questions what exactly does the sql lineno reserved word do know theres function error line thats available inside of begin catch end catch error line does what need but want to use it from outside catch block anywhere in the file declare insertsource varchar1000 object name procid exec proc accounting transaction insert other parameters insertsource currently im just hard coding the call as it appears in the stored procedure body but its getting old quickly declare insertsource varchar1000 set insertsource object name procid exec set insertsource object name procid exec
139106 is there sql equivalent of the and patterns that will let me pull values from column that contains punctuation for example create table test value varchar10 insert into test values 123a 456b 12abcab23cd789 select from test where value like this would return values where the first characters are numbers between and and the last character will be letter between and so would return things like 123a and 456b but wouldnt return value of 12abc want to know if there is an equivalent for punctuation as is for numbers and is for letters so that it would return ab23 and cd789 if could use regular expression might use the expression za z0 to match alphanumeric characters in string where value like za z0 is there sql equivalent for this know this kind of thing that can be done in regex but need it in sql am not able to load any custom assemblies onto this server so cant use regular expressions the real column is varchar200 the collation is latin1 general ci as am using sql server standard edition
139131 our setup master mariadb slave mariadb replication was working fine until recently at which point the slaves dbs had to be restored from dump performed all of the necessary steps dump the masters dbs transfer the dump to the slave drop the old dbs execute the dump to restore the dbs execute the appropriate change master command and finally start slave am receiving the error got fatal error from master when reading data from binary log could not find first log file name in binary log index file the first log file that the slave needs from the master is mysql bin can see that this is present on the master can also see that the binary log index on the master seems to have an entry for this log file still replication is not working keep getting the same error im out of ideas what should check next updated output of show slave status as requested mariadb none show slave status show slave status row slave io state master host master user replication master port connect retry master log file mysql bin read master log pos relay log file mysqld relay bin relay log pos relay master log file mysql bin slave io running no slave sql running yes replicate do db xxx yyyxxx zzz replicate ignore db replicate do table replicate ignore table replicate wild do table replicate wild ignore table last errno last error skip counter exec master log pos relay log space until condition none until log file until log pos master ssl allowed no master ssl ca file master ssl ca path master ssl cert master ssl cipher master ssl key seconds behind master null master ssl verify server cert no last io errno last io error got fatal error from master when reading data from binary log could not find first log file name in binary log index file last sql errno last sql error replicate ignore server ids master server id master ssl crl master ssl crlpath using gtid no gtid io pos row in set sec additional requested information root master var lib mysql ls var lib mysql mysql bin rw rw mysql mysql may var lib mysql mysql bin root master var lib mysql ls mysql bin mysql bin mysql bin mysql bin yes it was created root master var lib mysql mysql uroot enter password welcome to the mariadb monitor commands end with or your mariadb connection id is server version mariadb log mariadb server copyright oracle mariadb corporation ab and others type help or for help type to clear the current input statement mariadb none show binary logs log name file size mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin mysql bin rows in set sec mariadb none exit bye root master var lib mysql mysqlbinlog mysql bin tmp somefile txt root master var lib mysql tail tmp somefile txt at server id end log pos xid commit at server id end log pos rotate to mysql bin pos delimiter end of log file rollback added by mysqlbinlog set completion type old completion type set session pseudo slave mode root master var lib mysql etc my cnf server cnf excerpt binary logging log bin var lib mysql mysql bin expire logs days sync binlog edit postion does seem to exist root master var lib mysql grep end log pos tmp somefile txt server id end log pos binlog checkpoint mysql bin
139213 anyone here had luck with creating and using udfs on azure data warehouse database am in the middle of migrating an on prem warehouse from sql server to azure datawarehouse and ran into an issue with udfs create function dbo fn getimpliedrate multiple float term int returns float as begin declare impint float if term set impint select from dbo impliedrate where multiple multiple if term set impint select from dbo impliedrate where multiple multiple if term set impint select from dbo impliedrate where multiple multiple if term set impint select from dbo impliedrate where multiple multiple return impint end go this udf works perfectly on sql server when create this on azure data warehouse it gets created but it doesnt work when query it it returns null have verified obvious things like whether the target table exists etc all check looked at create function documentation for azure data warehouse and it has an example udf that converts int to decimal this works flawlessly on azure dw the moment write simple function that has select it fails unfortunately azures documentation here is not really helpful and was wondering if any of you ran into this issue if yes how did you resolve just tested another use case and it also doesnt work create function dbo fn getnumberbusinessdays startdate datetime enddate datetime returns int as begin declare ndays int select ndays isnull datediffdd startdate enddate datediffwk startdate enddate case when datenamedw startdate sunday then else end case when datenamedw enddate saturday then else end select ndays ndays count from dbo fedholidays where dateofholiday between startdate and enddate return ndays end go
139290 am trying to get an accurate count of total days tool is out for rental here is data sample create table tmptoolrentaldays toolid bigint startdate datetime enddate datetime rentaldays float insert into tmptoolrentaldaystoolid startdate enddate rentaldays values select from tmptoolrentaldays tool can go out for day and return the same day then go back out again on the same day this should be day am trying to avoid counting date like twice my intention is to get two columns toolid rental days how can achieve that
139358 maybe for this community my problem is easy but for me simple java programmer its big problem have big db with more and more data so the external db admin had create job that show me in temporary table the data that need but he had create the table without primary key and when with my java project go to read this table obtain an error cant read this table because the primary key dont exist can insert in the procedure the possibility to create an autoincremental primary key without changing the structure of this complex procedure this is the begin of the stored procedure code use mydb go set ansi nulls on go set quoted identifier on go alter procedure dbo spschedula scadenzario as begin drop table mydb dbo tmptable select aa into mydb dbo tmptable from thanks in advance
139382 want to capitalize only the first letter of each word of each sentence in sql column for example if the sentence is like movies then need the output like movies query declare varchar15 set qwerty keyboard select as normal text upper as uppercase text lower as lowercase text upperleft a1 lowersubstring a2len as capitalize first letter only here did upper lower and capitalize first letter only in my column here put just random word here are my results is there any possibilities to do that any possibilities to get results without using user defined function need the output qwerty keyboard
139480 as you know union removes duplicated rows thought it removes from the latter ones but it doesnt found out that if theres no order by clause oracle sorts the merged dataset by the first column and removes the duplicates select from dual union select from dual union select from dual it results whereas expect failed many times with this and make the problem simple like the example above how do keep the union sequence without order by in my real case the first select is full match result and the second is partial matchlike result if search for roma for example full match result roma partial match result aroma and of course full match result should come first as said it seems order by the first column of union result but as you see select roma from dual result comes out anyway union select aroma from dual result aroma roma aroma is alphabetically ahead of roma it comes first maybe should try union all and remove the duplicates
139702 im adding the following cross reference table to my sql server hosted db company id bigint not null fk org path nvarchar not null the company id field refers to the id field in another table in which its the primary key given that there can also be multiple records with the same company id any primary key would have to use both fields however im unable to create key using both fields because org path is too long for sql server as for org path this is the only table in which it exists theres every likelihood that queries to this table will be asking for either all entries or all org path entries by company id or to put it another way it looks doubtful that this table will ever be queried by org path furthermore its unlikely that org path will be updated and more likely inserted and probably rarely deleted expect that the total number of rows will be in the low thousands also the reason its nvarchar is because the value has to mimic that in third party db typical example will be something like translation providers customer name order name and can contain diacritics so my question is this would it be more efficient to add an auto increment id field and use that in conjunction with company id as the primary key or would it add unnecessary overhead and does the fact that company id is the primary key in another table have any effect here
139746 have an availability group set up on sql server running on two node windows server failover cluster the setup consists of two stand alone instances synchronous with automatic failover many microsoft articles ive read advocate using multiple files for tempdb to increase performance it seems they recommend using files should do that in the case of this configuration will it improve performance
139758 inherited database systems like this currently have publisher database in sql server compatibility mode on windows server r2 with sql server 2008r2 sp2 machine distributor is on the same machine subscriber is 2008r2 sp2 and database is in sql server compatibility mode we are using transactional replication isolation level is read committed distributor resides on publisher even though when right click on publication and even though subscription shows as pull subscription think it wont matter since distributor resides on publisher itself please correct me if am wrong storage system is ibm flex which is shared by five servers including publisher and subscriber since couple of days see latency of few hours it catches up in the morning and starts going up again in the afternoon followed https www mssqltips com sqlservertip troubleshooting transactional replication latency issues in sql server to see exactly what was happening ran following query use distribution go exec sp browsereplcmds xact seqno start seq seq is same for start and end xact seqno end seq publisher database id publisher database id this is different than database id see that there are supposedly massive updates being done on few tables involved in replication and log reader is just scanning transaction log not able to replicate anything till transaction completes interestingly cant see any blocking on either publisher and or subscriber will changing isolation level to read committed snapshot isolation rcsi help here will it help to change polling interval to and readbatchsize to or whats the command to change that setting changed log reader agent default profile as follows polling interval from to and readbatchsize to this brought latency from hours to zero almost instantly but see that it went back to hours replication is in sync and dont have single clue for actual root cause which caused the latency and now it went away
139776 im running simple db on sql server express just today when backup the database the bak file size doubles to what it was from the previous backup just minutes before have done several backups today through sql server management studio backup type full and with each one the bak file keeps on doubling in sql server mgmt studio when right click on my db reports backup and restore events expand successful backup operations the report here shows that the latest backup size was recorded as 55mb but when go to the actual bak file it is 260mb each other backup today was recorded also as 55mb size while their corresponding bak files are multiple times larger and growing with each backup operation what could be going wrong havent made any changes to the database since this started happening
139912 query declare xml item id item id item item select value id int from nodes item as ix result null null execution plan the top branch shreds the xml to four rows and the bottom branch fetches the value for the attribute id what strikes me as odd is the number of rows returned from the stream aggregate operator the rows that comes from the filter is the id attribute from the first and second item nodes in the xml the stream aggregate returns four rows one for each input row effectively turning the inner join to an outer join is this something that stream aggregate does in other circumstances as well or is it just something odd going on when doing xml queries can not see any hints in the xml version of the query plan that this stream aggregate should behave any differently than any other stream aggregate have noticed before
139969 have database that due to some silly code issues has grown hugely it is now sitting at 24gb most of it is very unnecessary logging information system generated debug information my server lives on cloud hosted server am now sitting with issues because of the file size pay for storage am happy to pay for my core business but paying for silly data seems silly my backups are now huge also do nightly full backups and then ship them off to an ftp server this process is taking longer and longer given my issues is shrinking so bad will rebuild all my indexes did dry run and brought my database down to 6gb this guy who have always trusted as my sql guru says its bad mkay sql server shrinking database is bad increases fragmentation reduces performance
139986 when run the following query on my database select from sys sysfiles get the following results but when run dynamic query that gets the percent of free space get msg level state line arithmetic overflow error converting numeric to data type numeric declare command nvarcharmax select command select db name as db name casts size castfilepropertys name spaceused as int as int as freespacemb cast100 cast size castfilepropertys name spaceused as int size as decimal52 as varchar8 as freespacepct from sys sysfiles exec sp executesql statement command am struggling to find the reason of the arithmetic overflow why is it happening why divide by it is because both sys sysfiles and fileproperty give the number of 8k pages not mb and to convert from 8k pages to mb you divide by as it is explained here why is it dynamic because actually get the values from each database using sp foreachdb as you can see on the example below declare command varchar5000 select command use select db name as db name casts size castfilepropertys name spaceused as int as int as freespacemb cast100 cast size castfilepropertys name spaceused as int size as decimal42 as varchar8 as freespacepct from dbo sysfiles exec sp foreachdb command
140247 in the following query plan snippet it seems obvious that the row estimate for the concatenation operator should be billion rows or the sum of the row estimates for its two inputs however an estimate of million rows is produced leading to sub optimal sort stream aggregate strategy that spills hundreds of gb of data to tempdb logically consistent estimate in this case would have produced hash aggregate removed the spill and dramatically improved query performance is this bug in sql server are there any valid circumstances in which an estimate lower than the inputs could be reasonable what workarounds might be available here is the full query plan anonymized do not have sysadmin access to this server in order to provide outputs from querytraceon or similar trace flags but may be able to get these outputs from an admin if they would be helpful the database is in compatibility level and is therefore using the new sql server cardinality estimator stats are updated manually every time data is loaded given the volume of data we are currently using the default sampling rate its possible that higher sampling rate or fullscan could have an impact
140363 select fullname login from user order by fullname desc login asc results fullname login mh com ad com null and com null ben com null roc com what want fullname login ad com mh com null and com null ben com null roc com want the full name in abc order desc and same with login but want all nulls to go to the bottom
140369 this is simple one but cant seem to figure it out have two parameters and if one parameter is passed null value use the other parameter in there where clause ex select where case when parameter1 is null then field2 parameter2 when parameter2 is null then field1 parameter1 end know this is something simple or maybe im using my logic wrong any help or direction is appreciated
140493 on clustered windows r2 server with sql server enterprise just upgraded an instance from sp1 cu4 to rtm and now getting this error when attempting to start the sql server agent sql server agent log microsoft sqlserveragent version x64 unicode retail build process id the sql server agent startup service account is domain username sql server does not accept the connection error waiting for sql server to allow connections operation attempted was verify connection on start unable to connect to server servername instancename sqlserveragent cannot start sqlserver error sql server network interfaces error locating server instance specified xffffffff sqlstate odbc error login timeout expired sqlstate hyt00 sqlserver error network related or instance specific error has occurred while establishing connection to sql server server is not found or not accessible check if instance name is correct and if sql server is configured to allow remote connections for more information see sql server books online sqlstate logon to server servername instancename failed disableagentxps sqlserveragent terminated normally windows application log sqlserveragent could not be started reason unable to connect to server a08sql edi edi sqlserveragent cannot start the agent starts and runs for about seconds then dies with the above error anyone run into this issue and do you know how to resolve it
140558 ive been digging around in the adventureworks2012 database and see row guid used in several tables there are parts to my question when should include row guid column what are the uses and benefits of row guid column
140568 im trying to find my listener ora file to edit it but the docs say its in the oracle home directory but where is that im running on windows server
140576 have this code that sums up the qty for certain item itemid and by its product date code proddte select sumqty itemid proddte from testtable where group by itemid proddte what want to do is to get the total of all qty regardless of itemid proddte have tried select sumqty itemid proddte sumqty over as grandtotal from testtable where group by itemid proddte but it says should also have qty in the group by clause if did that the result will not be equal to my expected result it does not absolutely need to be represented as separate column with the same value in every row any representation is accepted as long as can display the overall total
140604 introduction in order for this question to be useful for future readers will use the generic data model to illustrate the problem face our data model consists of entities which shall be labeled as and in order to keep things simple all their attributes will be of int type entity has following attributes and entity has following attributes and entity has following attributes and since all entities share common attribute have decided to apply type subtype design important entities are mutually exclusive this means that entity is either or or problem entities and have yet another common attribute but this attribute is not present in the entity question would like to use the above described characteristic to further optimize my design if possible to be honest have no idea how to do this nor where to start trying hence this post
140632 have been researching the concept of rowguids recently and came across this question this answer gave insight but has led me down different rabbit hole with the mention of changing the primary key value my understanding has always been that primary key should be immutable and my searching since reading this answer has only provided answers which reflect the same as best practice under what circumstances would primary key value need to be altered after the record is created
140756 how would get the names of all the columns in table that are of specific type such as datetime even better how can do the same thing but with multiple tables joined together and then list the column name along with the table it comes from
140790 someone was reviewing my ddl code for creating tables and suggested when they saw saw using varchar256 fields for text expect to be pretty small like first name or whatever that should always just use varcharmax and linked why use anything but varcharmax read it but it seemed dated as it was focusing on and didnt seem to offer any real justification to allocate potentially up to gb per row on all text fields from performance storage etc standpoint how should one go about deciding whether to use varcharmax or smaller more specific type for modern versions of sql server
140896 this is my first dba se post so please inform me of any mistakes thanks am new dba not an it pro just no one else in the company to do it so the more basic the explanation the better have been reading about database backup strategies or as have learned to call them restore strategies understand what full differential and transaction log backups do but want to know why differential backup can only be based on the most recent full backup if differential backup is everything that has changed since the last full backup then why cant the differential be based off of any backup of my choosing to be more clear im asking about specifying the base when the backup is taken not when restoring am assuming that when restoring you would choose the correct base and corresponding differential to perform the restore not using differential made from base to restore from base what is the reason that prevents this functionality from being possible figure that there must be reason just dont know what it is note understand that the base cannot be specified but my question is why not im also not interested in discussion about why would you analogy heres an analogy for how understand differential backup have an excel file with some data in cells on day make copy of this file and store it somewhere else the full backup on day look at the file and compare it to the backup copy that made on day and note all the cells that have changed and what their new values are differential backup am not noting every change made to cell only what its final value is if cell a1 started as alfred changed to betty charlie then dave would only note that a1 is now dave on day compare the current file with the backup file again and note the changes another differential backup with the same base as day again only noting final values per cell at the time observed not all values that the cell has been throughout the day on day compare again and note changes again continuing with cell a1 now it says sarah even if it was other names throughout the day and all note is now a1 is sarah on day my file gets messed up so look at the backup copy that made on day then the final states noted on day and apply the changes noted to the backup copy and now have the file restored to how it was on day so look at the backup made on day see that on day cell a1 ended as sarah and change the backup cell a1 to be sarah why would it matter if had made another backup copy full of the file on day why wouldnt it still be possible to compare read take differential backup of the file on day or with the copy made on day as understand it sql server would require me to compare when taking another differential backup to full backup made on day if one had been made no other option
140909 how can use coalesce to check for all variables with different type precedence to all be null without encountering errors declare startdate datetime null enddate datetime null statecode char2 null countycode char3 null producername varchar64 null taxid varchar9 null farm int null select case when coalesce startdate enddate statecode countycode producername taxid farm is null then yes else no end output yes adding select statecode gives this error msg level state line conversion failed when converting date and or time from character string this is valid select case when coalesce startdate enddate is null and coalesce statecode countycode producername taxid is null and farm is null then yes else no end id love to know if theres another clever way to accomplish this in terse manner so dont have to remember to separate by type family
140943 ive run dbcc checkdb and it found allocation errors when run it with repair allow data loss will it need to take the same amount of time to scan for errors or will it attempt to use the last checkdb ran and go faster sql server yes its ancient no cant upgrade
141092 there are lot of trace flags out there some are well documented some are not and others found their way to default behavior status in the release aside from official support channels microsoft employees etc what are ways to find new trace flags ive read through couple recent posts by aaron bertrand here and here but didnt spot anything about new trace flags copied the data and log file of mssqlsystemresource to new location and attached it like regular database to poke through system tables and views but didnt spot anything immediately considered taking list of known trace flags and looping through numbers not on that list to see which ones dbcc traceon would allow but wanted to ask the question here first assuming that the dbcc command to enable them has to check in with some resource to make sure the trace flag is valid where does it reach out to is there dll or some other system file that holds list know the question casts wide net but what spurred this was reading about trace flag with specific intended behavior alongside new feature in that was not having the described effect my initial thought was that perhaps the numbers were transposed somehow like becoming was hoping to get list of valid trace flags within range say to look for permutations testing them all both as dbcc traceon flags and startup parameters would be quite nuisance combined with testing the results against the feature behavior
141129 say have the following schema and data create table images id int not null insert into images values1 want to perform query like select id from images where id not exists in4 but this doesnt work the case above should return since it doesnt exist in the table records
141175 in sql server have partitioned one of my large tables weekly and defined sliding window scenario to switch the oldest weeks data to the archive db and create new partition for the next week this is the result this is for an avl system vehicle tracking have partitioned on positiondate datetime all our queries have positiondate in the where clause and in many cases we have vehicleid in the where clause too so created two aligned indexes on vehicleid int one on positiondatevehicleid one on just vehicleid but in every query that contains vehicleid in its where clause neither of these two non clustered indexes is used according to the query plan have performance problem now compared the query plans between partitioned and non partitioned table for queries like below select from mynonpart table where positiondate between and select from partitinedtable where positiondate between and and awesomely see that the first query costs but the second have one file group with two files for the partitioned table my questions is the number of rows in each partition more than the optimal number of rows for partitioning if partition by day and hold last days data live will that help me improve performance are my non clustered indexes well defined or should remove them we have positiondate in the where clause of all queries and vehicleid in many of them am misusing partitioning for this scenario if define good indexes on my non partitioned table and move oldest data more than month old to the archive table will that work well for my case ddl for my indexes alter table dbo mytable add constraint pk primary primary key clustered positiondate asc id asc with pad index off statistics norecompute off sort in tempdb off ignore dup key off online off allow row locks on allow page locks on go create nonclustered index nonclusteredindex vehicleid on dbo mytable vehicleid asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on go create nonclustered index ncix vehicleid positiondate on dbo mytable vehicleid asc positiondate asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on go this is an example my queries are in sps that receive datetime type parameters
141239 have an azure asp net mvc web application which uses sql azure db it was working fine for some months someday for some reason azure changed the name of the server and had to set that again but everything works fine but now it does not work first thought they may have changed the server name again but opened my ssms and after resetting the password it connected again so change that password in my application connection strings and test everything worked and yes debug and tests work fine so published but once published the application throws error login failed for user my username check on this post azure sql database login failed for user in application but works fine in ssms which seemed similar but can not do that without getting errors changed my connection string from this server tcp server name database windows net1433 database my db user id my username qtdhcrrxmi password my password encrypt true trustservercertificate false connection timeout to this server tcp server name database windows net1433 initial catalog my db persist security info false user id my username password my password pooling false multipleactiveresultsets false encrypt true trustservercertificate false connection timeout as it is shown in the azure portal in the database connection strings only removed the attribute data source server name database windows net also checked that publish procedure does not change connection strings but it still works on debug and on ssms but not on production
141464 im trying to determine which to apply and when is the appropriate time to apply compression im posting this question to get insight from this community have read several articles but wanted to have place where this addressed in db administrators
141527 am trying to define way of working with test database in sql server for our integration testing my idea was to do these steps at the launch of the integration test assembly create totally empty database run the create database objects script to create all relevant database objects table views sequences etc fill the base data lookup values etc take database snapshot called db basis as the base line for future integration tests now before every test class containing tests was planning to simply do restore from snapshot to get back to the well defined more or less empty state of the database works like charm so far however there are set of integration tests that need to operate on large test database so was hoping to do this before each of those test fixtures classes with individual tests restore database from the db basis snapshot insert those rows of data into the database create another snapshot db with testdata snapshot and then for each test reset the database to the well defined db with testdata snapshot version run the tests verify the outcome and so forth trouble is cannot seem to have two db snapshots at the same time once do cannot restore my database to either of them keep getting this error msg level state line database cannot be reverted either the primary or the snapshot names are improperly specified all other snapshots have not been dropped or there are missing files msg level state line restore database is terminating abnormally is that really how sql server database snapshots work seems awfully restricting would understand if couldnt go back directly to the original db basis snapshot maybe but just because now have two snapshots cannot even go back to the most recent one
141594 the query select yearsummarksummaxmark from table1 group by parameterno yields an error message column table1 year is invalid in the select list because it is not contained in either an aggregate function or the group by clause
141789 have configuration table in my sql server database and this table should only ever have one row to help future developers understand this id like to prevent more than one row of data being added have opted to use trigger for this as below alter trigger onlyoneconfigrow on dbo configuration instead of insert as begin declare haszerorows bit select haszerorows case when count id then else end from dbo configuration if existsselect id from inserted and haszerorows begin raiserror you should not add more than one row into the config table end end this does not throw an error but is not allowing the first row to go in also is there more effective more self explaining way of limiting the number of rows that can be inserted into table to just than this am missing any built in sql server feature
141804 recently inherited an environment with around databases distributed between instances on huge physical servers databases are distributed between different instances based on the name so some instances are few times bigger than others schema is identical for all databases but data is not some databases are just few mb and other are gb management is considering whether we should try to keep as many databses as possible on each instance and have fewer huge instances another idea is to keep adding small instances to existing environment or even reduce the number of databases in each existing instance and increase the number of instances administration wise its probably easier to have fewer big instances than many small instances on the other hand if something happens to big instance then all databases on it will be affected so it may be safer to have many small instances what would be better from the performance point of view what other factors should take into account how many databases should aim to keep on each instance know that it depends but do you have some rough estimates for example is databases on one instance too many if you need any more information please let me know forgot to mention that all these databses are production databses of different clients and so all of them are equally important development testing etc are on different servers
142080 im on sql server and having blast my db mail isnt sending and im running out of places to look double checked the sql account permissions to the dbmail executable it has read and execute entered rule for the firewall outbound port tried another mail account and profile with the same unsent issues the only entries in the logs db mail logs are starting and ending of the service there are no errors anywhere that can find the emails appear to simply enter the send queue and never leave it the accounts can send and receive email on their own and from sql server instance on another machine ive got queue of items with sent status unsent and checked all the normal places with expected results in all of them aside from long queue of unsent mail select from msdb sysmail event log order by log id desc select from dbo sysmail mailitems select from dbo sysmail sentitems use msdb select sent status from sysmail allitems select is broker enabled from sys databases where name msdb execute msdb dbo sysmail help status sp have tried turning it off and on again so did miss dmv etc that could shed light on this situation is this known issue with sql server that havent in my searches any other possible steps to get this mail sent
142139 is it possible to retrieve the same data as the following with single seek or scan either by modifying the query or influencing the optimizers strategy code and schema similar to this are currently on sql server repro script setup use tempdb go if object iddbo testupload is not null drop table dbo testupload create table dbo testupload jobrunid bigint not null thinganame nvarchar255 not null thingatype nvarchar255 not null thingagranularity nvarchar255 not null thingbname nvarchar255 not null thingbtype nvarchar255 not null thingbgranularity nvarchar255 not null create clustered index ix jobrunid on dbo testupload jobrunid go insert into dbo testupload jobrunid thinganame thingatype thingagranularity thingbname thingbtype thingbgranularity values go insert into dbo testupload jobrunid thinganame thingatype thingagranularity thingbname thingbtype thingbgranularity values go query declare jobrunid bigint select jobrunid thinganame as name thingatype as type thingagranularity as granularity from dbo testupload where jobrunid jobrunid union select jobrunid thingbname as name thingbtype as type thingbgranularity as granularity from dbo testupload where jobrunid jobrunid tear down if object iddbo testupload is not null drop table dbo testupload think this is probably not modeled ideally im trying to get more information from the developer about how the schema was chosen but am curious if theres tsql trick im overlooking as it will be easier to change the query than the schema
142282 am designing database for an exam and am stuck do not know how to do it here is the relevant information student answers questions each question has options answers and student chooses one only one answer is correct other are wrong students can take this exam only once they can not try again there will be no other exams this is the only one need help with database design what have tried so far on my own table students id bigint primary key identity name nvarcharmax table questions id bigint primary key identity textofthequestion nvarcharmax table answers id bigint primary key identity textoftheanswer nvarcharmax questionid bigint foreign key to questions id iscorrectanswer bit table studentchoices studentid bigint primary key foreign key to students id answerid bigint primary key foreign key to answers id this is my personal design for learning purposes am trying to learn entity framework on my own
142305 im just wondering what happens if you begin transaction in db and forgot to commit or rollback it will the server be down lets say you left it for days there are also users who are using it assuming that the other users did not know that there is an unclosed transaction lets just assume that the users are just inserting data on the database what are the consequences of this action
142330 have two tables table id name and table id lets say on oracle 12c why does this query not return an exception select from table where name in select name from table from what understand oracle sees this as select from table where name name but what dont get is why
142331 see in the documentation that the database backup tools are divided into four categories hot backups cold backups physical backups and logical backups understand the the most important difference between cold and hot backup is that the latter can be done while the database is operating and receiving read write queries and the result will be consistent and atomic but then how do know if mysqldump has this hot aspect or cold aspect the documentation just places it in the logical backup category and is not clear about this yes it seems to mention that it uses the same snapshot of the db at the beginning of the mysqldump procedure which suggests its definitely hot backup tool but just wanted to double check here the tables use innodb engine
142552 am needing to find way to sum all of the positive values for num and return the sum of all positive numbers and an individual row for each negative number below is sample ddl create table be id int salesid int num decimal164 insert into be values and this is my desired output positive numbers for each salesid sum and negatives get an individual line returned salesid num
142714 in sql server or rdbms systems in general is there limit to the number of concurrent queries that can be made against table how is it that number of users can access the same physical data at the same time update actually more generally forgetting about limits how is it that two users can read from table at the same time can someone explain this from technical perceptive if data is stored in physical space on disk how is it that two users processes can query that same physical data at the same time
142825 lookup tables or code tables as some people call them are usually collection of the possible values that can be given for certain column for example suppose we have lookup table called party meant to store information about political parties that has two columns party code idn which holds system generated numeric values and lacking business domain meaning works as surrogate for the real key party code is the real or natural key of the table because it maintains values that have business domain connotations and let us say that such table retains the data that follows party code idn party code republican democratic the party code column which keeps the values republican and democratic being the real key of the table is set up with unique constraint but optionally added the party code idn and defined it as the pk of the table though logically speaking party code may work as the primary key pk question what are the best practices for pointing to lookup values from transaction tables should establish foreign key fk references either directly to the natural and meaningful value or to surrogate values option for example candidate idn party code city democratic alaska republican memphis has the following properties1 readable for the end user easy to import export across systems difficult to change the value as it needs modification in all referring tables adding new value is not costly think it is almost like pass by value to draw an analogy from function call in application programming jargon option for instance candidate idn party code idn city alaska memphis has the properties below not readable for the end user difficult to import export as we need to de reference it easy to change values as we are only storing references in transaction tables adding new value is not costly it is very similar to pass by reference if comparing to function call in app programming parlance import export can also be done in different way just by populating the look up table again and then re seed the surrogate column hope am getting this right this is something have just heard as possibility note that and indicate the benefit of those properties question quite importantly is there difference between lookup or code table and fk reference if we are just going to use the latter approach think they work just the same related resources how important are lookup tables best practices with code or lookup tables lookup tables best practices db tables or enumerations
142960 have ms access frontend that would like to install on computers so they can access sql server stored on shared network drive if possible is it safe and can my data be corrupted otherwise how have users access the sql server simultaneously by using an interface thank you very much for your time
143008 have table like this cookies id user id token expire dki3j4rf9u3e40 erhj5473fv34gv 8u34ijf34t43gf few days that think about it guess dont need to id column currently id column is pk and also have an unique index on token column to make it fast for searching now want to know can remove id column and make token column as pk is that normal to be honest ive never created table without id column its always been the pk so thats weird for me to choose token column as the pk
143132 have xml structure which is input param for my stored procedure it contains element with which is escaped in xml when extract that element to varchar receive which is not valid xml character need to escape it before converting to xml again how to do that without replace have the following text param1 xyz para2 dasdasdfdas param3 it is part of query string converted it to xml and send it as part of xml structure zzz xmlns http example com aaa aaa aaa bbb param1 xyz amp para2 dasdasdfdas amp param3 bbb zzz inside the stored procedure need to extract it do that with isnullnullifltrimrtrim xmlinput valuedeclare default element namespace http example com zzz bbb nvarchar250 after that the value contains normal text are not escaped after some processing need to put that string inside other xml make cast as xml because are not escaped got error
143157 need help to deal with problem about permissions in my database this is the scenario database schemas schemaa owner dbo schemab owner ownerx schemac owner ownerx schemad owner ownerx in schemad have view named viewabc that gets information from tables and views allocated on schemas schemaa schemab schemac user userx have select permission for viewabc with the following command grant permission grant select on schemad viewabc to userx when userx try to execute select against the view this way select from schemad viewabc we get the error the select permission was denied on object tablea database mydatabase schema schemaa do understand that the error happens because tablea is on different schema with different owner dbo instead ownerx and sqlserver applies userx permissions to grant or deny access once userx don have explicit access to tablea the query execution returns error how to solve this without grant select permission on tablea for userx
143342 im in the process of designing new system for large geospatial data set that will require rapid read query performance therefore want to see if anyone thinks it is possible or has experience advice about suitable dbmss data structure or alternative methods to achieve the required performance in the following situation data will be continuously produced from processed satellite radar data which will have global coverage based on the satellite resolution and land coverage of the globe estimate the full data set to produce values at billion discrete locations on the globe over the life span of single satellite the output will produce up to values at each of these locations so total data set of trillion values this is for one satellite and there is already second in orbit with another two planned in the new few years so there will be lot of data single data item is very simple and will only consist of longitude latitude value but due to the number of items estimate single satellite to produce up to 100tb the written data should never need updating as it will only grow as new satellite acquisitions are processed write performance is not important but read performance is crucial the goal of this project is to be able to visualize the data through simple interface such as layer over google maps where each point has colored value based on its average gradient or some function over time demo at end of post from these requirements the database needs to be scalable and we are likely to look towards cloud solutions the system needs to be able to deal with geospatial queries such as points near latlon and points within box and have read performance of 1s for locating single point and polygons which contain up to points although up to points would be preferable so far have test data set of million data items at million locations ive trialed postgres postgis instance which worked ok but without the possibility of sharding dont this this will be able to cope as the data grows have also trialed mongodb instance which again appears to ok so far and with sharding it might be sufficient to scale with the data volume ive recently learnt little about elasticsearch so any comments on this would be helpful as it is new to me here is quick animation of what we want to achieve with the full data set this gif from my postgres trial is serving 6x3 pre computed raster tiles each containing points and taking 17s to generate each by clicking point the graph is made by pulling all historic values at the nearest location in 1s apologies for the long post all comments advice are welcome
143601 just tried installing sql server enterprise but noticed that management studio was not installed by default also noticed there is separate link to ssms in the installation guide which points to https msdn microsoft com en us library mt238290 aspx what was the reason for this decision the following is noted on the above url this generally available release of ssms is free and does not require sql server license to install and use perhaps this is it
143861 went through at least question on se but must be doing something very bad out of frustration already have stored procedure that returns an xml like this pmsg messages pmsg message info blah pmsg messages need to get the value of info declare xmlmessage xml exec procreturnsxml xmlmessage output returns rows with xmlnamespacespmsg as pmsg select msgs msg value info nvarcharmax from xmlmessage nodespmsg messages pmsg message msgsmsg returns null with xmlnamespacespmsg as pmsg select xmlmessage value pmsg messages pmsg message info nvarcharmax returns null with xmlnamespacespmsg as pmsg select value pmsg messages pmsg message info nvarcharmax as info from xmlmessage nodes tx generally dont understand what these mean at all can you provide an example exactly on this case please
143941 have created job which will sets up its start date and end date automatically with dynamic value for example job will run for eight days and at eighth day it needs to automatically deletes itself irrespective of its status is there way to code it while creating or scheduling
144096 need to assign db owner to all users within an instance there are few hundred logins and same number of databases within the instance would like each user to get db owner within the database where that user exists know how to do it for individual users but do not know how its done for all of them at once am well aware that it is not good idea to give db owner to everyone but we are installing very specific application every users who wants to use this application will need db owner we already tried walkarounds but the aplicaton is rather primitive in nature so until application code is changed we will have to tolerate insane number of db owners there is no sensitive data on this particular instance anyway so we have management approval
144100 ok ive got monitoring process that runs the following query to make sure the databases are backing up regularly this process used to take seconds for connection query execution and everything now the process is timeing out and if run the query in smss it takes over minute select name as database name recovery model bs1 last backup bs1 last duration from sys databases left join select bs database name datediffhhmaxbs backup finish date getdate as last backup datediffmimaxbs backup start datemaxbs backup finish date as last duration from msdb dbo backupset bs where bs type group by bs database name bs1 on name bs1 database name order by name now the sub select by itself still takes less than second and returns about rows the msdb dbo backupset table has 500k rows the sys databases table has about only about rows so im lost as to why this query now takes minute to run the problem started after the drive was full for few minutes so thought maybe an index for corrupted so ive rebuilt all the indexes including pk on the msdb dbo backupset table cant find the sys databases table so im guessing its virtual table or something looked at the estimated execution plan but that stuff is mostly over my head any suggestions
144214 having the table with columns id category flag want to select all rows that have flag at least once per category expected results id category flag it can be solved using temporary table like this select id into temptable from sometable where flag select from sometable join temptable on sometable id temptable id but id prefer solution with grouping which struggle to came up with any help will be appreciated
144591 my team and have an issue where we need to be able to identify processes that are blocking other processes and kill them there are ton of scripts that are freely available to perform these actions we have tried various ones and scrutinized the code if you are someone that has posted one of these queries thanks before go further let me tell you that this is against vendor application that we cannot modify the vendor is also not willing to spend the time to figure out why processes are getting blocked the only choice we have at this point is to kill the long running processes as suggested by the vendor support before killing these processes we do review the query running but of the time it shows up as fetch api cursor00000000000a7e1f which tells us nothing this has been manual process up to this point now we want to automate the killing of these long running blocking processes instead of someone manually killing them we want to test this script before putting it into production we would like some help creating script that would intentionally create blocking processes we have tried with the test environment with this application but unfortunately we have had no luck replicating blocking processes thanks in advance for your assistance
145163 productive work has crawled to stop while we try and figure out what the red thing on the cover of microsoft sql server internals is we think its surgical or electrical tool of some sort thoughts
145204 while comparing the execution plan of stored procedures on the second one get warning sign marked by the red arrow what does it mean
145665 our dba is out and im trying to back up our remote dev database to unc path im running the back up operation while logged into management studio as particular windows domain account corp myuser ive checked that this domain account has full control rights the unc path nameofmachine backup when attempt to execute the following as corp myuser backup database dev to disk nameofmachine backup dev bak with copy only init receive the error cannot open backup device nameofmachine backup dev bak operating system error 5access is denied which seems to indicate that whatever account sql server is using to run the backup command does not have rights to that unc path do not have way to rdp to the dev database server and find out which account sql server is using to execute the backup command above is there way to find this out from management studio using dynamic management view
145982 is there any difference between updating the statistics of table using sp updatestats with out resample and updating stats of table using update statistics without sample optionsfullscansample percentresample exec sp updatestats vs update statistics tablename updating the tables using sp updatestats with default value no will update the statistics with default sampling rate similarly updating statistics of table using update statistics without sample optionsfullscansample percentresample will also update the table statistics with default sampling so is there any difference between both methods am missing anything here update know that sp updatestats runs on all the tables but using update statistics we can update statistics of specific table
146011 find am spending lot of time lately examining lists of proposed indexes that were generated by an index tuning adviser or some other automated process back in the day circa found the suggestions to be rather awful in that they seemed geared to one specific query and more often than not were merely covering index that had every column in the table whether or not those colums were already indexes as time has progressed feel the index tuning advisers have improved dramatically but still have built in mistrust and still want to spend the time to review each one am trying not to be pessimistic but it is difficult when vendor microsoft suggests to add multiple overlapping covering indexes to improve performance especially when we are paying for storage space also dont want to spend my own time needlessly would rather be fixing poorly written queries
146543 have need to increase wal keep segments on our master server can do that on the fly or does it require restart
146610 faced strange phenomenon today ran the below query to find all requests with waits other than broker receive waitfor select from sys dm exec requests where session id and wait type not like broker receive waitfor the query ran fine but it failed to list any request with no waits ie with wait type as nullthere were quite lot of running queries without any waits at this time why is this happening isnt it supposed to list all records with wait type not like broker receive waitfor including nulls im running sql server 2008r2 sp3
146750 have create the following table create table dbo teststructure id int not null filler1 char36 not null filler2 char216 not null and then created clustered index create clustered index idx cl id on dbo teststructureid next populated it with rows each size is byte based on table declaration declare as int while begin set insert into dbo teststructure id filler1 filler2 values end now based on information read in training kit exam querying microsoft sql server itzik ben gan book sql server internally organizes data in data file in pages page is an kb unit and belongs to single object for example to table or an index page is the smallest unit of reading and writing pages are further organized into extents an extent consists of eight consecutive pages pages from an extent can belong to single object or to multiple objects if the pages belong to multiple objects then the extent is called mixed extent if the pages belong to single object then the extent is called uniform extent sql server stores the first eight pages of an object in mixed extents when an object exceeds eight pages sql server allocates additional uniform extents for this object with this organization small objects waste less space and big objects are less fragmented so here have the first mixed extent 8kb page populated with bytes have inserted times byte size row so to check the size have run size check proc it returns the following result index type desc clustered index index depth index level page count record count avg page space used in percent name teststructure rows reserved kb data kb index size kb unused kb so kb are reserved for the table first kb page is for root iam page the second one is for leaf data storage page which is 8kb with occupation of kb now when insert new row with byte insert into dbo teststructure id filler1 filler2 values it is not stored in the same page although it have space of byte which is still smaller than 8kb new data page is created but that new row could be fit on the same old page why does sql server create new page when it could save space and searching time buy inserting it in the existing page note the same thing is happening in heap index
146993 need to rebuild the indexes in two very large almost table database but want to prevent users from accessing the database while this happens is there state can put the database in that prevents the users from accessing it but still allows me to rebuild the indexes for reference plan on the script provided at http instadba com quick script to defragment your sql server indexes to do the rebuild
147039 am querying data from linked server through view on the origin server the view has to include couple of standardized columns such as created modified and deleted but in this case the table on the source server doesnt have any suitable info the columns are therefore explicitly cast to their respective types updated the view changing column from null as modified to castnull as datetime as modified however after performing this update the view is triggering the following error message msg level state line cannot get the current row value of column user generated expression expr1002 from ole db provider sqlncli11 for linked server we have done this explicit cast change generally across the origin server without worries and suspect the issue might be related to the version of the servers involved we dont really need to apply this cast but it feels cleaner right now im just curious as to why this is happening server version origin microsoft sql server x64 may copyright microsoft corporation enterprise edition bit on windows nt build service pack hypervisor server version linked microsoft sql server r2 sp1 x64 jun copyright microsoft corporation enterprise edition bit on windows nt build service pack hypervisor edit just realized made mistake by not posting all the columns in question and must apologize for leaving out an important detail dont know how didnt notice this sooner the question still remains though the erroneous cast does not happen with the cast to datetime but with column being cast to uniqueidentifier this is the culprit castnull as uniqueidentifier as guid uniqueidentifiers are supported on sql server r2 and as mentioned in the comments the query performed by the view runs fine on the linked server
147112 im allowing the end user to define how many rows are returned by query select top is there value that can be entered where all rows are returned or do have to dynamically create the query without the top if they want all rows returned im using sql server
147775 we had sql server 2008r2 enterprise edition for our database to support front end application we never had any timeout issues before recently the company decided to upgrade the database in to sql server enterprise edition with node always on cluster setup the new server has better cpu memory than the old server after the upgrade made all the necessary modifications checking database consistency run dbcc updateusage update statistics index rebuilding recompiling the stored procedures and so on the database switching and migration all went well however our users start complaining about timeout issues have been reviewing different articlesblog posts and made some modification like changing the connection string and add multisubnetfailover true which seems helped lot and minimize the frequency of timeout but still the issue is there does anyone know what can cause this issue and how to resolve it would highly appreciate your suggestions and recommendation to resolve this problem
147976 consider the following two ways to convert datetime varchar string into date field select convertdate only date is needed select cast2012 as date only date is needed both return what expect the date excluding the time in as date datatype my question is is there any pro and cons of doing either way
148386 ive just upgraded our data warehouse to sql im seeing some really interesting graphs in the query store love this feature below is the weirdest example ive seen plans for the same query its making me consider performance tuning of my etl process and the pros and cons of temporary tables and how you could influence execution plan behavior my etl process uses number of stored procedures which use mix of standard and temporary tables as staging tables the tables are typically used once and then dropped some are only few thousand rows some are millions ssms advises that there are missing indexes but on smaller tables would they make enough of difference to be worth the effort of adding them are better statistics sufficient ive just read this brent ozar blog post about statistics on temp tables and paul whites article on temporary tables in stored procedures it says that statistics are created automatically when the table is queried and then presumably used by the optimizer my questions are is there much point or benefit in creating an index on table and or is it worth explicitly updating statistics as step in the stored procedure before using it in queries given theyre only used once are the additional steps and overhead worth it would it result in significantly better or different execution plans
148395 weve been using mongodb for several weeks now the overall trend that weve seen has been that mongodb is using way too much memory much more than the whole size of its dataset indexes ive already read through this question and this question but none seem to address the issue ive been facing theyre actually explaining whats already explained in the documentation the following are the results of htop and show dbs commands know that mongodb uses memory mapped io so basically the os handles caching things in the memory and mongodb should theoretically let go of its cached memory when another process requests free memory but from what weve seen it doesnt oom kicks in an starts killing other important processes postgres redis etc as can be seen to overcome this problem weve increased the ram to 183gb which now works but is pretty expensive mongos using 87gbs of ram nearly 4x of the size of its whole dataset so is this much memory usage really expected and normal as per documentation wiredtiger uses at most of ram for its cache but considering the dataset size does it even have enough data to be able to take 86gbs of ram even if the memory usage is expected why wont mongo let go of its allocated memory in case another process starts requesting for more memory various other running processes were being constantly killed by linux oom including mongodb itself before we increased the ram and it made the system totally unstable thanks
148443 after updating to release my ssms keeps on disconnecting automatically and brings up the connect to database engine pop up screenshot below additionally keep on getting the system outofmemoryexception when run queries even though my machine has 16gb ram and is only currently using about 4gb these errors happen with simple queries like select getdate any suggestions on what could be the issue
148523 am trying to understand how sql server try to estimate for greater than and greater than equal to where clauses in sql server think do understand the cardinality estimation when it hits the step for example if do select from charge where charge dt the cardinality estimation is which can be easily calculated as 32eq rows 6624range rows eq rows histogram in below screenshot but when do select from charge where charge dt increased the time to so its not step the estimate is how is that calculated
148641 im trying to build history table out of an audit log ultimately to build out type dimension table unfortunately the audit log only records the specific fields being changes heres rough example of what im talking about create table staff id int surname varchar5 firstname varchar4 office varchar9 date varchar10 insert into staff id surname firstname office date values smith bill melbourne null null sydney brown mary melbourne jones null adelaide null null sydney null null perth the first entry for particular staff member is for when their record is created and each subsequent record is an update but only shows the update to the field that was updated want to fill out the update row with the rest of the employee record as it currently stands ie result like this smith bill melbourne smith bill sydney brown mary melbourne jones mary adelaide jones mary sydney jones mary perth know can do this using while loop or cursor but suspect there is probably more performant option null always means value didnt change rather than value changed to null
148665 when applying service pack to production sql server usually have planned downtime window of around minutes having been bitten before and discovering that required precondition like patch is not in place that can add several minutes to your downtime window and maybe taking you over the window or forcing reschedule now when the update is in planning stage move the service pack to the server and test it through check files in use or ready to update cancel the update at that point and feel as confident as can that during the update will not run over the scheduled downtime window you can cancel from either screen but as the update button on ready to update is in the same location as the next button on check files in use an accidental double click could accidentally start an update while validating my question should stop validation at check files in use or ready to update have validated everything can by the end of check files in use does going to ready to update add value to the validation
148784 my table looks like this groupid int not null somevalue int not null id like to keep rows with the same groupid together in the result set the groups themselves should be ordered randomly though somevalue is supposed to be the secondary sort criterion like this groupid somevalue order by groupid somevalue does not order groups randomly orderby newid is totally random order by somehashgroupid somevalue comes to mind but need new random order each time how can this be done
148788 when run the following query select engines manufacturer model maxseats from planes group by engines am getting the correct engines and seats results but not the correct manufacturer model combination in addition there are multiple rows with the same maximum value for seats number which need but just getting one result by engines seats have viewed other stack exchange posts and elsewhere but cannot seem to find good solution to fix the query any advice
148858 am curious how this statement updated rows in postgres all the other times ran it it would update or is there way to find out which rows bestsales update keyword set revenue random where id castrandom as int update id is the primary key id integer not null default nextvalkeyword id seq regclass keyword pkey primary key btree id tried to run it as select bestsales select from keyword where id castrandom as int id keyword seed id source search count country language volume cpc competition modified on google violation revenue bing violation vizio m190mv google shiatsu massage mat spyfu granary flour spyfu rows and sometimes it would return more than one how is that possible postgresql
148876 have function that ends with insert into configuration dates cols values values returning id into ret id return ret id and would like to remove the into ret id part and instead do something like return insert into configuration dates weekly date configuration id from to price activity configuration id values wdc id from ts from ts wdc duration wdc price wdc activity configuration id returning id but havent found how to achieve this edit adding entire function create function make date from configurationwdc id uuid from date date returns integer as declare from dow int extractisodow from from date day of the week wdc weekly date configurations from ts timestamp ret id integer begin select into wdc from weekly date configurations wdc where id wdc id if found and wdc valid through from date starts in the date range and get bitwdc weekdays from dow valid day of the week then from ts from date wdc start time insert into configuration dates weekly date configuration id from to price activity configuration id values wdc id from ts from ts wdc duration wdc price wdc activity configuration id returning id into ret id return ret id else return null end if end language plpgsql
149459 need to troubleshoot an issue am having and need some help understanding how sp msforeachdb works in order to overcome my issue what happens is every time run sp msforeachdb get an error msg level state incorrect syntax near an example of my code is as follows exec sp msforeachdb select as database from sys objects where name like aetna however it does not matter what query have as parameter to sp msforeachdb every time get the same error do have database that starts with 61s1d so that makes me think it has an issue with the db name but honestly do not know what is going behind the scenes on sp msforeachdb things to note it is the only database that starts with number can try to use code like if database is like dont do but still the same error cannot test changing the database name too many things connected to it if create test db that starts with then also get the error for that database how can overcome this
149630 have table with column named prefix storing aaaa my application has long string eg aaaabbbbccccdddd want to select all rows in which particular column is prefix of aaaabbbbccccdddd the length of the prefix is variable how can do it in postgres tried select from my table where prefix ilike aaaabbbbccccdddd but it does not match
149796 have query that is used to search for list of sub divisions in our network when the query is executed without any filter it returns different amount of rows on every execution select distinct t1 zresubdiv t1 subdiv name from stage wp pfunct loc t1 where rownum normally filter goes here order by t1 subdiv name execution zresubdiv subdiv name alexandria allanwater ashcroft bala beachburg bridge caramat central butte chapais chatham chicago execution zresubdiv subdiv name albreda alexandria allanwater ashcroft bala beachburg blackfoot bridge camrose caso central butte there is also missing data the subdivision baton rouge never appears although it is in the data if remove the rownum all data is correctly returned need to leave rownum since the query support where clause with filter on the subdivision name what is the cause of this
149811 in the queries below both execution plans are estimated to perform seeks on unique index the seeks are driven by an ordered scan on the same source table so seemingly should end up seeking the same values in the same order both nested loops have nestedloops optimized false withorderedprefetch true anyone know why this task is costed at in the first plan but in the second the reason for the question is that the first query was suggested to me as an optimisation due to the apparent much lower plan cost it actually looks to me as though it does more work but im just attempting to explain the discrepancy setup create table dbo targetkeycol int primary key othercol char32 not null create table dbo stagingkeycol int primary key othercol char32 not null insert into dbo target select top row number over order by spid leftnewid32 from master spt values v1 master spt values v2 insert into dbo staging select top row number over order by spid leftnewid32 from master spt values v1 query paste the plan link with as select from target as where keycol in select keycol from staging as merge using staging on keycol keycol when not matched then insert keycol othercol valuess keycol othercol when matched and othercol othercol then update set othercol othercol query paste the plan link merge target using staging on keycol keycol when not matched then insert keycol othercol values keycol othercol when matched and othercol othercol then update set othercol othercol query query the above was tested on sql server sp2 kb3171021 x64 joe obbish points out in the comments that simpler repro would be select from staging as left outer join target as on keycol keycol vs select from staging as left outer join select from target as on keycol keycol for the row staging table both of the above still have the same plan shape with nested loops and the plan without the derived table appearing cheaper but for row staging table and same target table as above the difference in costs does change the plan shape with full scan and merge join seeming relatively more attractive than expensively costed seeks showing this cost discrepancy can have implications other than just making it harder to compare plans
150158 we can store date and time information in couple of ways what is the best approach for storing datetime information storing date and time in separate columns or one column using datetime can you explain why that approach is better link to mysql docs for reference the question is general not specific to mysql date and time types date and time
150207 if have cte such as the below code how many times does the table people get queried against was under the impression that it was only called time and stored in memory but some of my queries have been running seem to be running lot longer than they should be which leads me to believe that it may be hitting the people table times with ctegeneric as select person from people where person dumb select from ctegeneric union all select from ctegeneric union all select from ctegeneric
151169 the following is an excerpt from book about db design beginning database design isbn the danger with using views is filtering query against view expecting to read very small portion of very large table any filtering should be done within the view because any filtering against the view itself is applied after the query in the view has completed execution views are typically useful for speeding up the development process but in the long run can completely kill database performance the following is an excerpt from postgresql documentation making liberal use of views is key aspect of good sql database design views allow you to encapsulate the details of the structure of your tables which might change as your application evolves behind consistent interfaces the two sources seem to contradict each other do not design with views vs do design with views however in pg views are implemented using the rule system so possibly and this is my question any filtering against the view is rewritten as filter within the view resulting in single query execution against the underlying tables is my interpretation correct and pg combines where clauses into and out of the view or does it run them separately one after another any short self contained correct compilable examples
151183 ive tried every solution on the internet but my mariadb server continue to fail continue to betray me continue to destroy my tiny devops world my attempts to smooth the situation included all sorts of satisfaction changing permissions configs removing log files upgrading reinstalling moving her internal files up and around removing other dbms removing everything except her but she has never been resisting so much for so long my last and only hope for you guys to light the way through such critical moment in our relationships im using vagrant and the problem is in datadir option when use default path everything is ok but when change it to vagrant shared folder maria does not even start have copied all the var lib mysql files to new folder have windows host centos guest and my configurations are mariadb version mysql ver distrib mariadb for linux x86 using readline vagrantfile mode ruby env vagrant default provider virtualbox vagrant configure do config config vm box url https github com tommy muehle puppet vagrant boxes releases download centos x86 box config vm box centos7 config vm network private network ip config vm synced folder mysql vagrant mysql owner mysql group mysql config vm provider virtualbox do vb vb customize modifyvm id memory vb customize modifyvm id cpus vb customize modifyvm id hwvirtex on vb customize modifyvm id audio none vb customize modifyvm id nictype1 virtio vb customize modifyvm id nictype2 virtio end end etc my cnf server cnf mysqld user mysql datadir vagrant mysql socket var lib mysql mysql sock symbolic links default storage engine innodb tmpdir tmp character set server utf8 init connect set names utf8 expire logs days skip external locking key buffer size 32m max allowed packet 32m table open cache table definition cache sort buffer size 16m net buffer length 16k read buffer size 8m read rnd buffer size 8m thread cache size thread concurrency query cache size 1024m query cache limit 2m join buffer size 32m max connections max connect errors connect timeout innodb file per table innodb buffer pool size 2048m innodb read io threads innodb write io threads innodb lock wait timeout innodb flush log at trx commit innodb flush method dsync innodb log file size 64m innodb log buffer size 32m innodb log files in group innodb thread concurrency innodb open files innodb sync spin loops skip name resolve log error var log mariadb mysqld log mariadb error log note innodb using mutexes to ref count buffer pool pages note innodb the innodb memory heap is disabled note innodb mutexes and rw locks use gcc atomic builtins note innodb gcc builtin atomic thread fence is used for memory barrier note innodb compressed tables use zlib note innodb using linux native aio note innodb using sse crc32 instructions note innodb initializing buffer pool size 0g note innodb completed initialization of buffer pool note innodb highest supported file format is barracuda note innodb rollback segments are active note innodb waiting for purge to start note innodb percona xtradb http www percona com started log sequence number note innodb dumping buffer pools not yet started note plugin feedback is disabled error cant init tc log error aborting
151199 in postgresql given simple table created with create table tbl id serial primary key val integer run sql to insert value then update it in the same statement with newval as insert into tblval values returning id update tbl set val from newval where tbl id newval id the result is that the update is ignored testdb select from tbl id val why is this is this limitation part of the sql standard present in other databases or something specific to postgresql that might be fixed in future the with queries documentation says multiple updates are not supported but does not mention inserts and updates
151431 im having an issue with using the new upsert feature in postgres have table that is used for aggregating data from another table the composite key is made up of columns of which can be nullable below have created smaller version of the issue im having specifically with null values create table public test upsert upsert id serial name character varying32 not null status integer not null test field text identifier character varying255 count integer constraint upsert id pkey primary key upsert id constraint test upsert name status test field key unique name status test field running this query works as needed first insert then subsequent inserts simply increment the count insert into test upsert as tunamestatustest fieldidentifier count values shaun1test valueident on conflict namestatustest field do update set count tu count where tu name shaun and tu status and tu test field test value however if run this query row is inserted each time rather than incrementing the count for the initial row insert into test upsert as tunamestatustest fieldidentifier count values shaun1nullident on conflict namestatustest field do update set count tu count where tu name shaun and tu status and tu test field null this is my issue need to simply increment the count value and not create multiple identical rows with null values attempting to add partial unique index create unique index test upsert upsert id idx on public test upsert using btree name collate pg catalog default status test field identifier however this yields the same results either multiple null rows being inserted or this error message when trying to insert error there is no unique or exclusion constraint matching the on conflict specification already attempted to add extra details on the partial index such as where test field is not null or identifier is not null however when inserting get the constraint error message
151453 am using sql server standard edition my objective is to trace the history of operations on the tables insert update delete and to display on the web so the users are able to tell what happened to the data in the past so came to know there is partial support of audit feature in the sql server standard edition and was able to log some operations as attached my question is how to make those logs stored inside proper sql table so that the records could be accessed retrieved and displayed on the web page currently the audit destination allowed file security log application log which dont seem to be able to be retrieved directly from an asp net api is it possible to log into an sql table correct me if am wrong many have said that the standard edition only allows server level audit but not database level audit however for some unknown reasons if added these audit types yup or none am able to capture and view the database level operations
151522 have date dimension table in which need to add new column in which define the iteration of the day of the week within the month for the second mon tue wed thu fri sat sun etc is it possible to do this be making calculations solely on the date column of the table which is of type date
151550 have table with an ntext column called comments have second string lets call it anothercomment varchar that needs placing inside given comments string after the word updatehere casting to nvarcharmax truncates the comments string so cannot use the likes of charindex msg level state line string or binary data would be truncated have used datalength to check that there are few thousand columns that are characters an example of what want to achieve albeit with much longer strings comments this is test updatehere this is the end of the test anothercomment this is inserted resulting string this is test updatehere this is inserted this is the end of the test realize that this is trivial with normal varchar nvarchar but ntext is complete and utter nightmare to work with realize its deprecated data type but did not write the application in question
151691 if run this code select result if run this code select result msg level state line arithmetic overflow error converting expression to data type int and if run the following code select result why am getting this arithmetic overflow error within specific range
151813 lets imagine you have table with this definition create table public positions id serial latitude numeric1812 longitude numeric1812 updated at timestamp without time zone and you have rows in this table now for testing purposes you will run an update like this update positions set updated at now where latitude between and that statement will update rows from the in this specific dataset if you run such query in different threads mysql innodb will succeed and postgresql will fail with lots of deadlocks why im comparing the latest version of mysql innodb vs postgres this is concurrent update case production cases imagine stocks being updated with the latest price available constantly
152301 the problem have client who is running their application on sql server they use sql server analysis services for additional reporting based on the cube thats been provided it turns out there is one query they are looking to run that is written in sql and is fairly complex the cube doesnt give them the data that they are looking for the workaround as the client they dont have access to directly query the database hence they send me this query and every week send them back the results on spreadsheet proposed solution believe would be able to use sql server integration services to work around this whereby can automate the creation of table with the results of said query which they would be able to query using their existing access to analysis services am correct in saying this if so believe need to create an ssis package with certain control and data flows what is the high level structure of what needs to be built in looking so far see control flow task called execute sql task imagine would have to build on this
152488 in permanent discussion with the developers of the company where work because they say it is better to get rid of relationship enforcement via foreign key constraint definitions in relational database in order to speed up large queries and to gain better performance the platform under consideration is mysql and no foreign key has been set up even some primary key constraints of the relevant tables are missing which at least for me is not reasonable maybe they re right and wrong but don have enough arguments to discuss about this situation this has been the preferred approach for three years now new in this company only one month but as the product works there is hesitation to enhance the database nevertheles the first thing noticed is one page taking minute to load yes seconds one of the claims behind the current state of affairs is that denormalized database is faster than normalized one but don believe that true most of the relevant queries include join operations which makes them run very very very slow with large amounts of data the database contains millions of rows commonly the handling of crud operations is implemented at the application program code level for example in order to delete some data from let say tablea it is necessary to first check on the fly if there is some relationship between the rows of tablea and tableb in case that said relationship is detected then the app program code won allow to delete the pertinent rows but if for some reason the app program code fails then the delete operation will succeed no matter if there is any relationship regarding the involved rows and tables question could you help me to elaborate good accurate and solid answer to enrich the debate note maybe something like this has been asked and answered before but couldn find anything by means of google
152501 ive found some execsql statements buried in the code theyre there for good reason because these statements couldnt be written directly however they are an obvious attack vector is there safe alternative to execsome sql something that will be paremetrised correctly including table names in statements
152659 if have an optimize for unknown in my stored procedure am able to see what value the database decided was optimal
153274 scenario table my table has primary key constraint pk my table table my table has also an index called idx pk my table which is enforces the uniqueness for constraint pk my table if disable constraint pk my table and then enabled it back does index idx pk my table get rebuilt
153295 was under the impression that when using the like operator in all optimise for unknown scenarios both the legacy and new ces use estimate assuming that relevant statistics are available and the query optimiser doesnt have to resort to selectivity guesses when executing the below query against the credit database get different estimates under the different ces under the new ce receive an estimate of rows which was expecting under the legacy ce receive an estimate of and cant figure out how this estimate is derived is anyone able to shed any light new ce estimate declare lastname varchar15 ba select from credit dbo member where lastname like lastname forcing legacy ce estimate declare lastname varchar15 ba select from credit dbo member where lastname like lastname option querytraceon querytraceon querytraceon querytraceon in my scenario already have the credit database set to compatibility level hence why in the second query im using trace flags to force the legacy ce and to also provide information on what statistics are used considered by the query optimiser can see the column statistics on lastname are being used but still cant work out how the estimate of is derived couldnt find anything online other than this itzik ben gan article which states when using the like predicate in all optimize for unknown scenarios both the legacy and new ces use percent estimate the information in that post would appear to be incorrect
153392 here is simple table where records may reference parent records in the same table create table foo id serial primary key parent id int null num int not null txt text null foreign key parent id references fooid with the added requirement that one of the other field values num must be identical between parent and child records thought composite foreign key should do the trick changed the last line to foreign key parent id num references fooid num and got error there is no unique constraint matching given keys for referenced table foo can easily add this constraint but dont understand why it is necessary when one of the referenced columns id is already guaranteed to be unique the way see it the new constraint would be redundant
153420 have table in which store all the forum messages posted by the users on my website the messages hierarchy strucrue is implement using nested set model the following is simplified structure of the table id primary key owner id foreign key references to id parent id foreign key references to id nleft nright nlevel now the table is looking something like this id owner id parent id nleft nright nlevel null note that the first row is the root message and the tree of this post can be displayed as select from forumtbl where owner id order by nleft message id message id message id message id my problem occurs when try to delete all the rows under the same owner id in single query example delete from forumtbl where owner id order by nright the above query fails with the following error error code cannot delete or update parent row foreign key constraint fails forumtbl constraint owner id frgn foreign key owner id references forumtbl id on delete no action on update no action the reason is that the first row which is the root node id also has the same value in its owner id field owner id and it causing the query to fail due to the foreign key constraint my question is how can prevent this foreign key constraint circularity and delete row that reference to itself is there way to do it without first having to update the the owner id of the root row to null created demo of this scenario http sqlfiddle com fd1b1 thank you
153458 just hoping to confirm my observation and get an explanation about why this is happening have function defined as create or replace function public post users id coin coins integer userid integer returns table id integer as update users set coin coin coins where userid users id returning users id language sql cost rows volatile returns null on null input security invoker when call this function from cte it executes the sql command but does not trigger the function for example with test as select from post users id coin10 select select but update not performed on the other hand if call the function from cte and then select the result of the cte or call the function directly without cte it executes the sql command and does trigger the function for example with test as select from post users id coin10 select from test select result and update performed or select from post users id coin101 since dont really care about the result of the function just need it to perform the update is there any way of getting this to work without selecting the result of the cte
153519 would like to implement sql server clustered columnstore indexes on some very large wide tables will need more ram to support this if so how do determine how much
153586 select statement returns several rows select cola from tablea where cola is null get rows that have null for cola within tablea cola null null null etc if add an aggregate to this query select cola countcola as thecount from tablea where cola is null group by cola get cola thecount null why is this happening and what can do to avoid this
153738 know how to write stored procedure with output parameters but dont know the reason why would use it over just using simple select statement normal stored procedure can still return an output in grid see below examples can anyone give real life example where can use an sp with output parameter over with sp without output parameter examples using output parameter select var count from table1 where gender gender without output parameter select count from table where gender gender
153747 wonder why sql server does not use all available memory when am quite sure there is memory bottleneck for example one server administer starts with using of memory and gradually increases up to but not increase trend is as follows after restart at after restart and reaches throughout the night suppose this should be due to overnight back ups taken by next day by steady trend throughout the day until when memory utilization increases to next morning at at next day at and stays there what might be causing that not being utilized is it possible that windows is preventing it if so how can prove that indeed windows is keeping it to itself am referring to the overall memory usage by the server the os max memory configuration is already set well beyond the servers current memory have monitoring tool at hand for tracking memory utilization memory utilization by the server is no less nor above this is dedicated single instance two database sql server on which no other services applications etc are running sql server might be using most of the allocated memory but my question is why is it not using the rest im not sure if am becoming too peevish in questioning that thanks to the query determining current memory allocation in monitor memory usage confirm memory usedby sqlserver gb is out of gb total ram was unable to find the column for max server memory and total ram in the output of sys dm os process memory dmv maybe due to sql server r2 but may confirm it is gb server and max server memory is set to mb dont know if my server will utilize any extra memory once they are put in the slots am trying to prove if the server needs more memory if so will make use of it and thus make cause to push for purchase will be left in an awkward position if the memory is bought and memory utilization does not increase and stays in its current state gb seeing gb of ram being utilized after increasing the servers memory from gb to gb will be waste of money want to make sure before the fact if the server will benefit from the new extra rams or be indifferent
154032 first create two tables create table xyz id int create table abc id int please observe the following sql code delete from abc begin tran exec insert into xyz values declare int select id from xyz insert into abc values commit select from abc running it outputs the following error msg level state line subquery returned more than value this is not permitted when the subquery follows or when the subquery is used as an expression and there is no result all as expected now let us change the error to syntax error by replacing xyz with xyz here is the error msg level state line incorrect syntax near xyz but this time there is result why what is going on
154061 have created fresh db dump from production server with the data only and column inserts flags so only have bunch of insert statements to insert data when performing restore on staging server pg dump localhost adminuser data only column inserts maindb maindb sql how do delete all data in the staging server database first before restoring the data from the production dump want to delete all data only so dont have to drop and create the database and all that stuff just want to remove data and insert new data that is all dont have the option to drop and create the database for several reasons will have to remove all data and just insert only so whatever it takes to find how to do this am willing to go for it but need help obviously to start with also need to automate this process will automate dumping data from production db then deleting data on staging db and then restoring data to staging db just need help on the deleting data on staging db part am running on postgresql
154227 have very large database roughly gb im executing query select from table name and want to show only the 100th to 200th rows want to understand how this happens internally does the database fetch all of the records from disk into memory and sends back 100th to 400th rows to the querying client or does there exist any mechanism so that only those records 100th 200th are fetched from database by using indexing mechanism like trees etc found that this is related to pagination concept but could not exactly find how it happens internally at the database level
154251 watching some brent ozar videos like this one for instance and he suggests not prefixing tables with tbl or tbl on the internet found some blogs saying it adds nothing to documentation and also that it takes longer to read it questions and considerations it this really problem because prefixing tables with tbl since my first dba job the senior dba told me to do that for organization is this something that need to get rid of made some tests copying really big table and giving it the tbl prefix while keeping the other one without it and didn notice any performance issue
154262 have an oracle db working in production for more than year have my listener working oracle base admin lsnrctl status lsnrctl for linux version connecting to address protocol tcphost port status of the listener alias listener version tnslsnr for linux version production start date nov uptime days hr min sec trace level off security on local os authentication snmp off listener log file app oracle diag tnslsnr base listener alert log xml listening endpoints summary description address protocol tcphost baseport services summary service base has instances instance base status ready has handlers for this service service basexdb has instances instance base status ready has handlers for this service the command completed successfully but have no listener ora ls app oracle product db se dbhome network admin samples shrept lst tnsnames ora this is the content of my tnsnames ora cat tnsnames ora base description address list address protocol tcphost 19port load balance yes connect data service name base orcl description address protocol tcphost 20port connect data server dedicated service name orcl how is this possible as far as know for the listener to work have to have listener ora file have another oracle database installed and if execute lsnrctl status in the output can see line containing listener parameter file app oracle product dbhome network admin listener ora and am not seeing it here also have executed locate listener ora and nothing
154471 if am going to set an alwayson availability group with nodes with read only routing do need to configure quorum
154574 am designing table of items which will potentially contain tens of millions of records some items will not be available for use until they are approved by administrator by use mean that such items will not be referenced in any other table until they are approved up to of items may be unapproved at any given time records may become approved but not vice versa consider two design options bit flag separate table of unapproved items when item is approved it is moved to regular table renewal of items id is not an issue think the second option is much better bit flag takes only byte per row so it is not an issue but if we have million of approved and million of unapproved records in same table scan time increases for operations with approved records question is should consider first bit flag option instead does it have any benefits in described situation
154766 load data into data warehouse presently have script that use to drop foreign key constraints and indexes prior to loading the data for convenience and speed there is big window in which can do the load so dont need to worry about users accessing the data during the load but dont want to impact unrelated data in other tables in the database have done some research here and elsewhere to come up with this script but wonder if there are some things that might be overlooking which could either make performance sub optimal or if might be missing something important dont know calculated columns or something or maybe im doing things in the wrong order etc any advice appreciated to make this robust and performant disable constraints and indexes edit removed the while loop which commenters helped me realise was redundant declare schema varchar128 dbo declare sql nvarcharmax indices select list of indexes in the schema and generate statements to disable them select sql sql alter index quotenameidx name on quotename schema quotenameobj name disable char13 from sys indexes as idx join sys objects as obj on idx object id obj object id where obj type and idx type in non clustered index columnstore on table or obj type all indexes on indexed views and obj schema id select schema id from sys schemas where name schema order by obj name idx name execute sp executesql sql foreign key constraints build list of foreign keys constraints in the schema and generate statements to disable the constraint checking select sql sql alter table quotename schema quotenameobj name nocheck constraint quotenamefk name char13 from sys foreign keys as fk join sys objects as obj on fk parent object id obj object id where obj schema id select schema id from sys schemas where name schema execute sp executesql sql enable constraints rebuild indexes and update statistics declare schema nvarchar128 dbo declare sql nvarcharmax indices build list of tables in the schema and generate statements to enable the indices on them select sql sql alter index quotenameidx name on quotename schema quotenameobj name rebuild iifidx type with maxdop with fillfactor char13 from sys indexes idx join sys objects obj on obj object id idx object id where obj type and idx type in non clustered index on table or obj type all indexes on indexed views and obj schema id select schema id from sys schemas where name schema and idx is disabled dont rebuild indexes that are already online and idx is hypothetical dont rebuild hypothetical indexes order by iifidx type obj name idx name execute sp executesql sql foreign key constraints build list of foreign keys constraints in the schema and generate statements to enable them with checking select sql sql alter table quotename schema quotenameobj name with check check constraint quotenamefk name char13 from sys foreign keys fk join sys objects obj on obj object id fk parent object id where obj schema id select schema id from sys schemas where name schema order by obj name fk name execute sp executesql sql statistics build list of tables in the schema and generate statements to update the statistics on them select sql sql update statistics quotename schema quotenameobj name with columns char13 from sys objects obj where obj type user defined and obj schema id select schema id from sys schemas where name schema order by obj name execute sp executesql sql
154841 which database role membership grants permission to execute all existing stored procedures in sql server tried adding user to each of them and am still unable to execute stored procedure dont want to grant execute for each stored procedure separately want to add the user to role and he be able to execute any of them
155219 should reindex rebuild while my db is up and running or should take it offline also after doing some digging around came across this article that recommends rebuilding after fragmentation and reindexing between to can anyone verify this my db is quite large and is slowing down more and more
155332 some times ago created postgresql user named user1 postgresql want to drop this user so first revoke all permissions on tables sequences functions default privileges and ownership too alter default privileges in schema public revoke all on sequences from user1 alter default privileges in schema public revoke all on tables from user1 alter default privileges in schema public revoke all on functions from user1 revoke all on all sequences in schema public from user1 revoke all on all tables in schema public from user1 revoke all on all functions in schema public from user1 reassign owned by user1 to postgres however it seems that one object remains linked to this user in databases postgres drop role user1 error role user1 cannot be dropped because some objects depend on it detail object in database db1 object in database db2 it even seems to be function postgres db1 you are now connected to database db1 as user postgres db1 drop role user1 error role user1 cannot be dropped because some objects depend on it detail privileges for function textboolean object in database db2 but can not determine which object is owned or related to user1 if pg dump db1 grep user1 get no result could it be global object how can identify the missing object have executed the commands in each database db1 and db2 do not want to drop objects owned by user1 just want to reassign or remove grants for this user
155580 in an update script im locking several tables begin transaction select top null from tablea with holdlock tablockx select top null from tableb with holdlock tablockx want to suppress results for it in order to focus on real script results is there an elegant way to xlock table without getting result set
155650 consider this answer on so that reassures the asker about the operator that is the same as but then commenter pipes up and says its true that they are functionally the same however how the sql optimizer uses them is very different are simply evaluated as true false whereas means the engine has to look and see if the value is greater than or less than meaning more performance overhead just something to consider when writing queries that may be expensive am confident this is false but in order to address potential skeptics wonder if anyone can provide an authoritative or canonical source to prove that these operators are not just functionally the same but identical in all aspects
155684 im trying to insert the result set from select from sys database scoped configurations into temp table because want to check the settings for all databases on my server so wrote this code drop table if exists create table hdbname sysname configuration id int name sysname value sql variant value for secondary sql variant exec sys sp msforeachdb use insert into hdbname configuration id name valuevalue for secondary select as dbname from sys database scoped configurations select from but then there will only be one row per database not the four rows that expect from running plain select in each database know there are better ways to code this than using sp msforeachdb and tried several but still only get one row per database ive tried this on both sql server rtm and sp1 is this bug with sql server or am doing something wrong
155952 can we enable disable sql server agent service by sql script or powershell cmd
155998 id value select maxvalue from tablename generally we know we will get but need the next value how do get the value using sql
156028 my issue is as follows im trying to create global temp table using data from db1 then use that temp table in query executed in db2 and then have the results imported into new table on db2 since the query is bit more complicated simple merge join wont do as need to transform the data and add an extra column dense rank based on data from both databases according to one of my colleagues this should all be possible to do in ssis simplified query that would like to execute on connection from db2 select db2 adb2 bdb2 cdb1 db1 dense rank over partition by db2 order by db1 asc as finalrank from db2 table as db2 left join globaltemp as db1 on db2 db1 after extensive googling and different tries the db2 source connection wont find my temp table globaltemp which has been created in db1 is such an operation even possible tldr is it possible to create global temp table in ssis session and pull data from it no matter the database
156068 consider table that records visits create table visits person varchar10 ts timestamp somevalue varchar10 consider this example data timestamp simplified as counter ts person somevalue bob null bob null jim null bob bob null bob jim jim jim null im trying to carry forward the last non null somevalue of the person to all his future visits until that value changes ie becomes the next non null value expected result set looks like this ts person somevalue carry forward bob null null bob null null jim null null bob bob null bob jim jim jim null my attempt looks like this select first valuesomevalue over partition by person order by somevalue is null ts rows between unbounded preceding and current row as carry forward from visits order by ts note the somevalue is null evaluates to or for the purposes of sorting so can get the first non null value in the partition the above doesnt give me the result im after
156164 have stored procedure that contains business logic inside it have around variables dont ask me why this is how the engine works try to set variable to the concatenated value of all other variables as result during creation get the error msg level state procedure xxx line yyy internal error server stack limit has been reached please look for potentially deep nesting in your query and try to simplify it figured out that the error is due to the number of variables that need to use in the set operation can perform the assignment by splitting it in two my question is are there some restrictions in this area checked but did not find any we checked the error described in this kb but this is not our case we dont use any case expressions inside our code we use that temporary variable to prepare list of values that have to be replaced using clr function we updated our sql server to sp3 cu6 latest up to date but we still experience the error
156359 in sql server how do we find all tables which do not have either of foreign key constraints references by other tables foreign keys
156545 while 7daysearlierpartitionintegerid currentpartitionintegerid begin set 7daysearlierpartitionid cast 7daysearlierpartitionintegerid as char set sqlcommand select from quotename requestusage partition 7daysearlierpartitionid where userlogin liker2 rohit kharade exec sqlcommand set 7daysearlierpartitionintegerid 7daysearlierpartitionintegerid end get the error unclosed quotation mark after the character string r2 rohit kh how to execute the exec command with userlogin r2 rohit kharade
156553 have two tables one titled planning constraints which contains the sot allowed time intervals and one titled planning which contains the sot contribution time interval here are the schema of the two tables edited for readiability table public planning constraints column type modifiers start time timestamp with time zone end time timestamp with time zone sot allowed interval table public planning column type modifiers start time timestamp with time zone end time timestamp with time zone sot contribution interval can query them separately and produce the totals that want the query of the planning constraints table is select date truncday start time interval hours date as planning day sumsot allowed as minutes allowed from planning constraints where start time and start time and comment like group by planning day order by planning day which produces planning day minutes allowed rows the query of the planning table is select date truncday start time interval hours date as planning day sumsot contribution as minutes planned from planning where start time and start time group by planning day order by planning day which produces planning day minutes planned rows would like to present the results of these queries in adjacent columns along with the difference between them here is my attempt to do so select date truncday start time interval hours date as planning day sumc sot allowed as minutes allowed sump sot contribution as minutes planned sumc sot allowed sump sot contribution as diff from planning constraints planning where start time and start time and comment like and start time and start time group by planning day order by planning day which produces planning day minutes allowed minutes planned diff rows the minutes allowed and minutes planned columns do not present the correct values feel like am missing something small here however have been unable to find solution
156827 what is the difference between the smallint type and the bool type for storing boolean values this question arose in the comments to question on geographic information systems stack exchange
157045 whether read about cap or acid see that consistency is referred to ensure the db integrity constraints so do not understand why two terms are used to refer the same thing or there is difference between the integrity and consistency read that anyhow atomic consistent isolated durable are properties of the transactions it is true that atomicity isolation is enough for you to roll your own consistency but we can also roll our own atomicity roll our own isolation and roll our own persistence durability when we roll our own we must pay for features with our own blood sweat and parentheses we dont say the properties were given to us by transaction system which suggests that consistency is what user application can provide himself on top of database integrity constraints this is not property that is provided by database as aid properties are why to give the title as you do to the other system provided aid properties
157334 have the following table in ms sql create table dbo dbip locations ip from bigint not null ip to bigint not null country code nvarchar not null region name nvarchar not null city name nvarchar not null latitude float not null longitude float not null the ip from and ip to columns are calculated from ipv4 addresses as such return convertbigint parsename ip convertbigint parsename ip convertbigint convertbigint parsename ip convertbigint convertbigint parsename ip convertbigint am then using an ipv4 address converted to bigint using the calculation above to search for the row in which the ip address falls between the ip from and ip to columns ill always find one row while it is not enforced by the schema this is the reality in the data here is the query select top1 latitude longitude from dbo dbip locations where ip int between ip from and ip to currently have two non clustered non unique indexes on both the ip from and ip to columns the query is executing pretty quickly but am performing huge number of these queries per second and would like to know if could be getting better performance through the usage of different indexes perhaps clustered multi column index or by using unique indexes are there any better indexes could be using
157501 so postgres database specifically an amazonrds instance fwiw its easy enough to arrange things to get the last updated or created time for any record that information would be datetime what want to achieve imagine an integer sequencenumber which is atomic and global to the database simply every table would have sequencenumber field any time any record ie in any table is updated or created it gets the next sequencenumber you know how you have programmer friends who know nothing about databases wave my idiot solution would be in the entire codebase every time you create or modify record be sure to remember to update sequencenumber on it of course just have one global system that gives the next atomic sequencenumber whats the sound solution for this indeed is it completely available already in some way dont know or
157515 can set logical primary key constraint served by physical nonclustered index so that it only checks certain values of the constrained and indexed column or another column in the relevant table and if so can that column be established as foreign key constraint referenced from other tables im trying to basically see if can remove certain rows from parent table without removing those rows from child table but still enforce the relationship for ids or createddatetime removaldate for example you can have that kind of constraint alter table mytable with nocheck add constraint pk mytable check id but how would you set this column as primary key fixed with nonclustered index and can do something like the following definition alter table mytable with nocheck add constraint pk mytable check createdon but have an index for the column named id
157696 im doing some testing on sync vs async auto statistics updates id like to quickly invalidate all statistic objects headers density vectors and histograms to ensure that next time the statistic is used that it will be updated im trying to simulate an auto update of statistics not an auto creation ideally dont want to change the row count so ive dismissed insert delete operations ideally dont want to change any data values either have considered using update statements but im thinking this could take too long on some of my larger tables had looked at update statistics with rowcount pagecount but dont think this is what im after was hoping there was maybe trace flag or undocumented command that would invalidate statistics is there an quick efficient way to do what want to achieve that havent considered im testing on sql server
157790 heres my code right now begin try insert into table f1f2f3 values end try begin catch throw end catch works great unless its run on machine with sql id like to have the catch block do check against the sql version and run throw if its equal or higher to and raiserror if its keep running into syntax errors and im wondering if its even possible even something simple like this is not working for me begin catch if select serverpropertyproductversion throw end catch any advice is appreciated
157893 am going to create history table which is populated by trigger after insert update delete as only of the columns are going to be updated decided to log only the changed values if the values are not changed null value is going to be used in the history table for example the history table columns will be sparse and am going to save lot of space versus ordinary implementation which is logging all data this is due to my test and my bussness cases as the sql server sp1 standard edition supports change data capture am wondering are their any pros cons differences between using it and trigger based logging have check few artciles here and here and cannot see what more change data capture can give me
157912 forgive me am developer who has moved over to the world of sql thought could improve some sql by adding variables but it did not function like expected can someone tell me why this does not work dont want work around want to know the reasons why this doesnt work like imagine it should as am sure there is good reason but currently it doesnt jump out at me declare databasename varchar150 set databasename myamazingdatabasename create database databasename go use databasename go
158092 we often encounter the if not exists insert situation dan guzmans blog has an excellent investigation of how to make this process threadsafe have basic table that simply catalogs string to an integer from sequence in stored procedure need to either get the integer key for the value if it exists or insert it and then get the resulting value there is uniqueness constraint on the dbo namelookup itemname column so data integrity isnt at risk but dont want to encounter the exceptions its not an identity so cant get scope identity and the value could be null in certain cases in my situation only have to deal with insert safety on the table so im trying to decide if its better practice to use merge like this set nocount xact abort on declare vvalueid int declare inserted as table id int not null merge dbo namelookup with holdlock as using select vname as val where vname is not null and len vname as new item on itemname new item val when matched then update set vvalueid id when not matched by target then insert itemname values vname output inserted id as id into inserted select vvalueid id from inserted as could do this witout using merge with just conditional insert followed by select think that this second approach is clearer to the reader but im not convinced its better practice set nocount xact abort on insert into dbo namelookup itemname select vname where not exists select from dbo namelookup as where vname is not null and len vname and itemname vname declare vvalueid int select vvalueid id from dbo namelookup as where itemname vname or perhaps there is another better way that havent considered did search and reference other questions this one https stackoverflow com questions sql server insert if not exists best practice is the most appropriate could find but doesnt seem very applicable to my use case other questions to the if not exists then approach which dont think is acceptable
158201 am experiencing what think is an impossibly high cardinality estimate for the following query select dm primary id from select coalesced1 join id d2 join id d3 join id primary id from driving table dt left outer join detail d1 on dt id d1 id left outer join detail link lnk on d1 link id lnk link id left outer join detail d2 on dt id d2 id left outer join detail d3 on dt id d3 id dm inner join last table lst on dm primary id lst join id the estimated plan is here am working on statistics copy of the tables so cannot include an actual plan however dont think that its very relevant for this problem sql server estimates that rows will be returned from the dm derived table it then estimates that rows will be returned after the join to last table is performed but join id is the primary key of last time would expect join cardinality estimate between and rows instead the row estimate appears to be of the number of rows would get when cross joining the outer and the inner tables the math for this works out with rounding which is rounded to am primarily looking for root cause for this behavior am interested in simple workarounds as well but please do not suggest changing the data model or using temp tables this query is simplification of logic within view know that doing coalesce on few columns and joining on them isnt good practice part of the goal of this question is to figure out if need to recommend that the data model be redesigned am testing on microsoft sql server with the legacy cardinality estimator enabled tf and others are on can give full list of trace flags if that ends up being relevant here is the most relevant table definition create table last table join id numeric18 not null constraint pk last table primary key clustered join id asc also scripted out all of the table creation scripts along with their statistics if anyone wants to reproduce the issue on one of their servers to add few of my observations using tf fixes the estimate but that is not an option for me tf does not fix the estimate removing one of the tables fixes the estimate bizarrely changing the join order of detail link also fixes the estimate by changing the join order mean rewriting the query and not forcing the join order with hint here is an estimated query plan when just changing the order of the joins
158205 problem below is query put together in mysql workbench that thought would be relatively simple but when running it on larger data set rows find it taking over seconds uses 1gb of ram and downloading data from the mysql server at rate of mb that entire time there isnt 100kb worth of data in the entire table so clearly im creating some kind of god awful loop here somehow but im not seeing it what im doing effectively what this query is doing is taking the specified columns from each row escaping single and double quotes then contacting them together into json format exactly as needed for 3rd party api after this part of the query is done then select the last row and return it not seen here it inserts this data into cache table for quick access from the api my query set rownum set json set rowtotal select count fromselect from job1111 civiltrackerdetails group by bidid tb select rownum rownum as id json concat json concat if rownum ctd bidid replacereplacectd description replacereplacectd foundationdescription replacereplacectd engdrawingnumber replacereplacectd detaildrawingnumber ctd takeoffquantity if rownum rowtotal as jsondata from job1111 civiltrackerdetails ctd question can you see why this is looping so badly do you know better way to accomplish this task preferably within the given methodology of arranging the data within mysql vs out of mysql the explain shows nothing much just derived ctd from full table scan to be expected im using version have often wondered if some of the json compatibility that are supposed to be available in may help with this scenario have not looked into what is all available in yet though would be interested in input on that as well ultimately the solution to this issue will be implemented in stored procedure
158291 heres the run down im doing select query every column in the where and order by clauses are in single non clustered index ix machineryid daterecorded either as part of the key or as include columns im selecting all the columns so that will result in bookmark lookup but im only taking top so surely the server can tell the lookup only needs to be done once at the end most importantly when force the query to use index ix machineryid daterecorded it runs in less than second if let the server decide which index to use it picks ix machineryid and it takes up to minute that really suggests to me that have made the index right and the server is just making bad decision why create table dbo machineryreading id int identity not null location sys geometry null latitude float not null longitude float not null altitude float null odometer int null speed float null batterylevel int null pinflags bigint not null daterecorded datetime not null datereceived datetime not null satellites int not null hdop float not null machineryid int not null trackerid int not null reporttype nvarchar null fixstatus int default not null alarmstatus int default not null operationalseconds int default not null constraint pk dbo machineryreading primary key clustered id asc constraint fk dbo machineryreading dbo machinery machineryid foreign key machineryid references dbo machinery id on delete cascade constraint fk dbo machineryreading dbo tracker trackerid foreign key trackerid references dbo tracker id on delete cascade go create nonclustered index ix machineryid on dbo machineryreading machineryid asc go create nonclustered index ix trackerid on dbo machineryreading trackerid asc go create nonclustered index ix machineryid daterecorded on dbo machineryreading machineryid asc daterecorded asc include operationalseconds fixstatus the table is partitioned into month ranges though still dont really understand whats going on there alter partition scheme partitionschememonthrange next used primary alter partition function partitionfunctionmonthrange split rangen2016 01t00 alter partition scheme partitionschememonthrange next used primary alter partition function partitionfunctionmonthrange split rangen2016 01t00 create unique clustered index pk dbo machineryreadingps on machineryreadingdaterecorded id on partitionschememonthrangedaterecorded the query that would normally run select top id location latitude longitude altitude odometer reporttype fixstatus alarmstatus speed batterylevel pinflags daterecorded datereceived satellites hdop operationalseconds machineryid trackerid from dbo machineryreading withindexix machineryid daterecorded this makes all the difference where machineryid linq and daterecorded linq and daterecorded linq and operationalseconds order by daterecorded asc query plan https www brentozar com pastetheplan id r1c rpxnx query plan with forced index https www brentozar com pastetheplan id sywwtagve the plans included are the actual execution plans but on the staging database about 100th of the size of live im hesitant to be fiddling with the live database because only started at this company about month ago have feeling its because of the partitioning and my query typically spans every single partition when want to get the first or last operationalseconds ever recorded for one machine however the queries have been writing by hand are all running good times faster than what entityframework has generated so im just going to make stored procedure
158602 have the following query to get me the values corresponding with the latest date select maxrowaddeddate from dbo mytable group by this is fine but need to get the id of each row in this query if add the id though get everything as the id needs to be in the group by how do solve this
158617 executing this request update table t1 set t1 column where t1 column2 getting this error column t1 of relation table does not exist this request runs fine in mysql why do get this error in postgresql
158693 am using this great example https dba stackexchange com from bluefeet to create pivot and transform it to xml data declaring the param declare cols as nvarcharmax query as nvarcharmax next there is cte with lot of code the endresult of the cte is put in temp db same as in the example select staydate this is date dd mm yyyy guid into tempdates from baseselection generating the cols same as the example select cols stuffselect distinct quotenameconvertchar10 staydate from tempdates for xml path type value nvarcharmax the result set is what should expected set query select guid cols from select staydate guid from tempdates pivot count staydate for staydate in cols exec sp executesql query when try to transform it to xml my attributes are only partially converted set query select guid cols from select staydate guid from tempdates pivot count staydate for staydate in cols for xml auto when using for xml path will get error for xml path rootroot msg level state line column name contains an invalid xml identifier as required by for xml 20x0032 is the first character at fault exec sp executesql query resultset guid 3c3359e3 cfe5 e511 80ca 005056a90901 x0032 should be x0032 should be x0032 should be have missed something why is only portion of the date converted to unicode how can fix this
158957 only heard about robert martin today and it seems like hes notable figure in the software world so dont mean for my title to appear as if its click bait or me putting words in his mouth but this is simply how interpreted what heard from him with my limited experience and understanding was watching video today on software architecture on talk by robert martin and in the latter half of the video the topic of databases was the main focus from my understanding of what he said it seemed like he was saying that ssds will reduce the usefulness of databases considerably to explain how came to this interpretation he discussed how with hdds spinning disks retrieving data is slow however these days we use ssds he noted he starts off with ram is coming and then continues by mentioning ram disks but then says he cant call it ram disk so resorts to just saying ram so with ram we dont need the indexes because every byte takes the same time to get this paragraph is paraphrased by me so him suggesting ram as in computer memory as replacement for dbs as thats what interpreted his statement as doesnt make sense because thats like saying all the records are in memory processed in the lifetime of an application unless you pull from disk file on demand so resorted to thinking by ram he means ssd so in that case hes saying ssds reduce the usefulness of databases he even says if was oracle id be scared the very foundation of why exist is evaporating from my little understanding of ssds unlike hdds which are on seek time id think ssds are near o1 or almost random so his suggestion was interesting to me because ive never thought about it like that the first time was introduced to databases few years ago when professor was describing the benefits over regular filesystem concluded the primary role of database is essentially being very indexed filesystem as well as optimizations caching concurrent access etc thus if indexes arent needed in ssd this kind of does make databases less useful regardless of that though prefacing that im newb find it hard to believe that they become less useful as everyone still uses dbs as the primary point of their application instead of pure filesystem and felt as if he was oversimplifying the role of databases note did watch till the end to make sure he didnt say something different for reference is when the whole database topic comes up is when he starts off with why do we even have databases this answer does say ssds speed dbs up considerably this question asks about how optimization is changed to tl dr my question does the advent of widespread ssd use in the server market whether its upcoming or has happened already reduce the usefulness of databases it seemed like what the presenter was trying to convey was that with ssds one can store the data on disk and not have to worry about how slow it would be to retrieve it as with older hdds as with ssds seek times are near o1 think so in the event of that being true that would hypothetically lose one of the advantages it had indexing because the advantage of having indexes for faster seek times is gone
158988 was reviewing the commit fest scheduled for for postgresql and saw that pg is likely going to get identity columns sometime soon found some mention in information schema columns but nothing much is identity yes or no applies to feature not available in postgresql identity generation character data applies to feature not available in postgresql identity start character data applies to feature not available in postgresql identity increment character data applies to feature not available in postgresql identity maximum character data applies to feature not available in postgresql identity minimum character data applies to feature not available in postgresql identity cycle yes or no applies to feature not available in postgresql the wikipedia page doesnt say much either an identity column differs from primary key in that its values are managed by the server and usually cannot be modified in many cases an identity column is used as primary key however this is not always the case but dont see anything else on them how do identity columns work do they provide any new functionality or is this just standard method to create sequences any breakdown of the new feature and how it works
159069 inherited database that has varchar column that really should be date theres no checking on the form need to fix that so dates constantly get entered as instead of and so forth im pretty sure there are historically lot of bad dates so im little skeevy about just changing the data type to date but im curious about what would be good strategy for changing that background im front end developer not dba so while can get around in sql dont know what dont know and im always afraid ill make mistake that corrupts the database
159115 have table with data as follows no amount need query to find the maxs no according to the sumamount for when sumamount need as result can get the desired output by using cursor but it would be easier if could use query tried query as follows select maxs no from table having sumamount but it doesnt work would appreciate any kind of help thank you
159227 have two fields in my table int int want to add two computed fields cf1 cf2 that uses these two fields cf1 case when then cf1 else case when then cf1 else cf1 end end cf2 case when then cf2 else case when then cf2 else cf2 end end this works fine but when the two fields are null cf1 and cf2 are how do avoid this mean when either one of the fields or is null both cf1 and cf2 should also be null in other words there should only be value of or in cf1 and cf2 if there is value not null in fields cf1 cf2 null null null null null null null null null null
159229 have table with an identity column that is also primary key currently it has million rows with the highest value of the identity column sitting at the table has lot of deletes and inserts performed on it hence the high value we want to change the data type from int to bigint to prepare for the addition of more rows note that there are no references to the pk column what is the best way to do this with minimal downtime have two options drop the pk and alter the column or the copy drop rename method as described here
159413 whenever need to check for the existence of some row in table tend to write always condition like select from table where exists select this is what normally write from another table where another table table some other people write it like select from table where exists select this nice is what have seen other people use from another table where another table table when the condition is not exists instead of exists in some occasions might write it with left join and an extra condition sometimes called an antijoin select from table left join another table on another table table where another table primary key is null try to avoid it because think the meaning is less clear specially when what is your primary key is not that obvious or when your primary key or your join condition is multi column and you can easily forget one of the columns however sometimes you maintain code written by somebody else and it is just there is there any difference other than style to use select instead of select is there any corner case where it does not behave the same way although what wrote is afaik standard sql is there such difference for different databases older versions is there any advantage on explicity writing an antijoin do contemporary planners optimizers treat it differently from the not exists clause
159462 we are administering sweepstakes for client where we have guid tied to contestant and the number of entries the contestant has acquired want to be able to draw winners based on the chance of winning corresponding to the number of entries guid entries so the results would be guid basically plan put the results in spreadsheet and then use random number generator to pick row number between and where is the total number of entries but of course if there is way to easily select the winner programmatically im all ears or eyes as the case may be thanks in advance
159710 was just reviewing some old code written for pre postgresql and saw something really nifty remember having custom function do some of this back in the day but forgot what pre array agg looked like for review modern aggregation is written like this select array aggx order by desc from foobar however once upon time it was written like this select arrayselect from foobar order by desc so tried it with some test data create temp table foobar as select from generate series11e7 as tx the results were surprising the oldschoolcool way was massively faster speedup moreover simplifying it without the order showed the same slowness explain analyze select arrayselect from foobar query plan result cost rows width actual time rows loops initplan returns seq scan on foobar cost rows width actual time rows loops planning time ms execution time ms rows test explain analyze select array aggx from foobar query plan aggregate cost rows width actual time rows loops seq scan on foobar cost rows width actual time rows loops planning time ms execution time ms rows so whats going on here why is array agg an internal function so much slower than the planners sql voodoo using postgresql on x86 pc linux gnu compiled by gcc ubuntu 5ubuntu12 bit
160216 have table that looks like this id code category mq weight weave show minprice dt450r carbon plain dt450r carbon plain dt450r carbon plain pp120q carbon twill pp120q carbon twill pp120q carbon twill zx300r carbon plain zx300r carbon plain zx300r carbon plain ive created sqlfiddle here want min price from table in each code tried using the following query select id code category mq weight weave price show minprice as total from product group by code why is the group by getting the wrong result its returning id instead of id incorrect output id code category mq weight weave show minprice dt450r carbon plain pp120q carbon twill zx300r carbon plain expected output id code category mq weight weave show minprice dt450r carbon plain zx300r carbon plain pp120q carbon twill
160281 have the following query select id email first name as firstname last name as lastname is active as isactive password access case when access then select case when count then true else false end from user rating entity ure where ure user id id and ure rating entity id re id else true end as isresponsible from users where id id if access field isresponsible should be directly set to true and the subquery should not be executed used explain analyze with both cases where access and to but get the same output why is that so
160290 all ive read about is how potentially damaging it is to stop sql server because it creates cold cache and sucks up memory so why would someone want to stop the sql server if you can provide any links to articles so that can read more into this would really appreciate it this question was posed by my teacher unless its some trick question it has me absolutely stumped his exact question was conduct research using the internet and learn why someone would want to stop the sql server explain your answer this was in the context of us exploring how to use sql server r2 im not sure if he is asking for the obvious answer or if there is something im missing
160343 for logging purposes in table design want to store the ip information of users when they logged in when user logs in ipv4 ipv6 or both ip addresses should be stored but want to set constraint that both cannot be null is it possible to do that in standard sql with constraints or should do that using pl pgsql in postgresql or in the business tier
160354 given two numbers and want to generate series of the form and repeat it times for instance for and want sequence of the following numbers know how to achieve this result in postgresql by either of two methods using the following query which uses the generate series function and few tricks to guarantee that the order is the right one with parameters as values select xi from select as xi from parameters generate series1 parameters as xi union all select parameters parameters as xi from parameters generate series1 parameters as xi as s0 cross join generate series select from parameters as xj order by or use function for the same purpose with adjoint and nested loops create function generate up down series elements integer repetitions integer returns setof integer as body declare integer integer begin for in repetitions loop for in elements loop return next end loop for in reverse elements loop return next end loop end loop end body language plpgsql immutable strict how could possibly do the equivalent in either standard sql or in transact sql sql server
160530 have this table create table table01 column01 nvarchar100 and want to create unique index on column01 with this condition lencolumn01 tried create unique index uix on table01column01 where lencolumn01 got incorrect where clause for filtered index uix on table table01 and alter table table01 add column01 length as lencolumn01 create unique index uix on table01column01 where column01 length produces filtered index uix cannot be created on table table01 because the column column01 length in the filter expression is computed column rewrite the filter expression so that it does not include this column
160794 have accfb file from an access database and am trying to import this database into sql server developer edition instance while found posts online discussing using import export data tools in sql server and also migration assistant for access neither have worked out for me does anyone know how to easily move database from access to sql server more specifically an accdb file type all suggestions are greatly appreciated
160872 is it possible in sql server to determine whether mixed mode authentication is enabled without logging on to sql server
160924 given hierarchical table like this create table dbo btree id int primary key parent id int references dbo btree id name nvarchar20 would like to obtain the whole tree structure for instance using this data insert into btree values null root insert into btree values group insert into btree values group insert into btree values group insert into btree values group insert into btree values group insert into btree values group insert into btree values items insert into btree values items insert into btree values items insert into btree values items insert into btree values items insert into btree values items insert into btree values items would like to obtain id parent id description null root group group items items group items items group group items items group items im fetching records using recursive query like this with tree as select c1 id c1 parent id c1 name level from dbo btree c1 where c1 parent id is null union all select c2 id c2 parent id c2 name level tree level from dbo btree c2 inner join tree on tree id c2 parent id select tree level tree id parent id replicate tree level tree name as description from tree option maxrecursion and this is the current result id parent id description null root group group group group items items items group group items items items items cant figure out how to order it by levels is there way to set rank for each sub level ive set up rextester
161220 sql server through management studio offers quite few standard reports but dont see how you could change the reports dont even see an option for extracting obtaining the sql code behind it is there any way to achieve this specifically am more interested in security reports although itd be nice to get code for all of them thank you in advance
161586 currently we execute two queries to get the count and its result with pagination filter while we can easily combine both of these into single network call is there anyway to do this in single query following the dry principle count select count from table result with pagination select from select row number over order by tbl idn as row from tbl tbl where row and row single query has lesser maintenance overhead some of these queries used in production are written like short stories and it is really challenging to modify both of them
161752 have an sql agent job on production server that keeps failing with the below messages it is supposed to be capturing sql server activity using the sp whoisactive stored proc at regularly scheduled intervals executed as user warning null value is eliminated by an aggregate or other set operation sqlstate message warning null value is eliminated by an aggregate or other set operation sqlstate message warning null value is eliminated by an aggregate or other set operation sqlstate message violation of primary key constraint pk whoisactive cannot insert duplicate key in object monitoring whoisactive the duplicate key value is jan 25am sqlstate error the statement has been terminated sqlstate error the step failed any idea what may be causing this what steps should follow to fix this error
161824 select value from persons join persons2 p2 on leftp lastname1 leftp2 lastname1 sql server is there any way to make this sargable run faster cant create columns on persons table but can create columns on persons2
161849 in craig freedmans blog nested loops join he explains why the nested loops join cannot support right outer join the problem is that we scan the inner table multiple times once for each row of the outer join we may encounter the same inner rows multiple times during these multiple scans at what point can we conclude that particular inner row has not or will not join can someone please explain this in really simple and educational way does it mean that the loop starts with the outer table r1 and the scans the inner r2 understand that for r1 value that doesnt join with r2 it should be replaced with null so the result set becomes null r2 for me it seems impossible to return an r2 value when r1 does not join for the reason that it cannot know which r2 value to return but thats not the way its explained or is it sql server does in fact optimize and often replaces right join with left join but the question is to explain why its technically impossible for nested loops join to use support right join logic
162042 would like to get some clarity on the usage best practices of sp send dbmail and cannot find any answers online our developers love to use sp send dbmail to send out application related emails from within the application say for instance user forgets their password they will send the password reset email that user requests by executing sp send dbmail this is just one of the many types of emails they send using this stored procedure is this good practice to me it feels like dirty solution as there are many apis out there that can handle email sending in your application and as dba dont like enabling these options on the production servers just dont have choice maybe with enough motivation and proof that its bad practice and or potentially dangerous for various reasons can sway them to change it
162227 moving an application from oracle to sql server have this pseudo oracle pl sql select ltrimmycolumn from mytable im using oracles ltrim with second argument specifying the characters to trim from the left side of the string unfortunately the sql version of ltrim doesnt allow me to specify the characters to trim currently im rather clueless how to migrate that ltrim im even thinking about processing the results in my hosting application after read the mycolumn this looks rather inelegant to me my question is there any meaningful way of getting an ltrim like functionality for sql to pass the characters to trim away edit need to replace and from the beginning of the string this is test would result in this is test edit strongly hope this isnt an xy problem maybe rewriting my whole query would remove the need for ltrim altogether although would rather focus on porting it as as possible and later question the usefulness of the ltrim
162232 this is example of the table have part no start date end date cost abcd1 bde2 abcd1 and this is the output want to see part no start date end date cost period abcd1 abcd1 abcd1 bde2 bde2 abcd1 want to add certain number of rows depending of the range of dates also in each row add the column period that is basically month and year
162452 my question is how does sql server handle query that needs to pull more volume of data into the buffer cache than there is space available this query would contain multiple joins so the result set does not exist in this format already on disk and it would need to compile the results but even after the compilation it still requires more space than is available in the buffer cache will give an example suppose you have sql server instance that has 6gb total of buffer cache space available run query with multiple joins that reads 7gb of data how is sql server able to respond to this request does it temporarily store the data in tempdb does it fail does it do something that just reads data from disk and compiles segments at time in addition what happens if am trying to return 7gb of total data does that change how sql server handles it am already aware of several ways to address this am just curious how sql server handles this request internally when it runs as stated also am sure this information exists somewhere but have been unsuccessful in finding it
162514 have html code stored in the data base and want to read it as xml my codes http rextester com rmeho89992 this is an example of the html code have div section h4 span span h4 ul li span ab span ad span ac span li li span ag span span al span li ul h4 span span h4 ul li span bb span bd span bc span li li span bg span span bl span li ul section div and this is an example of the output need category selection value ab ad ag al bb bd bg bl need to get the value inside the h4 tag as category the first span tag as selection and the rest of the values as concatenated string ive tried the following query select isnullt valueh4 span span text nvarcharmax isnullt valueh4 span text nvarcharmax isnullt valueh4 span span text nvarcharmax as category isnullc valuespan text nvarcharmax isnullc valuespan span text nvarcharmax isnullc valuespan text nvarcharmax as selection isnullc valuespan text nvarcharmax isnullc valuespan span text nvarcharmax isnullc valuespan text nvarcharmax as value from htmlxml nodesdiv section as tv cross apply nodes ul li as cg and select value nvarcharmax isnullt valueh4 span span text nvarcharmax isnullt valueh4 span text nvarcharmax isnullt valueh4 span span text nvarcharmaxas category isnullc valuespan text nvarcharmax isnullc valuespan span text nvarcharmax isnullc valuespan text nvarcharmaxas selection isnullc valuespan text nvarcharmax isnullc valuespan span text nvarcharmax isnullc valuespan text nvarcharmaxas value from htmlxml nodesdiv section h4 span as tv cross apply htmlxml nodesdiv section ul li as cg but it only gets the first category and doesnt get all the values togheter category selection value ab ac ab ac ag al ag al bb bc bb bc bg bl bg bl there can be categories and the values might or might not be inside span tags how can get all the categories with their corresponding value or get category h4 number mean h4 first mean h4 second ul number selection value ab ad ag al bb bd bg bl relation between column ul number and h4 number cannt
162906 have simple test table like this create table mytable int within transaction try to add column and then insert into the newly created column begin transaction print adding column supplementaldividends to mytable table alter table mytable add supplementaldividends decimal186 print column added successfully print ready to insert into mytable insert into mytable supplementaldividends values print changes complete committing commit transaction the problem is an error message when run the above code invalid column name supplementaldividends why is this causing an error if add the column in different batch outside the transaction itll work my problem is that want to add the column within the transaction why the error
163154 within one web application am working on all database operations are abstracted using some generic repositories defined over entity framework orm however in order to have simple design for the generic repositories all involved tables must define an unique integer int32 in int in sql until now this has been always the pk of the table and also the identity foreign keys are heavily used and they reference these integer columns they are required for both consistency and for generating navigational properties by the orm the application layer typically does the following operations initial data load from table select from table update update table set col1 val1 where id idval delete delete from table where id idval insert insert into table cols values less frequent operations bulk insert bulk insert into table followed by all data load to retrieve generated identifiers bulk delete this is normal delete operation but bulky from orms perspective delete from table where otherthanidcol somevalue bulk update this is normal update operation but bulky from orms perspective update table set somecol someval where otherthanidcol othervalue all small tables are cached at application level and almost all selects will not reach database typical pattern is initial load and lots of inserts updates and deletes based on current application usage there is very small chance of ever reaching 100m records in any of the tables question from dbas perspective are there significant problems can run into by having this table design limitation edit after reading the answers thanks for the great feedback and referenced articles feel like have to add more details current application specifics did not mention about current web application because want to understand if the model can be reused for other applications as well however my particular case is an application that extracts lots of metadata from dwh source data is quite messy denormalized in weird way having some inconsistencies no natural identifier in many cases etc and my app is generating clear separated entities also many of the generated identifiers identity are displayed so that the user can use them as business keys this besides massive code refactoring excludes usage of guids they should not be the only way to uniquely identify row aaron bertrand that is very good advice all my tables also define an unique constraint to ensure that business duplicates are not allowed front end app driven design vs database driven design design choice is caused by these factors entity framework limitations multiple columns pks are allowed but their values cannot be updated custom limitations having single integer key greatly simplifies data structures and non sql code all lists of values have an integer key and displayed values more important it guarantees that any table marked for caching will be able to put into unique int key value map complex select queries this will almost never happen because all small 30k records tables data is cached at application level this makes life little harder when writing application code harder to write linq but the database is hit much nicer list views will generate no select queries on load everything is cached or queries that look like this select allcolumns from bigtable where filter1 in val1 val2 and filter2 in val11 val12 all other required values are fetched through cache lookups o1 so no complex queries will be generated edit views will generate select statements like this select allcolumns from bigtable where pkid value1 all filters and values are ints
163448 table status statusid status opened closed reopened pending table claims claimid companyname statusid abc abc abc abc xyz xyz expected result companyname totalopenclaims totalclosedclaims totalreopenedclaims totalpendingclaims abc xyz how do need to write the query so that could get the result as expected
163528 am struggling to find any documentation on how sql server actually stores non persisted computed column take the following example schema create table dbo invoice invoiceid int identity1 primary key customerid int foreign key references dbo customercustomerid invoicestatus nvarchar50 not null invoicestatusid as case invoicestatus when sent then when complete then when received then end go index create nonclustered index ix invoice on invoice customerid asc include invoicestatusid go get that it is stored at the leaf level but if the value is not persisted how is anything stored at all how does the index help sql server find these rows in this situation any help greatly appreciated many thanks edit thanks to brent aaron for answering this heres the pastetheplan clearly showing what they explained
163557 am trying to calculate running total but it should reset when the cummulative sum greater than another column value create table reset runn total id int identity11 val int reset val int grp int insert into reset runn total values select row numberoverpartition by grp order by idas rn into test from reset runn total index details create unique clustered index ix load reset runn total on testrn grp sample data id val reset val grp expected result id val reset val running tot greater than reset val reset greater than reset val reset greater than reset val reset query got the result using recursive cte original question is here https stackoverflow com questions reset running total based on another column with cte as select rnid val reset val grp val as running total iif val reset val as flag from test where rn union all select iifc flag val running total val iifiifc flag val running total val reset val from cte join test on grp grp and rn rn select from cte is there any better alternative in sql without using clr
163875 have table and need to update some names but was wondering about the following queries will both do the same query1 update mytable set name replacenamejeffjoe query2 update mytable set name joe where name jeff
164043 given the use case tenant data should not cross talk one tenant does not need another tenants data each tenant could potentially have large historical data volume sql server is hosted in aws ec2 instance each tenant is geographically distant there is an intention to use third party visualization tools such as powerbi embedded the data volume is expected to grow over time the cost of the system is constrained the solution must be maintainable without production dba the solution should be able to scale horizontally total number of tenants is less than what would be recommended architecture are there any reference implementations for this use case believe many people might have already faced this problem for enterprise software development think this is different situation from handling growing number of tenants in multi tenant database architecture the use case mentioned in that question deals with higher number of tenants which is very different from having very few large tenants the architecture mentioned might be solution here which is what want to know more about
164046 have tables that am using right join on the data is setup like below and my issue with it is that numbers being returned are inaccurate such that when execute the query get this returned which is double and triple the accurate value vendor totalsales totalcases vendor but if you manually do the math it should be vendor totalsales totalcases vendor what must change in my query so that the above results are returned declare bbc table vendor varchar250 vendorcasenum varchar100 casenumdate date declare allvendor table vendor varchar250 declare totalsalesamt table vendor varchar250 saleamt decimal102 insert into totalsalesamt vendor saleamt values vendor vendor vendor vendor vendor vendor insert into allvendor vendor values vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor vendor insert into bbc vendor vendorcasenum casenumdate values vendor a12344 vendor a12311 vendor a12889 vendor a12988 vendor a12931 vendor a12199 vendor e12331 vendor e12391 vendor e12300 vendor e11001 vendor e12301 vendor e12221 select av vendor totalsales sumisnulltsa saleamt0 totalcases countbbc vendorcasenum from allvendor av left join bbc bbc on av vendor bbc vendor left join totalsalesamt tsa on tsa vendor av vendor group by av vendor order by av vendor asc edit also tried using cte to achieve my desired outcome but came out with the same incorrect results with tsa as select vendor saleamt from totalsalesamt bbc as select vendor vendorcasenum from bbc select av vendor totalsales isnullsumtsa saleamt0 totalcases countbbc vendorcasenum from allvendor av left join tsa tsa on tsa vendor av vendor left join bbc bbc on bbc vendor av vendor group by av vendor order by av vendor
164166 consider the following example declare suppliednumber money set suppliednumber select ah firstname ah lastname from accountholders as ah join dbo accounts as on ah id accountholderid group by ah firstname ah lastname having suma balance suppliednumber as you can see there is no ordering of any kind in the query but looking in the execution plan see implicit sorting why is this happening whats the technical reason behind this im using microsoft sql server rtm cu14 kb3158271 x64 may copyright microsoft corporation express edition bit on windows nt build
164260 was watching years old webinar done by brent ozar https youtu be kle3gkahc and heard of several items being recommended at that time sqldiag utility sqlnexus pal tool database tuning advisor wizard bpa best practices analyzer sql server policy based management are all of them still to be used considered or there is something newer that replaced them
164329 have two tables products and products discounts if product is designated as top product and it is missing defined discount in products discounts want to insert record into products discounts for each missing record the inserted values would consist of the product id and two other hard coded values in the insert into products id product id name top ten products discounts id product id discount amount discount description the following is how plan to do this manually but since ill be repeating this process few times year im looking to see if this can be done in few lines of sql get the list of products that meet my criteria select product id pd discount amount from products as left outer join products discounts as pd on product id pd product id where top ten and pd product id is null use text editor to construct each individual insert into statement id use the output of step to get the product ids but the other two values would remain the same hard coded insert into products discounts product id discount amount discount description values product id goes here top product can these steps be combined into few lines of sql
164382 am getting confused with choosing index reorganize rebuilding of indexes based on avg fragmentation in percent returned by sys dm db index physical stats dm function msdn says and also avg fragmentation in percent value says percentage of logical fragmentation this is the percentage of out of order pages in the leaf pages of an index existed in index reorganize of index will always fixes physical ordering of pages so even avg fragmentation in percent is greater than also can consider reorganize of index it will be decreasing the percentage of fragmentation in index please look below example select database id object id avg fragmentation in percent avg page space used in percent page count avg fragment size in pages from sys dm db index physical stats db idbpigtn gal app tst object idnm ppa projection master null null sampled result database id object id avg fragmentation in percent avg page space used in percent page count avg fragment size in pages in above example we can see of average fragmentation it means index is having more logical fragmentationunordering of pages is more so am going to reorganizing of index which we have seen in above image please look below example for further alter index pk nm ppa projection master projection details sid rs contract sid on nm ppa projection master reorganize go select database id object id avg fragmentation in percent avg page space used in percent page count avg fragment size in pages from sys dm db index physical stats db idbpigtn gal app tst object idnm ppa projection master null null sampled result database id object id avg fragmentation in percent avg page space used in percent page count avg fragment size in pages above we have re organized the index so after reorganize average fragmentation reduced to percent and also space used is increased from percent to percent msdn suggesting avg fragmentation in percent value to be consider for reorganize rebuild of index but avg fragmentation in percent value says only logical fragmentation not amount of space which is not used in index so am little bit confused about choosing between re organize re build of index can we consider both avg fragmentation in percent and avg page space used in percent values for choosing rebuild an index or reorganize an index any one please suggest me the exact parameters to be consider for choosing rebuild or re organize of an index
164463 having read great deal about the differences between temporary tables and table variables in sql server am experimenting with switching from mostly using temporary tables to mostly using table variables they seem to be better fit for the types of queries usually work with in these queries the tables hold unique identifiers that drive the lookup process its been my habit when working with temporary tables to include primary key constraint so that the query optimizer is aware that it wont see any duplicates however given that the optimizer in most circumstances and for my queries assumes that table variable only holds single row which is unique by definition is the query optimizer going to make choices any differently if theres primary key constraint technically it assumes there are no rows but replaces the zero with one because the zero interacts very poorly with the rest of the estimation process but it also depends on whether the table variable is populated or not when the query is compiled there is some background information here whats the difference between temp table and table variable in sql server im currently using sql server but would be curious if the behavior changes in newer versions as has been pointed out primary key constraint comes with clustered index that gives the query optimizer more choices on how to get data out of the table variable was aware of this and thinking about the rest of the query plan but after attempting to clarify my question ive decided that the question was attempting to ask was too broad and probably particular to my extreme situation nothing but navigational type queries into half trillion row tables with an expectation of sub second performance so am going to leave my question as is
164526 have query which runs in an acceptable amount of time but want to squeeze the most performance possible from it the operation im trying to improve is the index seek at the right of the plan from node ive added appropriate indexes but the estimates that get for that operation are half of what they are supposed to be ive looked for changing my indexes and adding temporary table and re writing the query but couldnt simplify it more than this in order to get the right estimates does anyone have any suggestions on what else can try the full plan and its details can be found here the non anonymized plan can be found here update have feeling the initial version of the question raised lot of confusion so im going to add the original code with some explanations create procedure dbo someprocedure astype int customattrvalids idlist readonly as begin set nocount on declare dist ca id int select into temp from customattrvalids where id is not null select dist ca id countdistinct customattrid from customattributevalues inner join temp on id id select id assortmentid from assortments inner join assortmentcustomattributevalues acav on id acav assortment id inner join customattributevalues cav on cav id acav customattributevalue id where assortmenttype astype and acav customattributevalue id in select id from temp group by assortmentid id having countdistinct cav customattrid dist ca id optionrecompile end answers why the odd initial naming in the pastetheplan link answer because used anonymize plan from sql sentry plan explorer why option recompile answer because can afford recompiles in order to avoid parameter sniffing the data is could be skewed have tested and am happy with the plan that the optimizer generates while using option recompile with schemabinding answer id really want to avoid that and would use it only when have indexed view anyway this is system function count so no use for schemabinding here answers to more possible questions why do use insert into temp from customattrributevalues answer because noticed and now know that when using variables plugged into query any estimates that come out of working with variable is always and tested putting the data into temp table and the estimated is then equal with actual rows why did use and acav customattributevalue id in select id from temp answer could have replaced it with join on temp but developers were very confused and proffered the in option dont really think there would be difference even by replacing and either way there is no problem with this
164711 been looking to pivot rows progress check into columns check check etc no sum or totals required just to display the results hopefully can anyone help thanks ad
164922 while working on the query below in order to answer this question how to query chart data in database agnostic way having the following tables create table dbo foo creation datetime not null value money null dt as convert date creation persisted add clustered index on the dt column create clustered index ci foo on foodt go and this other table for joining create table bar dt date primary key clustered go the loading of data into these tables can be found here but when running the following query with radhe as select the row row number overpartition by dt order by dt the date dt the number of records on this day case when dt is null then else count over partition by dt end the total value for the day coalescesumf value over partition by dt from bar left outer join foo on dt dt get rid of the duplicates and present the result select the date the number of records on this day the total value for the day from radhe where the row get something like this picture below which is exactly what was looking for but the execution plan generated has several sort and nested loops operations as you can see on the picture below the full query plan can be found here this is very simple operation left outer join between tables the indexes are already ordered and therefore was wondering if could simplify the query plan alternatively could change the query code why exactly do we need nested loops times and sort times in the query plan
165230 using always encrypted we are attempting to encrypt one column of type varchar50 in table that contains million rows with the deterministic encryption after while the wizard returns failure due to the following error exception of type system outofmemoryexception was thrown any ideas on how to prevent this error
165439 ive got database running on sql server dbms consider that the server running sql server is not part of the network domain when execute this sql command alter database mydatabase set trustworthy on service broker queue early setted and working receives messages but the activeted stored procedure associated to the queue doesnt start messages continue to flow in to the queue but nothing else happens in the sql server log ive found this message the activated proc proofschema mysp running on queue proofschema myqueue output the following the database owner sid recorded in the master database differs from the database owner sid recorded in database mydatabase you should correct this situation by resetting the owner of database mydatabase using the alter authorization statement if run this same configuration on machine inside network domain everything works cant understand what is happening and why trustworthy crush with servi broker queue
165798 have solved the query problem by using row number over partition by this is more general question on why we cannot use columns with null values in joins why cant null be equal to null for the sake of join
165912 have generated trace file in sql server not really sure about the unit of duration it is measured by millisecond
165966 am mainly net developer using entity framework orm however because dont want to fail in using the orm am trying to understand what happens within the data layer database basically during the development start the profiler and check what some parts of code generate in terms of queries if spot something utterly complicated orm can generate awful queries even from rather simple linq statements if not carefully written and or heavy duration cpu page reads take it in ssms and check its execution plan it works fine for my level of database knowledge however bulk insert seems to be special creature as it does not seem to produce showplan will try to illustrate very simple example table definition create table dbo importingsystemfileloadinfo importingsystemfileloadinfoid int not null identity1 constraint pk importingsystemfileloadinfo primary key clustered environmentid int not null constraint fk importingsystemfileloadinfo references dbo environment importingsystemid int not null constraint fk importingsystemfileloadinfo importingsystem references dbo importingsystem filename nvarchar64 not null fileimporttime datetime2 not null constraint uq importingsystemimportinfo envxis tablename unique environmentid importingsystemid filename fileimporttime note no other indexes are defined on the table the bulk insert what catch in profiler one batch only insert bulk dbo importingsystemfileloadinfo environmentid int importingsystemid int filename nvarchar64 collate latin1 general ci as fileimporttime datetime27 metrics items inserted cpu reads writes duration total table count for my application thats ok although the reads seems rather large know very little about sql server internals so comparing to the 8k page size and the small record information have question how can investigate if this bulk insert can be optimized or it does not make any sense since it is arguably the fastest way to push large data from client application to sql server
166024 would like to create stored procedure that will create row in table for every day in given date range the stored procedure accepts two inputs start date and end date of the date range desired by the user so lets say have table like so select day currency from conversiontable day is datetime and currency is just an integer to keep things simple lets just say always want the currency column to be for each of these inserted rows so if someone inputs march as the start date and april as the end date would like the following rows created whats the best way to code the stored procedure to do this am using sql server r2 in my test environment but our real environment uses sql server so can upgrade my test machine if there is new functionality introduced in that makes this task easier
166117 im building database with postgres where theres going to be lot of grouping of things by month and year but never by the date could create integer month and year columns and use those or could have month year column and always set the day to the former seems bit simpler and clearer if someone is looking at the data but the latter is nice in that it uses proper type
166205 need to restore sql server database to sql server is there any possible way to do this without having to install complete copy of sql server restore to change the compatibility level and then restore the backup to know this would work but really dont want to install r2 just for this single purpose unless have no other choice
166374 have situation think can be solved using window function but im not sure imagine the following table create table tmp date timestamp id type integer insert into tmp date id type values id like to have new group at each change on column id type 1st group from to 2nd starting and finishing at and so on after it works want to include more criteria to define groups at this moment using the query below select distinct minmindate over as begin maxmaxdate over as end id type from tmp group by id type window as partition by id type order by begin get the following result begin end id type while id like begin end id type after solve this first step ill add more columns to use as rules to break groups and these others will be nullable postgres version we have postgres with postgis so it is not easy to upgrade postgis functions changes names and there are other problems but hopefully we are already re writing everything and the new version will use newer version with postgis
166493 have query that takes about hours to run on our server and it doesnt take advantage of parallel processing about million records in dbo deidentified records in dbo namesmultiword the server has access to cores update dbo deidentified with tablock set indexedxml dbo replacemultiwordindexedxml de461 dbo replacemultiwordde461 de87 dbo replacemultiwordde87 de15 dbo replacemultiwordde15 where inprocess and replacemultiword is procedure defined as select body replace bodynamesreplacement from dbo namesmultiword order by wordlength desc return body nvarcharmax is the call to replacemultiword preventing forming parallel plan is there way to rewrite this to allow parallelism replacemultiword runs in descending order because some of the replacements are short versions of others and want the longest match to succeed for example there may be george washington university and another from washington university if the washington university match were first then george would be left behind technically can use clr im just not familiar with how to do so
166747 im trying to delete bunch of rows from table matching query the general form of my query is delete from mytable where id in select id from mytable where the id column is serial primary key if run the inner select query on its own it runs in about second and returns about rows but when add the delete call onto it it seems to just sit and chug away and chug away let it run for about minute or so and then cancelled it assuming no progress was being made added explain analyze onto the front of it to see if could check what was taking so long but that call also hangs for very long period of time can anybody tell me what could be going on such that its very quick to identify the rows id like to delete but takes an indeterminate amount of time to delete them update the full query is as follows delete from cards where id in select id from cards left join game results on game results card id cards id where not available and game id is null simplified version of the tables would be create table cards id integer primary key available boolean create table games id integer primary key create table game results game id integer references games card id integer references cards im sure this query would eventually finish running im just surprised at how long its taking given that retrieving all of the ids to delete is so quick solution the problem was three fold had one of the earlier very slow queries running that had been triggered by an aborted script the query was still running even though the script had been aborted as such operations like trying to drop indexes were locked waiting for the query to finish manually cancelling that query using pg cancel backend allowed me to experiment with dropping indexes and foreign key constraints foreign key constraints seem to be the issue slowing everything down the fact that the game results table referenced cards made the delete take forever the funny thing is was only deleting cards explicitly unused in the game results but of course that didnt stop the foreign key checks from happening upon delete that made things very slow dropped the foreign key constraint before running the delete and that sped things up to level felt was appropriate for whatever reason the second delete query in the accepted answer ran much quicker than the first by combining these three things was able to do the whole delete in few seconds whereas until had combined these three factors together my queries were running to minutes before was cancelling them thanks to all for the help
166792 does postgresql support generated columns also know as virtual columns am not talking about identity columns can find any information on this remarkable feature but know that it is available on sql server and in the latest versions of mariadb mysql the feature is mentioned in the sql standard and there was some discussion on the postgresql forums around but can find anything substantial on the matter there is some discussion on so but it is quite old now so it may well be out of date
166843 lets make few assumptions have table that looks like this facts about my set size of the whole table is rows have 100k rows with value in column similar for other values that means 100k distinct values in column most of my queries will read all or most of the values for given value in select sumb from where the table is written in such way that consecutive values are physically close either its written in order or we assume cluster was used on that table and column the table is rarely if ever updated were only concerned about read speed the table is relatively narrow say bytes per tuple bytes overhead now the question is what kind of index should be using my understanding is btree my issue here is that the btree index will be huge since as far as know it will store duplicate values it has to since it cant assume the table is physically sorted if the btree is huge end up having to read both the index and the parts of the table that the index points to we can use fillfactor to decrease the size of the index bit brin my understanding is that can have small index here at the expense of reading useless pages using small pages per range means that the index is bigger which is problem with brin since need to read the whole index having big pages per range means that ill read lot of useless pages is there magic formula to find good value of pages per range that takes into account those trade offs gin gist not sure those are relevant here since theyre mostly used for full text search but also hear that theyre good at dealing with duplicate keys would either gin or gist index help here another question is will postgres use the fact that table is clustered assuming no updates in the query planner by binary searching for the relevant start end pages somewhat related can just store all my columns in btree and drop the table altogether or achieve something equivalent believe those are clustered indices in sql server is there some hybrid btree brin index that would help here id rather avoid using arrays to store my values since my query will end up less readable that way understand this would reduce the cost of the bytes per tuple overhead by reducing the number of tuples
166877 am using sql server management studio on windows im tired of using shift ctrl up down right left keys to select the code want to run im wondering whether there are shortcuts snippets to select block of code that separated from other code by blank lines here is code example select from tab1 select from tab2 select from tab3 say my cursor is inside the middle block and whats the best way to select the middle block
167031 title says it all strangely cant find result on this
167036 cleared my stats and ran my query the actual execution plan has total estimated cost of used the total worker time column from the dm exec query stats dmv to calculate an average of microseconds of cpu time sumquery stats total worker time sumquery stats execution count the plan recommended an index and created that index cleared my stats and ran the query again this time the plan has total estimated cost of at the top level checked the dm exec query state dmv again and now the average cpu time is microseconds was expecting the worker time to be around half not over double am missing something here why is the improvement to the query plan not reflected in the exec query stats the two plans are uploaded here plan1 plan2
167037 have timestamp column used for scheduling calendar events would like to change only the time part while retaining the date part untouched how can possibly do it im on postgresql current to
167086 was reviewing the sql server physical operators listed on technet dont judge you know youve done it and read that the hash match physical operator is sometimes used to implement the union logical operator have never seen that done and would like to learn more an example query would be great when is it used and when is it better than the alternatives those are usually the same but not always
167096 in this answer erwin brandstetter says countstep or null over order by date is the shortest syntax that also works in postgres or older count only counts non null values in modern postgres the cleaner equivalent syntax would be countstep filter where step over order by date im unsure of why countstep or null is preferred though in my query do the following renamed my variables to match his while maintaining the syntax case when lagid type over order by date id type then end as step were counting the values returned by that note that the case can only return or null if the two are not equal is returned if they are equal it returns null which isnt counted erwins answer has this assumes involved columns are not null else you need to do more so im even more confused what is the point of adding countstep or null what is this protecting our query against could anyone break this down and perhaps show two examples with data wherein only one of them the one with countx or null works
167201 im just getting started with postgres reading this document came across this query select title ts rank cdtextsearch query as rank from apod to tsqueryneutrino dark matter query where query textsearch order by rank desc limit can understand everything in this query except for this from apod what does this mean im used to joins but not to multiple from statements separated by comma searched the net for no avail after looking at it and thinking it seems to me that its declaring variable called query so it can use it multiple times but if this is true what does it have to do with from
167486 over in gis se many of us use esri geodatabases esri describes geodatabases as being object relational what are object relational databases and why is it necessary to use this model in spatial databases it seems like theyve taken something simple the relational database model and made it into something complicated id like to understand what the benefit is im not dba or developer so laymans terms would be appreciated
167489 have inherited some sql server databases there is one table ill call with about million rows and columns wide from source database ill call on sql server standard that gets etld over to target database ill call with the same table name on sql server r2 standard edit some people have asked if the source table is the only source to the target table yes it is the only source as far as the etl goes there isnt any real transformation happening it effectively is intended to be copy of the source data therefore there are no plans to add additional sources to this target table little over half of the columns in are varchar source table of the columns are varchar80 of the columns are varchar30 of the columns are varchar8 similarly the same columns in are nvarchar target table with the same of columns with the same widths in other words same length but nvarchar of the columns are nvarchar80 of the columns are nvarchar30 of the columns are nvarchar8 this is not my design id like to alter target columns data types from nvarchar to varchar want to do it safely without data loss from conversion how can look at the data values in each nvarchar column in the target table to confirm whether or not the column actually contains any unicode data query dmvs that can check each value in loop of each nvarchar column and tell me if any of the values is genuine unicode would be the ideal solution but other methods are welcome
167709 is there anyway to change column data type so that it will accept only negative values its sql server azure
167727 lets say have query select from mytable where myparam or mycolumn myparam here myparam is parameter and optional so it only check mycolumn myparam if myparam is not but our dba is saying or will makes it slow and db will suffers another option is if myparam select from mytable where mycolumn myparam the problem with this approach is that we have lot of optional parameters so our query become very very big another option is case so what you guys suggest am talking about in general whether oracle or sql server
168022 suppose we have the following queries select count from some big table where some col some val select count from select from some big table where some col some val does any of the previous queries perform better or are they the same im using postgresql but are there big difference to another dbms ps im asking this question because sqlalchemy python based orm executes the count operation by default on subquery but there is an option to force it to execute the count directly on the query
168116 we currently have some indexes where allow pagelocks is set to off this presumably was done in order to reduce deadlocks however doubt that it would really had an effect back then now am trying to understand when sql server actually chooses to start locking pages rather than keys in clustered index asked jonathan keyhaisas recently and he told me that this could happen if am touching rows on several subsequent pages however didnt manage to get any exclusive page locks by updating rows in clustered index with sample query could you help me understand page locks better with sample query and table am running sql server sp4 thanks in advance martin
168134 im using postgres want to search for rows in which my name column does not contain space im little murky on how to define space to you though thought it would just be the space bar on my keyboard so ran where name not like but then got some results like this jason falkner that sure looks like space to me but there are probably some other things going on is there better way can scan for rows in which my name column doesnt contain space using regexp not name still returned columns that looked like they had space using select castname as bytea where name not like like returned x4a41534f4ec2a0424c414b45 however im still little unclear how use that data to figure out how to screen spaces from my results tried where not name space and its returning jason blake with the same byte sequence above x4a41534f4ec2a0424c414b45
168276 what this isnt about this is not question about catch all queries that accept user input or use variables this is strictly about queries where isnull is used in the where clause to replace null values with canary value for comparison to predicate and different ways to rewrite those queries to be sargable in sql server why dont you have seat over there our example query is against local copy of the stack overflow database on sql server and looks for users with null age or an age select count from dbo users as where isnullu age the query plan shows scan of quite thoughtful nonclustered index the scan operator shows thanks to additions to actual execution plan xml in more recent versions of sql server that we read every stinkin row overall we do reads and use about half second of cpu time table users scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads sql server execution times cpu time ms elapsed time ms the question what are ways to rewrite this query to make it more efficient and perhaps even sargable feel free to offer other suggestions dont think my answer is necessarily the answer and there are enough smart people out there to come up with alternatives that may be better if you want to play along on your own computer head over here to download the so database thanks
168303 want to extract complete list of the node names and their paths from any arbitrary well formed json document stored as nvarchar value in sql server is there reliable way to do this for example for json value declare json doc nvarchar4000 name1 value1 name2 value2 get this result upon querying json doc node name name1 name2
168402 am running postgresql on windows it seems it has an update monitor that does not use any information about the proxy agent from windows we have corporate proxy it also does not look at the environment variables http proxy or https proxy which have set to add credentials via cntlm for our corporate firewall this is useful for programs such as npm and git which need to get past our outgoing firewall so am getting the following message every or some minutes as pop up from program called program files x86 postgresql updatemonitor bin updmanager exe execute program files postgresql bin stackbuilder exe the message is very deceiving but tracked down the program and it is the one listed above
168590 not using fk constraints is my companys untold rule fk constraints are used only when designing erd and not used when creating tables according to my senior in real practice those are very time consuming obstacles when we are dealing with urgent issues he says when we need to use insert update delete statements immediately the constraints block those statements to be executed and writing statements while keeping the constraints is time consuming too even heard many other companies are doing this while kinda understand the struggles im not sure if this is good way since its quite opposite to my understanding to db this company is my first job too so dont know how other companies dealing with this in practical way what is your opinion about this can this be justified are there better ways how other companies are doing about this matters update it seems it is quite common approach for south korean companies asked few other seniors who worked for other companies and most of them are saying they all doing like this and even one of them worked for financial company interesting
168726 is there way how to inject cardinality estimation to sql server optimizer any version something similar to oracles cardinality hint my motivation is driven by the article how good are query optimizers really where they test the influence of the cardinality estimator on selection of bad plan therefore it would be sufficient if could force the sql server to estimate the cardinalities precisely for complex queries leis viktor et al how good are query optimizers really proceedings of the vldb endowment
168815 select power2 returns instead of it seems to have only digits of precision rounding the 17th even making the precision explicit select powercast2 as numeric380cast64 as numeric380 it still returns the rounded result this seems like pretty basic operation for it to be flaking out arbitrarily at digits of precision like this the highest it can calculate correctly is only power2 failing for power2 what is going on here whats really terrible is that select actually returns the right value so much for terseness
168833 am wanting to divide int quantity by number but my output is not what am after this is my ddl create table orders id int identity11 primary key not null partid varchar100 not null qtyordered int default orderedby varchar100 not null ordereddate date default getdate insert into orders partid qtyordered orderedby values ss100 james rr200 bob nn300 jake oo400 blue now have trired using the sql server functions ceiling and floor but am not getting my desired output this is the query tried what do need to do in sql server to get an output of select partid ceilingqtyordered as first want to be ceilingqtyordered as second want to be floorqtyordered as third want to be from orders where partid rr200
169123 we are beginning to provision set of physical servers for virtual cluster of sql server nodes within vmware we will be utilizing enterprise edition licenses we plan on setting up nodes but there is bit of debate on what the ideal way to provision the physical servers with regards to cpu clock speed versus cpu core count know this is largely dependent on transaction volume and number of databases stored among other software specific factors but is there general rule of thumb that is advised for instance is dual core ghz physical server cores more preferential to dual core ghz server cores has anyone come across white paper that further delves into this type of topic
169372 trying to run an update but want to step through it as its quite bit of data and dont want to blast it all at once in oracle its bit easier to select the rows want because you can include it in the where clause but sql you include it in the select im wanting to write it something along the lines of this update instances set thing new thing where rownum basically want to do at time is this syntax possible in tsql
169458 am trying to create column for my table only if it does not exist have researched lot but could not find any solution yet is this really possible to conditionally create column
170579 have sql query given below want to select multiple value using like operator is my query correct select top employee id employee ident utc dt rx dt from employee inner join employee mdata history on employee ident employee mdata history employee ident where employee id like emp1 emp3 order by rx dt desc if not can anyone correct me my table has large amount of data starting with emp1 and emp3 can filter the result by top emp1 and top emp3 based on rx dt
170803 have made script that one at time deletes all the foreign keys from database just like this alter table mytable1 drop constraint fk mytable1 col1 alter table mytable2 drop constraint fk mytable2 col1 alter table mytable2 drop constraint fk mytable2 col2 what surprises me is that the script takes long time on average seconds for each drop fk now understand that creating fk may be big deal because server has to go and check that the fk constraint is not infringed from the beginning but dropping what does server do when dropping fks that takes so long this is both for my own curiosity and to understand if there is way to make things faster being able to remove fk not just disable them would allow me to be much faster during migration and therefore minimize downtime
170849 can use truncate command with where clause need to remove specific rows from several tables how can delete specific data from the entire database select dimemployee firstname dimemployee lastname salesordernumber shipdatekey from dimemployee join factresellersales on dimemployee employeekey factresellersales productkey where dimemployee firstname like kevin have to truncate this specific name from entire db is there any other method to remove specific data from entire db in my database there are tables wanted to delete specific name and its corresponding columns from the entire database the name is spread across entire database hence want to remove it in single shot instead of going to each table and deleting it individually
171030 as my performance tuning skills never seem to feel sufficient always wonder if there is more optimization can perform against some queries the situation that this question pertains to is windowed max function nested within subquery the data that im digging through is series of transactions on various groups of larger sets ive got fields of importance the unique id of transaction the group id of batch of transactions and dates associated with the respective unique transaction or group of transactions most times the group date matches the maximum unique transaction date for batch but there are times where manual adjustments come through our system and unique date operation occurs after the group transaction date is captured this manual edit doesnt adjust the group date by design what identify in this query are those records where the unique date falls after the group date the following sample query builds out rough equivalent of the my scenario and the select statement returns the records im looking for however am approaching this solution in the most efficient manner this takes while to run during my fact table loads as my record counts number in the upper digits but mostly my disdain for subqueries makes me wonder if theres better approach here im not as concerned about any indexes as im confident those are already in place what im looking for is an alternative query approach that will achieve the same thing but even more efficiently any feedback is welcome create table example uniqueid int identity11 groupid int groupdate datetime uniquedate datetime create clustered index cx on example uniqueid asc set nocount on populate some test data declare int int uniquedate datetime groupdate datetime while begin if begin set uniquedate getdate end else begin set uniquedate getdate end set groupdate getdate insert into example groupid groupdate uniquedate values groupdate uniquedate set if begin set end end set nocount off create nonclustered index ix on example groupid asc uniquedate asc groupdate asc include uniqueid identify any uniquedates that are greater than the groupdate within their groupid select uniqueid groupid groupdate uniquedate from select uniqueid groupid groupdate uniquedate maxuniquedate over partition by groupid as maxuniquedate from example calc maxud where maxuniquedate groupdate and maxuniquedate uniquedate drop table example dbfiddle here
171194 have several sql objects that need to take alternate actions based on desired state of the request is there way to create database level constants enumerations that can be passed to stored procedures table valued functions and used in queries without using clr create procedure dbo dosomework param1 integer enumvalue myenumtype as and then use it exec dosomework myenumtype enumvalue1 myenumtype enumvalue2 where myenumtype would hold few enumeration values in the procedure would be able to use enumvalue and test it against values in myenumtype to do the required work would make the values of myenumtype bitmask for the case am considering for simple example consider an expensive process that takes huge dataset and reduces it to smaller but still very large dataset in this process you need to make some adjustment in the middle of that process that will affect the result say this is filter for or against some types of records based on some status of an intermediate calculation within the reduction the enumvalue of type myenumtype could be used to test for this select from where enumvalue myenumtype enumvalue1 myenumtype enumvalue1 and or enumvalue myenumtype enumvalue2 myenumtype enumvalue2 and or are these sort of database level constants possible in sql server without the use of clr am seeking database level enumeration or set of constants that can be passed as parameters to stored procedures functions and so on
171337 edit sql server im hoping this is generic enough question need not specify version but most of the instances im working with are or later am not good enough to mock up data and actually test this so im hoping someone could look at it and answer with simple experience imagine you have state table with american state abbreviations in column and its indexed like good lookup column when writing ad hoc queries users will often hit up this column to filter on but using criteria that represents information that is not implicit in the database for example if they want to get big states they may include filter in their ad hoc query that shows something like where stateabbreviation in ak tx aka the dreaded business rules so this query is fine it performs well and makes use of the index but man what bear to write every time we need to query the big states im tempted to create view with this filter in its definition to make it easier on them the problem here is that those business rules are specific to some number of ad hoc queries supporting line of business but dont really have universal uses so creating view that filters out data in this way will have little utility so instead of writing view where that criteria is in the filter want to write view where the criteria result is calculated such as select stateabbreviation isbig case when stateabbreviation in ak tx then else end from tblstates now when they want to write query for big states they just include where isbig in the query so my question is simple if the view were called with that criteria can the index on stateabbreviation be used know there are all kinds of things could do inside case statement that may change the answer so for the purposes of answering this very specific question assume the case statement will only ever look like that it will not make use of multiple fields it will not aggregate just simple in or out calculation of literals to expose complex filter criteria to report writers in simpler way
171365 have script to generate the create table script you can see it on my answer to this question however now need to script my partition functions and partition schemes on this question here see the answer has examples on how to add partition to table and how to remove partition from table and would like to have look at the scripts at any stage have found this link how to find partition function text applied to table how to generate the scripts for create partition function and partition schema was working on script to generate the create partition function as you can see below but it only works for my own pf year partition function because it was tricky to get over the fact that sys partition range values has sql variant data type as value when used case got the following error msg level state line operand type clash int is incompatible with date then in his great answer dan guzman showed us sql variant property some wonderful thing that was not aware of now am new dba and person set transaction isolation level read uncommitted select radhe create partition function space1 quotenamespf name coalescebaset namedata type not found char13 as range case when castspf boundary value on right as int then right else left end char13 for values select stuff select text cast case st system type id when then convert date sprv value126 when then convert int sprv value else convert int sprv value end as nvarchar from sys partition range values sprv where sprv function id spf function id order by sprv boundary id for xml path 1n st system type id spf name as name spf function id as id castspf boundary value on right as int as rangetype spf create date as createdate spf fanout as numberofpartitions from sys partition functions as spf inner join sys partition parameters as spp on spp function id spf function id inner join sys types as st on st system type id st user type id and spp system type id st system type id left outer join sys types as baset on baset user type id spp system type id and baset user type id baset system type id or baset system type id spp system type id and baset user type id spp user type id and baset is user defined and baset is assembly type where spf name pf year
171425 inside stored procedure have the following sql server set transaction isolation level serializable begin transaction getstuff begin try some selects updates etc etc commit transaction getstuff end try begin catch end catch since this is transaction based my thought was the rest of the database connections will not be affected by the serializable do need to implicitly set isolation level to read committed after my commit will this adversely effect other connections between my application server and database server
171510 am just trying to look up more verbose information for maintenance plan task that failed open up the log file viewer and check the box to view the logs for my maintenance plan there are no active filters besides that in this example then play the waiting game had it take almost minutes to get the logs to show so thought that it might just taking the time to load all logs from the beginning of this servers time reduced the filter to about days since that is all needed anyway that came up faster but still took minutes to show note this is the first time on my computer that have been trying to view these logs so that might also play factor also tried to look at the logs directly on the server but was getting similar time results is this par for the course should expect viewing the logs to be an experience like this is there something that should be doing or checking do plan to check the log age and see if it can be purged but would that still affect looking at logs for only short day period
171654 consider the following contrived but simple query select id case when id then select top id from other table else select top id from other table end as id2 from heap would expect the final row estimate for this query to be equal to the number of rows in the heap table whatever im doing in the subquery shouldnt matter for the row estimate because it cannot filter out any rows however on sql server see the row estimate reduced to because of the subquery why does this happen what can do about it its very easy to reproduce this issue with the right syntax here is one set of table definitions that will do it create table dbo heap id int not null create table dbo other table id int not null create table dbo other table id int not null insert into dbo heap with tablock select top row number over order by select null from master spt values create statistics heap id on heap id with fullscan db fiddle link
171855 im looking for an option to create an alias command for psql console connection which allowed setting the search path but am not finding any option on psql util nor the option to execute command without exiting any idea would like to avoid preconditions as setting environment options and have oneliner if possible
172275 work for the insurance company little more than year my sql experience around years including ssis ssrs we have approximately tb of data is it possible to build data warehouse by myself should do that with my experience there are lots of materials and sql groups can get help from but still is it too complicated thanks
172289 have table of zip codes which includes the center lat lng for each zip code use it to get list of zip codes within given mile radius from any arbitrary point it just occurred to me that just because zips center point is not within given radius does not mean that the zip itself is not within the radius used my super advanced art skills to illustrate the point here the green stripy blobs represent zip codes and the red smudges are the geographic centers for each zip code the fuchsia dot is the target location and the lumpy blue circle is mile radius from the target location if run query for all the zip codes within mile radius from the pink smudge only zip codes and will be returned as the center point for zip is not within the one mile radius even though the pink smudge itself is clearly in zip code select distance unit degreesacoscosradiansp latpoint cosradiansz cosradiansp longpoint radiansz sinradiansp latpoint sinradiansz as dist from standard zip as join these are the query parameters select lat as latpoint lng as longpoint miles as radius as distance unit as on where between latpoint radius distance unit and latpoint radius distance unit and between longpoint radius distance unit cosradiansp latpoint and longpoint radius distance unit cosradiansp latpoint order by dist how the heck do write query that will include zip in the results have access to spatial geometry for each zip code that can add to the table if needed but have no idea how would use it for this purpose in mysql edit spent day reading the oracle and mysql docs for spatial data and managed to successfully convert my spatial data to mysql how do go about writing similar query that uses the geometry column instead of the lat and long am using 2d data the geometry are polygons and multipolygons only think sort of figured it out select from select minst distancegeom point as miles zip from zip spatial group by zip order by miles asc where miles ill leave the bounty open for now in case someone has better more efficient solution
172521 inline views allow you to select from subquery as if it were different table select from selecting from query instead of table select c1 from t1 where c1 where c1 ive seen this referred to using different terms inline views with clause cte and derived tables to me it seems they are different vendor specific syntax for the same thing is this wrong assumption are there any technical performance differences between these
172903 am using windows language serbian sql management studio and cant use database reports because there is error an error occurred during local report processing culture is not supported parameter name culture 0x0c00 is invalid culture identifier try with different settings for windows language english try with different settings for ssms english but got same error do you have idea how to solve this my windows system locale settings get winsystemlocale lcid name displayname sr latin rs serbianlatin serbia
173190 have query which selects rows from source database databasea and inserts them into target database database the collation type differs between the databases and they cannot be changed need to address the collation difference in my query by explicitly specifying the collation for varchar fields currently my query looks like this insert into databaseb dbo users id usernumber firstname surname address1 address2 addresstown addresscity select id usernumber firstname collate sql latin1 general ci as surname collate sql latin1 general ci as address1 collate sql latin1 general ci as address2 collate sql latin1 general ci as addresstown collate sql latin1 general ci as addresscity collate sql latin1 general ci as from databasea dbo users my question is can avoid typing the collation type for every string based field is there way that can specify the collation type for the whole query at once if this is not possible are there any other shortcuts
173309 this is bit of an diversion from the real problem if providing context helps generating this data could be useful for performance testing ways of processing strings for generating strings which need to have some operation applied to them within cursor or for generating unique anonymous name replacements for sensitive data im just interested in efficient ways of generating the data within sql servers please dont ask why need to generate this data ill try to start with somewhat formal definition string is included in the series if it only consists of capital letters from the first term of the series is the series consists of all valid strings sorted by length first and typical alphabetical order second if the strings were in table in column called string col the order could be defined in sql as order by lenstring col asc string col asc to give less formal definition take look at alphabetical column headers in excel the series is the same pattern consider how you might convert an integer to base number aa ab the analogy isnt quite perfect because behaves differently than in base ten below is table of selected values that will hopefully make it more clear row number string aa ab ay az ba bb zzz aaaa zzzy zzzz aaaaa hjunyv the goal is to write select query that returns the first strings in the order defined above did my testing by running queries in ssms with the result set discarded as opposed to saving it to table ideally the query will be reasonably efficient here im defining efficient as cpu time for serial query and elapsed time for parallel query you may use whatever undocumented tricks that you like relying on undefined or non guaranteed behavior is okay as well but it would be appreciated if you call that out in your answer what are some methods of efficiently generating the data set described above martin smith pointed out that clr stored procedure probably isnt good approach due to the overhead of processing so many rows
173335 occasionally see questions asking how to safely store user passwords for web application using an rdbms im not talking of facebook or twitter the usual answer is salt the password then hash it with strong algorithm such as tdes or sha512 my question is as an rdbms user why should bother at all with the password storing problematic at all since most engines have built in authentication mechanism for example if some user wants to create an account user password on my web application how is issuing the following query wrong create user with encrypted password in group baseuser then within my application the user can open connection to the database using his credentials and dont have to bother at all of password management see multiple advantages to this method if the rdbms decides that the encryption algorithm needs to be changed dont need to touch anything just to apply the security updates it is easy for me to manage the users authorizations if user is promoted to an administrators role just have to add the user to the corresponding group sql injections are now meaningless for manage permissions to allow exactly what want to allow to each user in the database for example in forum like so adding new posts answering to posts commenting and editing deleting his own questions answers comments an user account anonymous can be used for unauthenticated connections to my application each user is the owner of the data he provided but on virtually every question see on this topic there seems to be general consensus that this is not the way things have to be done my question is why note the third point is allowed by policies in postgresql and security policies in microsoft sql server realize that these concepts are newcomers but anyway now that they are here why doesnt the technique describe become the standard way to handle users accounts
173367 am attempting to only select the 1st row returned from the query this is my syntax insert into temp id salesid select rn row number overorder by psuserid psuserid from select distinct psuserid rstln bama from rusticlines rstln and tnr active where rn and get the error msg level state line invalid column name rn what do need to change so that it only selects the 1st row number
173435 usually design my databases following next rules nobody else than db owner and sysadmin have access to the database tables user roles are controlled at application layer usually use one db role to grant access to the views stored procedures and functions but in some cases add second rule to protect some stored procedures use triggers to initially validate critical information create trigger triggername on mytable before after insert as if exists select from inserted where field1 some initial value or field2 other initial value begin update mytable set field1 some initial value field2 other initial value end dml is executed using stored procedures sp mytable insert field1 field2 field3 sp mytable delete key1 key2 sp mytable update key1 key2 field3 do you think that on this scenario worth it to use default constraints or im adding an extra and unnecessary job to the db server update understand that by using default constraint im giving more information to someone else that must to administer the database but im mostly interested on performance assume that the database is checking always default values even if supply the correct value hence im doing the same job twice for example is there way to avoid default constraint within trigger execution
173831 when using join on many to many relationship the result is split on multiple rows what id like to do is convert the right side of join into an array so the result is one row an example will explain it better than can so have those tables create table items id serial primary key title text create table tags id serial primary key title text create table items tags item id int references itemsid tag id int references tagsid primary key item id tag id when selecting items with their tags can do it this way select id title title from items inner join items tags it on it item id id inner join tags on id it tag id and the result will come up as item n1 sport item n1 soccer item n2 adventure item n2 mountain climbing item n2 sport item n2 nature what id like to have is this item n1 sport soccer item n2 adventure mountain climbing sport nature
173868 want to know if is possible to transform numeric return int column in sequence of symbols direct in mysql select my table is like below id category product product product want sql select like select id some mysql fancy funccategory as symbol category from mytable that could returns the following id symbol category product product product is it possible can do it programmatically but this is not my intention want some built in or even custom function that can achieve this result direct in the select statement
173895 from this microsoft doc defines the string length and can be value from through max indicates that the maximum storage size is bytes gb the storage size is the actual length of the data entered bytes please help me understand this the max characters for varchar seems to be which is way less than 2gb worth of data see that there are records in this varcharmax column of specific table that have lenmycolumn thus know can get way more than characters into varcharmax column question how does the characters come into play and where should be aware of it question will net datareader query to this column always return the full result with character
174044 have question regarding piece of documentation on temp tables that recently read on technet the fourth paragraph of the temporary tables section on that page reads as follows if temporary table is created with named constraint and the temporary table is created within the scope of user defined transaction only one user at time can execute the statement that creates the temp table for example if stored procedure creates temporary table with named primary key constraint the stored procedure cannot be executed simultaneously by multiple users work in an environment where we make significant use of handful of stored procedures that use indexed temp tables and weve never encountered an issue where users are having to wait for one execution to complete before the next begins hope that will continue to be the case but im concerned that it could become an issue if this caveat is not properly understood specifically am unclear on the following points does this apply only to global temp tables or to local ones as well it seems strange that table that isnt visible outside of the session as in the latter case would prevent another session from executing simultaneously what qualifies as named constraint dont all constraints have names even if they are system generated is this referring to constraints with user defined alias this seems like poor phrasing to me does multiple users actually mean multiple sessions these procedures are called through our application using single service account so of calls to our scripts are made to the db by that single account and im unconcerned about the occasional call an admin may make on the backend if the service account can run the sproc in multiple sessions simultaneously then this issue is moot for my purposes
174069 in english we might talk about the relation between say bob and tim perhaps theyre cousins the term relation in this context makes sense to me in the context of relational databases understand what the term refers to but dont understand why it is used figure that understanding why it is used will help me to better understand the field so id like to understand why it is used why is for example person considered to be relation in english relation is noun that describes how two entities are associated it doesnt refer to the entities themselves in the context of relational databases relation refers to the entities themselves why understand that relational model came after the hierarchical and network models ex parent neighbor but in those models the entities also have relations to one another so why call this model the relational model is there more specific phrase term or maybe we should say that all three models are relational models but the hierarchical and network models are specific types of relational models what if we have standalone entities that dont relate to one another say person door and tree is the term relational still applicable perhaps this should be multiple questions figured that the answers are highly related perhaps theres just one answer so figured itd make sense for this to be single question if im wrong let me know and ill create separate questions instead edit this diagram may be useful to visualize that relation is relating different domains to one another
174219 im trying to create trigger to alter collation of database on its creation but how can catch the database name to use inside the trigger use master go create trigger trg ddl changecollationdatabase on all server for create database as declare databasename varchar200 set databasename db name alter database databasename collate xxxxxxxxxxxxxxxxxxx go obviously this is not working
174249 im using sql server enterprise ive come across sql plan that is exhibiting some behavior dont find entirely intuitive after heavy parallel index scan operation parallelism repartition streams operation occurs but it is killing the row estimates being returned by the index scan object10 index2 reducing the estimate to ive done some searching but havent come across anything that explains this behavior the query is quite simple though each of the tables contain records in the low millions this is part of dwh load process and this intermediate data set is touched few times throughout but the question have is related to the row estimates in particular can someone explain why accurate row estimates go to within the parallelism repartition strems operator also is this something should be concerned with in this particular situation ive posted the full plan to paste the plan heres the operation in question including the plan tree in case that adds any more context could be running into some variation of this connect item filed by paul white further in depth explination on his blog here at least its the only thing ive found that seems to be even remotely close to what im running into even though there is no top operator in play
174353 when run this command with sum select count as records sumt amount as total from dbo t1 as where id and id im getting arithmetic overflow error converting expression to data type int any idea on what is the cause of it im just following the instructions in this answer
174694 ill try to make graph from the data from my sql server database ill have all streets with the count of the users who are living in this street even the count is zero for this ive tried this query create table streets id int identity primary key name varchar100 create table users id int identity primary key username varchar100 streetid int references streetsid insert into streets values 1st street 2nd street 3rd street 4th street 5th street insert into users values pol doortje marc bieke paulien fernand pascal boma goedele xavier select name as street counts name as count from users inner join streets on streetid id group by name and gives me this output street count 1st street 2nd street 3rd street 4th street the problem is that ill to have the 5th steet where lives no one could do this with sql server here youve got fiddle update if do right join ive got this result street count 1st street 2nd street 3rd street 4th street 5th street see this fiddle
174739 before post connect item regarding the lack of documentation about this will someone confirm that am not simply missing something here on the docs page where format is listed as string function all built in string functions are deterministic string functions transact sql there is also no mention of format being nondeterministic on related pages deterministic and nondeterministic functions format transact sql however when attempting to create persisted computed column create table date col date insert into values getdate alter table add date formatted as formatdate colyyyy persisted returns the following error computed column date formatted in table cannot be persisted because the column is non deterministic the documentation states that if the culture argument is not provided the language of the current session is used but adding culture argument doesnt change things this also fails alter table add date formatted as formatdate col en us persisted rextester demo http rextester com zms22966 dbfiddle uk demo http dbfiddle uk rdbms sqlserver next fiddle 7fc57d1916e901cb561b551af144aed6
175073 we are troubleshooting server that has high cpu utilization after finding that the queries werent really causing it we started looking into compilations performance monitor is showing less than compilations sec and less than recompilations sec after running an xe session looking for compilations we are seeing thousands of compilations per second this system is using triggers to audit changes most of the compilations are due to triggers the triggers reference sys dm tran active transactions our first thought was that maybe referencing dmv in trigger would cause it to compile each time or maybe just this specific dmv would cause it so started testing that theory it does compile each time but hadnt checked if trigger compiles each time it is triggered when it doesnt reference the dmv and instead hardcodes value it was still compiling each time it got triggered dropping the trigger stops the compiles we are using sqlserver query pre execution showplan in an xe session to track the compilations why is there discrepancy between that and the perfmon counter is it normal that you get compilation event each time trigger runs repro script create table t1 transaction id int column2 varchar100 create table t2 column1 varcharmax column2 varchar100 go create trigger t2 ins on t2 after insert as insert into t1 select select top transaction id from sys dm tran active transactions column2 from inserted go both of these show compilation events insert into t2 values row1 value1 insert into t2 values row2 value2 go alter trigger t2 ins on t2 after insert as insert into t1 select column2 from inserted go both of these show compilation events insert into t2 values row3 value3 insert into t2 values row4 value4 drop trigger t2 ins these do not show compilation events insert into t2 values row5 value5 insert into t2 values row6 value6 drop table t1 t2
175674 often see statements like sql server log records every transction and opeation but am confused about what happens when transaction is eventually rolled back say an explicit transaction has statements statement statement statement and finally rollback statement now say when the execution has not reached the rollback statement will the modifications resulting from statements through get recorded to sql server log understanding statements through all get recorded sql server records everything no matter what understanding modifications are only stored somewhere in memory and only recorded to log when sql server see commit statement if it turns out to be rollback statement sql server simply ignore the transacrion no writing to log happens because it serves no purpose in other words sql server logs when there is net result before and after the transactions both seems logical at least to me but they cant both be right thanks for any help
175682 was working on demo involving ccis when noticed that some of my inserts were taking longer than expected table definitions to reproduce drop table if exists dbo stg create table dbo stg id bigint not null insert into dbo stg select top row number over order by select null rn from master spt values t1 cross join master spt values t2 drop table if exists dbo cci bigint create table dbo cci bigint id bigint not null index cci clustered columnstore for the tests im inserting all rows from the staging table thats enough to fill exactly one compressed rowgroup as long as it doesnt get trimmed for some reason if insert all of the integers mod it takes less than second truncate table dbo cci bigint insert into dbo cci bigint with tablock select id from dbo stg option maxdop sql server execution times cpu time ms elapsed time ms however if insert the same integers mod it sometimes takes over seconds truncate table dbo cci bigint insert into dbo cci bigint with tablock select id from dbo stg option maxdop sql server execution times cpu time ms elapsed time ms this is repeatable test that has been done on multiple machines there seems to be clear pattern in elapsed time as the mod value changes mod num time in ms if you want to run tests yourself feel free to modify the test code that wrote here couldnt find anything interesting in sys dm os wait stats for the mod insert wait type diff wait ms xe dispatcher wait qds persist task main loop sleep lazywriter sleep logmgr queue dirty page poll hadr filestream iomgr iocompletion sqltrace incremental flush sleep request for deadlock search xe timer event sleep task broker to flush checkpoint queue sos scheduler yield why does the insert for id take so much longer than the insert for id
176023 support an application in big enterprise one of my roles is to clean up data there is query need to execute every hour and would like to automate it due to organization policies cant create sql server agent jobs or modify schema can only manipulate data an endless while1 begin waitfor delay do work end does the job for me but shrug at the thought of perma open connection ideally would script the ms ss itself to execute given piece of code every hour but im not sure if that is possible is there any solution to this problem
176246 have sql server table which contains columns like this id int not null eventdate datetime not null other columns where the table has about half billion rows across range of about distinct id values the table has unique clustered index like this create unique clustered index myindex on dbo mytable id asc eventdate asc need to find the earliest per id eventdate which can obtain using query such as select id mineventdate from dbo mytable group by id however this query takes just under minutes to complete cant share the specifics query plans etc of the problem im looking at due to nda constraints but can advise that im seeing clustered index scan so its checking all the rows in the table given that the data is organised in eventdate sequence id expect that the retrieval could be much quicker but im not sure quite how any other id specific range query responds in few milliseconds and the table has recently been rebuilt and reindexed so dont think there are any statistic updates which would help can anyone suggest better method of determining the minimum per id eventdate value which avoids scanning the entire clustered index do have table with the thousand distinct id values
176666 the sql server instance is accessible and seems to be fine microsoft sql server sp1 cu2 kb4013106 x64 mar copyright microsoft corporation enterprise edition bit on windows server r2 standard build hypervisor but what does the white question mark mean these icons dont go away when refresh am sysadmin inside sql server and outside am administrator on that box another thing noticed you can see on the picture below these are different management studio sessions on the top one am logged in as myself dba and sysadmin on the second one use management studio with run as different user and use domain account that use for the replication which is not sysadmin the second one has the blue icon in this and other servers as well whilst mine is the normal green one
176935 would need to track products price changes so that can query the db for product price at given date the information is used in system that calculates historical audits so it must return the correct price for the correct product based on the date of purchase would prefer to use postgres in building the db need with the design of the database but any and all best practice suggestions are also welcome
177009 my tables may or may not have the same employee names listed in them the problem is have no way of knowing if table1 will hold more names or if table2 will hold more names so was thinking union would solve this issue however as you see in the syntax below it produces to lines for since it exists in both tables this is my desired output empname totalsw totalsnt with the ddl below how can query be written to produce this output create table test1 empname varchar100 swas varchar100 create table test2 empname varchar100 swont varchar100 insert into test1 empname swas values res1 tim1 run34 insert into test2 empname swont values er12 nn12 23rw select empname totalsw countswas totalsnt from test1 group by empname union select empname totalsw totalsnt countswont from test2 group by empname
177022 may just be setting up my query completely incorrect but my expected result set that need returned is nuestra nosotros since they fall into the date range respectively nuestra feeduedate and fj feeduedate and for nosotros fe violationdate and fe violationdate they fall into place however when run this query attempting to return the result get returned in my result set what is incorrect with my query below is sample data ddl create table dbo fei violatorsname varchar not null violationnumber varchar not null violationdate date not null on primary create table dbo fji violatorsname varchar not null vin varchar not null vfees1 float not null vfees2 float not null vfees3 float not null vfees4 float not null vfees5 float not null vfees6 int null feeduedate date null totalvfees float not null totalvfeespaid float not null vfeesremaining float not null vfee7 float not null vfee8 float not null vfee9 float not null vfee10 float not null vfee11 float not null vfee12 float null vfee13 float null vfee14 float null vfee15 float null vfee16 float null on primary insert dbo fei violatorsname violationnumber violationdate values nnostra n3244 cast0xe63c0b00 as date insert dbo fei violatorsname violationnumber violationdate values nnuestra n408 cast0xe53c0b00 as date insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n41 cast0x353b0b00 as date insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n211 null cast0x2d3b0b00 as date insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n211 null cast0x2d3b0b00 as date insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n311 null insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n811 null insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n11111 null insert dbo fji violatorsname vin vfees1 vfees2 vfees3 vfees4 vfees5 vfees6 feeduedate totalvfees totalvfeespaid vfeesremaining vfee7 vfee8 vfee9 vfee10 vfee11 vfee12 vfee13 vfee14 vfee15 vfee16 values nnosotros n77711 null select isnullfe violatorsnamefj violatorsname totalsw countfe violationnumber totalsnt countfj vin totalvfees sumtotalvfees calculatedvfee sumcoalescetotalvfees0 coalescevfee90 adminfee sumcoalescevfee120 secfee sumcoalescevfee130 from fei fe full outer join fji fj on fj violatorsname fe violatorsname where fe violationdate and fe violationdate and fj feeduedate and fj feeduedate group by isnullfe violatorsname fj violatorsname strong edit strong br select isnullfe violatorsnamefj violatorsname totalsw countfe violationnumber totalsnt countfj vin totalvfees sumtotalvfees calculatedvfee sumcoalescetotalvfees0 coalescevfee90 adminfee sumcoalescevfee120 secfee sumcoalescevfee130 from fei fe full outer join fji fj on fj violatorsname fe violatorsname where fe violationdate and fe violationdate or fj feeduedate and fj feeduedate group by isnullfe violatorsname fj violatorsname
177032 as was putting some test sets of data together noticed some funny behavior with temp tables when working with large sets of data in clustered temp tables that are populated via parallel execution plan the clustered key does not look to be honored when selecting data this issue also seems to affect all versions of sql server that ive tested include vnext heres dbfiddle uk example of the test you may have to execute it couple of times to get the result am finding but it shouldnt take more than one or two executions to yield the same results additionally this is the local execution plan im getting on my environment which shows that the only difference between the large and small data sets is the way data is fed into the tables parallel vs serial plan if you want to play at home heres the test im running large data set create table tmp id int primary key clustered insert into tmp purposely insert in reverse order select top percent rn from select top row number over order by select null rn from master spt values t1 cross join master spt values t2 order by rn desc smaller data set create table tmp2 id int primary key clustered insert into tmp2 purposely insert in reverse order select top percent rn from select top row number over order by select null rn from master spt values t1 cross join master spt values t2 order by rn desc large record set clustered key not honored select top from tmp small record set clustered key honored select top from tmp2 drop table tmp drop table tmp2 ive not found any references indicating this is expected behavior but before submit connect item first wanted to reach out and confirm this isnt localized problem can someone either point me to documentation identifying this is expected behavior or alternatively confirm this is in fact bug edit in response to the comments about not including an order by clause was always under the assumption the top keyword returned the data in the order in which it was inserted which should in this case be the order dictated by the clustered key when running the same statement against formal table the expected behavior is returned large data set with formal data table create table tmp id int primary key clustered insert into tmp purposely insert in reverse order select top percent rn from select top row number over order by select null rn from master spt values t1 cross join master spt values t2 order by rn desc large record set clustered key not honored select top from tmp drop table tmp rows affected rows affected id rows affected rows affected even the execution plans are the same so why the different result sets between temp table and formally defined table finally shout out to joe obbish as gratuitously ripped off his cross join approach to build large sets of test data as its quite efficient
177083 am trying to optimize procedure there are different update queries present in the procedure update resultset set majorsector case when charindex sector then rtrimltrimsubstringsector charindex sector else ltrimrtrimsector end update resultset set majorsector substringmajorsector lenmajorsector where leftmajorsector4 in update resultset set majorsector substringmajorsector lenmajorsector where leftmajorsector3 in abcdefghijklmnopqrstuvwxyz to complete all three update queries it takes less than seconds execution plan for all three update queries https www brentozar com pastetheplan id r11blfq7b what planned is to change the three different update queries into one single update query so that the can be reduced with resultset as select case when lefttemp majorsector in then substringtemp majorsector lentemp majorsector when lefttemp majorsector in de hi lm pq tu xy then substringtemp majorsector lentemp majorsector else temp majorsector end as temp majorsector majorsector from select temp majorsector case when charindex sector then rtrimltrimsubstringsector charindex sector else ltrimrtrimsector end majorsector from resultseta update resultset set majorsector temp majorsector but this takes around minute to complete checked the execution plan it is identical as first update query execution plan for above query https www brentozar com pastetheplan id sjvttz9qw can somebody explain why it is slow dummy data for testing if object idtempdb resultset is not null drop table resultset with lv0 as select union all select lv1 as select from lv0 cross join lv0 lv2 as select from lv1 cross join lv1 lv3 as select from lv2 cross join lv2 lv4 as select from lv3 cross join lv3 lv5 as select from lv4 cross join lv4 tally as select row number over order by select null from lv5 select convertvarchar255 newid as sectorcast as varchar1000 as majorsector into resultset from tally where my original table record count order by note since this is not my original data the timings have mentioned above could be little different still the single update query is much slower than the first three tried executing the queries more than times to make sure external factors should not affect the performance all times first three updates ran much faster than the last single update
177114 there are some similar questions answers on the forums but think my problem is simpler have two quesries eg select count from agent select count from agent where active would like the output to be on single row for bonus points it would be nice to scan through the table only once and update both counters to get result like this active agents total agents so guess have two questions what is the neatest clearest way to do this and what is the fastest way to do it for very large tables
177162 have table station logs in postgresql database column type id bigint bigserial station id integer not null submitted at timestamp without time zone level sensor double precision indexes station logs pkey primary key btree id uniq sid sat unique constraint btree station id submitted at im trying to get the last level sensor value based on submitted at for each station id there are around unique station id values and around 20k rows per day per station id before creating index explain analyze select distinct onstation id station id submitted at level sensor from station logs order by station id submitted at desc unique cost rows width actual time rows loops sort cost rows width actual time rows loops sort key station id submitted at desc sort method external merge disk 681040kb seq scan on station logs cost rows width actual time rows loops planning time ms execution time ms creating index create index station id submitted at on station logsstation id submitted at desc after creating index for the same query unique cost rows width actual time rows loops index scan using station id submitted at on station logs cost rows width actual time planning time ms execution time ms is there way to make this query faster like sec for example sec is still too much
177369 would like to monitor transaction log usage regarding all the following aspects what task job query is making it fill the drive file log file usage percentage what time the transaction happened any relevant way of doing this which is tested would be helpful
177463 the page describing whats new in postgres mentions transition tables for triggers transition tables for triggers this feature makes after statement triggers both useful and performant by exposing as appropriate the old and new rows to queries before this feature after statement triggers had no direct access to these and the workarounds were byzantine and had poor performance much trigger logic can now be written as after statement avoiding the need to do the expensive context switches at each row that for each row triggers require what is transition table
177518 users were able to run reports before am after that same reports became very slow sometimes users just didnt have patience to wait after some troubleshooting found the column that was causing the delay it was computed column that uses function in order to bring the result approximately at the same time got another complain about slow running report that was always working fine after some troubleshooting found the columns that was causing delay where amount ptd and again the amount column is computed column so my question is why all of the sudden computed columns that were always part of the reports started to slow down the performance significantly what could really happen approx after am and what is the disadvantage if make those columns persisted thank you function that bring the column alter function dbo calcinvoiceamtptd sinvnum int entityguid uniqueidentifier returns money as begin declare amt money declare toplevel uniqueidentifier set toplevel select dbo gettoplevelentity entityguid declare table guid uniqueidentifier insert into select from dbo getlinkedentities toplevel where guid is not null declare tbl table amount money glacctid int select amt isnullsumamount from tblfin journalpostings jp inner join tblfin journal on transactnum jp transactnum and voiderfor is null and voidedby is nulland transdescid inner join tblfin glaccounttypes glt on glt glacctid jp glacctid and glt accounttype inner join on guid jp entityguid where invoicenum sinvnum return isnull amt end
177622 have had quite lengthy discussion with rick james on this this we came out with idea of having composite key to replace the autoincrement pk where the int is limited close billion my table will reach this limit in few months easily as monthly we are capturing close to few hundred million data below is how my table looks like the key table is the gdata so composite the primary using fields primary key alarmtypeidvehicleidgdatetime then have another table called alarm table the link between both is one to many meaning one data in gdata can have zero or more alarms related to it the link between them is vehicleid and gdatetime create table gdata alarmtypeid tinyint4 not null default fleetid smallint11 not null fleetgroupid smallint11 default null fleetsubgroupid smallint11 default null deviceid mediumint11 not null vehicleid mediumint11 not null gdatetime datetime not null insertdatetime datetime not null latitude float not null longitude float not null speed smallint11 not null see full text alter table gdata add primary key alarmtypeidvehicleidgdatetime add key gdatetime gdatetime add key fleetid fleetidvehicleidgdatetime commit here is the alarm table create table alarm alarmtypeid tinyint4 not null vehicleid mediumint9 not null gdatetime datetime not null insertdatetime datetime not null alarmvalue varchar5 not null readweb enumny not null default readwebdatetime datetime not null readmobile enumny not null default readmobiledatetim datetime not null engine innodb default charset latin1 alter table alarm add primary key alarmtypeidvehicleidgdatetime commit all looks good but recently was doing some googling on related topic and found that some discussion https www quora com is it bad idea to have primary key on or more columns are against composite primary key and would prefer to go with autoincrement mainly for insert purpose can some one shed more light on this to maintain composite key of primary or move back to autoincrement
177652 added script task to an ssis project in vs2015 when deployed to sql server got an error message that the version script is not supported where does this version come from reading other similar questions on stack overflow see that you can set the target version of the project to sql server which did the eventual deployment target is sql server also tried deleting and recreating the script task and in the information of the script it says its using v10 of how can resolve this script task error there was an exception while loading script task from xml system exception the script task st a1ad9dc5972c42b68c12a13155f10b6d uses version script that is not supported in this release of integration services to run the package use the script task to create new vsta script in most cases scripts are converted automatically to use supported version when you open sql server integration services package in sql product short name integration services at microsoft sqlserver dts tasks scripttask scripttask loadfromxmlxmlelement elemproj idtsinfoevents events have also opened the project in ssdt and rebuilt with different name same error it seems that there must be reference that wasnt deleted or something none of the solutions on this question https stackoverflow com questions ssis script task vs15 not work when deploy on sql server worked looking at the xml in the package where the script is can easily find that task and there is no reference to version anywhere edit after copying the project to the machine that hosts the database opening vs2015 and deploying from there the package executes and then when going back to my machine and building there it doesnt is this bug or am doing something stupid by expecting the build to produce the same deployment wizard as using the wizard from vs have sql server ssisdb has the schema version am using an integration services package created in visual studio the script component has the path program files x86 microsoft sql server dts binn vsta14 is st cs template vstax it wont let me execute the package through the integration services catalog due to the message experienced by zach however it appears that it will let me execute it through the file system using sql agent unsure if this is working will update this once the package has completed
177803 imagine am running multiple batches through management studio separated by the go command id like to know how implicit transactions will behave is the transaction committed on per batch basis or once for the entire execution
178056 ive got query where want to retrieve distinct child rows but ordered by column of parent if do the following get an error because the column specified in the order by is not included in the distinct list select distinct foo bar from parent join child on parentid id order by createddate however if add createddate to the select list will lose distinctness of child rows as createddate makes them all distinct if use cte or subquery to first do the ordering and then select distinct rows from that the outer query doesnt guarantee that it will maintain the order of the inner cte query is there way to achieve this
179500 an alter index all rebuild operation on sql server failed because the transaction log ran out of space the indexes have never been reorganized or rebuilt so fragmentation is over on nearly all of them the db uses simple recovery model assumed that following each index operation performed by the all form of the command the transaction log data would be flushed prior to the next index rebuild is that how it actually works or are the index rebuilds logged as if they are part of single transaction in other words could reduce transaction log growth by writing script to perform each rebuild individually are there any other factors to consider
179623 was experimenting the effect of giving sql server small amount of memory thought it was going to recover configured sql server to use 200mb of memory now it does not want to start did some searches on the internet and was advised to start sql server in single user mode however get the error login failed for user reason server is in single user mode only one administrator can connect at this time microsoft sql server error have stopped the sql server agent
180006 id value id parent id dropdown id name bbb comprable comprable rrr ttt have above table with data an using below query to get data from the table select d0 name as name0 d1 name as name1 d2 name as name2 d3 name as name3 from my table as d0 inner join my table as d1 on d1 parent id d0 value id inner join my table as d2 on d2 parent id d1 value id inner join my table as d3 on d3 parent id d2 value id where d0 dropdown id here am using inner join to get value id from its parent id basically it will check if there is parent available for current record and will get its data if any can get correct data if have as number of parents for record as the number of inner join like with above query get details like name0 name1 name2 name3 ttt comprable rrr comprable here want to get below table name0 name1 name2 name3 ttt comprable rrr comprable bbb null null here in last row do not have d2 and d3 tables available want to include that data too with null value any help is appreciated
180136 have database of size gb which has vlfs on other hand there is one with size of tb having vlfs does that indicate the smaller database with that high count of vlfs is actually problem also how can we decide that number to be too big or within ok range please suggest
180197 have table party with columns party id party type when use the below queries get two different types of result select from party where party type null returns nothing select from party where party type is null returns all possible rows need query which should accept both null and values
180461 ive been playing around investigating sampling thresholds with statistics updates on sql server and noticed some curious behaviour basically the number of rows sampled seems to vary under some circumstances even with the same set of data run this query drop table if exists if object iddbo test is not null drop table dbo test create table for testing create table dbo testid int identity11 constraint pk test primary key clustered textvalue varchar20 null insert enough data so we have more than 8mb the threshold at which sampling kicks in insert into dbo testtextvalue select top blahblahblah from sys objects sys objects sys objects sys objects create index on textvalue create index ix test textvalue on dbo testtextvalue update statistics without specifying how many rows to sample update statistics dbo test ix test textvalue view the statistics dbcc show statisticsdbo test ix test textvalue with stat header when look at the output of the show statistics im finding that the rows sampled varies with each full execution the table gets dropped recreated and repopulated for example rows sampled my expectation was that this figure would be the same each time as the table is identical by the way dont get this behaviour if just delete the data and re insert it its not critical question but id be interested in understanding whats going on
181593 our client has agreed to an rpo of one day data loss so am going to change back up strategy to eliminate space constraints database is in full recovery mode existing backup plan once daily we are taking full back up of the database thinking to change like below every week on sunday or saturday we are going to take full back up of the database and planning to take transaction backup or differential backup once every day on the database this will avoid space constraint for me if we take transaction log back up then will the rto be larger since we are taking daily only one transaction backup anyways we need to perform transaction log backup to make free space in log files so can you people suggest which back up type will be good either differential or transaction log if am going in the wrong direction please suggest backup strategy based on my rpo thanks
181594 im trying to execute query that times out after minutes from experience know that the query takes about minutes to execute its not ideal and this is being worked on however for now just need it to execute having changed the remote query timeout parameter to however the query is still timing out after minutes get the message the statement has been terminated msg level state line execution timeout expired the timeout period elapsed prior to completion of the operation or the server is not responding this is using ssms on remote computer why is the timeout setting not taking effect do need to restart the server this isnt possible right now
181598 there are two reasons that prompts me to ask this question tsqlt the sql testing framework tsqlt considers it an issue of high severity when there exists columns with non default collation the author of the test states the following am not suggesting that every string column should have collation that matches the default collation for the database instead am suggesting that when it is different there should be good reason for it yet the severity of the failed test is as mentioned considered high octopus deploy while configuring the octopus deploy server the setup fails with fatal error during the initialization of the octopusserver instance the article related to the error message does not explain why this is requirement but simply states that it will be requirement for future deployments from and including octopus version as side note redgates ci tool package the dlm automation suite supports deployments with varying collations without complaints the recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me why is it considered such serious error by some
181732 all the examples ive seen online imply that optimize for unknown is applied on specific query can optimize for unknown be applied to stored procedure as whole and not just to specific query if yes what would be the syntax
182189 am running ubuntu have installed postgresql postgresql used to work but then rebooted nmap commands show port is open postgres seems to be working correctly service postgresql status postgresql service postgresql rdbms loaded loaded lib systemd system postgresql service enabled vendor preset enabled active active exited since sat edt 1min 4s ago process execstart bin true code exited status success main pid code exited status success memory 0b cgroup system slice postgresql service ran this psql but got this psql could not connect to server no such file or directory is the server running locally and accepting connections on unix domain socket var run postgresql pgsql the file listed above does not seem to exist how do get into postgresql normally id run psql or sudo postgres then psql but these commands are not working keep getting an error about could not connect to server several reboots have not helped update ran this command dpkg grep postgres rc postgresql 0ubuntu0 amd64 object relational sql database version server ii postgresql client all front end programs for postgresql supported version ii postgresql client 0ubuntu0 amd64 front end programs for postgresql ii postgresql client common all manager for multiple postgresql client versions ii postgresql common all postgresql database cluster manager
182227 after upgrading sql server to the server keeps resetting cached execution plans and dm views like dm exec query stats etc every couple of hours as if someone executes dbcc freeproccache and dbcc dropcleanbuffers manually except for no one does it happens automatically the same very database worked fine on sql server and windows server things went south after moving to sql server and windows server things checked the database does not have auto close flag the sql server is ad hoc optimized set to true thought it would help it didnt the query store is off server has gb memory nothing helpful in the sql server log either just weekly backup message also checked this article https docs microsoft com en us sql sql statements alter database transact sql set options scroll down to examples section and right above it there is list of situations when the plan is cleared automatically none of those apply update unfortunately none of the suggestions helped granting lpim permissions detecting and fixing non parameterized queries that generated tons of plans for the same query lowering max server memory plans keep resetting randomly from every couple of hours to every minutes if the server was under memory pressure how come the version was working fine on the same machine heres the sp blitz output as requested priority performance query store disabled the new sql server query store feature has not been enabled on this database xxx priority server info instant file initialization not enabled consider enabling ifi for faster restores and data file growths priority performance resource governor enabled resource governor is enabled queries may be throttled make sure you understand how the classifier function is configured priority query plans implicit conversion affecting cardinality one of the top resource intensive queries has an implicit conversion that is affecting cardinality estimation missing index one of the top resource intensive queries may be dramatically improved by adding an index rid or key lookups one of the top resource intensive queries contains rid or key lookups try to avoid them by creating covering indexes priority file configuration system database on drive master the master database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space model the model database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space msdb the msdb database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space priority backup msdb backup history not purged msdb database backup history retained back to jun 47pm priority informational backup compression default off uncompressed full backups have happened recently and backup compression is not turned on at the server level backup compression is included with sql server 2008r2 amp newer even in standard edition we recommend turning backup compression on by default so that ad hoc backups will get compressed priority non default server config agent xps this sp configure option has been changed its default value is and it has been set to max server memory mb this sp configure option has been changed its default value is and it has been set to optimize for ad hoc workloads this sp configure option has been changed its default value is and it has been set to show advanced options this sp configure option has been changed its default value is and it has been set to xp cmdshell this sp configure option has been changed its default value is and it has been set to priority performance buffer pool extensions enabled you have buffer pool extensions enabled and one lives here sql buffer pool bpe its currently gb did you know that bpes only provide single threaded access 8kb one page at time cost threshold for parallelism set to its default value changing this sp configure setting may reduce cxpacket waits priority wait stats no significant waits detected this server might be just sitting around idle or someone may have cleared wait stats recently priority informational sql server agent is running under an nt service account im running as nt service sqlserveragent wish had an active directory service account instead sql server is running under an nt service account im running as nt service mssqlserver wish had an active directory service account instead priority server info default trace contents the default trace holds hours of data between aug 55am and aug 59pm the default trace files are located in program files microsoft sql server mssql13 mssqlserver mssql log hardware logical processors physical memory 15gb hardware numa config node state online online schedulers offline schedulers processor group memory node memory vas reserved gb locked pages in memory enabled you currently have gb of pages locked in memory memory model unconventional memory model lock pages server last restart aug 32pm server name xx services service sql full text filter daemon launcher mssqlserver runs under service account nt service mssqlfdlauncher last startup time not shown startup type manual currently running service sql server mssqlserver runs under service account nt service mssqlserver last startup time aug 32pm startup type automatic currently running service sql server agent mssqlserver runs under service account nt service sqlserveragent last startup time not shown startup type automatic currently running sql server last restart aug 33pm sql server service version patch level sp1 edition enterprise edition bit alwayson enabled alwayson mgr status virtual server type hypervisor windows version youre running pretty modern version of windows server 2012r2 era version priority rundate captains log stardate something and something
182233 this is my sql server stored procedure which is below create procedure passenger details as begin select full name age nationality category airline name class type from passenger ticket airline class where passenger passenger no ticket passenger no and airline airline no ticket airline no and class class no ticket class no end execute passenger details the above stored procedure in sql server works successfully then tried to execute the same stored procedure in oracle pl sql which is shown below create or replace procedure passenger details passenger details out sys refcursor as begin open passenger details for select full name age nationality category airline name class type from passenger ticket airline class where passenger passenger no ticket passenger no and airline airline no ticket airline no and class class no ticket class no end passenger details the above stored procedure in oracle pl sql is compiled successfully then tried to execute it which is shown below set serveroutput on execute passenger details while trying to execute the stored procedure im getting the following error message which is shown below error starting at line in command begin passenger details end error report ora line column pls wrong number or types of arguments in call to passenger details ora line column pl sql statement ignored line column cause usually pl sql compilation error action
182275 need to come up with the best approach possible to enable page compression on table gb in size with rows the database size itself is tb and holds archive data if understand correctly it needs disk space possibly the same amount just to be on the safer side so that it can create copy of the table with page compression enabled and release the space this is in full recovery model so it will be heavily logged have spoken to my capacity planning resource and he agreed to give additional temporary required space for this maintenance and take back the space once this activity is complete having said this do you think the best approach to be take full backup restore the backup onto new temp drives change the recovery model to simple enable page compression take full backup after compression and restore original database change recovery model to full also instead of enabling page compression after step truncate the largest table enable page compression and then do select into replicadb dbo replicatbl does this enable page compression on existing indexes do not have test environment to test the above steps alternatively if any better approach is available then please let me know the goal is to minimize the future disk space required at its current growth we are an erp software company and we have licensed enterprise edition this table is only for archive when some checks are performed and all the data resides in this table have indexes ci non ci none of the columns are varchar max they are either nvarchar int or date type
182331 have where clause that want to use case expression in however my case expression needs to check if field is null if the userrole variable value analyst then the supervisorapprovedby column value must be null otherwise am saying return all data supervisorapprovedby supervisorapprovedby what do need to change to the below where supervisorapprovedby case when userrole analyst then null else supervisorapprovedby end cannot use for null tested and does not work would want all rows returned including those where supervisorapprovedby is not null
182410 have table like this create table updates updateid int not null identity11 primary key objectid int not null essentially tracking updates to objects with an increasing id the consumer of this table will select chunk of distinct object ids ordered by updateid and starting from specific updateid essentially keeping track of where it left off and then querying for any updates ive found this to be an interesting optimization problem because ive only been able to generate maximally optimal query plan by writing queries that happen to do what want due to indexes but do not guarantee what want select distinct top objectid from updates where updateid fromupdateid where fromupdateid is stored procedure parameter with plan of select top hash match flow distinct rows touched index seek due to the seek on the updateid index being used the results are already nice and ordered from lowest to highest update id like want and this generates flow distinct plan which is what want but the ordering obviously isnt guaranteed behavior so dont want to use it this trick also results in the same query plan though with redundant top with ids as select objectid from updates where updateid fromupdateid order by updateid offset rows select distinct top objectid from ids though im not sure and suspect not if this truly guarantees ordering one query hoped sql server would be smart enough to simplify was this but it ends up generating very bad query plan select top objectid from updates where updateid fromupdateid group by objectid order by minupdateid with plan of select top sort hash match aggregate rows touched index seek im trying to find way to generate an optimal plan with an index seek on updateid and flow distinct to remove duplicate objectids any ideas sample data if you want it objects will rarely have more than one update and should almost never have more than one within set of rows which is why im after flow distinct unless theres something better dont know of however there is no guarantee that single objectid wont have more than rows in the table the table has over rows and is expected to grow rapidly assume the user of this has another way to find the appropriate next fromupdateid no need to return it in this query
182420 at university my professor taught me this year that this sql statement select countlength from product will return with the following dataset product id length code x00 c02 a31 she justified it by saying that count doesnt count duplicates told my professor that thought she made an error she answered me that some dbms may or may not count duplicates after trying lot of dbms ive never found one that has this behaviour does this dbms exist is there any reason for professor to teach this behaviour and without even mentioning that other dbms may behave differently fyi the course support is available here in french the concerned slide is in the lower left corner at page
182580 have table like this create table aggregated master user bigint type text date timestamp operations bigint amount numeric primary key user type date this table is the master from which lot of partitions inherit the partitions are done by month in the date field for example partition for aug would be agg and its pk would be pk agg there is the usual trigger before insert to redirect the insert to the proper partition the thing is that want to do an upsert into this table the do conflict part is not working the code first was like this insert into aggregated master user type date oeprations amount select user type date sumops sumamt from where group by user type date on conflict on constraint pk aggregated do update set operations excluded operations amount excluded amount but then noticed that the constraint pk aggregated is the one on the master table and not on the child table where the insert will really be performed due to the trigger changed the clause conflict to on conflict user type date which are the fields of the pk but this doesnt work either any idea how to make this work
182728 ive come across this multiple times and im sure there is good reason for it but how do avoid it im sure it has to do with the quirks around isnumeric in english have view that filters on isnumericsomefield then try to query it using an int in my where clause and because some other field in the table has character values the whole thing fails created sql fiddle that shows the error tried doing cast convert in the select on the view but the query engine seems to ignore it so why does this happen and is there clean way to deal with it the original fiddle had very similar error due to me missing quotes on the insert that is not the problem was trying to highlight have fixed the view by adding quotes to all values being inserted
182731 im getting an error that thought was impossible have sql server ver table that looks like this create table idmaster id int not null constraint pk idmaster primary key clustered id asc on primary on primary its basically supposed to do what the identity does now its been around while it is used to get new unique integer values that are used in few other tables now have sql statement which selects the max value adds to it and inserts that value into the same table all inside one statement so in theory its not possible to get pk violation on this statement although think could possibly get deadlock but somehow it is getting one violation of primary key constraint pk idmaster cannot insert duplicate key in object idmaster the duplicate key value is dont know how this is possible but apparently it is because it just happened imagine any crazy odd mixture of other ways to try generating new id value from this same table and they probably arent far off from what exists in this system but cant think of any way that they could cause that particular error it was my understanding that the transaction locking used by sql server enforces that within one sql statement that doesnt have nolock on it or anything there cant be any interference with the rows that it is referencing the only other factor which dont think could be related but who knows is that this sql is wrapped in try with some options set heres the sql that got that error set xact abort on set implicit transactions on begin try insert into idmaster id select coalesce make sure the new value is odd case when maxcoalesceid0 then maxcoalesceid0 else maxcoalesceid0 end1 from idmaster end try begin catch while trancount rollback throw end catch while trancount commit please explain how is it possible to get that error if possible how can prevent it from ever happening without altering the table
182860 apologies if havent used the correct terminology am not dba but right now am all that is here situation is last week database server was misbehaving it was about years old the hosting company built new server and moved all the current data across to the new server which is running well however somehow the new server is missing hours of data the hosting company has provided us with backup that contains this data amongst other things as some of the tables have auto generated keys type integer is there any way of restoring just set of data from date range back into my live database obviously cant just cut and paste as all the integrity of the data will be lost thoughts windows server and microsoft sql server
183046 we use sql spotlight in our environment its pretty handy we use particularly the output of sys dm exec requests and sys dm exec query stats spotlight pulls the query plan from the plan cache for you using the hash which is nice problem is unless youre experienced with the code base its quite difficult to know where that query came from had an idea that if could parse the codebase pull the sql queries hash them the same way microsoft does this way could do quick matches of the hash to be able see where in the codebase particular query came from alternatively id have to do some very slow regex
183087 given the following data create table histories username varchar10 account varchar10 assigned date insert into histories values philaccount12017 peteraccount12017 daveaccount12017 andyaccount12017 daveaccount12017 fredaccount12017 jamesaccount12017 daveaccount22017 philaccount22017 joshaccount22017 jamesaccount22017 daveaccount22017 philaccount22017 which represents when given user was assigned to an account am looking to establish who owned given account on the last day of each month the assigned date is the date that the account transferred ownership with any missing month ends populated possibly created from handy dates table that have available with useful columns datekey date and lastdayofmonth courtesy of aaronbertrand the desired results would be peter account1 peter account1 dave account1 dave account1 fred account1 fred account1 fred account1 james account1 phil account2 phil account2 phil account2 james account2 phil account2 doing the initial part of this with windowing function is trivial its adding the missing rows that im struggling with
183891 im playing with postgres table validation rules and trying to set check constraint for an array column an idea is to allow only arrays with length here is how want to implement it create table words table id serial primary key words varchar20 check array lengthwords but looks like it doesnt work insert into words table words values insert how to implement such constraint
183984 if database is detached from an instance permanently are there any cleanup tasks that should be done
184149 stumbled upon this question on twitter conversation with lukas eder although the correct behavior would be to apply the order by clause on the outermost query because here we are not using distinct group by join or any other where clause in the outermost query why wouldnt rdbms just pass the incoming data as it was sorted by the inner query select from select from table order by time desc as when running this example on postgresql at least you get the same execution plan for both the inner query and this derived table example as well as the same result set so would assume that the planner will simply discard the outermost query because its redundant or simply pass through the results from the inner table does anyone think this might not be the case
184288 im trying to better understand numeric types in sql and have read that the decimal type will always require bytes however the ms docs list table indicating the amount of space used depends on the decimals precision so tried to test it using the datalength function create table tbl testdecdec1 decimal194 dec2 decimal204 dec3 decimal94 insert into tbl testdec select select datalengthdec1 datalengthdec2 datalengthdec3 from tbl testdec this outputs was expecting either or im using sql server are all decimals vardecimal or am misunderstanding the datalength function
184525 using sql server have several tables where the pk field is an int but also have uniqueidentifier field that need to query by this field will always be unique am adding indexes for these and have the option of choosing index or unique key is there any benefit to choosing one or the other
184617 im setting up saas system where were planning to give each customer their own database the system is already set up so that we can easily scale out to additional servers if the load becomes too great were hoping to have thousands or even tens of thousands of customers questions is there any practical limitation on the number of micro databases you can should have on one sql server can it affect performance of the server is it better to have databases of mb each or one database of tb additional information when say micro databases dont really mean micro just mean that were aiming for thousands of customers so each individual database would only be thousandth or less of the total data storage in reality each database would be around the 100mb mark depending on how much usage it gets the main reason to use databases is for scalability fact is v1 of the system has one database and we have had some uncomfortable moments when the db was straining under the load it was straining cpu memory all of the above even though we fixed those problems they made us realize that at some point even with the best indexing in the world if were as successful as we hope to be we simply cant put all our data in one big honkin database so for v2 were sharding so we can split the load between multiple db servers ive spent the last year developing this sharded solution its one license per server but anyway thats taken care of since were using vms on azure reason the question comes up now is because previously we were offering only to large institutions and setting up each one ourselves our next order of business is self service model where anyone with browser can sign up and create their own database their databases will be much smaller and much more numerous than the large institutions we tried azure sql database elastic pools performance was very disappointing so we switched back to regular vms
185044 im unclear about the true meaning in the definitions for immutable volatile and stable functions read the documentation specifically the definitions of each immutable indicates that the function cannot modify the database and always returns the same result when given the same argument values that is it does not do database lookups or otherwise use information not directly present in its argument list if this option is given any call of the function with all constant arguments can be immediately replaced with the function value stable indicates that the function cannot modify the database and that within single table scan it will consistently return the same result for the same argument values but that its result could change across sql statements this is the appropriate selection for functions whose results depend on database lookups parameter variables such as the current time zone etc it is inappropriate for after triggers that wish to query rows modified by the current command also note that the current timestamp family of functions qualify as stable since their values do not change within transaction volatile indicates that the function value can change even within single table scan so no optimizations can be made relatively few database functions are volatile in this sense some examples are random currval timeofday but note that any function that has side effects must be classified volatile even if its result is quite predictable to prevent calls from being optimized away an example is setval my confusion comes in with the condition for immutable and stable that the function always or consistently returns the same result given the same arguments the immutable definition states that the function does not database lookups or otherwise use information not directly present in its argument list so to me that means such functions are used to manipulate data provided by the client and should not have select statements although that just sounds bit odd to me with stable the definition is similar in that it says it should consistently return the same result so to me that means that everytime the function is called with the same arguments it should return the same results same exact rows every single time so to me that means that any function that performs select on table or tables that can be updated should only be volatile but again that doesnt sound right to me bringing this back to my use case am writing functions that perform select statements with multiple joins on tables that are constantly being added to so the function calls would be expected to return different results each time its called even with the same arguments so does that mean that my functions should be volatile even though the documentation indicates relatively few database functions are volatile in this sense thank you
185123 need to be able to locate missing element from table with tens of millions of rows and has primary key of binary64 column which is the input value to calculate from these values are mostly inserted in order but on occasion want to reuse previous value that was deleted its infeasible to modify the deleted records with isdeleted column as sometimes row is inserted that is many millions of values ahead of the currently existing rows this means the sample data would look something like keycol binary64 0x 0x 0x ffffffffffff so inserting all the missing values between 0x000000000002 and 0xffffffffffff is infeasible the amount of time and space used would be undesirable essentially when run the algorithm expect it to return 0x000000000003 which is the first opening ive come up with binary search algorithm in which would query the database for each value at position and test if that value was expected for context my terrible algorithm https codereview stackexchange com questions binary search for missing or default value by given formula this algorithm would run for example sql queries on table with items that doesnt seem like lot but its going to be occurring very frequently currently this table has approximately rows in it and performance is becoming noticeable my first alternative thought is to translate this to stored procedure but that has its own hurdles have to write binary64 binary64 algorithm as well as slew of other things this would be painful but not infeasible ive also considered implementing the translation algorithm based on row number but have really bad gut feeling about this bigint is not nearly big enough for these values im up for other suggestions as really need this to be as quick as possible for what its worth the only column selected by the query is the keycol the others are irrelevant for this portion also for what its worth the current query that fetches the appropriate record is along the lines of select keycol from table order by keycol asc offset value rows fetch first rows only where value is the index supplied by the algorithm also havent had the bigint issue with offset yet but will only having rows right now means that it never asks for an index above that value but at some point itll get above the bigint range some additional data from deletions the gap sequential ratio is about the last rows in the table have values bigints maximum
185376 have xml response with this structure but with some different nodes xml version encoding utf orders order orderid orderid amountpaid currencyid eur amountpaid userid marc58 userid shippingaddress name marc jupp name address rue address city paris city stateorprovince stateorprovince country fr country phone phone postalcode postalcode shippingaddress shippingcosts shippingcosts items item details itemid itemid store store title mcpu dda010 title sku mmx sku details quantity quantity price currencyid eur price item item details itemid itemid store store title mcpu dfz42 title sku mmy sku details quantity quantity price currencyid eur price item items order orders need to store this info into different tables and for item table need to create record for each different item but inserting also the order node details like this itemid store title sku quantity price orderid amountpaid userid shippingcost dda010 mmx marc58 dfz42 mmy marc58 to write in the different tables the required info automatically built with great help of the community this query set t1 orders set f1 orderid set select iif charindex valuelocal name nvarchar100 concat isnull quotenamet valuelocal name nvarchar100 iif charindex valuelocal name nvarchar100 cp concat isnull valuentext nvarcharmax iif charindex valuelocal name nvarchar100 cp concat isnull quotename valuelocal name nvarchar100 valuentext nvarcharmax iift valuelocal name nvarchar100 f2 valuetext nvarchar100 from xml nodes countchild as tx where valuenlocal name nvarchar500 in select name from db1 sys columns where object id object id t1and is identity select stuff select stuff select stuff set nif not exists select from t1 where f1 insert into t1 values else update t1 set where f2 print exec sp executesql set t1 users ok better to use loop here this query although can be surely improved and optimized has been working well until now since there was only item node but now with more item nodes it returns only the first one tried to modify the from clause trying to refer to the item collection but without success but think that even if succeed iterating the items nodes have no idea how to get the order node details that are parent of item node can you suggest solution thanks
185662 we have situation where we need to change the relationship between tables from to so we need to create cross reference table in between those two tables after migrating all existing data from the child table into the cross reference table would it be bad idea to delete the original foreign key column in the child table if we leave it there we basically have technical debt but im not dba and dont have good grasp of the implications of deleting column from table know it is possible but is it bad idea will my database hate me for it thanks
186257 am trying to return multiple records using record data type is there way can append to record and add append new value with each iteration to this record that is want to append to rec so that rec becomes set of rows when the loop is over which can just return at the end of my function currently am doing this select temp table col1 temp table col2 temp table col3 into rec from temp table where temp table col3 false my full code is here create or replace function validation returns record as declare rec record temp row record begin create temporary table temp table col1 text col2 integer col3 boolean on commit drop for temp row in select from staging validation loop raise notice sql temp row sql execute formatinsert into temp table temp row sql if select distinct temp table col3 from temp table where temp table col3 false false then raise notice there is false value select temp table col1 temp table col2 temp table col3 into rec from temp table where temp table col3 false end if end loop return rec end language plpgsql current output after select validation validation crea ddf8095f desired output validation crea ddf8095f some source systemsome countf some other source systemsome countf
186339 database sql server have several queries which are exactly the same in all ways except for the value of one of the filter terms same table not copy of the schema on another server therefor same indexes resources etc everything is absolutely identical this query runs in under one second select maxmessageid as maxid from boothcomm universalmessagequeue where messageplatform linux this query runs in under one second select maxmessageid as maxid from boothcomm universalmessagequeue where messageplatform linux and messagecategory accounting this query runs in under one second select maxmessageid as maxid from boothcomm universalmessagequeue where messageplatform windows so why does this one take nearly seconds to run select maxmessageid as maxid from boothcomm universalmessagequeue where messageplatform windows and messagecategory accounting colleague of mine added another index to the table which resolved the business problem of latency this index reduced seconds to full second while speeding the other queries up to instantaneous again execution plans are exactly the same index scan should be have taken advice from other forums and made sure that the column order in the query matched the order that they are stored in the index create nonclustered index messageid and platform and category on boothcomm universalmessagequeue messageid asc messageplatform asc messagecategory asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary go am also providing the table schema in the event that that is helpful create table boothcomm universalmessagequeue messagequeueid bigint identity11 not null messageid bigint not null messageplatform nvarchar null assetnumber nvarchar not null messagestate int null messagestatelabel nvarchar null messagetype int null messagetypelabel nvarchar null messagecategory nvarchar null messagesource int null messagesourcelabel nvarchar null messagesourceserialnumber nvarchar null messagecreatedate datetime null messagetransmitdate datetime null messagereceiveddate datetime null messagestoreddate datetime null xmlpayload nvarchar max null jsonpayload nvarchar max null semanticxml nvarchar max null semanticjson nvarchar max null messagesequencenumber int null erpimportdate datetime null erpimportstatus int null erpmsg nvarchar max null normalizationdate datetime null normalizationstatus int null normalizationdesc nvarchar max null semanticdate datetime null semanticstatus int null semanticdesc nvarchar max null createddate datetime not null default getdate createdby nvarchar not null default etl updateddate datetime not null default getdate updatedby nvarchar not null default etl etl id uniqueidentifier null primary key clustered messagequeueid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary constraint ck etl unique messageid platform unique nonclustered messageid asc messageplatform asc messagetype asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary textimage on primary go this should provide you with enough code to reproduce the problem just fill the table with about million records and you can see the problem since it was brought up couple of times and had not even thought to check looked to see how many windows records there were versus linux records select count from boothcomm universalmessagequeue where messagecategory accounting and messageplatform linux returned select count from boothcomm universalmessagequeue where messagecategory accounting and messageplatform windows returned so am guessing that recordcount is not the issue
186597 we have single instance of sql server sp1 running in vmware virtual machine it contains databases each for different application those applications are all on separate virtual servers none of them are in production use yet people testing the applications are reporting performance issues though these are the stats of the server gb ram 110gb max memory for sql server cores ghz gbit network connection all the storage is ssd based program files log files database files and tempdb are on separate partitions of the server asd the users are performing single screen access via based erp application when stress test the sql server with microsofts ostress using either many small queries or big query get max performance only thing throttling is the client because he cant answer fast enough but when there are barely any users the sql server is barely doing anything yet people have to wait forever just to save anything in the application according to paul randals tell me where it hurts query of all wait events are async network io this could either mean network issue or performance issue with the application server or client neither of those are even remotely using their resources at maximum capacity most of the time cpu is around on all machines client appserver db server latency of network connection is around 3ms the io of the db server is at max 20mb write speed during normal usage with the application avg is 9mb when stress test get around max 5gb buffer cache size is at 60gb for the db of our erp system 20gb for our financing software 1gb for quality assurance software 3gb for document archiving system gave the sql server account the right to use instant file initialization that didnt increase performance in the slightest page life expectancy is at around 15k during normal use drops to around 05k during the end of heavy stress testing which is to be expected batches sec is at around 8k depending on workload id say the erp app is just badly written but cant because all applications are affected even at minimal workload yet cant pinpoint at what is causing this are there any tips hints tutorials applications best worst practices documents or anything else you guys have in mind regarding this problem these are the results from sp blitzfirst ran it seconds started it during high workload of the app of the time its async network io also tested the network connection with ntttcp psping ipferf3 and pathping nothing unusual response times are at max 3ms avg 3ms throughput is at around mb my investigation always results in async network io being the number one waitstat we investigated the result of disabling the large receive offload feature in vmware we are still testing but the results seem inconsistent our first benchmark resulted in duration of minutes top result is minutes which is only achieved when the app is running on the vm with the sql server itself second result is minutes which is really bad first result of our benchmark was minutes which is good because the top result was minutes which is only achievable when the application benchmarks on the vm with the sql server itself this strongly hints at some network related issue or issue with the vmware configuration am currently lost at what methods to use to nail it down to the bottleneck maximum performance with the app is only achievable when the app is running on the vm with the sql server itself if the app is executed on any other vm or virtual desktop the duration of our benchmark gets tripled from minutes duration to minutes or more all the endpoints vm of sql server vm of app server and the virtual desktop are using the same physical hardware weve moved all other endpoints to other hardware edit seems like the problem is back after setting the energy savings mode from balanced to high performance we actually improved response times dramtically but today ran sp blitzfirst again with seconds sample this is the result it shows more second of wait time for async network io than the seconds sp blitzfirst ran
186631 use the following statement to monitor activity in sql server this is out of some book select des session id des status des login name des host name der blocking session id db nameder database id as database name der command des cpu time des reads des writes dec last write des program name der wait type der wait time der last wait type der wait resource case des transaction isolation level when then unspecified when then readuncommitted when then readcommitted when then repeatable when then serializable when then snapshot end as transaction isolation level object namedest objectid der database id as object name substringdest text der statement start offset case when der statement end offset then datalengthdest text else der statement end offset end der statement start offset as executing statement deqp query plan from sys dm exec sessions des left join sys dm exec requests der on des session id der session id left join sys dm exec connections dec on des session id dec session id cross apply sys dm exec sql textder sql handle dest cross apply sys dm exec query plander plan handle deqp where des session id spid order by some activities are not shown here create index alter table how to monitor all activities in sql server activity monitor profiler is no option only plain sql no sysadmin only view server state
186638 have query which runs much faster with select top and much slower without top the number of returned records is could you explain the difference in query plans or share links where such difference explained the query without top text select top from inventtrans join inventdim on inventdim dataareaid dat and inventdim inventdimid inventtrans inventdimid where inventtrans dataareaid dat and inventtrans itemid and inventdim inventlocationid and inventdim ecc businessunitid the query plan for the above without top https pastebin com cbtjpxff the io and time statistics without top sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms rows affected table inventdim scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table inventtrans scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms the used indexes without top inventtrans 177transididx keys dataareaid inventtransid inventdimid recid inventtrans 177itemidx keys dataareaid itemid datephysical inventdim 698dimididx keys dataareaid inventdimid the query with top select top from inventtrans join inventdim on inventdim dataareaid dat and inventdim inventdimid inventtrans inventdimid where inventtrans dataareaid dat and inventtrans itemid and inventdim inventlocationid and inventdim ecc businessunitid the query plan with top https pastebin com 0dyu6qzd the query io and time stats with top sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table inventtrans scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table inventdim scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads rows affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms sql server execution times cpu time ms elapsed time ms the used indexes with top inventtrans 177transididx keys dataareaid inventtransid inventdimid recid inventtrans 177dimididx keys dataareaid inventdimid itemid inventdim 698dimididx keys dataareaid inventdimid inventdim 698ecc busunitlocidx keys dataareaid ecc businessunitid inventlocationid will deeply appreciate any help on the topic
186778 we have recently had an issue where the transaction log grew and maxed out the storage space this happened during maintenance window ola hallengren index optimize scripts are being used for some reason someone in the past has set the transaction log backups to stop for hours it seems to be while the indexing jobs are running they are set to run for hours my organization recently migrated to sql server from 2008r2 the person who worked on the migration changed the window of not doing transaction logs to be min shorter so they now start at 30am think he believed they were being stopped for the full backup but believe they were stopped to let the index maintenance run is that necessary in either case it seems like stopping tlog back ups would be doing more harm than helping the sql server 2008r2 schedule was 00am transaction log backups are stopped 00am weekly index optimize job starts this failed after hours min due to log space 00am full backup 00am changed to 30am transaction log backups start but the log remains full additional info my organization has recently upgraded to sql server from 2008r2 in 2008r2 we had never experience this both are enterprise edition could it just be coincidence or could the upgrade have changed something can any one offer advice on what to check for as to the cause the server admin has set the jobs back to the original schedule no tran log back ups from 3am to 6am believing that the overlap of index maintenance and transaction log backups is causing the issue could that be true the more research do on the subject of transaction logs the more it seems like we should actually be doing the log backups during this maintenance window not shutting it off im bit confused as to why they are being stopped in the first place the original admin is no longer with the company im even more confused as to why it seems to be an issue after migration
187090 reviewing this question it seems like thats lot of work that shouldnt be needed theyre trying to extend range with date in other databases you would just use greatest and least leastextenddatemin greatestextenddatemax when try to use these though get least is not recognized built in function name greatest is not recognized built in function name that would cover extension in either direction for the purposes of the question you would still have to do exclusive range replacement im just wondering how sql server users implement query patterns to mimic least and greatest functionality postgresql greatest least mysql greatest least mariadb greatest least db2 greatest least oracle greatest least do you unroll the conditions into case statements or is there an extension third party add on or license from microsoft that enables this functionality
187207 we have sql server r2 running on virtual windows r2 server after upgrading the cpu from core to and the ram from gb to gb weve noticed the performance is worse some observations see query that took seconds to run is now taking seconds the cpu is pegged at with sqlservr exe as the culprit select count on table with million rows took over seconds the processes that are running on the server havent changed the only change was to increase the cpu and ram other sql servers have static paging file where this server is set to manage it on its own has anyone run into this issue before per sp blitzerik ran exec dbo sp blitzfirst sincestartup giving me these results
187342 much has been written about the perils of scalar udfs in sql server casual search will return oodles of results there are some places where scalar udf is the only option though as an example when dealing with xml xquery cant be used as computed column definition one option documented by microsoft is to use scalar udf to encapsulate your xquery in scalar udf and then use it in computed column this has various effects and some workarounds executes row by row when the table is queried forces all queries against the table to run serially you can get around the row by row execution by schemabinding the function and either persisting the computed column or indexing it neither of those methods can prevent the forced serialization of queries hitting the table even when the scalar udf isnt referenced is there known way to do that
187369 have column that has as delimiter it looks like so abc efg hij want query that turns this into three columns col1 col2 and col3 am wondering what the fastest way to do this is so far havent been able to do very well with my limited database experience ive got function create function dbo split delimited nvarcharmax delimiter nvarchar100 returns table id int identity11 val nvarcharmax as begin declare xml xml set xml replace delimited delimiter insert into tval select value varcharmax as item from xml nodes as recordsr return end this is how im doing it right now but believe it could be made to go much faster im also open to significantly better function or outside the box ideas for splitting strings believe im running this dbo splitname three times and could only be running it once select col1 select val from dbo splitname where id col2 select val from dbo splitname where id col3 select val from dbo splitname where id from mains any help would be greatly appreciated
187568 sql server standard ed on windows we have an core 128gb machine vm actually we max this out for our prod db 122gb for the instance cores for the instance we are setting up 2nd db to be on its own spindle set slower disks and do not want it to impact the prod db this will be on the same vm as we understand it we can use the existing sql license this way must this 2nd db be served by its own sql instance in order to restrict ram consumed to very little just or gb restrict to cpu cores or can we restrict this db within the main instance note express edition is not an option our db will be over gb
187688 when we implement the always encrypted feature in sql server do the mdf files ldf files and bak files get encrypted as well would like to know if the data log backup files gets encrypted or not
187807 im just curious say you have table of million records rows select order value from store orders does it make difference whether that table has field fields or fields in actual query time mean all fields other than order value right now im pushing data to data warehouse sometimes dump fields into the table that may be used in the future someday but they arent being queried right now by anything would these extraneous fields affect select statements that do no include them directly or indirectly no mean
188016 just finished reading an excellent article on isolation levels here our company will soon start development on rewrite and expansion of our current product my desire is to have an oltp database and separate more denormalized reporting database assuming were somewhat disciplined and most of our ad hoc and reporting type queries actually go to the reporting database does it sound appropriate that our oltp database have default isolation level of read committed we wont need more stringent isolation level for oltp and our reporting database be snapshot isolation probably rcsi my thinking is that if our oltp database is actually true oltp database and not serving double duty as reporting db we wont need snapshot isolation and the associated overhead it entails but snapshot isolation would be desirable on the reporting database so that readers are not blocked by the constant flow of data coming in and reading the last saved version of row would be acceptable
188044 im doing training and one of the scripts has the following command select sumcol2 from clust table where col1 would like to know what this snippet is for in the where clause col1 did some research on the internet and found no references about this command
188203 ive been doing deep dive into the sql server query store and often see references to ad hoc queries however havent seen what the query store determines an ad hoc query to be ive seen places where it could be inferred to be query without parameters or query executed just one time does formal definition exist for this dont mean in general mean as it relates to the query store as an example this page shows an example of removing ad hoc queries from the query store but it appears the criteria its using is an execution count of just one this seems to be an odd definition of an ad hoc query btw if you go to the page search for delete ad hoc queries
188231 am interested in the terminology for the characters you can put around table names column names etc when referencing them in query name that in microsoft sql server would be for and in mysql would be for or possibly am unable to find generic term for them anywhere and am wondering if one exists
188335 sorry for probably very basic question but in the literature and online have came across two different definitions of weak entities which may sometimes be contradictory weak entity is an entity which cannot exist without some other owner entity weak entity does not have primary key but rather partial key and can be only uniquely identified by combining this partial key with foreign key from owner entity which one of these is true let us take the example of customers orders relationship where orders have unique orderid here an order cannot exist without customer however it still has its own primary key would it be strong or weak entity then
188667 am writing an application that needs to store and analyze large amounts of electrical and temperature data basically need to store large amounts of hourly electricity usage measurements for the past several years and for many years to come for tens of thousands of locations and then analyze the data in not very complex manner the information that need to store for now is location id timestamp date and time temperature and electricity usage about the amount of the data that needs to be stored this is an approximation but something along those lines locations records per month hourly measurements approximately hours per month months for years back and many years into the future simple calculations yield the following results locations records months years back records these are the past records new records will be imported monthly so thats approximately new records per month the total locations will steadily grow as well on all of that data the following operations will need to be executed retrieve the data for certain date and time period all records for certain location id between the dates and and between and simple mathematical operations for certain date and time range min max and avg temperature and electricity usage for certain location id for years between and the data will be written monthly but will be read by hundreds of users at least constantly so the read speed is of significantly more importance have no experience with nosql databases but from what ive gathered they are the best solution to use here ive read on the most popular nosql databases but since they are quite different and also allow for very different table architecture have not been able to decide what is the best database to use my main choices were cassandra and mongodb but since have very limited knowledge and no real experience when it comes to large data and nosql am not very certain also read that postresql also handles such amounts of data well my questions are the following should use nosql database for such large amounts of data if not can stick to mysql what database should use should keep the date and time in separate indexed if possible columns to retrieve and process the data quickly for certain time and date periods or can this be done by keeping the timestamp in single column is time series data modeling approach appropriate here and if not could you give me pointers for good table design thank you
188793 plan here plan here have almost identical tables with very small difference in row count one on and the other on indexing is identical these vms are in exact same environment with upgraded os and sql server versions same number vcores same memory same server settings max degree of parallelism cost threshold for parallelism this drop dead simple query for single record using single column for filter and single column return the column in the where filter is the sole column in the index version has rows version has rows they are the same queries is missing the index and doing full table scan for no apparent reason does rid lookup heap on the table after the index scan tried with index contact rc nui1 on server and the cost jumped from to on the cost was tried adding and select and that made no difference tried removing parameter sniffing as possible problem by using option recompile and that made no difference the dba ran index rebuilds after restoring the database both servers had fairly recent index stats updates we run olas index update script and to be sure rebuilt the index on which had no effect on the explain plan added hint below in the query select address1 stateorprovince from dla dcrm contact rc with index contact rc nui1 where wv partyid this resulted in taking cost from to even though it shows almost identical to explain plan just added parallelism gather streams what appears to be doing is costing the index for only but the rid lookup is in this was reversed it appears used the index to scan all the entries and then looked up every rid in the table could that be true think optimizer has been smoking something seriously strong wv party id is nvarchar100
189484 in dynamics ax there is caching mechanism where tables can be configured to be loaded into memory and cached this cache is limited to certain amount of kb to prevent memory issues the setting im talking about is called entiretablecache and loads the whole table in memory as soon as single record is requested up to recently we relied on some scripts to verify the size of the tables that have this setting to see if the table size is above this limit now however compression comes into play and things like sp spaceused or sys allocation units seem to report the space actually used by the compressed data obviously the application server is working with uncompressed data so the data size on disk in sql server is irrelevant need the actual size the uncompressed data will have know of sp estimate data compression savings but as the name says this is just an estimate would prefer to have size as correct as possible the only way could think of was some convoluted dynamic sql creating uncompressed tables with the same structure as the compressed tables inserting the compressed data in that shadow table and then check the size of that shadow table needless to say this is bit tedious and takes while to run on database of several hundreds of gb powershell could be an option but wouldnt like to iterate over all tables to perform select on them to check the size in the script as that would just flood the cache and would probably take long time too in short need way to get the size for each table as it will be once uncompressed and with fragmentation out of the equation as presented to the application if thats possible im open to different approaches sql is preferred but im not opposed to powershell or other creative approaches assume the buffer in the application is the size of the data bigint is always the size of bigint and character data type is bytes per character unicode blob data takes the size of the data too an enum is basically an int and numeric data is numeric3812 datetime is the size of datetime also there are no null values they are either stored as an empty string or zero there is no documentation on how this is implemented but the assumptions are based on some testing and the scripts used by pfes and the support team which also ignore compression apparently since the check is built in the application and the app cant tell if the underlying data is compressed which also check the table sizes this link for example states avoid using entiretable caches for large tables in ax over kb or pages in ax over entire table cache size application setting default 32kb or pages move to record caching instead
189607 have to delete millions records from million row table and it is going extremely slowly appreciate if you share suggestions to make code below faster set transaction isolation level read committed declare batchsize int iteration int totalrows int msg varchar500 set deadlock priority low set batchsize set iteration set totalrows begin try begin transaction while batchsize begin delete top batchsize from mysourcetable output deleted into mybackuptable where not exists select null as empty from dbo vendor as where vendorid id set batchsize rowcount set iteration iteration set totalrows totalrows batchsize set msg castgetdate as varchar iteration cast iteration as varchar total deletes cast totalrows as varchar next batch size cast batchsize as varchar print msg commit transaction checkpoint end end try begin catch if error and trancount begin print there is an error occured the database update failed rollback transaction end end catch go execution plan limited for iterations vendorid is pk and non clustered where clustered index is not in use by this script there are other non unique non clustered indexes task is removing vendors which do not exist in another table and back them up into another table have tables vendors specialvendors specialvendorbackups trying to remove specialvendors which do not exist in vendors table and to have backup of deleted records in case what im doing is wrong and have to put them back in week or two
189676 would like to understand why cyber security teams in general more than one organization ive dealt with is dead set against granting bulk insert tsql rights to applications and database programmers cant believe the filling up the disk abuse excuse unless im missing something as the end result is no different than an application doing something like for long long max executesql insert into table values and insert is common dml command that anyone with basic write permission can execute for an applications benefit bulk insert is far more efficient faster and relieves the programmer of the need to parse files outside of sql edit originally asked this question on the information security site for reason it is not the dbas that are against using bulk insert it is the information assurance ia for short the cybersecurity folks that are forcing the issue ill let this question stew for another day or two but if the bulk operation does indeed bypass constraints or triggers can see that being an issue
189928 im trying to backup restore mongodb database to from gz files as sample script here create gz backup ok for r3 and r3 mongodump db db name gzip archive backup file gz restore from gz file not ok for r3 mongorestore gzip archive backup file gz nsfrom db name nsto db name restore step back up is good for both mongodb version r3 and r3 though step not works for r3 how can get mongorestore version r3 to restore from gz file and be able to rename the database we have the solution here but that requires the backup to be folder my backup files are huge 1gb 2gb so the extraction is too much time consuming
190073 hi im trying to truncate some tables from my db but they are related with foreign key constraints so every time try it sql server throws an error like this cannot truncate table table because it is being referenced by foreign key constraint dont want to drop the tables or delete them friend told about truncate cascade for this case but havent found any info related also other user told me to try this did try it but im still not getting my tables truncated also read about script for foreign keys drop and re create for use with truncate and the script is supposed to affect my db or at least thought so but after running it was unable to truncate my tables and it threw the same error im using sql server r2 and running my queries with that version of ssms
190090 any suggestions on how to deal with this error title microsoft sql server setup the following error has occurred vs shell installation has failed with exit code for help click https go microsoft com fwlinklinkid prodname microsoft 20sql 20server evtsrc setup rll evtid prodver evttype 0x5b39c8b9 buttons ok this is new laptop running sql server express visual studio ssms tried uninstalling anything related to sql server or visual studio log 03t16 e000 error 0x80070666 cannot install product when newer version is installed detailed results feature full text and semantic extractions for search status failed reason for failure an error occurred for dependency of the feature causing the setup process for the feature to fail next step use the following information to resolve the error and then try the setup process again component name microsoft visual redistributable component error code component log file program files microsoft sql server setup bootstrap log vcruntime140 x64 cpu64 log error description vs shell installation has failed with exit code error help link https go microsoft com fwlinklinkid prodname microsoft sql server evtsrc setup rll evtid prodver evttype vcruntime140 x64 40install 400x1638 feature database engine services status failed reason for failure an error occurred for dependency of the feature causing the setup process for the feature to fail next step use the following information to resolve the error and then try the setup process again component name microsoft visual redistributable component error code component log file program files microsoft sql server setup bootstrap log vcruntime140 x64 cpu64 log error description vs shell installation has failed with exit code error help link https go microsoft com fwlinklinkid prodname microsoft sql server evtsrc setup rll evtid prodver evttype vcruntime140 x64 40install 400x1638
190782 in our environment the network storage is low on space at the same time would like to make sure that we take transaction log backups every minutes instead of current every hours my question is will changing the log backup interval from hours to every minutes consume more disk space
191146 am writing batch processing insert statement and would like to use temp table to keep track of inserted ids instead of looping through the items myself and calling scope identity for each inserted row the data that needs to be inserted has temporary ids linking it to other data that also should be inserted into another table so need cross reference of the actual id and the temporary id this is an example of what have so far the existing table declare mytable table id int identity11 name nvarcharmax my data want to insert declare myinsertdata table id int name nvarcharmax insert into myinsertdata idname values bla 2test 3last declare mycrossref table newid int oldid int insert into mytable name output inserted id ins id into mycrossref select name from myinsertdata ins check the result select from mycrossref the problem is that cannot get the output into clause to accept the id ive tried myinsertdata id and other tricks joining the table to itself but nothing seems to work
191393 recently upgraded localdb from version to using the sql server express installer and this instruction after the installation stopped the existing default instance mssqllocaldb of version and created new one which automatically used the v14 server engine often use localdb for database integration tests in my xunit tests create temporary database which is deleted when the test finishes since the new version unfortunately all my tests fail because of the following error message create file encountered operating system error 5access is denied while attempting to open or create the physical file users kepfldbd0811493e18b46febf980ffb8029482a mdf the odd thing is that the target path for the mdf file is incorrect backslash is missing between users kepfl and dbd0811493e18b46febf980ffb8029482a mdf which is the random database name for single test the databases are created via the simple command create database databasename nothing special here in ssms see that the target locations for data log and backup are the following however when try to update the location get another error message how can update the default locations so that localdb is able to create databases again its obvious that localdb does not correctly combine the default location directory and the database file name is there registry entry that can edit or anything else update after dougs answer and sepupics comment according to this stackoverflow question the default locations should also be changeable via the registry however if try to find the corresponding keys defaultdata defaultlog and backupdirectory cannot find them in my registry did sql server v14 rename these registry keys or moved these information out of the registry
191440 is it even possible my use case is ledger table with requirement that once record is created it should be read only no one should be able to edit or delete it this only applies to the ledger table and tables with direct relation to it there are other tables in the same schema which will be updated deleted as normal my understanding is that for data integrity purposes these sorts of constraints should be applied at the database layer but cant find clean widely accepted way of doing this is this use case where id just be better doing it in the application layer the ideal would be for way to do it in plain sql so as to be agnostic of what db platform is used since that may be subject to change but realise that may be too much to ask for so if it has to be platform dependent some flavour of mysql is preferred thank you
191552 have requirement to find potentially matching customer records in the same table the logic is as per below however this seems to perform at on is there anyway to improve the performance here ive tried setting indexes hashing the columns and comparing on that etc but performance remains terrible over large dataset ive also added the query plan below select c1 customerid as customer1 c2 customerid as customer2 from customer c1 inner join customer c2 on c1 customerid c2 customerid and c1 firstname c2 firstname or c1 birthdate c2 birthdate and c1 emailaddress c2 emailaddress or c1 mobilephonenumber c2 mobilephonenumber or c1 homeaddressline1 c2 homeaddressline1 and c1 homepostcode c2 homepostcode or c1 homesuburb c2 homesuburb
191595 given this table create table test id int not null description nvarchar100 collate modern spanish ci as not null insert into test id description values co2 ive realised cant fix typographic issue select from test where id update test set description co where id select from test where id because the update matches but has no effect id description co2 affected rows affected rows id description co2 affected rows its as if sql server determines that since is obviously just tiny the final value wont change so its not worth changing it could someone shed some light on this and maybe suggest workaround other than updating to an intermediary value
191803 how do move my tempdb data or log files from wherever they are now to different drive or folder
191825 ive noticed that on server running sql server sp1 cu6 sometimes an extended events session shows select query causing writes for example the execution plan shows no obvious cause for the writes such as hash table spool or sort that could spill to tempdb variable assignment to max type or an automatic statistics update could also cause this but neither was the cause of the writes in this case what else could the writes be from
192208 have read about json objects and the json object type only want to do select and it return json do not necessarily want to store json object serialization per se it not my question the columns are regular varchar int etc columns no json objects normal database rows can do regular old select and return json for mysql isnt this what for json in sql server and rows for json do in postgresql they seemed ahead in this are but didnt want to fool myself found this question from https stackoverflow com questions mysql return row as json using new json features
192364 have the following configuration host machine that runs three docker containers mongodb redis program using the previous two containers to store data both redis and mongodb are used to store huge amounts of data know redis needs to keep all its data in ram and am fine with this unfortunately what happens is that mongo starts taking up lot of ram and as soon as the host ram is full were talking about 32gb here either mongo or redis crashes have read the following previous questions about this limit mongodb ram usage apparently most ram is used up by the wiredtiger cache mongodb limit memory here apparently the problem was log data limit the ram memory usage in mongodb here they suggest to limit mongos memory so that it uses smaller amount of memory for its cache logs data mongodb using too much memory here they say its wiredtiger caching system which tends to use as much ram as possible to provide faster access they also state its completely okay to limit the wiredtiger cache size since it handles operations pretty efficiently is there any option to limit mongodb memory usage caching again they also add mongodb uses the lru least recently used cache algorithm to determine which pages to release you will find some more information in these two questions mongodb index ram relationship quote mongodb keeps what it can of the indexes in ram theyll be swaped out on an lru basis youll often see documentation that suggests you should keep your working set in memory if the portions of index youre actually accessing fit in memory youll be fine how to release the caching which is used by mongodb same answer as in now what appear to understand from all these answers is that for faster access it would be better for mongo to fit all indices in ram however in my case am fine with indices partially residing on disk as have quite fast ssd ram is mostly used for caching by mongo considering this was expecting mongo to try and use as much ram space as possible but being able to function also with few ram space and fetching most things from disk however limited mongo docker containers memory to 8gb for instance by using memory and memory swap but instead of fetching stuff from disk mongo just crashed as soon as it ran out of memory how can force mongo to use only the available memory and to fetch from disk everything that does not fit into memory
192524 what is the best way to flatten tables into single row for example with the following table id hprop idayofmonth dbltargetpercent would like to produce the following table hprop idatetarget1 dblpercenttarget1 idatetarget2 dblpercenttarget2 idatetarget3 dblpercenttarget3 idatetarget4 dblpercenttarget4 have managed to do this using pivot and then rejoining to the original table several times but im fairly sure there is better way this works as expected select x0 hprop x0 idatetarget1 x1 dbltargetpercent dblpercenttarget1 x0 idatetarget2 x2 dbltargetpercent dblpercenttarget2 x0 idatetarget3 x3 dbltargetpercent dblpercenttarget3 x0 idatetarget4 x4 dbltargetpercent dblpercenttarget4 from select hprop max idatetarget1 max idatetarget2 max idatetarget3 max idatetarget4 from select rank over partition by hprop order by iweek rank from table pivot maxiweek for rank in pv group by hprop x0 left join table x1 on x1 hprop x0 hprop and x1 iweek x0 idatetarget1 left join table x2 on x2 hprop x0 hprop and x2 iweek x0 idatetarget2 left join table x3 on x3 hprop x0 hprop and x3 iweek x0 idatetarget3 left join table x4 on x4 hprop x0 hprop and x4 iweek x0 idatetarget4
192901 note understand the risks involved and the possibility of destroying production system by doing this im interested in doing it anyway whenever try to play with system catalogs gets these weird errors update sys sql logins set password hash pwdencryptpass where name sa error produced msg ad hoc updates to system catalogs are not allowed ive tried numerious ways to take the training wheels off sp configure allow updates go reconfigure go but cant figure out the right option it even says in this answer this is no longer possible at least not without jumping through ton of additional hoops beyond just an sp configure option its not something you ever want to do on production system and all of the system catalog is now exposed through read only views like sys objects well what if want to jump through those hoops how can do it
193138 have user ls readonly which is supposed to have db datareader privileges on several databases thought had set it up right but when connect to the server as ls readonly and try to open the db in the object explorer get an error the database wtest is not accessible objectexplorer open query window in master and try run use wtest this responds with msg level state line the server principal ls readonly is not able to access the database wtest under the current security context what am missing update heres clue if delete ls readonly as user from under the databases security context then go to the user under the servers security context and under user mapping grant access to the database then it starts working it could be that the database was originally restored from different server which also has ls readonly user guess then that the user identification is not based on the username
193154 have table like this id date name score erin erin emil erin paul john john erin want the data back years based on the year provide am using this select from tblstdnt where date dateaddyeardatediffyear0 but this gives me data starting from how do fix this any ideas it should give me all data from to the year period regardless of the month or day update dbfiddle link
193287 im working on project that is using the instagram key format tl dr bit integer ids theyll be used for lookups and we also like them for sorting and batching since they will naturally sort by creation time the values are between and so just too big to fit inside bigint so it seems our options for storage are numeric20 or varchar varchar is not as ideal since wed have to zero pad them for sorting to work but would there be performance hit to using numeric for lookups
193362 were having problems with some queries which are similar to this select counta from dbo oinv t0 inner join dbo ocrd t2 on t2 cardcode t0 cardcode where t0 cardcode p2 or t2 fathercard p3 query plan the indexes that its hitting are defined as nonclustered index ocrd father on dbo ocrd fathercard asc include cardcode nonclustered index oinv customer on dbo oinv cardcode asc theyre currently taking seconds to run and returns count of which is what were expecting im incredibly surprised that its not filtering the nonclustered indexes before it feeds into the hash match its feeding every single row these are vendor software queries so unfortunately theres no way for us to rewrite them why is this and is there any way to change it to filter before it does the hash match without rewriting the query example data setup create table dbo ocrd fathercard nvarchar50 cardcode nvarchar50 insert into dbo ocrd select top newid newid from master spt values v1 master spt values v2 master spt values v3 create table dbo oinv cardcode nvarchar50 insert into dbo oinv select top newid from master spt values v1 master spt values v2 master spt values v3 create nonclustered index ocrd father on dbo ocrd fathercard asc include cardcode create nonclustered index oinv customer on dbo oinv cardcode asc
193394 say table car has one to one relationship to tables electric car gas car and hybrid car if car is electric car it can no longer appear in gas car or hybrid car etc is there anything wrong with such design some problems that may occur down the road
193777 im trying to run recursive alter table reading couple of parameters from my sys table declare altercmd nvarcharmax select top altercmd alter table dbo table alter column column name nvarchar character maximum length from information schema columns where table name table name and data type like char print altercmd in this sample code just want to validate if the command is coming right but its not possible to run it due an error in the concatenate according to the error message get msg level state line conversion failed when converting the varchar value to data type int
193783 from onward the sql server docs show they support offset fetch which im trying to use instead of limit the following works fine in postgresql to sample result set select from values as tx offset rows fetch next rows only however with sql server get msg level state line invalid usage of the option first in the fetch statement whats going on here does sql server support the standardized offset fetch
193875 found postgresql are create function and drop function locking when used inside different transactions but there are no answers and isnt exactly the same as my question though very similar lets say do the following create function myfunc start transaction from client start transaction from client in transaction use create or replace function to revise the definition of myfunc commit transaction call myfunc from transaction what happens in step am calling the original function as defined in step or the modified form from step committed in step and if the function is dropped in step rather than being modified will step fail or succeed this is probably the same question but modifications may work differently where is the documentation about this
194131 have table storing parent child records as such custid custname deptid company parentcustid enrolled sally ab1 comp1 null ajit ab7 comp2 rahul de1 comp3 uday tg6 comp4 john hy7 comp5 netaji hy5 comp6 prakriti gt6 comp7 sachin kl7 comp8 santosh kk5 comp9 ravi pp0 comp10 is it possible to return records where parent record is enrolled and related child record is not enrolled this returns what need for specific customer select custid custname deptid company parentcustid enrolled from customer where company comp1 and enrolled union all select custid custname deptid company parentcustid enrolled from customer where parentcustid and enrolled custid custname deptid company parentcustid enrolled sally ab1 comp1 null sachin kl7 comp8 how can structure the query to return that type of result set for all parent child records in the table
194222 recently upgraded from postgresql to postgresql one of the nifty features in postgresql is the new identity column type an alternative to postgresql serial pseudo type official documentation for identity column can be found one the create table page however when inserting multiple rows into table with generated by default as identity column and using the keyword default to get the next id value the default value is coming back as null for example lets say have table create table test id int generated by default as identity text create table inserting single row with the default keyword seems to work fine insert into test id values default insert inserting multiple rows does not insert into test id values default default error null value in column id violates not null constraint detail failing row contains null inserting multiple rows using an implicit default also works insert into test values insert the problem specified above does not appear to be present when using the serial column pseudo type create table test2 id serial text create table insert into test2 id values default default insert so my question is am missing something is the default keyword just not expected to work with the new identity column or is this bug
194315 im out of ideas so im asking for your help have weird problem which cannot find cause for no matter how much search on the internet thing is have two laptops l1 hdd and kingston hyperx fury sata iii ssd 240g l2 hdd and samsung evo nvme ssd 500g and have procedure to test the performance of the database which inserts selects and deletes from different tables in numbers of and l2 was bought after l1 and tried to migrate everything had on l1 to l2 including the databases after running the performance procedure noticed something weird l2 was taking way more than l1 was taking an example being the insertion of lines in table l1 would take second where l2 would take seconds of course didnt have the patience to try the insertions on l2 where on l1 would take only seconds so tried to find the root of the cause opened task manager im using windows performance tab and found out that when its about to insert the ssd is writing only with 200kb whereas l1s ssd writes with 4mb of course took the possibility that the ssd might be defected so used benchmarks to see first one used was samsung magician crystaldiskmark and as ssd benchmark all of those benchmarks said there is no problem and my ssd is working at the speed it should be working have to note that all my drivers are up to date and the firmware of the ssd is the latest one tried reinstalling the sql server tried changing the editions from enterprise to developer developer being the one use on l1 another thing tried was detaching the database moving it to the other drive and attaching it back to find out that it works better than on the ssd the results for each performance check inserts select delete incepela startsat seincheiela endsat l2 on ssd l2 on hdd l1 on ssd in the end the question is why does sql server perform significantly worse on piece that is supposed to be superior to an hdd edit edit added another image that shows an interesting thing regarding the stall time ugly code declare id test int declare name test nvarchar50 declare ctest cursor for select codtest nume from teste open ctest fetch next from ctest into id test name test while fetch status begin if name test insert name test testetabele begin declare cte int declare cta int declare nr int declare int insert into rulariteste descriere incepela seincheiela values add in table sysdatetime null declare cursor for select from testetabele open fetch next from into cte cta nr while fetch status begin insert into rularitestetabele codrularetest codtabel incepela seincheiela values cte cta sysdatetime execaddfields cta nr update rularitestetabele set seincheiela sysdatetime where codrularetest cte and codtabel cta fetch next from into cte cta nr end close deallocate update rulariteste set seincheiela sysdatetime where id test codrularetest end else if name test select name test testeviewuri begin declare ct int declare cv int insert into rulariteste descriere incepela seincheiela values select view sysdatetime null declare cursor for select from testeviewuri open fetch next from into ct cv while fetch status begin insert into rularitesteviewuri codrularetest codview incepela seincheiela values ct cv sysdatetime select from select nume from viewuri where codview cv as aview update rularitesteviewuri set seincheiela sysdatetime where codrularetest ct and codview cv fetch next from into ct cv end close deallocate update rulariteste set seincheiela sysdatetime where id test codrularetest end else if name test delete begin insert into rulariteste descriere incepela seincheiela values delete from table sysdatetime null delete from topic follows delete from topics delete from users update rulariteste set seincheiela sysdatetime where codrularetest select top1 codrularetest from rulariteste order by codrularetest desc end fetch next from ctest into id test name test end close ctest deallocate ctest the addfields functions are supposed to add fields for the tables with while edit to make matters more clear its not about the algorithm used ill demonstrate by making while in which add values once when the db is on the ssd and once when the db is on the hdd ill use only l2 for this test an image consisting of both because its the last one can post dont have reputation to post more than images as it can be noticed when the db is on the hdd it executes the inserts in seconds whereas when its on the ssd it inserts them in seconds
194684 understand the distinction between scalar functions set returning functions srfs internal functions window functions aggregate functions of all sorts user implemented functions which in postgresql can be implement in any language etc in sql server stored procedures are permitted through exec what does that provide over any other function executed with select that returns null when postgresql gets stored procedures what will they bring me and what is the formal distinction if any between function and stored procedure in the spec read this question but it seems to predate the announcement of the implementation
194975 just discovered that can insert values of any type into postgresql column of type text drop table if exists cascade create table text insert into values insert into values true select pg typeof from pg typeof text true text rows is this an intentional feature have done something wrong is there setting to avoid this doesnt this violate the assumption that rdbmss should be typesafe know that text acts like catch all in postgresql which is often convenient because you can then write string representations of arbitrary types but surely sometimes you want to make sure that only strings get inserted into given column to the exclusion of implicitly cast values is there anything can do to avoid casual type casts
195085 am used to working in very secure environments and so design my permissions to very fine degree of granularity one thing that normally do is to explicitly deny users the ability to update columns that should never be updated for example create table dbo something created by varchar50 not null created on datetimeoffset not null these two columns should never be changed once the value has been set therefore would explicitly deny the update permission on them recently during team meeting developer raised the point that the logic to ensure the fields never get updated should be contained within the application layer and not the database layer in the event that they need to update the values for some reason to me that sounds like typical dev mentality know used to be one am the senior architect at my company and have always worked on the principle of least amount of privileges required to get the app to work all permissions are audited regularly what is the best practice in this scenario
195338 little backstory some time ago we started to experience high cpu system time on one of our mysql databases this database was also suffering from high disk utilization so we figured that those things are connected and since we already had plans to migrate it to ssd we thought that it will solve both issues it helped but not for long for couple of weeks after migration cpu graph was like this but now we are back to this this happened out of nowhere without any apparent changes in load or application logic db stats mysql version os debian db size 2tb ram 700gb cpu cores peek load about 5kq read 600q write although select queries are often pretty complex threads running connected it has about tables all innodb mysql config client port socket var run mysqld mysqld sock mysqld safe pid file var run mysqld mysqld pid socket var run mysqld mysqld sock nice mysqld user mysql pid file var run mysqld mysqld pid socket var run mysqld mysqld sock port basedir usr datadir opt mysql data tmpdir tmp lc messages dir usr share mysql explicit defaults for timestamp sql mode strict trans tablesno zero in dateno zero dateerror for division by zerono auto create userno engine substitution log error opt mysql log error log replication server id gtid mode on enforce gtid consistency true relay log opt mysql log mysql relay bin relay log index opt mysql log mysql relay bin index replicate wild do table dbname log bin opt mysql log mysql bin log expire logs days max binlog size 1024m binlog format row log bin trust function creators log slave updates disabling symbolic links is recommended to prevent assorted security risks symbolic links important additional settings that can override those from this file the files must end with cnf otherwise theyll be ignored includedir etc mysql conf here goes skip name resolve general log slow query log slow query log file opt mysql log slow log long query time max allowed packet 16m max connections max execution time open files limit table open cache thread cache size innodb buffer pool size 550g innodb buffer pool instances innodb log file size 15g innodb log files in group innodb flush method direct max heap table size 16m tmp table size 128m join buffer size 1m sort buffer size 2m innodb lru scan depth query cache type query cache size innodb temp data file path ibtmp1 12m autoextend max 30g other observations perf of mysql process during peak load mysqld kernel kallsyms raw spin lock raw spin lock 0x7fd118e9dbd9 0x7fd118e9dbab mysqld libc so 0x00000000000f4bd9 mysqld libc so 0x00000000000f4bab mysqld libpthread so start thread mysqld mysqld pfs spawn thread mysqld mysqld handle connection mysqld mysqld do commandthd mysqld mysqld dispatch commandthd com data const enum server command mysqld mysqld mysql parsethd parser state mysqld mysqld mysql execute commandthd bool mysqld mysqld handle querythd lex query result unsigned long long unsigned long long mysqld mysqld 0x0000000000374103 mysqld mysqld join exec mysqld mysqld sub selectjoin qep tab bool mysqld mysqld row search mvccunsigned char page cur mode row prebuilt unsigned long unsigned long mysqld mysqld ha innobase general fetchunsigned char unsigned int unsigned int mysqld unknown 0x00007f40c4d7a6f8 mysqld mysqld 0x0000000000828f74 mysqld mysqld handler ha index next sameunsigned char unsigned char const unsigned int it shows that mysql is spending lot of time on spin locks was hoping to get some clue on where are those locks are coming from sadly no luck query profile during high load shows extreme amount of context switches used select from mytable where pk mytable has about 90m rows profile output status duration cpu user cpu system context voluntary context involuntary block ops in block ops out messages sent messages received page faults major page faults minor swaps source function source file source line starting checking permissions check access sql authorization cc opening tables open tables sql base cc init handle query sql select cc system lock mysql lock tables lock cc optimizing optimize sql optimizer cc statistics optimize sql optimizer cc preparing optimize sql optimizer cc executing exec sql executor cc sending data exec sql executor cc end handle query sql select cc query end mysql execute command sql parse cc closing tables mysql execute command sql parse cc freeing items mysql parse sql parse cc cleaning up dispatch command sql parse cc peter zaitsev made post recently about context switches where he says in the real world though would not worry about contention being big issue if you have less than ten context switches per query but it shows more than switches what can cause these symptoms and what can be done about it ill appreciate any pointers or info on the matter everything that come across so far is rather old and or inconclusive will gladly provide additional information if required output of show global status and show variables cannot post it here because contents exceed post size limit show global status show variables iostat avg cpu user nice system iowait steal idle device rrqm wrqm rkb wkb avgrq sz avgqu sz await await await svctm util fd0 sda sdb avg cpu user nice system iowait steal idle device rrqm wrqm rkb wkb avgrq sz avgqu sz await await await svctm util fd0 sda sdb avg cpu user nice system iowait steal idle device rrqm wrqm rkb wkb avgrq sz avgqu sz await await await svctm util fd0 sda sdb update show global status like uptime uptime uptime since flush status show global status like rollback com rollback com xa rollback handler rollback handler savepoint rollback
195763 want to store coordinate points latitude longitude in table variable have tried declare coordinates tablelatitude1 decimal129 longitude1 decimal129 latitude2 decimal129 longitude2 decimal129 select latitude longitude into coordinates from loc locations where place name in delhi mumbai select coordinates its showing error msg level state line incorrect syntax near coordinates the result of the select query select latitude longitude from loc locations where place name in delhi mumbai is latitude longitude how can store the values in table datatype ran the query select version and got the result microsoft sql server rtm x64 apr copyright microsoft corporation standard edition bit on windows enterprise build
195775 have function dbo fn calcaerialdistance which accepts parameters and returns its result now that parameters come from the same table do not want to hit database multiple times to bring same data have tried select dbo fn calcaerialdistance select latitude from loc locations where place name delhi select longitude from loc locations where place name delhi select latitude from loc locations where place name mumbai select longitude from loc locations where place name mumbai any way to optimise the code have also tried to access table datatype like an array
195923 sometimes store object names identifiers in some of our databases for example in some parameter tables because select records from these tables using the or like comparison operators must take care to store these names always with or without brackets if exists select from mytable where obj name table name or if exists select from mytable where obj name table name however ms sql has some functions where you can use object names with or without brackets for example the object id function ive set up minimal example on dbfiddle uk create table test id int identity11 primary key object sysname not null go insert into test values obj1 obj2obj3 obj4 go now can use object id to check if the table test exists in this way if object idtest is not null begin select test exists object id end go object id test exists if object id test is not null begin select test exists object id end go object id test exists it doesnt matter if pass the identifier test with or without brackets parser is smart enough to remove the brackets well can simulate this by adding scalar function that remove brackets from one string create function unquotename txt nvarcharmax returns nvarcharmax as begin return iifleft txt and right txt substring txt len txt txt end go and then use it in this way select dbo unquotename field name1 nfield name2 go name1 name2 field field select id object from test where object like obj go id object obj2 obj3 select id dbo unquotenameobject from test where dbo unquotenameobject like obj go id no column name obj1 obj2 obj3 obj4 but my question is is there any hidden built in function that removes brackets using sql dbfiddle here
195937 recently noticed my companys application using addwithvalueto pass parameter values to dynamic parameterized queries example cmd parameters addwithvalue vehicleid vehicles vehicleid in the database the data type for vehicleid is int as understand it because addwithvalue which is deprecated does not specify the data type length this vehicleid is converted and potentially converted incorrectly does that conversion affect sql server performance in the case of an int would this then cause issues beyond plan cache pollution is there performance hit caused by the conversion
196030 quick easy filter question what would be the difference in output or what impact would it have moving filter condition out of where clause into the join condition for example select a1 name a2 state from student a1 left join location a2 on a1 name id a2 name id where a1 name like and a2 state new york to this select a1 name a2 state from student a1 left join location a2 on a1 name id a2 name id and a2 state new york where a1 name like thanks all
196158 im in the process of rewriting queries that no longer pull in all the required data my question is in regard to practice ive never seen and havent found any question on stackexchange that specifically addresses the issue know the point of the having statement is to introduce conditions on aggregations just like where introduces conditions on individual rows however what im seeing in this code is having being used in lieu of where on queries with aggregations the conditions in having are not applied against the aggregations but on the non aggregated columns for example select id filedate sumamount from sales group by id filedate having id and filedate as opposed to select id filedate sumamount from sales where id and filedate group by id filedate are there performance implications or other advantages disadvantages to this strategy havent tried running diagnostics myself not priority and id have to do it on my own time however think may if there isnt clear answer on this my concern is how the optimizer views this query does it aggregate all data and then restrict the result set based on the having clause or does it realize it can apply the having conditions on the individual rows since they are specifically referencing non aggregated columns edit for my example queries and the actual sql im rewriting the plans are identical but the queries are of similar complexity and im not yet knowledgeable enough to draw conclusions from the identical plans
196219 of the sql system database master model msdb tempdb query store can only be used on msdb looked and dont find any documentation about query store on msdb while you cant see it in the gui it can be validated on your sql instance validate query store is off use msdb select from sys database query store options turn query store on use master go alter database msdb set query store on go alter database msdb set query store operation mode read write interval length minutes max storage size mb query capture mode auto go validate query store is on use msdb select from sys database query store options of all the system database why is msdb the only one with the option to use query store and what value does it add stop query store use master go alter database msdb set query store off go
196220 beginner question have an expensive function fx on two columns and in my database table want to execute query that gives me the result of the function as column and puts constraint on it something like select fx as func from table name where func however this doesnt work so will have to write something like select fx as func from table name where fx will this run the expensive function twice whats the best way to do this
196330 have docker installed and am running mongodb container for my local development on my mac the problem is that cant connect to said db easily from cli have robo 3t installed but would prefer to use the cli client instead is there known way to install just the mongo shell command mongo and not the full db distribution on os
196570 have table with varchar column it is allowing trademark copyright and other unicode characters as shown below create table varcharunicodecheck col1 varchar100 insert into varcharunicodecheck col1 values mycompany insert into varcharunicodecheck col1 values mycompany insert into varcharunicodecheck col1 values mycompany insert into varcharunicodecheck col1 values mycompany insert into varcharunicodecheck col1 values mycompany select from varcharunicodecheck but the definition of varchar says it allows non unicode string data but the trademark and registered symbols are unicode characters does the definition contradicts the property of varchar datatype read couple of links like first one and second one but still could not understand why it allows unicode string when the definition says that it allows only non unicode string values
196600 im testing for resilience against injection attacks on an sql server database all table names in the db are lower case and the collation is case sensitive latin1 general cs as the string can send in is forced to uppercase and can be maximum of characters in length so cant send in drop table because the table name would be in uppercase and thus the statement would fail due to the collation so whats the maximum damage could do in characters edit know all about parameterised queries and so forth lets imagine that the person who developed the front end that builds the query to send in didnt use params in this case im also not trying to do anything nefarious this is system built by somebody else in the same organisation
196931 installed postgresql under centos environment the service is started postmaster pid file present under var lib pgsql data but need to reload configuration or restart the server following change in pg hba conf however trying different commands get the following pg ctl reload var lib pgsql data bash pg ctl command not found service postgresql reload redirecting to bin systemctl reload postgresql service failed to reload postgresql service unit not found
196938 is there way to link two tables in order table actual ex status dlv actual fee expected fee act mail act email act pickup non mail non email non pickup table expected ex status dlv expected fee act mail act email act pickup non mail non email non pickup table expected result ex status dlv actual fee expected fee act mail act email act pickup non mail non email non pickup want to incorporate the follow logic within the join ex ex join on first but then on the combination of the other since they are unique tried this but of course it didnt work from dbo claims left outer join dbo pricing on ex ex and status status and dlv dlv was hoping to link table to table to get the expected fee if ex then its regardless of status or dlv if ex then have to depend on the other two columns im sorry if this is confusing
197142 read this article quite recently on the performance issues to do with functions im currently in the development phase of new database on the azure sql database platform it wont go live for new months yet im using variety of scalar valued functions with one being to convert utc to local date where it is specified in the config table aus eastern standard time the function is called as the following select dbo fngetlocaldatedatecolumn as datecolumn from table does anyone have any suggestions on how to avoid functions in cases such as this find them useful for code reuse and im not sure how to avoid it here im also wondering whether with vnext fixing the performance issues with functions in the azure platform that im better off continuing to work with functions create function dbo fngetlocaldate datetoconvert datetimeoffset null returns datetimeoffset as begin declare timezone varchar50 return case when datetoconvert is null then null else convertdatetimeoffset datetoconvert at time zone select value from lookup config where property timezone end end
197360 have script file in my local machine have to open ssms and open the query in ssms and to execute the query for opening the ssms and query have used the following powershell script to do it ssms exe powershell prod screenshot serverdetails sql localhost how can run this query using powershell and how can close this query window the reason im doing this is have to take the evidence of the sql server properties and other information for that have powershell script to capture the screenshot im preparing the powershell script that runs every query in ssms and captures the outcome of that and save into specified path now am able to open the query in ssms and capture the screenshot but im unable to run the query
197363 scientific names are well accepted unique identifiers of species however scientific names have special characters such as spaces and dots can it be good unique identifier for databasing or should one assign implement unique numerical identifiers if it is with the latter is there globally known numeric identifier or should one assign own identifiers for the rows
197789 have bunch of sql servers registered within my ssms lots of different sql versions editions and db compatibilities want to check out if any of those servers have any of the sp blitz or sp whoisactive stored procedures installed and which versions they are know can right click on my registered servers group and open one query window that connects to all the servers in the group but is there reliable way to check which sprocs and versions have
197870 were using vendor app running on sql server enterprise and it has rather annoying quirk of executing count statements on the items table while processing most financial documents orders invoices etc select counta from dbo items t0 im sure that would normally be fine but theres over million records and it takes 400ms to count them all this can constitute substantial portion of the overall processing time the table already has an extremely narrow nonclustered index tinyint plus clustered key on it which is what sql is using when it does the table scan so dont think we can do any better in that regard theres few solutions im aware of which wed like to avoid if possible indexed view using count big use faster processors were using cloud servers and theres very limited options in this regard deleting records not really an option in this case do we have any other options to speed this up heres gist showing the setup https gist github com elvishfiend 5094f120b14f8ecfb325623edcb5f3eb
197937 am working on setting up secondary copy of our prod db for data warehousing and even something we can possibly use to offload other processes and reporting on basically just read only copy of prod issues is am running into options that breaks our point in time backup strategies and that can reliably handle tb of data after reading this post always on availability groups looks promising but have few questions and am new to dba and most of microsofts documentation on the subject is another language to me at this point does always on availability groups mean the database will basically be mirrored and that can use that server to offload the heavy reporting to does that break point in time transaction logs backup of the prod server can this cause strain on the prod server will this cause prod traffic to be redirected to this server do not want this to become ha db server just want read only copy of few prod dbs
198170 want to install sql server on linux from ms site read that red hat suse and ubuntu are supported but want to use it under debian since ubuntu is based on debian is there chance to succeed with installation https docs microsoft com en us sql linux sql server linux setup
198408 when loading data into table get the following error error row is too big size maximum size the table has columns in it which appears to be the problem the general internet advice is refactor or normalize for instance this post unfortunately dont believe such advice applies to my situation the table is to store data collected from device the device produces png image as part of an analysis the png consists of pixels each pixel has an associated numeric value along with the pixel data are various other fields related to the analysis breaking the table into parts doesnt really make sense the fields are all logically associated with the particular object being analyzed postgres doesnt seem to like that each pixel has its own field the table has fields of the form pixel pixel pixel note that this is fundamentally different from the usual example of phone number phone number etc each pixel is unique object by virtue of its location pixel has different position than pixel and each pixel has an associated value the common aspect between them is that they both are used to describe the same analysis object they are the quantitative analog to the visual representation given in the png is there way to increase the row size if the table simply cannot have columns how could refactor this assuming the first two answers are no should just stick the columns into an xml and throw that in text field hope have made the context clear have tried to boil the problem down to its essence but suspect some clarification may be needed please let me know if clarification is needed edit as an experiment tried breaking the pixels into separate table that seems to be the only possible way to refactor but the columns produces the same error
198473 not sure how to go about this have query select name city salary from employee can get sum by doing grouping select name city sumsalary total salary from employee group by name city how can get the total per city as separate row under each city expected sample result name city salary n1 c1 n2 c1 t1 c1 n3 c2 n4 c2 t2 c2
198513 am having problem with sql query that is supposed to find authors that have published at least one book priced or more it is supposed to display first and last name sorted by last name as long as the author has published at least one book priced or more below is my query so far select authors au fname authors au lname titles price from authors titles order by au lname asc where titles price the first picture shows the dbs first table authors and the second shows titles which holds the price column what am doing wrong cannot figure out what is wrong with my syntax thanks
198611 it appears setting filegroup to read only prevents dbcc checkdb for the entire database if the filegroup contains columnstore index when attempting to run checkdb or checkfilegroup for any filegroup in the database including read write secondaries and primary the below error is returned msg level state line check terminated failure was detected while collecting facts possibly tempdb out of space or system table is inconsistent check previous errors is there supported method for having columnstore data in read only filegroup or am precluded from integrity checks in this scenario repro create database check fg ro go use check fg ro go exec sp changedbowner sa go alter database check fg ro add filegroup check fg ro alter database check fg ro add file name check fg ro filename check fg ro ndf to filegroup check fg ro go create table foo int not null primary key on check fg ro go create columnstore index ccix foo on fooi go use master go alter database check fg ro modify filegroup check fg ro read only go dbcc checkdb check fg ro with no infomsgs all errormsgs extended logical checks msg level state line check terminated failure was detected while collecting facts possibly tempdb out of space or system table is inconsistent check previous errors go disclaimer cross posted to technet forums
198789 in postgres im thinking of query pg type for an up to date list of enumerations im using on regular basis id be using something like this select pg type typname as enum type pg enum enumlabel as enmu label from pg type join pg enum on pg enum enumtypid pg type oid or select distinct pg type typname as enum type from pg type join pg enum on pg enum enumtypid pg type oid is this bad practice
199144 quarterly check the vlf on all of my servers through cms with query from the tigertoolbox fixing vlfs on github this includes suggestions and code for correcting what is found always try to get full understanding of what is going before making any adjustments of the time the vlf solution apply is not the same as the recommendation though it is usually close using dbcc loginfo find that several of the vlfs are not used in sequential order am trying to understand why this highly voted answer to dbcc shrinkfile on log file not reducing size even after backup log to disk says it can happen but not why because virtual log files are not always allocated in order this would seem to conflict with jonathan kehayias an xevent day of how it works multiple transaction log files we can see that of the vlf only two line are currently used the largest fseqno is subtract dont see any fseqno lower then so presumably all have been used in the last vlf rollovers if we look at lines we see line is written to first then line then line also the parity is off not sure what this adds to the scenario the lsn show they are created in the opposite order would expect that once whatever caused the disruption in sequence order has passed the next time through the vlfs would be written to in order created why are they not notice line is in both images to show consistency the example is from sql server the database is currently in full recovery and has been for year or more the server has is part of node ag the results above are taken from the secondary server where the logs are backed up hourly the last grows occurred through auto grow of 1000mb creates 1vlf the log file currently has initial size of 115000mb and autogrow of 1000mb edit max vernon thank you for the link in your answer understood how they got written out of order but am making an assumption that they continued to written out of order once the blocking event was cleared by following fsegno backwards you can recreate how it occured line is the current vfl with fseqno at line the fseqno is the fseqno is at line last line presumably vfl was created when fseqno was being written but there have been events of writing to the next vlf with vlf that would be more than cycles through the log couple of hours later the list looks this
199202 am using temporal tables within my database and when right click on my table in management studio v17 do not see the select top rows in the context menu there is no problem with non temporal tables any ideas how to get this context menu back have feeling it is to do with the version of sql server am running sql express edition
199226 in this scheme id uid content if the user decided that he doesnt need the rows with id is it better to make him set their content to null or just delete the row this process occur many time and he maybe use the null slots again do still use update and set it to null in case he will change it in the future or just delete the entire row the question is like is it better to delete the row insert delete insert delete insert or update the row insert update to null update to value update to null update to value
199487 have table created below that contains rows expect the last query below to use the nonclustered index and then key lookup however instead it uses clustered index scan only single row is returned as expected why does it do the scan rather than use the nonclustered index is it because the table contains only rows create table dbo testindexsample code char4 not null name nvarchar200 not null modifieddate datetime not null constraint df testindexsample modifieddate default getdate constraint pk testindexsample code primary key clusteredcode go create nonclustered index ix testindexsample name on dbo testindexsamplename go insert into dbo testindexsamplecode name select codename fullname from dbo sourcetest go select from dbo testindexsample select from dbo testindexsample where code x132ey select from dbo testindexsample where name user
199842 querying the sys dm tran locks dmv shows us which sessions spids are holding locks on resources like table page and row for each lock acquired is there any way to determine which sql statement delete insert update or select caused that lock know that the most recent query handle column of the sys dm exec connections dmv gives us the text of the last query executed but several times other queries ran before under the same session spid and are still holding locks already use the sp whoisactive procedure from adam machanic and it only shows the query that is on the input buffer at the moment think dbcc inputbuffer spid which not always and in my case usually never is the query that acquired the lock for example open transaction session exec statement that holds lock on resource exec another statement on the same session open another transaction session and try to modify the resource locked at step the sp whoisactive procedure will point out the statement at step which is not the responsible for the lock and thus not useful this question came from doing an analysis using the blocked process reports feature to find the root cause of blocking scenarios in production each transaction runs several queries and most of time the last one that is shown on input buffer at bpr is rarely the one holding the lock have follow up question framework to effectively identify blocking queries
199853 why do these two select statements result in different sort order use tempdb create table dbo oddsort id int identity11 primary key col1 nvarchar2 col2 nvarchar2 go insert dbo oddsort col1 col2 values ne nea go select from dbo oddsort order by col1 collate latin1 general cs as id col1 col2 ea should be id select from dbo oddsort order by col2 collate latin1 general cs as id col1 col2 ea
199961 this is question about logic that could help me understand more how to iterate into my dbs on my server have several adventureworks databases have query that cold help me iterate through all my databases adventureworks and not create table list db name nvarchar128 insert into list db select name from sys databases where database id and state select from list db drop table list db for each adventureworks database want to list awbuildversion next to each db at this point im puzzled because there are problems dont know how to solve how to run select database version from awbuildversion for each database what to do if database has no awbuildversion table here found this query declare sql nvarcharmax set sql stuff select union select quotenamename as db name name collate sql latin1 general cp1 ci as as table name from quotenamename sys tables where name tablename from sys databases order by name for xml path type value nvarcharmax print sql execute sp executesql sql tablename varchar30 tablename awbuildversion that brings me close to my goal but it shows me the table name awbuildversion while need the column database version
200161 am little stumped with this cte update stmt declare table id int value int declare table id int value int insert values insert values with cte as select from update cte set value value from cte as inner join as on id id select from go why does this result in table having for both rows thought it should be for id and for id get expected results if use table instead of common table expression to do the updates declare table id int value int declare table id int value int insert values insert values select from update set value value from as xx inner join as on id xx id select from this results in table with and but arent we supposed to get both the same values based on the previous explanation of referencing issue update being done to table and not the referenced xx
200269 were getting ready to perform large upgrade on our sql servers and are noticing some unusual behavior with distributed availability groups that im trying to resolve before moving forward last month upgraded remote secondary server from sql server to sql server this server is part of multiple distributed availability groups dags and separate availability group ag when we upgraded this server we were unaware that it would get into an unreadable state so during the past month weve solely been relying on the primary server as part of the upcoming upgrade applied the cu patch to the server and rebooted it when the server came back online the just patched secondary showed all of the dags ags were syncing without any issues however the primary was showing very different story it was reporting that the separate ag was syncing without any issues but the dags were in not synchronzing not healthy state after initially panicking attempted the following things to get things synchronizing again in the dags from the primary stopped and resumed the data movement this did not start syncing the data on the secondary the one just patched ran alter database database set hadr resume which execute without errors but did not resume any syncing my last attempt at syncing the data again was to login to the secondary and manually restart the sql server service manually restarting the service seems bit extreme as id expect the server being rebooted would have been enough has anyone run into this issue where dag doesnt start syncing to secondary after reboot if so how was it resolved checked both the sql server error log and the event viewer on the secondary server there was nothing out of the ordinary that could see
200279 have couple sql server express databases running on windows server r2 on vps couple of websites access them these are local connections right website connections use integrated security also connect using remote desktop sql server express is accessible remotely through port same as remote desktop problem logs are showing failed login attempts every seconds they will be from ip for few minutes then from another ip they usually try user sa dont have one or variation have blocked several hundred of these using windows firewall but they keep coming so what else should be doing is this something should be worrying about
200352 am trying to ignore duplicate rows from cte but am not able to do that it seems like cte does not allow to use rownum variable to where clause as it is showing invalid column name numrows error while trying to do so how do ignore the duplicate rows while using select in cte sql query declare batchid uniqueidentifier newid declare clusterid smallint declare batchsize integer declare mytablevariable table eventid bigint hotelid int batchstatus varchar50 batchid uniqueidentifier with pendingextressvceventsdata batch as select top batchsize eventid hotelid batchstatus batchid row number over partition by eventid order by eventid numrows from extressvcpendingmsg with nolock where clusterid clusterid and numrows exclude extressvceventid and hotelid which are partly included in in progress batch and not exists select from extressvcpendingmsg t2 where t2 batchstatus batched and t2 eventid eventid and t2 hotelid hotelid update pendingextressvceventsdata batch set batchstatus batched batchid batchid output inserted into mytablevariable where numrows select extressvceventid hotelid id1 id2 extressvceventtype hostid statuscode channelid requestattime processtime datebegin dateend statusmsg em msgbodyout em msgbodyin channelresid from extressvcevent with nolock inner join mytablevariable on extressvceventid eventid inner join extressvceventxml em with nolock on eventid em extressvceventid order by extressvceventid
201598 all of this works create database go use go create schema go create table nvarchar20 go create unique clustered index on go insert into values go create view vw as select from go create proc sp shrug nvarchar20 as select from vw where shrug go exec sp shrug go but you can probably see where im going with this dont want shrug want neither of these work on any version from create proc sp nvarchar20 as select from vw where go create proc sp nvarchar20 as select from vw where go so is there way to use unicode stored procedure parameter names
201719 have the table courses where we have some fields and also the field coursestring favoritedbool favoriteddatedatetime dateupload the teacher uploads the course and this date is stored in the dateupload field if the teacher flag this course as favorite this information is number in the field favorited and the date it was flagged is stored in the field favoriteddate now need to retrieve all courses from specific teacher and then order the list by date if the course is flagged favorited should ordered by favoriteddatedateupload desc if favorited should be ordered only by dateupload desc this is what have so far but it is not working select from courses where teacherid order by case favorited when then favoriteddate dateupload else dateupload end desc any idea how to solve this
202000 found that if script sql server agent job as create to that the script starts with use msdb so assume that jobs are stored in the msdb database what is the best way to add sql server jobs to visual studio database project added the msdb database to the database solution but that doesnt seem to have any reference to jobs would like the jobs to be deployed updated along with database it seems like this is not possible
202086 we have third party application that sends sql statements in batches the database is hosted on sql server enterprise sp1 cu7 cores and 256gb memory optimize for ad hoc is enabled this is dummy example of the queries that are being executed exec sp executesql if trancount set transaction isolation level snapshot select field1 field2 from table1 where field1 optionkeep plan keepfixed loop join select field3 field4 from table2 where field3 optionkeep plan keepfixed loop join nvarchar6 ntest when monitor the database and look at batches sec and compiles sec notice they are always the same under heavy load this can be batches sec and compiles sec under average load there are batches sec analyze the query cache for recently compiled plans select top qs creation time databasename db namest dbid qs execution count st text qs plan handle qs sql handle qs query hash from sys dm exec query stats qs cross apply sys dm exec sql textqs plan handle as st order by creation time desc when run above query only see new query plans sec its like every sp executesql call triggers compile but the queryplan is not cached what can be the cause of batches sec being equal to compiles sec
202211 im trying to see if theres way to trick sql server to use certain plan for the query environment imagine you have some data which is shared between different processes so suppose we have some experiment results which take lot of space then for each process we know which year month of experiment result we want to use if object iddbo shareddata is not null drop table shareddata create table dbo shareddata experiment year int experiment month int rn int calculated number int primary key experiment year experiment month rn go now for every process we have parameters saved in the table if object iddbo params is not null drop table dbo params create table dbo params session id int experiment year int experiment month int primary key session id go test data lets add some test data insert into dbo params session id experiment year experiment month select union all select go insert into dbo shareddata experiment year experiment month rn calculated number select row number overorder by v1 name abschecksumnewid from master dbo spt values as v1 cross join master dbo spt values as v2 go insert into dbo shareddata experiment year experiment month rn calculated number select row number overorder by v1 name abschecksumnewid from master dbo spt values as v1 cross join master dbo spt values as v2 go fetching results now its very easy to get experiment results by experiment year experiment month create or alter function dbo getshareddata experiment year int experiment month int returns table as return select rn calculated number from dbo shareddata as where experiment year experiment year and experiment month experiment month go the plan is nice and parallel select calculated number count from dbo getshareddata2014 group by calculated number query plan problem but to make usage of the data bit more generic want to have another function dbo getshareddatabysession session id int so straightforward way would be to create scalar functions translating session id experiment year experiment month create or alter function dbo fn getexperimentyear session id int returns int as begin return select experiment year from dbo params as where session id session id end go create or alter function dbo fn getexperimentmonth session id int returns int as begin return select experiment month from dbo params as where session id session id end go and now we can create our function create or alter function dbo getshareddatabysession1 session id int returns table as return select rn calculated number from dbo getshareddata dbo fn getexperimentyear session id dbo fn getexperimentmonth session id as go query plan the plan is the same except its of course not parallel because scalar functions performing data access make the whole plan serial so ive tried several different approaches like using subqueries instead of scalar functions create or alter function dbo getshareddatabysession2 session id int returns table as return select rn calculated number from dbo getshareddata select experiment year from dbo params as where session id session id select experiment month from dbo params as where session id session id as go query plan or using cross apply create or alter function dbo getshareddatabysession3 session id int returns table as return select rn calculated number from dbo params as cross apply dbo getshareddata experiment year experiment month as where session id session id go query plan but cant find way to write this query to be as good as the one using scalar functions couple of thoughts basically what id want is to being able to somehow tell sql server to pre calculate certain values and then pass them further as constants what could be helpful is if we had some intermediate materialization hint ive checked couple of variants multi statement tvf or cte with top but no plan is as good as the one with scalar functions so far know about coming improvement of sql server froid optimization of imperative programs in relational database im not sure it will help though it wouldve been nice to be proven wrong here though additional information am using function rather than selecting data directly from the tables because it is much easier to use in many different queries which usually have session id as parameter was asked to compare actual execution times in this particular case query runs for 500ms query runs for 1500ms query runs for 1500ms query runs for 2000ms plan has an index scan instead of seek which is then filtered by predicates on nested loops plan is not that bad but still does more work and works slower that plan lets assume that dbo params is changed rarely and usually have around rows not more than lets say is ever expected its around columns now and dont expect to add column too often the number of rows in params is not fixed so for every session id therell be row number of columns there is not fixed its one of the reasons dont want to call dbo getshareddata experiment year int experiment month int from everywhere so can add new column to this query internally id be glad to hear any opinions suggestions on this even if it has some restrictions
202254 have this relationship create table auth user id integer not null primary key username character varying150 not null unique create table auth group id integer not null primary key name character varying80 not null unique create table auth user groups id integer not null primary key user id integer references auth userid not null group id integer references auth groupid not null constraint user groups uniqueuser id group id insert into auth user values user1 insert into auth user values user2 insert into auth group values group1 insert into auth group values group2 insert into auth user groups values insert into auth user groups values insert into auth user groups values how to select all usernames which are in the group group1 use postgresql but sql which works everywhere is preferred
202419 have basic useractivity table that captures an activitytypeid per userid and the activitydate at which the activity occurred am writing query stored procedure that allows for input of the userid fortypeid as well as the durationinterval and durationincrement to dynamically return results based on number of seconds minutes hours days months years given that the datepart argument within dateadd datediff does not allow parameters had to revert to bit of trickery in order to get the desired results within the where clause initially wrote the query using datediff but immediately after writing and taking peek at the execution plan remembered that it is not sargable function along with the fact the precision levels could offer for some dates falling off in leap year so re wrote the query to utilize datepart thinking that would hit an index seek instead of an index scan and generally perform better unfortunately ive found that writing the query as dateadd offers up the same results an index scan is occurring and query optimizer is not leveraging the non clustered index against activitydate read aaron bertrands blog post performance surprises and assumptions dateadd and implemented the changes he described to convert the dateadd portion into the equivalent datetime2 column definition due to weird trickery involved with datetime2 however the issue was still present even after doing so to better illustrate the scenario here is comparable table definition drop table if exists dbo useractivity if object id dbo useractivity is null begin create table dbo useractivity userid int not null useractivityid bigint identity11 not null activitytypeid tinyint not null activitydate datetime2 not null constraint df useractivity activitydate default getdate constraint pk useractivity primary key clustered useractivityid asc index ix useractivity userid nonclustered userid asc index ix useractivity activitytypeid nonclustered activitytypeid asc index ix useractivity activitydate nonclustered activitydate asc end go populate the table with dummy data recursively for different users with random activitytypeid between and with new activitydate every minutes declare userid int select isnullselect top userid from dbo useractivity order by userid desc with useractivityseed as select convertdatetime20 as activitydate union all select dateaddminute activitydate from useractivityseed where activitydate insert into dbo useractivity userid activitytypeid activitydate select userid abschecksumnewid activitydate from useractivityseed option maxrecursion go alter index all on dbo useractivity rebuild below is the first query wrote with datediff note am excluding the userid and fortypeid predicates intentionally so to avoid those key lookups and reduce noise within the plans attached as youll find on pastetheplan for this query it is performing an index scan as expected given that datediff is not sargable declare userid int declare fortypeid int declare durationinterval varchar6 hour declare durationincrement int select countua useractivityid as activitytypecount from dbo useractivity ua where exclude the userid and fortypeid predicates ua userid userid and ua activitytypeid fortypeid and case when durationinterval in year yy yyyy then datediffsecond ua activitydate getdate when durationinterval in month mm then datediffsecond ua activitydate getdate when durationinterval in day dd then datediffsecond ua activitydate getdate when durationinterval in hour hh then datediffsecond ua activitydate getdate when durationinterval in minute mi then datediffsecond ua activitydate getdate when durationinterval in second ss then datediffsecond ua activitydate getdate end durationincrement below is the dateadd query pastetheplan here unfortunately an index seek is not occurring this may be an incorrect assumption on my part but im perplexed as to why it isnt occurring at all declare userid int declare fortypeid int declare durationinterval varchar6 hour declare durationincrement int select countua useractivityid as activitytypecount from dbo useractivity ua where exclude the userid and fortypeid predicates ua userid userid and ua activitytypeid fortypeid and durationinterval in year yy yyyy and ua activitydate convertdatetime20 dateaddyear durationincrement getdate or durationinterval in month mm and ua activitydate convertdatetime20 dateaddmonth durationincrement getdate or durationinterval in day dd and ua activitydate convertdatetime20 dateaddday durationincrement getdate or durationinterval in hour hh and ua activitydate convertdatetime20 dateaddhour durationincrement getdate or durationinterval in minute mi and ua activitydate convertdatetime20 dateaddminute durationincrement getdate or durationinterval in second ss and ua activitydate convertdatetime20 dateaddsecond durationincrement getdate what is the cause of this is the behavior im seeing result of my usage of or negating any potential for it to even get to using the index am overlooking something painstakingly obvious here update my second question above lead me to perform query foregoing the or operations the query performed the index seek so something is occurring during these comparisons that sql server does not like pastetheplan here declare durationincrement int select countua useractivityid as activitytypecount from dbo useractivity ua where ua activitydate convertdatetime20 dateaddhour durationincrement getdate update solution shared here
202707 in my application have query which performs search in files table the table files is partitioned by created see the table definition and has million rows for the client cid im using this query from the answer to my previous question slow order by sql server with partitionnumbers as each partition of the table select partition number from sys partitions as where object id object idndbo files nu and index id select ff id ff name ff year ff cid ff created vnve0 keywordvaluecol0 numeric from partitionnumbers as pn cross apply select f100 from rows in order for year select id name year cid created from dbo files as where grapado is null and masterversion is null and year and cid and eid and partition pf files partitioningf created pn partition number order by name offset rows fetch first rows only union all rows in order for year select id name year cid created from dbo files as where grapado is null and masterversion is null and year and cid and eid and partition pf files partitioningf created pn partition number order by name offset rows fetch first rows only as f100 as ff outer apply lookup distinct values select keywordvaluecol0 numeric case when vn value is not null and vn value then convertdecimal28 vn value else convertdecimal28 end from dbo value number as vn where vn id file ff id and vn id field group by vn value as vnve0 order by ff name offset rows fetch first rows only the point here is this query is performing well but if change eid in where to eid not in the query becomes too slow more than minutes execution plan using eid https www brentozar com pastetheplan id hj fbb2qm execution plan using eid not in https www brentozar com pastetheplan id b1 zmbnqz ive tried doing join with the entidades table whose id is referenced by eid in the files table execution plan doing join with entidades https www brentozar com pastetheplan id rjarhzh5m how could improve the performance here additional info partition function pf files partitioning create partition function pf files partitioning datetime27 as range left for values partition scheme ps files partitioning create partition scheme ps files partitioning as partition pf files partitioning all to primary note will have around million rows in each partition table files create table dbo files id bigint identity11 not null cid tinyint not null eid bigint not null cat id bigint not null tip id bigint null sub id bigint null year smallint not null caducidad smallint null grapadopri int not null grapado bigint null name nvarchar not null extension tinyint not null size bigint not null id doc bit not null observaciones nvarchar not null indexed bit not null signed bit not null created datetime2 not null name lower nvarchar not null modified datetime2 null related bit not null masterversion bigint null versioned bit not null hwsignature tinyint not null blockeduserid smallint null constraint pk files id primary key clustered id asc created asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on ps files partitioning created constraint files estructure unique unique nonclustered cat id asc tip id asc sub id asc year asc name asc grapado asc created asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on alter table dbo files with nocheck add constraint fk files entidad foreign key eid references dbo entidades id on update cascade on delete cascade alter table dbo files check constraint fk files entidad table value number create table dbo value number id bigint identity11 not null id file bigint not null default id field bigint not null default value nvarchar null default null id doc bigint null default null constraint pk value number id primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on also the table value number is partitioned by this partition function create partition function pf value number bigint as range left for values and this partition scheme create partition scheme ps value number as partition pf value number all to primary create table dbo value number id bigint identity11 not null id file bigint not null default id field bigint not null default value nvarchar null default null id doc bigint null default null constraint pk value number id primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on ps value number id file table entidades create table dbo entidades id bigint identity11 not null cid bigint not null mid bigint null id doc bigint null name nvarchar not null sincro tinyint not null comprobar tinyint not null op1 tinyint not null op2 tinyint not null op3 tinyint not null index tinyint not null can be parent tinyint not null accounting nchar null nota nvarchar null alias nvarchar null element type tinyint not null size bigint not null constraint pk entidades id primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on constraint codigo unique nonclustered cid asc name asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on alter table dbo entidades with check add constraint fk entidades entidad foreign key mid references dbo entidades id go indexes of the files table create nonclustered index files clientes on dbo files cid asc include id with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index files grapado on dbo files grapado asc include id name with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index files mv on dbo files masterversion asc year asc cat id asc cid asc eid asc grapado asc sub id asc tip id asc include id name with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index files ocr on dbo files cid asc grapado asc indexed asc masterversion asc extension asc include id eid cat id tip id sub id year name with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index files ocr2 on dbo files cid asc eid asc grapado asc indexed asc masterversion asc extension asc include id cat id tip id sub id year name with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index files ocr3 on dbo files cid asc cat id asc grapado asc indexed asc masterversion asc extension asc include eid tip id sub id year name with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index busqueda name on dbo files cid asc eid asc grapado asc year asc include id cat id tip id sub id grapadopri name size id doc signed created modified related masterversion with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index busqueda2 on dbo files cid asc eid asc cat id asc grapado asc masterversion asc year asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index cid on dbo files cid asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index eid on dbo files eid asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index extension on dbo files extension asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index fk files archivo on dbo files grapado asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index fk files tipo on dbo files tip id asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index grapadopri on dbo files grapadopri asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index index all on dbo files cid asc eid asc grapado asc masterversion asc include cat id tip id sub id year grapadopri name size id doc signed created modified related versioned with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index missing index on dbo files cid asc eid asc grapado asc name asc year asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index ocrcloudclients on dbo files grapado asc indexed asc extension asc include cid eid cat id tip id sub id with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index searchentity on dbo files cid asc eid asc grapado asc masterversion asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index sub id on dbo files sub id asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index ix dbo files cid year eid grapado is null and masterversion is null on dbo files cid asc year asc eid asc include grapado masterversion where grapado is null and masterversion is null with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index ix dbo files cid year name grapado is null and masterversion is null on dbo files cid asc year asc name asc include grapado masterversion where grapado is null and masterversion is null with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created create nonclustered index ix dbo files cid year eid name grapado is null and masterversion is null on dbo files cid asc year asc eid asc name asc include grapado masterversion where grapado is null and masterversion is null with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps files partitioning created indexes of the value number table create nonclustered index searchvalues on dbo value number id field asc include id file value with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps value number id file create nonclustered index search on dbo value number id file asc id field asc include value with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps value number id file create nonclustered index id field on dbo value number id field asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps value number id file create nonclustered index fk valueesn documento on dbo value number id doc asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps value number id file create nonclustered index fk valueesn archivo on dbo value number id file asc with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on ps value number id file
203130 have an internal web application running and everytime user goes to the search view it queries three different tables in the db to generate values for three dropdowns in the view its basically running select distinct portname from ports order by portname asc but the table contains rows and is under quite heavy load which means that from time to time the loading time for the page due to loading the dropdowns with data can be upwards of seconds so is there better way to do this for example running some script at certain intervals and creating table view whatever at different location so as to offload querying the big table just to have rows returned from the in the main table
203402 have question about the installation of sql server we installed trial version tha was made on according to the script that execute in the database the expiration date was but we can still use it without problems the sharepoint site that uses this db does not give any errors or warnings legends that it is expired or that some features are no longer available can also enter from msqlserver management studio and execute scripts create new db etc how can exactly know the expiration date select servername as servername create date as instalaltiondate serverpropertyedition as version dateadddd create date as expiry date from sys server principals where sid 0x010100000000000512000000 servername installationdate version expiry date saharcvsps02 enterprise evaluation edition bit
203499 am recently seeing the unknown netowrk error which saves the full back ups to file share on some of the servers the back ups happens at different times ola halengren scripts are being used currently and the sql server edition used are enterprise edition see there are suggestions for to create dword with name sesstimeout and value in hklm system currentcontrolset services lanmanworkstation parameters my question is should we create this on the target server file share or the server from where the sql server resides
203612 the following image describes the format of the gst identification number the first digits denote the unique state code in accordance with the indian census for instance the state code of new delhi is and that of karnataka is the next characters denote the pan permanent account number of the taxpayer the 13th digit denotes the registration number or entity number of the tax payer with the same pan the 14th digit is by default for all not intending anything currently the 15th digit is the check sum digit can be number or an alphabetical character perhaps can carry out the validation by means of patindex or regex
203877 there are queries where when we hit execute it shows some rows and it keeps growing but the query is not over yet yet sometimes it waits until the end of the query why does this happen is there way to control this
203981 just installed pgadmin version for macos sierra replacing the pgadmin that came bundled with postgres running locally on the same mac opened the pgadmin app app icon which caused safari browser to come to the front with tab showing the usual pgadmin user interface good closed that tab in safari now when open the pgadmin app icon either double clicking or choosing file open nothing happens is pgadmin supposed to launch in browser window rather than in the apps own window how to re open pgadmin again
204011 we have some consultants working on expanding an inhouse data warehouse was doing code review and ran across this pattern in all of the load procs merge edhub customer class as target using select columns from dbo vw customerclass where jhapostingdate postingdate as source on target bankid source bankid this join is on the business keys and target code source code when not matched by target then insert statement when matched and target islatest and exists select source hash except select target hash then update statement the gist is if we have new business key insert but if the business key exists and the hash of the attributes dont match our current row then update the old row and insert new one later in the code it all works fine but paused when got to this code and exists select source hash except select target hash it seems overly complicated compared to source hash target hash the except will do an accurate null comparison but in our case hashes will never be null or we have bigger problems want our code to be easy to read so that when someone has to maintain it it doesnt confuse asked our consultants about it and they speculated that it might be faster because of set operations but decided to write simple test test code below the first thing noticed was the exists except had more complicated query plan but thats not always bad ran each select client statistics on and the join yielded total execution time of vs with the exists except want to take this to our consultants with the request to refactor that statement but wanted to get feedback here on is this good test am missing anything is there case where exists except would be better comparison test script create table hash varbinary8000 create table hash varbinary8000 set nocount on declare int while begin insert into dbo hash select hashbytessha2 256castnewid as varchar200 insert into dbo hash select hashbytessha2 256castnewid as varchar200 set end insert into dbo hash values null insert into dbo hash values null select count1 from dbo cross join dbo where isnullr hash0 isnulll hash0 select count1 from dbo cross join dbo where existsselect hash except select hash
204087 have an analysis services cube project that my company received from an outside contractor and im trying to get it so that developers can work on it on their local machines im pretty sure that the original developers generated the schema for the backend database from the cube project and then worked against that therefore id like to generate the schema from visual studio rather than just exporting the database in sql server management studio currently im receiving an error saying that the dimensions are bound to user tables when try to generate the relational schema is my approach reasonable and if so how would go about doing this if my approach isnt reasonable why not note id asked this question few days ago on stackoverflow when came back to it it occurred to me that this would be better place for it so moved it here
204096 have run into an odd issue where sql server standard edition bit has seemed to have capped itself off at precisely half of the total memory allocated towards it 64gb of 128gb the output of version is microsoft sql server sp1 cu7 gdr kb4057119 x64 dec copyright microsoft corporation standard edition bit on windows server r2 datacenter build hypervisor the output of sys dm os process memory is when query sys dm os performance counters see that the target server memory kb is at and total server memory kb is at just under half of that at in most scenarios would understand this to be normal behavior as sql server has not yet determined that it needs to allocate any further memory for itself however it has been stuck at 64gb for over months now during this timeframe we have performed significant amount of memory intensive operations on some of the databases and have added close to more databases to the instance we are sitting at databases total each with pre allocatted data files at 4gb with 256mb autogrowth rate and 2gb log files with 128mb autogrowth rate perform full backup once nightly at 00am and begin transaction log backups monday through friday starting at 00am through 00pm on an interval of every minutes these databases are relatively low on their overall throughput but im skeptical that something is awry given that sql server hasnt crept up towards the target server memory naturally through new database additions normal query executions as well as memory intensive etl pipelines that have been ran the sql server instance itself is sitting atop virtualized vmware windows server 2012r2 server with cpu 144gb of memory 128gb to sql server 16gb reserved for windows and total virtual disks that sit atop vsan with 15k sas drives windows sits naturally on 64gb disk with page file of 32gb the data files sit on 2tb disk the log files sit on top of 2tb disk and tempdb sits on 256gb disk with 8x16gb files with no autogrowth have verified that there are no other instances of sql server running on the server besides mssqlserver this server is entirely dedicated to only the sql server instance so we have no other applications or services running on it that might consume memory utilize redgate sql monitor for analysis and below is history of the past days of total server memory as you can see the memory utilization has remained entirely stagnant aside from single uptick of 300mb in early april what might be the cause of this what can take closer look at in order to determine why sql server isnt wanting to use the additional 64gb of memory allocated towards it the output of running sp blitz sp blitz outputtype markdown checkserverinfo priority performance cpu schedulers offline some cpu cores are not accessible to sql server due to affinity masking or licensing problems memory nodes offline due to affinity masking or licensing problems some of the memory may not be available priority reliability remote dac disabled remote access to the dedicated admin connection dac is not enabled the dac can make remote troubleshooting much easier when sql server is unresponsive priority performance many plans for one query plans are present for single query in the plan cache meaning we probably have parameterization issues server triggers enabled server trigger rg sqllighthouse ddltrigger is enabled make sure you understand what that trigger is doing the less work it does the better server trigger ssmsremoteblock is enabled make sure you understand what that trigger is doing the less work it does the better priority performance queries forcing join hints instances of join hinting have been recorded since restart this means queries are bossing the sql server optimizer around and if they dont know what theyre doing this can cause more harm than good this can also explain why dba tuning efforts arent working queries forcing order hints instances of order hinting have been recorded since restart this means queries are bossing the sql server optimizer around and if they dont know what theyre doing this can cause more harm than good this can also explain why dba tuning efforts arent working priority file configuration system database on drive master the master database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space model the model database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space msdb the msdb database has file on the drive putting system databases on the drive runs the risk of crashing the server when it runs out of space priority informational agent jobs starting simultaneously multiple sql server agent jobs are configured to start simultaneously for detailed schedule listings see the query in the url tables in the master database master the commandlog table in the master database was created by end users on jul 22pm tables in the master database may not be restored in the event of disaster traceflag on trace flag is enabled globally trace flag is enabled globally trace flag is enabled globally priority non default server config agent xps this sp configure option has been changed its default value is and it has been set to backup checksum default this sp configure option has been changed its default value is and it has been set to backup compression default this sp configure option has been changed its default value is and it has been set to cost threshold for parallelism this sp configure option has been changed its default value is and it has been set to max degree of parallelism this sp configure option has been changed its default value is and it has been set to max server memory mb this sp configure option has been changed its default value is and it has been set to optimize for ad hoc workloads this sp configure option has been changed its default value is and it has been set to show advanced options this sp configure option has been changed its default value is and it has been set to xp cmdshell this sp configure option has been changed its default value is and it has been set to priority reliability extended stored procedures in master master the sqbdata extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbdir extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbmemory extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbstatus extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbtest extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbtestcancel extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbteststatus extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqbutility extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning master the sqlbackup extended stored procedure is in the master database clr may be in use and the master database now needs to be part of your backup recovery planning priority non default database config read committed snapshot isolation enabled this database setting is not the default redgate redgatemonitor snapshot isolation enabled this database setting is not the default redgate redgatemonitor priority wait stats sos scheduler yield hours of waits minutes average wait time per hour signal wait waiting tasks ms average wait time priority informational sql server is running under an nt service account im running as nt service mssqlserver wish had an active directory service account instead priority server info default trace contents the default trace holds hours of data between apr 21pm and apr 13am the default trace files are located in program files microsoft sql server mssql13 mssqlserver mssql log drive space 00mb free on drive drive space 00mb free on drive drive space 00mb free on drive drive space 00mb free on drive hardware logical processors physical memory 144gb hardware numa config node state online online schedulers offline schedulers processor group memory node memory vas reserved gb node state offline online schedulers offline schedulers processor group memory node memory vas reserved gb instant file initialization enabled the service account has the perform volume maintenance tasks permission power plan your server has 60ghz cpus and is in balanced power mode uh you want your cpus to run at full speed right server last restart mar 27am server name redacted services service sql server mssqlserver runs under service account nt service mssqlserver last startup time mar 27am startup type automatic currently running service sql server agent mssqlserver runs under service account localsystem last startup time not shown startup type automatic currently running sql server last restart mar 27am sql server service version patch level sp1 cumulative update cu7 edition standard edition bit availability groups enabled availability groups manager status virtual server type hypervisor windows version youre running pretty modern version of windows server 2012r2 era version priority rundate captains log stardate something and something
204302 alter table xxx alter column yyy varchar max null suppose there are gb data space and 2gb index space about million rows in this table column yyy is varchar8000 now and can be updated is writable the table has about other columns there are about 3000g data from other database tables on this machine some other information of rows have null in this varchar8000 the web application may hit this table times per minute the hardware is enterprise level core cpu and 256gb ram there are other tables and database on this machine about 3000gb data relevant version details microsoft sql server x64 enterprise edition bit
204339 am working on sql server r2 have table benefit which has after insert update trigger named tiu benefit want to write an update statement for this table to update row but dont want its trigger to fire know can disable trigger before update and then enable the trigger after update disable trigger tiu benefit on benefit go update benefit set editor srh where benefit id go enable trigger tiu benefit on benefit go but this disable and enabling trigger will affect all users logged in currently so there is possibility that another user run an update insert while the trigger is disabled by my script which is not good thats why only want to disable and enabling trigger for my current session is it possible if yes please tell how thanks
204532 have an aggregate query that has lot of columns and generate huge data set select column1 column2 column20 sumcolumn21 sumcolumn40 into output from ledger group by column1 column20 the input table ledger has 18m rows and the result table output has 600k rows the query took minutes is there anything can do to make it faster heres the execution plan theres no index on the ledger table and the exclamation point in sort gives the following warning operator used tempdb to spill data during execution with spill level and spilled threads sort wrote pages to and read pages from tempdb with granted memory 3752160kb and used memory 3681824kb
204545 am working with the programmers on database solution they want to add computed column to mimic the old keys for the older queries procedures and systems and index it the new keys will be guids to do this they want to create function for the computed column that creates value and persist it it will not let them persist the column dont have any warm fuzzies about the idea and also can not find any info on the web about the technique is it technique am thinking they need to add trigger instead does anyone have any ideas the function will be run as this select int identity field from table where guid column guidkey it returns an int identity field based on the guid this will be run on ever insert into related table so if table one holds the primary key the related table two will update using the guid passed in to get the key from the table one and insert it into table two
204565 salutations check one well trained professional casual reader hapless wanderer have check all that apply query stored procedure database thing maybe that was running fine if applicable yesterday in recent memory at some point but is suddenly slower now ive already checked to make sure its not being blocked and that its not the victim of some long running maintenance task report or other out of band process what is the problem what should do and what information can provide to get some help insert appropriate closing remarks
204597 consider below table tbl id date how can get only rows with same id but different date right now can only get ids with more than count like so select id date from tbl group by id having countid how can only get id and along with date
204757 have this scenario it looks like mysql is taking the largest decimal value and tries to cast the other values to that the problem is that this query is generated by an external library so dont have control over this code at this level at least do you have some idea how to fix this select as union select null union select why from to null expected result or doesnt matter really in my case null adding more context im using entity framework with an extension library http entityframework extensions net to save changes in batches specifically the method context bulksavechanges this library creates queries using select union
204920 consider the following simple xml xml customer name max email address me you com customer customer name erik email address erik your mom com customer customer name brent email address brentcom customer xml want to get list of customer sequences where the address attribute of the email item does not contain an so want output that looks like customer name brent email address brentcom customer mcve declare xml xml customer name max email address me you com customer customer name erik email address erik your mom com customer customer name brent email address brentcom customer xml this query select withvalidemail query xml customer email contains address withinvalidemail query xml customer email contains address false returns withvalidemail withinvalidemail email address me you com email address erik your mom com false this query select withinvalidemail query xml customer email where exist xml customer email contains address returns withinvalidemail no results the where clause in the query above is eliminating the entire set of xml because at least single sequence exists where the email address contains an sign
205012 how to count how may true and false for the field public in postgresql user table have tried this query select sumcase when false then else end as false sumcase when true then else end as true from public user but am not getting any value and if remove public from query then will get correct counts only have value true table name dob public values bb true op true and false but am getting the same answer when make public as false table name dob public values bb false op true and false so someone please help me to solve this
205026 im looking for good resource that gives breakdown of the feature differences between sql server enterprise and standard editions want to make sure theres no gotchas or miss anything truly beneficial by going with standard ive read over the licensing guide from microsoft but is there any other notable feature differences it doesnt cover what are the vaguely described feature differences like advanced adaptive query processing enterprise data management and advanced security etc
205065 it is knocking my socks off mysql db weight is defined as double insert into xxx weight values why does the db store
205073 ive got bunch of simple ssis packages that output sql data to excel then rename the files and move them out to the end user some of them use file paths with spaces in them and they arent excited about using new folders heres the code im using declare date varchar25 sql varchar1000 set date castdatepartmonth current timestamp as varchar2 castdatepartday current timestamp as varchar2 castdatepartyear current timestamp as varchar4 print date set sql copy data ed xlsx ehsintra3 ed weekly ed date xlsx print sql exec xp cmdshell sql but it wouldnt work without the underscores question how can make this work without underscores
205190 read through microsofts editions and supported features of sql server doc to compare the feature differences between enterprise and standard edition two things not available in standard that caught my eye were online indexing online schema change does this literally mean you cant create and modify indexes or tables and other objects without taking the database offline first or does microsoft mean there are only specific operations you cant do and if so where can learn more about which operations require taking the database offline
205341 am trying to tune query where the same table valued function tvf is called on columns the first thing did was convert the scalar function into an inline table valued function is using cross apply the best performing way to execute the same function on multiple columns in query simplistic example select col1 val col2 val col3 val do the same for other columns col21 col22 col23 from cross apply dbo function1col1 cross apply dbo function1col2 cross apply dbo function1col3 do the same for other columns are there better alternatives the same function can be called in multiple queries against number of columns heres the function create function dbo convertamountverified tvf amt varchar60 returns table with schemabinding as return with ctelastchar as select lastchar rightrtrim amt select amountverified castret as numeric182 from select outer apply select cast case when charindexl lastchar collate latin1 general cs as abcdefghi then charindexl lastchar collate latin1 general cs as abcdefghi when charindexl lastchar collate latin1 general cs as jklmnopqr then charindexl lastchar collate latin1 general cs as jklmnopqr when charindexl lastchar collate latin1 general cs as pqrstuvwxy then charindexl lastchar collate latin1 general cs as pqrstuvwxy else null end as varchar1 from ctelastchar num outer apply select case when charindexl lastchar collate latin1 general cs as abcdefghi then when charindexl lastchar collate latin1 general cs as jklmnopqrpqrstuvwxy then else end from ctelastchar neg outer apply select amt case when num is null then amt else substringrtrim amt1 len amt num end tp outer apply select case when neg then casttp amt as numeric when neg then cast tp amt as numeric end ret go heres the scalar function version that inherited if anyone is interested create function dbo convertamountverified amt varchar50 returns numeric as begin declare the return variable here declare amount numeric18 declare tempamount varchar declare num varchar1 declare lastchar varchar1 declare negative bit get last character select lastchar rightrtrim amt select num case lastchar collate latin1 general cs as when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then ascii when then when then when then when then when then when then when then when then when then when then else end select negative case lastchar collate latin1 general cs as when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then when then ascii when then when then when then when then when then when then when then when then when then when then else end add the sql statements to compute the return value here if num begin select tempamount amt end else begin select tempamount substringrtrim amt1 len amt num end select amount case negative when then cast tempamount as numeric when then cast tempamount as numeric end return the result of the function return amount end sample test data select dbo convertamountverified00064170 select from dbo convertamountverified tvf00064170 select dbo convertamountverified00057600a select from dbo convertamountverified tvf00057600a select dbo convertamountverified00059224y select from dbo convertamountverified tvf00059224y
205342 if my result is null or empty would like it not to add comma in front of the string how can achieve that update companies set address4 concataddress4 eircode from companies inner join temptbl1 on comp id comp id or address4 update just to make it clear when make this update v05 ttx1 waterfordx01 b234 b90 e902 co wexfordtd2 pve2 it adds comma regardless if it was empty or not what want to get back is this v05 ttx1 waterfordx01 b234 b90 e902 co wexfordtd2 pve2 if there is no value then just add the string without the comma in our case v05 ttx1 without at the start of the string create table dbo companies comp id int not null address4 varchar not null insert into dbo companies comp id address4
205587 question how to improve this query to increase the speed of the query shown below from seconds to milliseconds am using postgresql context have time series table buildings hispoint that stores historical data for data points in table buildings point need to aggregate the maxvalue minvalue as aggregation value for different time ranges year to date for large collection of data points this proves to be very slow and needs improvement table structure of buildings hispoint rows create table buildings hispoint id int value float datetime timestamp point id constraint foo foreign key point id references buildings point point id query select countdatetime as id maxvalue minvalue as aggregation value from buildings hispoint where point id and buildings hispoint datetime between and query plan aggregate cost rows width actual time rows loops buffers shared hit read dirtied bitmap heap scan on buildings hispoint cost rows width actual time rows loops recheck cond point id rows removed by index recheck filter datetime timestamp with time zone and datetime timestamp with time zone rows removed by filter heap blocks exact lossy buffers shared hit read dirtied bitmap index scan on buildings measurementdatapoint ffb10c68 cost rows width actual time rows loops index cond point id buffers shared read planning time ms execution time ms
205684 have table my tables which gets referenced as foreign key in several tables would like to select all rows of my table which are not referenced in an other table afaik it should be possible to do this in generic way with some introspection magic
205727 recently our school district has upgraded several servers from version to and from physical servers to vm servers our infrastructure team presented the vm servers for sql server installation with single core am attempting to explain to the head of that department that sql server hates being on single core and he wants to see proof none of the data that have presented has satisfied his need for proof and so he has asked for some articles on why vm sql server wont operate efficiently on single core what articles to you use for this any help for me making my case would be appreciated
205790 im developing sql server stored procedure and want to get the last characters of varchar38 column know there will always be at least characters and dont know the exact length of the column because it is variable think can get the length of the column and do subtract to use substring but cant do that because im doing this set externalcodes select serial aggregationlevel from externalcode where productionorderid productionorderid for json path im generating json and dont know how to get the length of each serial column inside select my question is how can get substring from string without the first characters without knowing its length one solution could be substringserial and it always return the substring from to the end of the string even if the string doesnt have length of
206055 this question is the postgresql version of question about mysql here originally it was one question for both rdbmss but it was suggested to me that given the different capabilities of the two systems should split the question in particular think that ctes with clause should make the query far more elegant and readable suppose have list of tumours this data is simulated from real data create table illness nature of illness varchar25 created at datetime insert into illness values cervix insert into illness values cervix insert into illness values cervix insert into illness values cervix insert into illness values cervix insert into illness values lung insert into illness values lung insert into illness values lung insert into illness values lung insert into illness values cervix with cervix and lung each for the month of jan tie insert into illness values cervix insert into illness values lung insert into illness values lung insert into illness values lung insert into illness values lung insert into illness values cervix you want to find out which particular tumour was most common in given month so far so good now you will notice that for month of there is tie so it makes no sense whatsoever to randomly pick one and give that as the answer so ties have to be included this makes the problem much more challenging have solution but its quite complex id like to know if my solution is optimal or not the postgresql fiddle is here the query in the fiddle is very cumbersome ill have look at using ctes my first answer which works with both postgresql and mysql is included in the fiddle but wont post it here as believe that it will be superceded by postgresqls superior capabilities and it would be just copy of my answer to the mysql question
206170 have the following foo table id name qty john john mary mary gary gary gary would like to select only id and name of minimal qty grouped by name with this result id name john mary gary the only way could do this was by means of the following operation select id name from select id name minqty from foo group by name as is there more beautiful or less redundant way to do that
206241 have developer that would like when doing select statement with no order by the rows in table to be in the order they were inserted the developer suggested changing from clustered to non clustered index by changing the index from clustered to non clustered does this make any guarantees about the order in which rows would appear in the table this question is mostly for my curiosity am going to suggest using an identity column instead but this request got me thinking timestamp could be used but there is chance rows can be inserted simultaneously thanks in advance for your help
206426 im trying to write query do delete rows based on two columns delete from us test where cell in ca001018611 ca001135126 and date in however this matches when cell ca001018611 is either or how can modify this to delete ca001018611 only when date is and ca001135126 only when date is edit have about 300k rows to delete based on similar criteria
206481 am writing custom json parser in sql for the purpose of my parser am using the patindex function that calculates the position of token from list of tokens the tokens in my case are all single characters and they include these usually when need to find the first position of any of several given characters use the patindex function like this patindex abc sourcestring the function will then give me the first position of or or whichever happens to be found first in sourcestring now the problem in my case seems to be connected with the character as soon as specify it in the character list like this patindex sourcestring my intended pattern apparently becomes broken because the function never finds match it looks like need way to escape the first so that patindex treats it as one of the lookup characters rather than special symbol have found this question asking about similar problem need help with like operator and square brackets however in that case the simply does not need to be specified in brackets because it is just one character and it can be specified without brackets around them the alternative solution which does use escaping works only for like and not for patindex because it uses an escape subclause supported by the former and not by the latter so my question is is there any way to look for with patindex using the wildcard or is there way to emulate that functionality using other transact sql tools additional information here is an example of query where need to use patindex with the pattern as above the pattern here works albeit somewhat because it does not include the character need it to work with as well with data as select cast f1 v1 v2 f2 v3 as varcharmax as responsejson parser as select level openclose substringd responsejson nullifp substringd responsejson nullifp responsejson substringd responsejson nullifp from data as cross apply select patindex responsejson as union all select level isnulld openclose level isnulloc openclose openclose oc openclose substringd responsejson nullifp responsejson substringd responsejson nullifp from parser as cross apply select patindex collate latin1 general bin2 responsejson as cross apply select substringd responsejson nullifp as cross apply select case when in then when in then end as oc openclose where and select from parser option maxrecursion the output get is level openclose responsejson f1 v1 v2 f2 v3 null f1 v1 v2 f2 v3 v1 v2 f2 v3 null v1 v2 f2 v3 null v2 f2 v3 null f2 v3 v3 you can see that the is included as part of in one of the rows the level column indicates the level of nesting meaning bracket and braces nesting as you can see once the level becomes it never returns to it would have if could make patindex recognise as token the expected output for the above example is level openclose responsejson f1 v1 v2 f2 v3 null f1 v1 v2 f2 v3 v1 v2 f2 v3 null v1 v2 f2 v3 v2 f2 v3 null f2 v3 null f2 v3 v3 you can play with this query at db fiddle we are using sql server and are unlikely to soon upgrade to version that supports json parsing natively could write an application to do the job but the results of the parsing need to be processed further which implies more work in the application than just the parsing the kind of work that would be much easier and probably more efficiently done with sql script if only could apply it directly to the results its very unlikely that can use sqlclr as solution for this problem however dont mind if someone decides to post sqlclr solution since that could be useful for others
206678 am trying to get disk space report from bunch of servers and insert them into sql table below is sample of what am trying to doserver names will be populated from table and passed as comma seperated list even passing one server also has the same result set ps powershell exe noexit computers get wmiobject class win32 volume servernames foreach computer in computers pscomputername computer pscomputername name computer name capacity computer capacity freespace computer freespace label computer label insertquery insert into dbo temp disksdata servername diskname capacitygb freespacegb label values pscomputername name capacity freespace label go invoke sqlcmd query insertquery serverinstance someserver database dbname now try to pass the variable to xp cmdshell like below execute xp cmdshell ps when invoked in ssmsabove returns nullbut works in powershell any ideas why below are few things tried same accountadmin is used in both shell and ssms tried multiple things like import modules xp cmdshell worksbut this returns null only for this query have tried to add nowaitbut that doesnt help as well have been trying to get this done from more than daybut this doesnt work have to use xp cmdshell because writing app is not allowed bat file doesnt help because server names are passed as comma seperated list am modifying the code and was told not to rewrite when asked please let me know if you need any further info repro below is entire command from printif you remove powershell exe and below will run in powershell powershell exe computers get wmiobject class win32 volume servername foreach computer in computers pscomputername computer pscomputername name computer name capacity computer capacity freespace computer freespace label computer label insertquery insert into dbo temp disksdata servername diskname capacitygb freespacegb label values pscomputername name capacity freespace label invoke sqlcmd query insertquery serverinstance servername database dbname
206815 have sql query that am trying to optimize declare id uniqueidentifier cec094e5 b312 4b13 997a c91a8c662962 select id minsometimestamp maxsomeint from dbo mytable where id id and somebit group by id mytable has two indexes create nonclustered index ix mytable sometimestamp includes on dbo mytable sometimestamp asc includeid someint create nonclustered index ix mytable id somebit includes on dbo mytable id somebit include totallyunrelatedtimestamp when execute the query exactly as written above sql server scans the first index resulting in logical reads and second duration when inline the id variable and execute the query again sql server seeks the second index resulting in only logical reads and second duration basically instant need the variable but want sql to use the good plan as temporary solution put an index hint on the query and the query is basically instant however try to stay away from index hints when possible usually assume that if the query optimizer is unable to do its job then there is something can do or stop doing to help it without explicitly telling it what to do so why does sql server come up with better plan when inline the variable
206832 wrote case statement with choices where am using the same statement in places in simple query the same query twice with union between them but also is doing count and therefore the group by also contains the case statement this is to relabel some company names where different records for the same company are spelled differently tried to declare variable as varcharmax declare caseforaccountconsolidation varcharmax set caseforaccountconsolidation case when ac accountname like air new then air new zealand when ac accountname like air bp then air bp when ac accountname like addiction advice then addiction advice when ac accountname like aia then aia when went to use it in my select statement the query just returned the case statement as text and didnt evaluate it also was unable to use it in the group by got this error message each group by expression must contain at least one column that is not an outer reference ideally would like to have the case in just single place so that there is no chance of me updating one line and not replicating that elsewhere is there some way of doing this am open to other ways like maybe function but am not sure how to use them like this here is sample of the select am currently using select sumc charge amount as gstexcl dl firstdateofmonth as monthbilled dl firstdateofweek as weekbilled case when ac accountname like air new then air new zealand when ac accountname like air bp then air bp when ac accountname like addiction advice then addiction advice when ac accountname like aia then aia else ac accountname end as accountname dl financialyear convertdatec date charged as date charged from accession left join account code ac on account code id ac account code id left join charge on accession id accession id left join datelookup dl on convertdatec date charged dl date where datecreated convertdatenow group by dl firstdateofmonth dl financialyear dl firstdateofweek convertdatec date charged case when ac accountname like air new then air new zealand when ac accountname like air bp then air bp when ac accountname like addiction advice then addiction advice when ac accountname like aia then aia else ac accountname end union select sumc charge amount as gstexcl dl firstdateofmonth as monthbilled dl firstdateofweek as weekbilled case when ac accountname like air new then air new zealand when ac accountname like air bp then air bp when ac accountname like addiction advice then addiction advice when ac accountname like aia then aia else ac accountname end as accountname dl financialyear convertdatec date charged as date charged from accession left join account code ac on account code id ac account code id left join charge on accession id accession id left join datelookup dl on convertdatec date charged dl date where datecreated dateaddyear 1convertdatenow group by dl firstdateofmonth dl financialyear dl firstdateofweek convertdatec date charged case when ac accountname like air new then air new zealand when ac accountname like air bp then air bp when ac accountname like addiction advice then addiction advice when ac accountname like aia then aia else ac accountname end the purpose for this union is to return all data for timeperiod and also to return data for the same timeperiod for months previously edit added missing catch all edit2 added second of the union statement edit3 corrected the group by to include some other necessary elements
206934 have two tables in mysql database posts and reasons each post row has and belongs to many reason rows each reason has weight associated with it and each post therefore has total aggregated weight associated with it for each increment of points of weight for etc want to get count of posts that have total weight less than or equal to that increment id expect the results for that to look something like this weight post count the total weights are approximately normally distributed with few very low values and few very high values maximum is currently but the majority in the middle there are just under rows in posts and around in reasons each post has on average or reasons the relevant parts of the tables look like this create table posts id bigint primary key create table reasons id bigint primary key weight int11 not null create table posts reasons post id bigint not null reason id bigint not null constraint fk posts reasons posts post id references postsid constraint fk posts reasons reasons reason id references reasonsid so far ive tried dropping the post id and total weight into view then joining that view to itself to get an aggregated count create view post weights as select posts id sumreasons weight as reason weight from posts inner join posts reasons on posts id posts reasons post id inner join reasons on posts reasons reason id reasons id group by posts id select floorp1 reason weight as weight countdistinct p2 id as cumulative from post weights as p1 inner join post weights as p2 on floorp2 reason weight floorp1 reason weight group by floorp1 reason weight order by floorp1 reason weight asc that is however unusably slow let it run for minutes without terminating which cant do in production is there more efficient way to do this in case youre interested in testing the entire dataset its downloadable here the file is around 60mb it expands to around 250mb alternately there are rows in github gist here
206975 will you please help me understand pros and cons of using ola solution over maintenance plan have prepared presentation based on sql pass http www pass org downloadfile aspxfile ebae1b31 which will present am also preparing few scenarios which ola solution addresses and maintenance plan solution doesn can you all please help me explain this more technically by the way we are managing almost servers mix of with ola solution on at least of them liked this article by brent ozar but in one the comments brent has recommended to use script based solution for the number of servers we have https www brentozar com archive maintenance plans roombas suck good way
207262 im using sqlserver and trying to create storedprocedure inside try catch block like this begin try create procedure ammeghezi1 int as select end try begin catch end catch it fails to run with the error of incorrect syntax near select expecting external then changed sp like create procedure ammeghezi1 int as begin select end cover sp with begin end block but the error did not change also add go after begin try statement and it just got worst im getting to conclude that creating sp inside try catch block is not practical is this even possible to create sp inside try catch block and how
207371 in this code am converting the subjectscolumns english hindi sanskrith into rows declare colsunpivot as nvarcharmax query as nvarcharmax select colsunpivot stuff select quotename name from sys columns where object id object iddbo result2 for xml path type value nvarcharmax print colsunpivot select query select name subjectmarks from result2 unpivot marks for subject in colsunpivot as tab exec sp executesql query please explain what does for xml path type value nvarcharmax do here
207456 create table script create table temp id int identity11 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 decimal62 insert script insert into temp abcdefghijklmnopqrstu values insert into temp abcdefghijklmnopqrstu values expected result median first row second row non working solutions tried the below query which is working fine in sql server but has issues in sql server r2 select id avgval from select id val count over partition by id as row number over partition by id order by val as rn from temp unpivot val for col in abcdefghijklmnopqrstu as as where rn in group by id ran the above query in version and its working properly but its not working in r2 get this error in sql server r2 incorrect syntax near the keyword for the reason must be because my databases compatibility level is but if change the compatibility level it will affect my application so cant do that ive also tried this query select id select castavgtotavgas decimal62 as median from values convertdecimal62 aconvertdecimal62 convertdecimal62 convertdecimal62 dconvertdecimal62 convertdecimal62 fconvertdecimal62 gconvertdecimal62 hconvertdecimal62 convertdecimal62 jconvertdecimal62 kconvertdecimal62 lconvertdecimal62 convertdecimal62 nconvertdecimal62 oconvertdecimal62 pconvertdecimal62 convertdecimal62 rconvertdecimal62 sconvertdecimal62 tconvertdecimal62 as totalavgtotavg median from tempone obviously it calculates the average but need the median
207496 is there way to determine whether or not user defined type in postgresql is an enum essentially we have the following create type foo as enum sometimes you wanna go where everybody knows your name with table instantiated by create table bar lyrics foo default wanna foo im able to determine the type of foo from the column lyrics however im having trouble finding way to determine whether or not foo is an enum for context need this information to programmatically get list of all possible values of foo when given column of lyrics
207671 im struggling to understand why row estimation is so terribly wrong here is my case simple join using sql server sp2 same issue on sp1 dbcompatiblity select amount transactioncurrency id currencyshareds id from currencyshareds inner join annexes on amount transactioncurrency id currencyshareds id option querytraceon querytraceon sql estimates row whereas its and chooses to do nested loop link to plan after statistics are updated on currencyshareds then estimation is fine and merge join is chosen link to new plan as soon as just one record is added to currencyshareds then statistics become stale and sql goes back to wrong estimation wouldnt worry that much about this simple query but this is just part of larger one and this is the begining of domino why adding one row to records table causes such damage when looking into the output of cardinality estimation trace see this warning warning badly formed histogram but couldnt find anything more on this topic here is output the full output from cardinality estimation begin selectivity computation input tree logop join cstcollbasetableid card tbl annexes cstcollbasetableid card tbl currencyshareds scaop comp cmpeq scaop identifier qcol test masterdata dbo currencyshareds id scaop identifier qcol test masterdata dbo annexes amount transactioncurrency id plan for computation cselcalcexpressioncomparedtoexpression qcol test masterdata dbo annexes amount transactioncurrency id cmpeq qcol test masterdata dbo currencyshareds id loaded histogram for column qcol test masterdata dbo annexes amount transactioncurrency id from stats with id loaded histogram for column qcol test masterdata dbo currencyshareds id from stats with id warning badly formed histogram selectivity 59503e stats collection generated cstcolljoinid card jtinner cstcollbasetableid card tbl annexes cstcollbasetableid card tbl currencyshareds end selectivity computation estimating distinct count in utility function input stats collection cstcollbasetableid card tbl annexes columns to distinct on qcol test masterdata dbo annexes amount transactioncurrency id plan for computation cdvcplanleaf multi column stats single column stats guesses covering multi col stats id using ambient cardinality to combine distinct counts combined distinct count result of computation estimating distinct count in utility function input stats collection cstcollbasetableid card tbl currencyshareds columns to distinct on qcol test masterdata dbo currencyshareds id plan for computation cdvcplanuniquekey result of computation and when update the statistics on currencyshareds the part with badly formed histogram changes and cardinality is calculated correctly plan for computation cselcalcexpressioncomparedtoexpression qcol test masterdata dbo annexes amount transactioncurrency id cmpeq qcol test masterdata dbo currencyshareds id loaded histogram for column qcol test masterdata dbo annexes amount transactioncurrency id from stats with id loaded histogram for column qcol test masterdata dbo currencyshareds id from stats with id selectivity stats collection generated cstcolljoinid card jtinner cstcollbasetableid card tbl annexes cstcollbasetableid card tbl currencyshareds end selectivity computation and stats info for this currencyshareds id from stats with id with warning about histogram which looks fine to me name updated rows rows sampled steps density average key length string index filter expression unfiltered rows persisted sample percent pk currencyshareds id may 43pm no null row affected all density average length columns id row affected range hi key range rows eq rows distinct range rows avg range rows and info for the second index name updated rows rows sampled steps density average key length string index filter expression unfiltered rows persisted sample percent ix fk amount transactioncurrency may 29pm no null row affected all density average length columns amount transactioncurrency id 932801e amount transactioncurrency id id rows affected range hi key range rows eq rows distinct range rows avg range rows
207706 have table named articles with column named price when user on my web app selects both price min and price max need to perform query like this pricemin pricemax query select from articles where price between pricemin and pricemax however when the selected price min is and price max is million dont have any output with price greater than even though rows with values above do exist the table is defined like this create table articles id int11 not null auto increment name varchar100 not null price varchar100 not null source varchar255 not null primary key id engine innodb auto increment default charset utf8
207984 select json array elements one two json gives result json array elements one two would like to have the same but without the quotes one two looks like cant use here because dont have field names in the json its just an array of strings postgres version postgresql on x86 apple darwin compiled by i686 apple darwin11 llvm gcc gcc based on apple inc build llvm build bit
208170 ive got the following working select info1 info2 from tablea where id in select xid from tableb where userid and active which works fine but in tableb also have team column which id like to select and return in the outermost select statement something like this pseudo query which of course doesnt work select info1 info2 team from tablea where id in select xid team from tableb where userid and active had read around and thought maybe as could help but dont really know for sure can anybody suggest solution is this even possible
208178 in got programming task in the area of sql task people want to get inside an elevator every person has certain weight the order of the people waiting in line is determined by the column turn the elevator has max capacity of lbs return the last persons name that is able to enter the elevator before it gets too heavy return type should be table question what is the most efficient way to solve this problem if looping is correct is there any room for improvement used loop and temp tables here my solution set rowcount the source table line has the same schema as result and temp use northwind go declare sum int declare curr int set sum declare id int if object idtempdb tempu is not null drop table temp if object idtempdb resultu is not null drop table result create table result id int not null name varchar255 not null weight int not null turn int not null create table temp id int not null name varchar255 not null weight int not null turn int not null insert into temp select from line order by turn while exists select from temp begin get the top record select top curr weight from temp order by turn select top id id from temp order by turn print curr print sum if sum curr begin print entering again print curr set sum sum curr print sum insert into result select from temp where id id id name turn delete from temp where id id end else begin print breaaaking break end end select top name from result order by turn desc here the create script for the table used northwind for testing use northwind go object table dbo line script date set ansi nulls on go set quoted identifier on go create table dbo line id int not null name varchar not null weight int not null turn int not null primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary unique nonclustered turn asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go alter table dbo line with check add check weight go insert into dbo line id name weight turn values gary jo thomas will mark james
208372 when im tracking waits with sp blitzfirst get this detail clicktoseedetails for seconds over the last seconds sql server was waiting on this particular bottleneck should that read for times over the last seconds finding was clr semaphore
208487 backstory im trying to restore copies of several sql databases to different environment but the application needs them to be roughly synchronized in time of course the ideal way to do this is by using full recovery mode using full and log backups and then do point in time restores using the same timestamp but in my case dont need them perfectly synchronized just within minutes or so and id rather deal with only full backups and restores do have control over when the full backups run but the problem is that the databases are of very different sizes so the full backups run for very different lengths of time so my question should synchronize the start time of the full backups or the finish time in other words when restore full backup will end up with how it looked when the backup started or when it finished
208520 given the band table with json column holding an array id people john thomas john james james george how to list the number of bands each name is part of desired output name count john james thomas george
208731 ive added solution without using window functions and benchmark with large data set below martins answer this is followup thread to group by using columns not in the select list when is this practical elegant or powerful in my solution to this challenge use query that groups by an expression that is not part of the select list this is frequently used with window functions when the logical grouping element involves data from other rows perhaps this is an overkill as an example but thought you may find the challenge interesting in its own right ill wait with posting my solution maybe some of you can come up with better ones challenge we have table of sensors that periodically logs reading values there is no guarantee on sample times being in monotonous intervals you need to write query that will report on the exceptions meaning the times that the sensors reported out of threshold reading either low or high each period of time the sensor was reporting over or under the threshold values is considered an exception once the reading got back to normal the exception ends sample tables and data the script is in sql and is part of my training materials here is link to the sqlfiddle sensor thresholds setup example create table sensors sensor nvarchar10 not null lower threshold decimal72 not null upper threshold decimal72 not null constraint pk sensors primary key clustered sensor constraint ck value range check upper threshold lower threshold go insert into sensors sensor lower threshold upper threshold values nsensor nsensor nsensor go create table measurements sensor nvarchar10 not null measure time datetime20 not null measurement decimal72 not null constraint pk measurements primary key clustered sensor measure time constraint fk measurements sensors foreign key sensor references sensors sensor go insert into measurements sensor measure time measurement values nsensor n20160101 nsensor n20160101 nsensor n20160101 nsensor n20160101 nsensor n20160102 nsensor n20160102 nsensor n20160101 note that this exception range has both over and under nsensor n20160101 nsensor n20160101 nsensor n20160102 nsensor n20160102 nsensor n20160101 nsensor n20160101 nsensor n20160101 nsensor n20160101 nsensor n20160101 go expected result sensor exception start time exception end time exception duration minutes min measurement max measurement lower threshold upper threshold maximal delta from thresholds sensor sensor sensor sensor
209084 we have encountered an interesting issue with sql server consider the following repro example create table test guid uniqueidentifier primary key insert into test guid values 7e28eff8 a80a 45e4 bfe0 c13989d69618 select guid from test where guid 7e28eff8 a80a 45e4 bfe0 c13989d69618 and guid newid drop table test fiddle please forget for moment that the guid newid condition seems entirely useless this is just minimal repro example since the probability of newid matching some given constant value is extremely small it should evaluate to true every time but it doesnt running this query usually returns row but sometimes quite frequently more than time out of returns rows have reproduced it with sql server on my system and you can reproduce it on line with the fiddle linked above sql server looking at the execution plan reveals that the query analyzer apparently splits the condition into guid newid or guid newid which completely explains why it fails sometimes if the first generated id is smaller and the second one larger than the given id is sql server allowed to evaluate as or even if one of the expressions is non deterministic if yes where is it documented or did we find bug interestingly and not guid newid yields the same execution plan and the same random result we found this issue when developer wanted to optionally exclude particular row and used guid isnull someparameter newid as shortcut for someparameter is null or guid someparameter am looking for documentation and or confirmation of bug the code is not all that relevant so workarounds are not required
209384 is there way any functions dmvs in sql server that will provide who executed stored procedure of particular database
209546 my vendor requires the data warehouse database to be case sensitive but need to do case insensitive queries against it in case sensitive database how would you write this to be case insensitive where name like hospitalist
209583 am working on reporting system that will require large select queries but is based on database that is only filled once the database management system is microsoft sql server there is probably better way to design system like this but lets approach this theoretically theoretically speaking if we have very large database 150m rows on several tables and we can assume the database will only be populated once could indexing every possible column combination have negative performance impact on select query
209695 do unused ctes in queries affect performance and or alter the generated query plan
209989 have the following table structure accountid property value status active city los angeles registrationdate status active city las vegas registrationdate status inactive city toronto registrationdate want to be able to select all rows where property status and value active and property registrationdate and value im figuring need to do some kind of group by and then select on that aggregation but cant get my head around itmy sql is really rusty want the following output for the above query accountid status registrationdate active active if all the columns had existed on the same row would just write something like this select accountid from property where status active and registrationdate
210066 does the order of operation that you place your ltrim and rtrim matter when used in conjunction with isnull for instance take the following example where user potentially enters bunch of spaces in field but we trim their input to be an actual null value to avoid storing empty strings am performing the trim operations outside of isnull declare test1 varchar16 if ltrimrtrimisnull test1 begin set test1 null end select test1 this appropriately returns true null value now lets place isnull on the outside declare test2 varchar16 if isnullltrimrtrim test2 begin set test2 null end select test2 this also returns null value both work well for the intended usage but im curious if there is any difference to how the sql query optimizer handles this
210249 notwithstanding that blindly creating suggested missing indices is less than ideal is there way to copy and paste the suggested missing indices from the execution plan tab in sql server management studio you get tool tip on mouse hovering over it but there does not appear to be right click menu copy them for years ive just typed out any needed but am curious if the base form can be easily gotten from the tab using ssms
210865 am trying to set up sql server query that lists the count of people signed up for tours during conference event and then displays the total sum this is my sql code that is coming close but it is ignoring the where clause for the grand total but not the individual totals select sitetour countsitetour as count from tblconference where registrationtype not like cancellation group by sitetour union all select total as sitetour countsitetour as expr1 from tblconference as tblconference here is what it is producing tour1 tour2 tour3 total tour1 count of is correct because there is one registrationtype cancellation however the total of should be also am getting an initial row guess based on counting nulls that shows is there some way prevent this please help me correct this thank you
211352 need to migrate an on premises sql server database to an azure sql database and im facing some challenges since theres quite bit of limitations to go through in particular since an azure sql database works only in utc time no time zones and we need the local time we have to change the use of getdate everywhere in the database which has proven to be more work than anticipated created user defined function to get the local time that works correctly for my time zone create function dbo getlocaldate returns datetime as begin declare datetimeoffset set convertdatetimeoffset sysdatetimeoffset at time zone pacific sa standard time returnconvertdatetime end the issue im having trouble with is to actually change getdate with this function in every view stored procedure computed columns default values other constraints etc what would be the best way to implement this change we are in the public preview of managed instances it still has the same issue with getdate so it doesnt help with this problem moving to azure is requirement this database is used and will be used always in this time zone
211700 recently have rebuild indexes uisng ola hallengreen script which were fragmented after rebuilt have noticed the physical reads have reduced lot does this has anything to do with index rebuilt
212003 am wanting to compare the distinct sn from hmi temp to sn in my table hmi if the sn exists want to update the values got this syntax check if the sn does not exist want to insert the data from hmi temp into hmi have tried this syntax but get errors of invalid column name cb invalid column name lp invalid column name cn invalid column name stn and this is my syntax how should re write this to accomplish my desired result merge into hmi as target using select distinct sn from hmi temp as source on target sn source sn when not matched then insert lp cb cn sn stn values source lp source cb source cn source sn source stn ddl create table dbo hmi temp sid int identity11 not null stn float null sn nvarchar null cn nvarchar max null lp nvarchar max null cb nvarchar max null primary key clustered sid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary textimage on primary go set identity insert dbo hmi temp on go insert dbo hmi temp sid stn sn cn lp cb values ntest ntest ntest123 ntest456 values nsecondtest nsecondtest nsecondtest123 nsecondtest456 go this is the update statement was using that worked but now throwing wrench in it for the insert if not exists update set lp lp cb cb cn cn from hmi inner join hmi temp on stn stn and sn sn and cn cn
212435 we have pcs running sql server sp4 and sp1 which regularly lose power obviously this sometimes leads to index corruption of the sql server database which we need to restore afterwards am aware that sql server is not designed for such scenarios and the correct solution is to fix the cause of the power loss more on that below if you are curious nevertheless are there any tuning options in sql server that can set to reduce the risk of database corruption on power loss background the pc is windows tablet mounted on forklift when the user turns off the forklift the tablet loses power we have tried to teach the users to properly shut down windows before turning off the forklift but failed probably because just turning it off works most of the time we are also currently investigating other options such as adding ups which signals the tablet to shut down on power loss
212533 im not dba ive only googled what msdb does its basically db of sql agent of its job and history now im running out of space in my cloud server and have year worth of msdb year is it okay to delete this or do keep it for backup purposes my msdb is 93gb in 250gb hdd
212707 im using mysql have three tables and in every table have time column with single query want to count the number of rows that are between two given dates have no need with connection between the tables although they are related so join is not necessary is it possible failed attempt select counta time countb time countc time from tbl1 as tbl2 as tbl3 as where time between and and time between and and time between and
212742 first trigger alter trigger dbo price modified on dbo stock recieve after update as set nocount on if update stock out begin update set value new item price new stock out from stock recieve as join inserted as new on new bill no bill no join deleted as old on old bill no bill no end second trigger alter trigger dbo newval on dbo stock recieve after update as set nocount on if update stock out begin update set uservalue new item price new stock out from stock recieve as join inserted as new on new bill no bill no join deleted as old on old bill no bill no end result to be obtained on first stock out update item price stock in stock out value uservalue result to be obtained on second stock out update item price stock in stock out value uservalue the first trigger does what its suppose to do but want the second trigger on uservalue to not add but instead replace the result hope this is clear ypercube
212770 have cursor that generates one record of json text from group of tables the cursor has been making ssms crash the script runs for time then ssms fails below is the code that have written that is causing the crash declare row id int here we create variable that will contain the id of each row declare json cursor cursor here we prepare the cursor and give the select statement to iterate through for select our select statement here you can do whatever work you wish row number over order by name 1name 2field 1field as rowid from select field field name name from select field 1field 2namevalue from table where name in name 1name as src pivot maxvalue for name in field field as pvt as t0 left join table as on t0 name name open json cursor this charges the results to memory fetch next from json cursor into row id we fetch the first result while fetch status if the fetch went well then we go for it begin select from select our select statement here you can do whatever work you wish field field field field t0 name row number over order by field 1field 2field 1field as rowid from select field field name name from select field 1field 2namevalue from table where name in name 1name as src pivot maxvalue for name in field field as pvt as t0 left join table as on t0 name name as t1 where rowid row id in regards to our latest fetched id order by field 1field 2field 1field for json path rootfield fetch next from json cursor into row id once the work is done we fetch the next result end we arrive here when fetch status shows there are no more results to treat close json cursor deallocate json cursor close and deallocate remove the data from memory and clean up the process from windows log the following net runtime error happened first error pm net runtime none application ssms exe framework version v4 description the process was terminated due to an unhandled exception exception info system componentmodel win32exception at system windows forms nativewindow createhandlesystem windows forms createparams at system windows forms control createhandle at system windows forms textboxbase createhandle at system windows forms control createcontrolboolean at system windows forms control createcontrolboolean at system windows forms control createcontrol at system windows forms control wmshowwindowsystem windows forms message byref at system windows forms control wndprocsystem windows forms message byref at system windows forms scrollablecontrol wndprocsystem windows forms message byref at system windows forms containercontrol wndprocsystem windows forms message byref at system windows forms updownbase wndprocsystem windows forms message byref at system windows forms control controlnativewindow onmessagesystem windows forms message byref at system windows forms control controlnativewindow wndprocsystem windows forms message byref at system windows forms nativewindow debuggablecallbackintptr int32 intptr intptr second application error error pm application error faulting application name ssms exe version time stamp 0x5b304116 faulting module name kernelbase dll version time stamp 0x5abda7d6 exception code 0xe0434352 fault offset 0x000daa12 faulting process id 0x3f6c faulting application start time 0x01d4205466d2b650 faulting application path program files x86 microsoft sql server tools binn managementstudio ssms exe faulting module path windows system32 kernelbase dll report id 03b3b0c6 a71f f5b4fc0a3029 faulting package full name faulting package relative application id the purpose of this script is to output single entry in json that will be posted to data service in the cloud also in ssms am sending the results to the grid here is the full query for the stored procedure that am creating object storedprocedure dbo sp acq zerion post http script date am set ansi nulls on go set quoted identifier on go fill in with db alter procedure dbo sp acq zerion post http id varchar50 as define variables declare hr int declare object as int declare responsetext as varchar8000 declare src varchar255 desc varchar255 status int msg varchar255 cursor pt declare row id int here we create variable that will contain the id of each row declare json cursor cursor here we prepare the cursor and give the select statement to iterate through for select our select statement here you can do whatever work you wish row number over order by name 1name 2field 1field as rowid from select field field name name from select field 1field 2namevalue from table where name in name 1name as src pivot maxvalue for name in field field as pvt as t0 left join table as on t0 name name where name in truefalse open json cursor this charges the results to memory fetch next from json cursor into row id we fetch the first result while fetch status if the fetch went well then we go for it begin declare records as varchar8000 cursor pt select from select our select statement here you can do whatever work you wish field field field field t0 name row number over order by field 1field 2field 1field as rowid from select field field name name from select field 1field 2namevalue from table where name in name 1name as src pivot maxvalue for name in field field as pvt as t0 left join table as on t0 name name where name in truefalse as t1 where rowid row id in regards to our latest fetched id order by field 1field 2field 1field for json path rootfield wrap records in json object declare body as varchar8000 records create xmlhttp object and send object via http post exec hr sp oacreate msxml2 serverxmlhttp object out if hr begin raiserrorsp oacreate msxml2 serverxmlhttp failed return end exec hr sp oamethod object open null posthttps dataflownode zerionsoftware com domain solutions services webhooks 4b9b0f4b8a4b4387ec1642fdaabec7b400d5c938 7be9d5a63b5cba8ab72cd3410429e2635f68a687 false if hr begin set msg sp oamethod open failed goto eh end exec hr sp oamethod object setrequestheader null content type application json if hr begin set msg sp oamethod setrequestheader failed goto eh end exec hr sp oamethod object send null body if hr begin set msg sp oamethod send failed goto eh end if status begin set msg sp oamethod http status str status goto eh end exec hr sp oamethod object responsetext responsetext out put select responsetext cursor pt fetch next from json cursor into row id once the work is done we fetch the next result end we arrive here when fetch status shows there are no more results to treat close json cursor deallocate json cursor close and deallocate remove the data from memory and clean up the process if hr begin set msg sp oamethod read response failed goto if hr begin exec sp oageterrorinfo object return goto eh end clean up after data is sent exec hr sp oadestroy object return eh raiserror msg return if hr begin exec sp oageterrorinfo object src out desc out raiserrorerror creating com component 0x s161 hr src desc return end go
212781 have road age test table create table road age test surface year int base year int insert into road age test surface year base year values insert into road age test surface year base year values 30null insert into road age test surface year base year values null40 insert into road age test surface year base year values nullnull insert into road age test surface year base year values commit select from road age test surface year base year null null null null would like to select the greatest value from each column even if one of the values is null greatest year null whats the simplest way to do this note where both values are null want to return null not
212797 have view that grabs lot of data that want to be able to search through the view is set up with the following columns part number description information supplier manufacturer category subcategory currency price discount am looking to create stored procedure which lets me search through the view for the data am specifically looking for something like select from partsdata where part number now my first thought was to set up the procedure so that the only parameter it was looking for was varchar which would essentially contain whatever the entire where clause was this sounds like it would work however would be able to instead set up stored procedure with optional parameters am not sure if this is something you can do in sql though if not is my initial idea for implementation my best bet because it seems like there should be better way to go about doing this sometimes the where clauses can be rather long such as where description like cap and supplier like a2a systems and manufacturer like vario
213010 how do check memory usage by my sql server in production box am using sql server when ever check task managerit shows above dont think that is the real memory usage by sql server have sql performance tool grafana which shows cpu usage very less than what see in task manager checked resource monitorthere is can see average cpu value am confused as to which is the sql server memory usage am trying to determine if memory pressure is an issue to some of my problem can someone direct to good proper explanation
213199 am currently using sql server sp standard edition and want to install services in the documentation is said that stand alone installation is allowed only on enterprise machine learning server standalone also found post where the guy is talking that on standard am not able to use parallel operations and there are memory limitations could anyone tell what are the exact memory processors limitations need such details as going to use heavy calculations and the tests for now show that the implementation in the standard edition is not working as hoped
213317 incarnations are explained in an answer to another question on this site the answer mentions orphaned incarnations there are other factors that result in orphaned incarnations and obsolete backups see from the oracle docs that database incarnation includes status column which can have values of orphan current or parent which must be related what are orphaned incarnations and what steps would result in row with status orphan in database incarnation
213608 have been puzzled by this issue for almost week hopefully someone in our community has experienced the same issue and already found solution so here is my problem as per our company policy we want database mail to be able to send emails over port with tls enabled and with tls tls disabled our mail server is exchange server our sql server developer and enterprise editions boxes have os of windows server standard editions our sql server version is select version microsoft sql server sp1 cu7 gdr kb4057119 x64 dec copyright microsoft corporation developer edition bit on windows server datacenter x64 build hypervisor we have the db mail configuration as shown here the issue is whenever we turn on ssl use msdb exec dbo sysmail update account sp account id enable ssl we cannot send db mail no matter whether our smtp authentication is windows authentication basic authentication or anonymous authentication the error message in db mail log is as follows message the mail could not be sent to the recipients because of the mail server failure sending mail using account 30t10 exception message cannot send mails to mail server failure sending mail but if we turn off this ssl there is no problem for db mail sent out so how can we enable ssl and uses tls for db mail have enabled tls by adding registry as shown below the details is from this link see the faq section
213791 need to confirm if particular table ever existed in our sql server is there an existing script or method one can use to list all dropped databases in an sql server instance
213922 consider the following relation instance mytable num1 num2 null null null null query select from mytable where num1 null query select from mytable where num1 null know that null values are looked up using is null and is not null but want to know how are the above two queries evaluated what is the number of tuples outputted by q1 and q2 by the ansi standards
214757 have clustered index primary key column and im doing range query on it the problem is the scan only uses the first range part for the seek predicate leaving the other side of the range as residual predicate which causes to read all the rows up to the upper limit im using two parameters for the range declare lower numeric180 upper numeric180 select from messages where msg id between lower and upper in that case the actual execution plan shows predicate messages msg id lower seek predicate messages msg id upper rows read table definition simplified create table dbo messages msg id numeric identity01 not null col2 varchar not null constraint pk message primary key clustered msg id asc more information when used with constants instead of variables both of the predicates are seek have tried option optimize for lower without success
214863 how to list all constraints primary key check unique mutual exclusive of table in postgresql
215043 im using this code to get mix of date and time to use it like unique id datetime now tostring yyyymmddhhmmssfffff in sql server im confused as to which best smallest size of data type would be that can use to store this string what is the best smallest data type bigint timestamp or varchar19
215078 have two tables with identically named typed and indexed key columns one of the them has unique clustered index the other one has non unique the test setup setup script including some realistic statistics drop table if exists left drop table if exists right create table left char4 not null char2 not null varchar13 not null bit not null char4 not null char25 null char25 not null char25 null and few other columns create unique clustered index ix on left update statistics left with rowcount pagecount create table right char4 not null char2 not null varchar13 not null bit not null char4 not null char25 null char25 not null char25 null and few other columns create clustered index ix on right update statistics right with rowcount pagecount the repro when join these two tables on their clustering keys expect one to many merge join like so select from left as left join right as on and and and and and and and where this is the query plan want never mind the warnings they have to do with the fake statistics however if change the order of the columns around in the join like so select from left as left join right as on and used to be third and used to be first and used to be second and and and and where this happens the sort operator seems to order the streams according to the declared order of the join which adds blocking operation to my query plan things ive looked at ive tried changing the columns to not null same results the original table was created with ansi padding off but creating it with ansi padding on does not affect this plan tried an inner join instead of left join no change discovered it on sp2 enterprise created repro on developer current cu removing the where clause on the leading index column does generate the good plan but it kind of affects the results finally we get to the question is this intentional can eliminate the sort without changing the query which is vendor code so id really rather not can change the table and indexes
215172 have sql server sp1 with data that am ready to share with partner developer except for few columns example email varchar64 masked with function email null give the partner the credentials for logging in to the test copy of the database as certain user without unmask permission understand this is reasonably safe now we are considering sharing the database backup with our partners so they can reload in their own server and environment would probably need to have contained users so that partner cannot create new users or connect them to existing logins not sure about this create user user name with password strong password is there way that can share database backup and still trust that the masked columns are safe or do have to physically delete the data
215212 am writing query that returns single record from the parent table would like to also return in this query if it has any children this is one to many relationship parent parent id name child child id name parent id my first instinct is to write the following query select name select countchild id from child where parent id parent id children from parent where name like some name but was wondering if there was more efficient way to do this since dont actually care about the count just whether or not it has children any pointers
215382 am having trouble understanding what exactly to expect from the cleanuptime option in the ola hallengren server maintenance solution im finding some related questions and elaborate answers but the explanations still puzzle me bit specifically am doing weekly full backup daily diff backup and an hourly log backup the full backup is using the default cleanuptime of 24h the diff and log backup have null as cleanuptime from the documentation of the cleanuptime paramter fail to understand if setting the cleanuptime setting for backup of backuptype full will also delete older diff and log backup files or only full backup files specify the time in hours after which the backup files are deleted if no time is specified then no backup files are deleted the latter paragraph makes me think that setting cleanuptime on backups of backuptype full will also delete older transaction logs yet it is unclear if this paragraph only applies to backups of the backuptype log or also to backups of the backuptype full databasebackup has check to verify that transaction log backups that are newer than the most recent full or differential backup are not deleted what am trying to achieve is that can do point in time recovery up to week we have very slowly changing database so this is feasible the way understand it now this would require week old full backup and weeks worth of transaction log backup since the full and differential backups can only be used to restore to one specific point in time so should just set the cleanuptime option of my full backup job to what im guessing now is that setting it to 24h will cause the next full backup to delete all older full diff and transaction log backup files leaving me with point in time recovery window of hours right
215491 have table that contains sparse columns along with column set column this is brief example drop table if exists columnset go create table columnset id int not null value1 varchar100 sparse null value2 varchar100 sparse null value3 varchar100 sparse null value4 varchar100 sparse null allvalues xml column set for all sparse columns go insert into columnset id value1 value2 value3 value4 values positive null null null negative null negative null null null negative positive negative null this is not positive result null go want to query the column set to identify rows where any of the columns has value of positive using the value method on the column set will concatenate all the values together into one string and could use like but dont want results where the value is within another string select from columnset where allvalues value nvarchar4000 like positive are there alternative methods of querying column set to achieve the above using apply along with the nodes method provides the same concatenated string output though my syntax could be incorrect the required output id
215534 am using mongodump from mongo tools to download backup of my server however as run the command the command returns unrecognized field snapshot the full error is failed error reading collection failed to parse find data skip snapshot true readpreference mode secondarypreferred db xxx unrecognized field snapshot mongodump version returns mongodump version built without version string git version built without git spec go version go1 os linux arch amd64 compiler gc openssl version openssl 2g mar might need to downgrade my version otherwise unsure of where to go from here
215699 for tables can implement if not exists and if exists as folows if table exists drop if object idau is not null drop table if table not exists create if object idau is null create table key varchar20 value varcharmax but it is not quite working the same on views and triggers can do if exists drop if object idvav is not null drop view va but when im trying the oposite if not exists create if object idvav is null create view va as select from im getting the following error incorrect syntax near the keyword view and the same is with triggers when do if not exists create if object idtrigger instr is null create trigger trigger ins on instead of insert as insert into select from inserted im getting error incorrect syntax near the keyword trigger but if exists drop if object idtrigger instr is not null drop trigger trigger ins is working did missed anything can anyone explain this difference between tables to triggers and views note im using sql server
215868 imagine the following situation person has passport person owns only one passport one passport can only be owned by single person this is clear case of one to one relationship for the sake of simplicity let us imagine that person only has name and the passport only has nationality what is bothering me the most is that it seems that everyone is doing this differently from what can tell there are four strategies that can be taken to map this relationship everything on the same table foreign key on the owner side that references the owned side same value for primary keys on both tables foreign key on the owned side with unique key on top number is pretty self explanatory and do not see any problem when the entities are small but if they have lot of fields we will have gigantic table number seems fine and this is the way have seen most people and frameworks do it entity framework hibernate and it is also the way was taught in school the big problem here is that because of referential integrity our deletion logic will be upside down should be able to delete passport without any problem but in this case will not be able to delete it without deleting the person as well which does not make any sense number and seem pretty much identical with the foreign key on the passport side am able to delete passports without deleting people which makes sense if remove person should have to remove their passport the main advantage that see for using option number instead of number or all the others is that if for ever reason decide that now want user to have multiple passports only have to remove the unique key constraint which is incredibly easy and lot less hassle than changing the keys from both tables so my question basically is reduced to the following with so many advantages with number why do people keep using the other strategies my biggest grip is with orms such as hibernate which in my opinion do things the opposite way the owning side of the relation tracked by hibernate is the side of the relation that owns the foreign key in the database so if tried to do this in java with hibernate my relationship would be swapped if included the foreign key to the passport on the person table like it wants me too it would ruin the deletion logic like have explained before have the impression that ef also works this way so with all of this why do people keep preferring the disadvantageous approaches
216417 while running select from sys server principals for public role in the column is fixed role is shown but the documentation states that it is fixed server role documentation link although it is stated that public is little bit different from other roles because we can assign permissions to it anyway it is mentioned as fixed role can anyone explain this dilemma
216816 ive started new job and it involves looking at bunch of big numbers is there an easy way to add commas to an int or decimal field to make it readable for example sql server outputs the column on the left but for my own sanity need it to look like the one on the right or would have to write some heinous leftrightvandalized data63 rightleftvandalized data63 function the perfect thing would be commas in the display grid then plain integers in the output
216919 have piece of sql that seems to run really fast in environment but the exact same query runs really slow in environment environments are supposed to be the same so what should do and or where should look in order to see why the query doesnt perform the same
216999 sp msforeachdb is an undocumented sp which is designed to run some sql against every database in the server instance why then does it appear that need to use the use keyword to do that exec sp msforeachdb command1 select db name prints the database name the sp msforeachdb command was run against times where is the number of databases on the instance exec sp msforeachdb command1 use select db name prints the name of each database why is it necessary to use the use statement shouldnt this behavior be inherent in the procedure
217019 want to create bible verse table that has one field representing range of bible verses currently represent this range as pair of values versebegid and verseendid want something like the sql server spatial data type instead of create table bibleverse as table versebegid int not null verseendid int not null want create table bibleverse as table verserange not null
217419 can run ola hallengrens checkdb job on secondary asynchronous replica dr that is readable during business hours currently the asynchronous replica is not readable first question is if it is possible secondly is it safe have primary server that is live and am doing checkdb on secondary server during business hours off hours
217424 for example if im creating view with name 4aii why does sql server care that it starts with could call the table fouraii or ivaii additionally what does do behind the scenes to allow for any string to be used as name strings string amirite
217653 in the dbo programs table the column id is the primarykey not par of composite key there are also quite few other indexes on that table when im running this simple query select id from dbo programs here is the execution plan my question is why is it not just using the pk index instead performance is not an issue as that table has rows but just find it odd and want to understand why sqlserver is right and why im wrong to assumed it would be better
217788 dont have clear understanding of query optimization in any database other than that it happens now ive just seen medium post that discusses the pitfalls of using postgresql cte in terms of optimization since the cte is only evaluated once and that any optimizations that may be applied in terms of how the cte is used the database just cant apply but the example in the blog post seems trivial to optimize select from foo where id vs with cte as select from foo select from cte where id if calculation of the cte is done lazily on first requirement then would imagine that these two queries could be optimized in the same way think at least and that made me wonder would sql server be able to optimize such query better than postgres are there known differences between databases to the extent ability they have in optimizing queries
218315 im stuck in way that how to do sum on column lets say first column of table result from another query without knowing the column name with something like column position id its something like this select sumwhat employid from select count employid from table1 union all select count employid from table2 union all select count employid from table3 or if its in single query single simple select query with using sum like select employname sumwhat employid from tablex how do tell sum function to sum based on column position index in table like sum2 note dont want to use column alias any possibility of doing sum not based on column name know can use column name or aliases but really want to know the possibility of not using only these which is why im asking this question if no possible ways will accept no as correct answer then
218479 with sql server microsoft introduces utf support for char and varchar data types and says this feature may provide significant storage savings depending on the character set in use for example changing an existing column data type with ascii strings from nchar10 to char10 using an utf enabled collation translates into nearly reduction in storage requirements this reduction is because nchar10 requires bytes for storage whereas char10 requires bytes for the same unicode string utf seems to support every script so basically we can start storing unicode data in varchar and char columns and as is said in the documentation this can reduce the size of tables and indexes and from there we can get even better performance because smaller amount of data is read am wondering does this mean we can stop to use nvarchar and nchar columns which implements utf can anyone point scenario and reason not to use the char data types with utf encoding and continue use the chars ones
218871 have two tables in mysql database t1 with column c1 and t2 with column c2 run this query select from t1 where c1 in select c1 from t2 the above query should give an error as c1 is not present in t2 instead it returns all the rows from t1 another version of the above query with delete which can be much more disastrous delete from t1 where c1 in select c1 from t2 the above query deletes all the rows from t1 when it is just supposed to give an error have noticed this behavior occurs only when the column in the subquery has the same name as the outer one meaning select from t1 where c1 in select c3 from t2 will throw an error as expected error 42s22 unknown column c3 in field list by the way have checked for the same issue on postgresql and the behavior is exactly the same any explanation for this strange behavior
218962 say we have production system with several databases encrypted using tde and self generated certificate we regularly need to take this database and refresh our non production systems this process involves backing up the source database restoring it to pre production obfuscating personal data and number of other items the key fact here is that the pre production system has copy of the encryption certificate from production am looking at ways to make this process more secure im not happy with the certificate sat on the pre production environment is there another way that will never have an unencrypted backup saved anywhere not allow propagation of personal data from production into non production systems
219936 when we do full data backup using ssms ui at the bottom of the window we have the option to specify the destination as disk and also to add multiple files my question is does adding multiple files create duplicate copies of the full backup or does it create split backup that is split the full backup into the specified files this book suggests it does duplication where as this link suggests that it does split please can someone clarify
220266 cant find documentation about whether agent jobs in sql server can be backed up if not how can back them up in order to restore during the future recovery in case of an instance failure
220270 historically it has been recommended not to use the default ports for connections to sql server as part of security best practice on server with single default instance the following ports would be used by default sql server service port tcp sql server browser service port udp dedicated admin connection port tcp questions is this advice still relevant should all of the above ports be changed
220363 have data warehouse comprised of four clustered columnstore index tables cci and nine rowstore tables these tables are used only for analytics and the cci data is inserted from staging tables every minutes am looking to optimize query performance by adding partitions and sorting all queries of this data are predicated on an integer field with about distinct values the leftmost cci has 100m records and columns there are three child ccis that have that same integer field cci has 15m records and columns cci and both have about 30m records and columns each of these distinct integers the distribution of record count in the leftmost table is as follows greater than 1m greater than 100k greater than 10k additionally there are nine other rowstore tables that also join to the ccis these have trickle inserts are children of the ccis and they all contain the same integer field these rowstores have similar or smaller record volumes columns each two contain lobs and two undergo mass updates frequently these updates are also predicated on the id field how many partitions should make should partition the rowstore tables also are there important considerations am overlooking note regarding the sorting mentioned earlier date field in the leftmost cci is often secondary predicate in these queries therefore am looking into re sorting that cci by date every four weeks or so as maintenance will achieve this sort by dropping the cci adding clustered rowstore index on the date dropping that index and then re adding the cci with maxdop am also looking at sorting the child ccis by the join key to their parent
220560 know this is not the first time this type of question has been asked but why in the following scenario is the persisted computed column being created non deterministic the answer should always be the same right create table dbo test id int eventtime datetime null posixtime int not null go declare eventtime datetime declare gpstime int datediffsecond eventtime insert into dbo testid eventtime posixtime values eventtime gpstime null gpstime go select from dbo test go alter table dbo test add utctime as convertdatetime2isnulleventtime dateaddsecond posixtime convertdate19700101112 persisted go msg level state line computed column utctime in table test cannot be persisted because the column is non deterministic think im following the deterministc rules here is it possible to create persisted computed column here
221191 have short question why do use use master to create database here is the example from the microsoft documentation use master go create database sales on name sales dat filename program files saledat mdf size maxsize filegrowth log on name sales log filename program files salelog ldf size 5mb maxsize 25mb filegrowth 5mb
221290 so im working on code golf puzzle and need to add an int number column to result while maintaining the current order lets say my source data is select value from string splitonetwothreefourfive which returns the items in the original desired order value one two three four five if try to use row number or rank im forced to specify an order by for which value is the only legal choice select value row number overorder by value from string splitonetwothreefourfive but this as expected sorts value alphabetically instead of leaving it in the desired original order value five four one three two joining to number table doesnt work since without where clause ill get full outer join about the best could come up with was using temp table with an identity field create table argg int identity11 varchar99 insert argg select value from string splitonetwothreefourfive select from argg drop table argg but this is really long and annoying any better ideas
221349 the debug button is present on this version of ssms but it is not present on version preview have tried in several ways to add the debug button to my ssms but not successful is there way to add the debug button to ssms v18
221531 im already quite comfortable with using compress and decompress in an internal forum software for our company currently in sql server but trying to make the database as efficient as possible is there an advantage to adding utf to my current collation as in latin1 general ci as sc utf8 upon future migration to sql server
222987 given the next example if object iddbo my table is not null drop table dbo my table go create table dbo my table id int identity not null primary key foo int null bar int null nki int not null go insert some random data insert into dbo my table foo bar nki select top abschecksumnewid abschecksumnewid convertint row number over order by s1 object id from sys all objects as s1 cross join sys all objects as s2 go create unique nonclustered index ix my table on dbo my table nki asc go if fetch all records ordered by nki non clustered index set statistics time on select id foo bar nki from my table order by nki set statistics time off sql server execution times cpu time ms elapsed time ms optimiser chooses the clustered index and then applies sort algorithm execution plan but if force it to use the non clustered index set statistics time on select id foo bar nki from my table withindexix my table set statistics time off sql server execution times cpu time ms elapsed time ms then it uses non clustered index with key lookup execution plan obviously if the non clustered index is transformed into covering index create unique nonclustered index ix my table on dbo my table nki asc include id foo bar go then it uses only this index set statistics time on select id foo bar nki from my table order by nki set statistics time off sql server execution times cpu time ms elapsed time ms execution plan question why does sql server use the clustered index plus sort algorithm instead of using non clustered index even if the execution time is faster in the latter case
223392 during one of the last lessons at university im student lecturer asked us to develop database mysql server if it matters and tiny client app that would consume the database as data source one of requirements was that the identity column which is the pk in every table must be sequential because it is good practice as per lecturer words that is when table row is deleted its pk must be reused in subsequent inserts have average knowledge in rdbms pks and identity columns from what understand that identity column is just way to let db to auto generate pks when inserting rows and nothing more and identity column value shall not be related to row attributes in any way as long as it is not natural key this requirement strictly sequential identity column was suspicious to me tried to ask the lecturer what is wrong if identity is not sequential with gaps caused by deletions but got very abstract answer like it is convenient for users and useful for db administrators who maintain the database no specific examples the argument convenient for users sounds silly because it doesnt have any meaning in business domain therefore im curious if these reasons are real can think only of one case when identity column reseed is required when identity space is exhausted but this is more design issue when identity column type was chosen incorrectly say simple int instead of bigint or uniqueidentifier when table contains billion rows suppose an identity column is clustered index can gaps in identity column affect index performance maybe there are other real world reasons for automatic identity column re seed after each delete im not aware of thanks in advance
223496 im inserting some xml data to an xml column in sql server but after the data has been inserted it has been changed by sql server here is the data insert xsl value of select name given xsl text xsl text xsl value of select name family when read it back it looks like this xsl value of select name given xsl text xsl value of select name family pay attention to the second line this is problem because it changes how the xslt transformation output will be the first example will create space between given and family name while the second will not create any space so it will be like johnjohnsen while the first one will be like john johnsen is there some way to solve this
223906 am trying to get count of ids on orders table for yesterday not hardcode where the date in the table corresponds to when the order was placed my table looks like this orders order id int primary key user id int date created date order value float city id int have used this code to get todays date in the table select dateadddd datediffdd getdate as today from orders this works fine but when try attempt where clause below this select dateadddd datediffdd getdate as today from orders where today date created receive this error invalid column name today so my question is simply how can get this table to recognise the the date added as new column and allow me to perform work on it thanks in advance
224871 have non binary tree of customer and need to obtain all the ids in tree for the given node the table is very simple just an join table with parent id and child id this is representation of the tree stored in my db in this example if search for node need in return if search for need in return the order is not important only need the id to avoid circularity when adding children to node created this query the ancestor part works fine retrieve all parent nodes but for the descendants have some trouble im only able to retrieve some part of the tree for example with node retrieve so all right part of the tree is missing with recursive starting nodes starting parent child as select parent child from public customerincustomer as where child node or parent node ancestors parent child as select parent child from public customerincustomer as where parent in select parent from starting union all select parent child from public customerincustomer as join ancestors as on child parent descendants parent child as select parent child from public customerincustomer as where parent in select parent from starting or child in select child from starting union all select parent child from public customerincustomer as join ancestors as on parent child table ancestors union all table descendants update see that many examples included in the tree table also the root in form root id null in my case dont have this record for example taking the smallest tree in my table have only one record parent child
224913 have query that always filters on status is there performance benefit of one of these ways over the other this is in the context of an ad hoc query the datatype of userstatus is int and userstatus or declare userstatus int and userstatus userstatus please dont talk about how params and literals are different when the values are unknown changing thats different topic
225240 am looking at this legacy sql query the bit am not able to get is why its inner joining same table twice on the same columns am talking about table1 and table1 joined with alias table1alias select distinct othercolumns table1alias columna from maintable inner join secondarytable on maintable id1 secondarytable id1 inner join table1 on secondarytable id2 table1 id3 inner join table1 table1alias on secondarytable id2 table1alias id3 inner join thirdtable on table1 id4 thirdtable id5 inner join fourthtable on thirdtable id6 fourthtable id7 inner join fivetable on thirdtable id8 fivetable id9 inner join sixthtable on table1alias columna sixthtable id10 left join seventhtable on thirdtable id11 seventhtable id12 where leftsecondarytable type123 between and and secondarytable type456 cate and table1 type and table1alias columna conn
225274 have person table that has created by column that references the primary id of the table itself so it could be an employee that adds another employee to the database it works fine but people can also add themselves signup so the value in the created by column should be the auto incremented value of the id column but that value is obviously not available until after the insert so could either make the reference not to check the values add default value in the beginning or make the column nullable all options seem bad to me the mysqls dialect has this set foreign key checks insert into employee values12345678906789012345 set foreign key checks but could not find something similar for sql servers sql
226356 wonder why sql server makes wrong estimations in such simple case there is scenario create partition function pf test int as range right for values create partition scheme ps test as partition pf test all to primary create table datekey int not null type int not null constraint pk primary key datekey type on ps testdatekey insert into datekey type select datekey n1 type n2 from dbo numbers n1 cross join dbo numbers n2 where n1 between and and n2 between and update statistics pk with fullscan incremental on create table datekey int not null subtype int not null type int not null constraint pk primary key datekey subtype on ps testdatekey insert into datekey subtype type select datekey subtype type type from cross join dbo numbers where between and update statistics pk with fullscan incremental on so setup is pretty straightforward statistics are in place and sql server can produce correct estimates when we query one table select count from where datekey select count from where datekey but in this simple select estimates are way off and see no explanation why select datekey type from join on datekey datekey and type type where datekey right after clustered index seek estimation is from actual real world query is even worse estimate is from actual numbers table to reproduce setup declare upperbound int with ctennumber as select row number over order by s1 object id from sys all columns as s1 cross join sys all columns as s2 select number into dbo numbers from cten where number upperbound create unique clustered index cix number on dbo numbersn with fillfactor in the event server default has been changed data compression row if enterprise table large enough to matter pps same scenario but non partitioned works perfectly
226610 im trying to trouble shoot slow performing query using show plan analysis ssms on the actual execution plan the analysis tool points out that estimates for number of rows are off from returned results in few places in the plan and further gives me some implicit conversion warnings dont understand these implicit conversions of int over to varchar the fields referenced are not part of any parameter filter on the query and in all tables involved the column data types are the same get the below cardinalityestimate warnings type conversion in expression convert implicitvarchar12 ccd profileid may affect cardinalityestimate in query plan choice this field is an integer everywhere in my db type conversion in expression convert implicitvarchar6 ccd nodeid may affect cardinalityestimate in query plan choice this field is an smallint everywhere in my db type conversion in expression convert implicitvarchar6 ccd sessionseqnum may affect cardinalityestimate in query plan choice this field is an smallint everywhere in my db type conversion in expression convert implicitvarchar41 ccd sessionid may affect cardinalityestimate in query plan choice this field is an decimal everywhere in my db edit here is the query and actual execution plan for reference https www brentozar com pastetheplan id sysyt0nzn and table definitions object table dbo agentconnectiondetail script date am set ansi nulls on go set quoted identifier on go create table dbo agentconnectiondetail sessionid decimal not null sessionseqnum smallint not null nodeid smallint not null profileid int not null resourceid int not null startdatetime datetime2 not null enddatetime datetime2 not null qindex smallint not null gmtoffset smallint not null ringtime smallint null talktime smallint null holdtime smallint null worktime smallint null callwrapupdata varchar collate sql latin1 general cp1 ci as null callresult smallint null dialinglistid int null convertedstartdatetimelocal datetime2 null convertedenddatetimelocal datetime2 null constraint pk agentconnectiondetail primary key clustered sessionid asc sessionseqnum asc nodeid asc profileid asc resourceid asc startdatetime asc qindex asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go object table dbo contactcalldetail script date am set ansi nulls on go set quoted identifier on go create table dbo contactcalldetail sessionid decimal not null sessionseqnum smallint not null nodeid smallint not null profileid int not null contacttype smallint not null contacttypedescription varchar collate latin1 general ci as null contactdisposition smallint not null contactdispositiondescription varchar collate latin1 general ci as null dispositionreason varchar collate latin1 general ci as null originatortype smallint not null originatortypedescription varchar collate latin1 general ci as null originatorid int null originatordn varchar collate latin1 general ci as null destinationtype smallint null destinationtypedescription varchar collate latin1 general ci as null destinationid int null destinationdn varchar collate latin1 general ci as null startdatetimeutc datetime2 not null enddatetimeutc datetime2 not null gmtoffset smallint not null callednumber varchar collate latin1 general ci as null origcallednumber varchar collate latin1 general ci as null applicationtaskid decimal null applicationid int null applicationname varchar collate latin1 general ci as null connecttime smallint null customvariable1 varchar collate latin1 general ci as null customvariable2 varchar collate latin1 general ci as null customvariable3 varchar collate latin1 general ci as null customvariable4 varchar collate latin1 general ci as null customvariable5 varchar collate latin1 general ci as null customvariable6 varchar collate latin1 general ci as null customvariable7 varchar collate latin1 general ci as null customvariable8 varchar collate latin1 general ci as null customvariable9 varchar collate latin1 general ci as null customvariable10 varchar collate latin1 general ci as null accountnumber varchar collate latin1 general ci as null callerentereddigits varchar collate latin1 general ci as null badcalltag char collate latin1 general ci as null transfer bit null nextseqnum smallint null redirect bit null conference bit null flowout bit null metservicelevel bit null campaignid int null origprotocolcallref varchar collate latin1 general ci as null destprotocolcallref varchar collate latin1 general ci as null convertedstartdatetimelocal datetime2 null convertedenddatetimelocal datetime2 null altkey as concat sessionid sessionseqnum nodeid profileid collate database default persisted not null prvseqnum smallint null constraint pk contactcalldetail primary key clustered sessionid asc sessionseqnum asc nodeid asc profileid asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go object table dbo contactqueuedetail script date am set ansi nulls on go set quoted identifier on go create table dbo contactqueuedetail sessionid decimal not null sessionseqnum smallint not null profileid int not null nodeid smallint not null targetid int not null targettype smallint not null targettypedescription varchar collate latin1 general ci as null qindex smallint not null queueorder smallint not null disposition smallint null dispositiondescription varchar collate latin1 general ci as null metservicelevel bit null queuetime smallint null constraint pk contactqueuedetail primary key clustered sessionid asc sessionseqnum asc profileid asc nodeid asc targetid asc targettype asc qindex asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go object index name of missing index sysname script date am create nonclustered index name of missing index sysname on dbo contactcalldetail convertedstartdatetimelocal asc include sessionid sessionseqnum nodeid profileid with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary go object index idx ccd contacttype desttype stdtlocal script date am create nonclustered index idx ccd contacttype desttype stdtlocal on dbo contactcalldetail destinationtype asc contacttype asc convertedstartdatetimelocal asc include sessionid sessionseqnum nodeid profileid convertedenddatetimelocal with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary go set ansi padding on go object index idx cqd profile traget targettype script date am create nonclustered index idx cqd profile traget targettype on dbo contactqueuedetail profileid asc targetid asc targettype asc include targettypedescription queueorder disposition dispositiondescription queuetime with pad index off statistics norecompute off sort in tempdb off drop existing off online off allow row locks on allow page locks on on primary go
226792 are sql server temporary tables prefixed with supported in localdb instances
226838 ive got bunch of financial reports and we want to be able to pass them two inputs year and quarter as variables im doing it this way but really dont like it declare quarter int year int date date set quarter set year set date cast year as varchar4 set date dateaddquarter quarter date print date question what is the best way to reconstruct date from integer inputs desired result
226946 coming to sql from other programming languages the structure of recursive query looks rather odd walk through it step by step and it seems to fall apart consider the following simple example create table nums bigint insert into nums values with as select from nums union all select as from where select from order by lets walk through it first the anchor member executes and the result set is put into so is initialized to then execution drops below the union all and the recursive member is executed for the first time it executes on that is on the that we currently have in hand this results in what does it do with this new result does it append to the existing label the resulting union and then carry on with the recursion from there or does it redefine to be only this new result and do all the union ing later neither choice makes sense if is now and we execute the next iteration of the recursion then we will end up with and weve lost if is now only then we have mislabeling problem is understood to be the union of the anchor member result set and all the subsequent recursive member result sets whereas is only component of it is not the full that we have accrued so far therefore to write the recursive member as selecting from makes no sense certainly appreciate what max vernon and michael have detailed below namely that all the components are created up to the recursion limit or null set and then all the components are unioned together this is how understand sql recursion to actually work if we were redesigning sql maybe we would enforce more clear and explicit syntax something like this with as select into from nums union all select as into from where select from order by sort of like an inductive proof in mathematics the problem with sql recursion as it currently stands is that it is written in confusing way the way it is written says that each component is formed by selecting from but it does not mean the full that has been or appears to have been constructed so far it just means the previous component
227267 have tmp table shown below in the above table ignore dup key is set to on and the id column is the primary key said table has only one key after inserting lots of data will get the duplicate key was ignored message because of some redundant data want to check which redundant row was tried to insert checked the origin of the message it was sys messages now how to store the row which failed while the insertion attempt took place because of the duplicate primary key value
227288 on server with 32gb we are running sql server sp2 with max memory of 25gb we have two tables here you find simplified structure of both tables create table dbo settings id int identity11 not null resourceid int null typeid int null remark varchar max null constraint pk settings primary key clustered id asc on primary go create table dbo resources id int identity11 not null resourceuid int null constraint pk resources primary key clustered id asc on primary go with following non clustered indexes create nonclustered index ix uid on dbo resources resourceuid asc create nonclustered index ix test on dbo settings resourceid asc typeid asc the database is configured with compatibility level when run this query there are spills to tempdb this is how execute the query exec sp executesql select idremark from resources inner join settings on resourceid id where resourceuid uid order by typeid uid int uid if dont select the remark field no spills occurs my first reaction was that the spills occurred due to the low number of estimated rows on the nested loop operator so add datetime and integer columns to the settings table and add them to my select statement when execute the query no spills are happening why are the spills only happening when remark is selected it has probably something to do with the fact that this is varcharmax what can do to avoid spilling to tempdb adding option recompile to the query makes no difference
228561 dont see why an index rebuild would require sch lock on the given index
228597 to help debugging batch of sql that is run inside transaction inside the transaction dump some data into global temporary table the global temporary table is created inside the transaction have no choice about that for reasons am not going into here thought might be able to select from the temp table from outside the transaction under different connection by using withnolock however the select is blocked from completing is there any way to select from the temp table from outside the transaction
228695 is there any limit for the content that the in filter can handle for example select name from people where job in all the values goes here microsoft docs for in says explicitly including an extremely large number of values many thousands of values separated by commas within the parentheses in an in clause can consume resources and return errors or to work around this problem store the items in the in list in table and use select subquery within an in clause but is there any exact or approximate number for many thousands of values
228789 as part of our etl process we compare rows from staging against the reporting database to figure out if any of the columns have actually changed since the data was last loaded the comparison is based on the unique key of the table and some kind of hash of all of the other columns we currently use hashbytes with the sha2 algorithm and have found that it does not scale on large servers if many concurrent worker threads are all calling hashbytes throughput measured in hashes per second does not increase past concurrent threads when testing on core server test by changing the number of concurrent maxdop queries from testing with maxdop showed the same scalability bottleneck as workaround want to try sql clr solution here is my attempt to state the requirements the function must be able to participate in parallel queries the function must be deterministic the function must take an input of an nvarchar or varbinary string all relevant columns are concatenated together the typical input size of the string will be characters in length is not max the chance of hash collision should be roughly equal to or better than the md5 algorithm checksum does not work for us because there are too many collisions the function must scale well on large servers throughput per thread should not significantly decrease as the number of threads increases for application reasons assume that cannot save off the value of the hash for the reporting table its cci which doesnt support triggers or computed columns there are other problems as well that dont want to get into what is scalable way to simulate hashbytes using sql clr function my goal can be expressed as getting as many hashes per second as can on large server so performance matters as well am terrible with clr so dont know how to accomplish this if it motivates anyone to answer plan on adding bounty to this question as soon as am able below is an example query which very roughly illustrates the use case drop table if exists changed ids select stg id into changed ids from select id cast hashbytes sha2 castfk1 as nvarchar19 castfk2 as nvarchar19 castfk3 as nvarchar19 castfk4 as nvarchar19 castfk5 as nvarchar19 castfk6 as nvarchar19 castfk7 as nvarchar19 castfk8 as nvarchar19 castfk9 as nvarchar19 castfk10 as nvarchar19 castfk11 as nvarchar19 castfk12 as nvarchar19 castfk13 as nvarchar19 castfk14 as nvarchar19 castfk15 as nvarchar19 caststr1 as nvarchar500 caststr2 as nvarchar500 caststr3 as nvarchar500 caststr4 as nvarchar500 caststr5 as nvarchar500 castcomp1 as nvarchar1 castcomp2 as nvarchar1 castcomp3 as nvarchar1 castcomp4 as nvarchar1 castcomp5 as nvarchar1 as binary32 hash1 from hb tbl with tablock stg inner join select id casthashbytes sha2 castfk1 as nvarchar19 castfk2 as nvarchar19 castfk3 as nvarchar19 castfk4 as nvarchar19 castfk5 as nvarchar19 castfk6 as nvarchar19 castfk7 as nvarchar19 castfk8 as nvarchar19 castfk9 as nvarchar19 castfk10 as nvarchar19 castfk11 as nvarchar19 castfk12 as nvarchar19 castfk13 as nvarchar19 castfk14 as nvarchar19 castfk15 as nvarchar19 caststr1 as nvarchar500 caststr2 as nvarchar500 caststr3 as nvarchar500 caststr4 as nvarchar500 caststr5 as nvarchar500 castcomp1 as nvarchar1 castcomp2 as nvarchar1 castcomp3 as nvarchar1 castcomp4 as nvarchar1 castcomp5 as nvarchar1 as binary32 hash1 from hb tbl with tablock rpt on rpt id stg id where rpt hash1 stg hash1 option maxdop to simplify things bit ill probably use something like the following for benchmarking ill post results with hashbytes on monday create table dbo hash me id bigint not null fk1 bigint not null fk2 bigint not null fk3 bigint not null fk4 bigint not null fk5 bigint not null fk6 bigint not null fk7 bigint not null fk8 bigint not null fk9 bigint not null fk10 bigint not null fk11 bigint not null fk12 bigint not null fk13 bigint not null fk14 bigint not null fk15 bigint not null str1 nvarchar500 not null str2 nvarchar500 not null str3 nvarchar500 not null str4 nvarchar500 not null str5 nvarchar2000 not null comp1 tinyint not null comp2 tinyint not null comp3 tinyint not null comp4 tinyint not null comp5 tinyint not null insert into dbo hash me with tablock select rn rn rn rn rn rn rn rn rn rn rn rn rn rn rn rn replicatechar65 rn replicatechar65 rn replicatechar65 rn replicatechar65 rn replicatechar65 rn from select top row number over order by select null rn from master spt values t1 cross join master spt values t2 option maxdop select maxhashbytessha2 castn as nvarcharmax castfk1 as nvarchar19 castfk2 as nvarchar19 castfk3 as nvarchar19 castfk4 as nvarchar19 castfk5 as nvarchar19 castfk6 as nvarchar19 castfk7 as nvarchar19 castfk8 as nvarchar19 castfk9 as nvarchar19 castfk10 as nvarchar19 castfk11 as nvarchar19 castfk12 as nvarchar19 castfk13 as nvarchar19 castfk14 as nvarchar19 castfk15 as nvarchar19 caststr1 as nvarchar500 caststr2 as nvarchar500 caststr3 as nvarchar500 caststr4 as nvarchar500 caststr5 as nvarchar2000 castcomp1 as nvarchar1 castcomp2 as nvarchar1 castcomp3 as nvarchar1 castcomp4 as nvarchar1 castcomp5 as nvarchar1 from dbo hash me option maxdop
228868 suppose we have table orders containing the columns order id total discount then we write query similar to the following select countorder id as num orders sumtotal countorder id as avg total sumdiscount countorder id as avg discount from orders is the value for countorder id preserved across columns or re computed eg is there performance hit or is it better to determine the computed values first and used those in the query for example declare order count as int select order count countorder id from orders select order count as num orders sumtotal order count as avg total sumdiscount order count as avg discount from orders note that while writing this question noticed that since sql server avg is supported however continued this question and intend this to be more general to wanting to understand how sql server handles identical aggregates across columns as do sometimes run into this in other forms
229012 im trying to make stored procedure who will return me the rowid with most matching parameters lets say we have table rowid documentid employeeid companyid null null and send lets say values documentid employeeid comanyid it should return me second row since documentid and companyid exist some other situation would be if send documentid employeeid and companyid it should return me first in the table if there is too little information about this problem or its not clear feel free to ask for more detail
229064 background im working on website for movie theather chain currently located in four different cities might expand in the future they use the same single database website for all cities which means have to have column in certain tables which holds the id of the city that each row belongs to right now have three different tables cinemas contains each citys cinema id and name movies contains all movies that has been will be shown at the cinema showtimes contains all showtimes for all movies in all cities the structure of the showtimes table is the following column name column type description id bigint primary unique id for each showtime perhaps unnecessary cinemaid tinyint foreign key bound to cinemas id movieid bigint foreign key bound to movies id showtime datetime at what date and time the movie will show will contain multiple rows for each movie one row for each showtime how this table will be used user of the website must be able to view all current upcoming movies and showtimes sorted by date in the selected city example query backend select movieid showtime from showtimes where cinemaid order by showtime select single movie and view all showtimes for that specific title only in the selected city example query select showtime from showtimes where cinemaid and movieid order by showtime select single day and view all movies and showtimes for that day only in the selected city example query select movieid showtime from showtimes where cinemaid and showtime between date am and date pm so naturally decided that needed to create indexes for the columns problem what im having trouble with is deciding determining how to index the columns properly one index for each column seems quite expensive so started looking into composite indexes which seems to be the right choice but also led to even more confusion from my understanding based on what ive read you should add the columns to the index by order of selectivity making the most selective im guessing that means the most unique with the most cardinality column the first in the composite index in my case that would be the showtime column the only problem with that is that the index can only be used by the database if the first column is included in the search query which it currently isnt in either of my queries question what kind of indexes should apply to my columns in order to cover all usage scenarios the last scenario may be omitted but the first two are required should use composite index on all columns for some columns or do need separate index for each column this table is updated at most few times per week to add new showtimes footnotes mysql indexes what are the best practices indexing every column in table how important is the order of columns in indexes question how important is the order of columns in indexes top voted answer when should use composite index
229070 have query where need to cross join small table with rows to table with roughly 31k rows the execution plan sql server came up with does index scans in both tables and joins them with nested loops inner join however when checking the execution plan noticed that the scan on the smaller table produced 31k 155k rows for the following step does that mean sql server is scanning the index of the smaller table 31k times here is small reproduction of the problem im having create table id int primary key create table id int primary key insert into aid values insert into bid values select from cross join here is the query live statistics of the select command my question is whether the index scan on the smaller table is performing multiple operations one for each row on the bigger table as opposed to doing it only once and having its result being fed to the following step only once
229103 am currently in the process of migrating data from oracle to sql server and im encountering an issue trying to validate the data post migration environment details oracle al32utf8 character set client nls lang we8mswin1252 varchar2 field sql server latin1 general ci as collation nvarchar field im using dbms crypto hash on oracle to generate checksum of the whole row then copying to sql and using hashbytes to generate checksum of the whole row which im then comparing to validate the data matches the checksums match for all rows except those with multibyte characters for example rows with this character do not match in the checksums even though the data is transferred correctly when use dump in oracle or convert to varbinary in sql server the data matches exactly except for the bytes for this character in sql server the bytes are 0xe625 and in oracle they are 0x25e6 why are they ordered differently and is there reliable way to convert one to the other to ensure the checksum at the other end matches for strings with multi byte characters
229559 have table that can be created and populated with the following code create table dbo examplegroupkey int not null recordkey varchar12 not null alter table dbo example add constraint iexample primary key clusteredgroupkey asc recordkey asc insert into dbo examplegroupkey recordkey values archimedes newton euler euler gauss gauss poincar ramanujan neumann grothendieck grothendieck tao for all rows that have finite collaboration distance based on recordkey with another row would like to assign unique value dont care how or what data type the unique value is correct result set that meets what im asking for can be generated with the following query select as supergroupkey groupkey recordkey from dbo example where groupkey in1 union all select as supergroupkey groupkey recordkey from dbo example where groupkey union all select as supergroupkey groupkey recordkey from dbo example where groupkey in5 order by supergroupkey asc groupkey asc recordkey asc to better aid what im asking ill explain why groupkeys have the same supergroupkey groupkey contains the recordkey euler which in turn is contained in groupkey thus groupkeys and must have the same supergroupkey because gauss is contained in both groupkeys and they too must have the same supergroupkey this leads to groupkeys having the same supergroupkey since groupkeys dont share any recordkeys with the remaining groupkeys they are the only ones assigned supergroupkey value of should add that the solution needs to be generic the above table and result set was merely an example addendum removed the requirement for the solution to be non iterative while would prefer such solution believe its an unreasonable constraint unfortunately am unable to use any clr based solution but if you want to include such solution feel free to will likely not accept it as an answer though the number of rows in my real table is as large as million but there are days when the number of rows will only be around ten thousand on average there are recordkeys per groupkey and groupkeys per recordkey imagine that solution will have exponential time complexity but am interested in solution nonetheless
229784 have the below table col1 col2 acaabsphr phr mcm abc now want to filter the data from this so if have filter parameter as say absmcm want to get only the rows which have at least one matching code so in this case should get the filtered result as col1 col2 acaabsphr mcm now can use the query select from mytable where col2 in absmcm but then it wont retrieve the first row acaabsphr can some one please tell me how can do text search for codes so as long as one code matches in col2 get the row so can pass in directly the delimited list and as long as it finds single match the row is retrieved thanks
230074 am in the process of inserting some data into sql server since this is manual one time load have not bothered spending lot of time writing it the most efficient way however some of the loads takes much longer than initially anticipated so here is my question can somehow tell the server to execute nother query immediately after the current one finalises would have done this manually by hitting f5 on the next part of the code but since the load takes longer than expected would like the server to somehow execute the next part of the code automatically when the first part finalises is this at all possible without either waiting for the current query to finalise or cancelling the current query and re running the whole lot at once including my next steps of my code hope this makes sense thanks
230303 we discovered sql sa account being used in way it should not have been so we are changing sa passwords on all our sql instances we have sql through servers running in mixed authentication mode all users and applications should be using either domain accounts or non sa sql accounts to connect have been monitoring but have not found any other apps users or non internal spids using the sa account few questions q1 does changing the sa password require sql restart found few references that say sql service restart is required after changing the sa account password dba se changing sa password sqlauthority change password of sa login using management studio is that true or only if im changing the authentication mode or only if routinely log on as sa this sql server central thread even suggests changing it could impact existing sql agent jobs and other stuff is that concern or only if someone has hard coded the sa account into an ssis package or something in case it matters we use domain accounts for the sql service and sql agent service and domain proxy accounts for jobs that call ssis packages or powershell scripts q2 can change the sa password the normal way can reset it like would any other account using ssms or more likely via alter login sa with password newpass or would have to enter single user mode or something that would require planned downtime note that id be running this from domain account not while connected as sa q3 should we try to do this password rotation on regular basis or only when we find an issue is this recommended best practice
230328 saw concise tsql statement that effectively splits string into its constituent characters one per line for the purpose of evaluating the ascii value on each character if am reading the query correctly effectively ctes are being used to prepare table of column containing rows each with the value fourth cte is defined as follows ctetallyn as select row number overorder by select null from e4 subsequently this cte is joined to table containing column with the strings of interest with the following select select substringlastname ascii substringlastname that is row number then the nth character in lastname then the ascii value of that character my questions relate to the over clause in the cte above essentially what exactly is it doing if we are querying row number from identical rows why do we need an order by clause at all why is the order by put into an over clause rather than as an order by clause for the select statement especially as the over clause isnt even specifying any partition presume this means the window over which row number operates is the full rows and what does it mean to order by select null
230491 today discovered the harddrive which stores my databases was full this has happened before usually the cause is quite evident usually there is bad query which causes huge spills to tempdb which grows till the disk is full this time it was bit less evident what happened as tempdb wasnt the cause of the full drive it was the database itself the facts usual database size is about gb it grew to gb log file has normal size datafile is huge datafile has available space interpret this as air space that was used but has been freed sql server reserves all space once allocated tempdb size is normal have found the likely cause there is one query which selects much too many rows bad join causes selection of billion rows where couple of hundred thousand is expected this is select into query which made me wonder whether the following scenario could have happened select into is executed target table is created data is inserted as it is selected disk fills up causing the insert to fail select into is aborted and rolled back rollback frees up space data already inserted is removed but sql server doesnt release the freed up space in this situation however wouldnt have expected the table created by the select into to still exist it should be dropped by the rollback tested this begin transaction select into tmp test from values1tx rollback select from tmp test this results in row affected msg level state line invalid object name tmp test yet the target table does exist the actual query wasnt executed in an explicit transaction though can that explain the existence of the target table are the assumptions sketched here correct is this likely scenario to have happened
230722 consider the following query that inserts rows from source table only if they arent already in the target table insert into dbo halloween is coming early this year with tablock select maybe new rows id from dbo heap of mostly new rows maybe new rows where not exists select from dbo halloween is coming early this year halloween where maybe new rows id halloween id option maxdop querytraceon one possible plan shape includes merge join and an eager spool the eager spool operator is present to solve the halloween problem on my machine the above code executes in about ms repro code to create the tables is included at the bottom of the question if im dissatisfied with performance might try to load the rows to be inserted into temp table instead of relying on the eager spool heres one possible implementation drop table if exists consultant recommended temp table create table consultant recommended temp table id bigint primary key id insert into consultant recommended temp table with tablock select maybe new rows id from dbo heap of mostly new rows maybe new rows where not exists select from dbo halloween is coming early this year halloween where maybe new rows id halloween id option maxdop querytraceon insert into dbo halloween is coming early this year with tablock select new rows id from consultant recommended temp table new rows option maxdop the new code executes in about ms can get actual plans and use actual time statistics to examine where time is spent at the operator level note that asking for an actual plan adds significant overhead for these queries so totals will not match the previous results operator first query second query big scan little scan sort merge join spool temp insert temp scan insert the query plan with the eager spool seems to spend significantly more time on the insert and spool operators compared to the plan that uses the temp table why is the plan with the temp table more efficient isnt an eager spool mostly just an internal temp table anyway believe am looking for answers that focus on internals im able to see how the call stacks are different but cant figure out the big picture am on sql server cu in case someone wants to know here is code to populate the tables used in the above queries drop table if exists dbo halloween is coming early this year create table dbo halloween is coming early this year id bigint not null primary key id insert into dbo halloween is coming early this year with tablock select top row number over order by select null from master spt values t1 cross join master spt values t2 cross join master spt values t3 option maxdop drop table if exists dbo heap of mostly new rows create table dbo heap of mostly new rows id bigint not null insert into dbo heap of mostly new rows with tablock select top row number over order by select null from master spt values t1 cross join master spt values t2
230993 first this is for sql server if was on would be using sp add trusted assembly just wanted to clarify that before asking the question how do you register the assembly system directoryservices accountmanagement dll without using trustworthy on cannot get it to work using an asymmetric key generated off of system directoryservices dll the accountmanagement dll is signed differently than system directoryservices dll ive even tried creating separate asymmetric key off of system directoryservices accountmanagement dll but that results in msg level state line xxxxx an error occurred during the generation of the asymmetric key here is test script have written to try to create this assembly use master if db idclr test is null begin create database clr test end go use clr test go exec sp configure configname clr enabled configvalue go reconfigure go drop objects if found first drop system directoryservices accountmanagement if existsselect from sys assemblies where name system directoryservices accountmanagement begin raiserror drop assembly system directoryservices accountmanagement with nowait drop assembly system directoryservices accountmanagement end drop system directoryservices protocols if existsselect from sys assemblies where name system directoryservices protocols begin raiserror drop assembly system directoryservices protocols with nowait drop assembly system directoryservices protocols end drop system directoryservices if existsselect from sys assemblies where name system directoryservices begin raiserror drop assembly system directoryservices with nowait drop assembly system directoryservices end go if existsselect from sys database principals dp where dp name msft clr login begin raiserror drop user msft clr login with nowait drop user msft clr login end go use master go if existsselect from master sys syslogins where name msft clr login begin raiserror drop login msft clr login with nowait drop login msft clr login end go if existsselect from master sys asymmetric keys where name msft clr key begin drop asymmetric key clrkey raiserror drop asymmetric key msft clr key with nowait drop asymmetric key msft clr key end go create the objects use master go if not existsselect from master sys asymmetric keys where name msft clr key begin drop asymmetric key clrkey raiserror create asymmetric key msft clr key with nowait create asymmetric key msft clr key from executable file windows microsoft net framework v4 system directoryservices dll end go if not existsselect from master sys syslogins where name msft clr login begin raiserror create login msft clr login with nowait create login msft clr login from asymmetric key msft clr key end go raiserror grant unsafe assembly with nowait grant unsafe assembly to msft clr login go raiserror grant external assembly with nowait grant external access assembly to msft clr login go use clr test go if not existsselect from sys database principals dp where dp name msft clr login begin raiserror create user msft clr login with nowait create user msft clr login for login msft clr login end go create the clr objects use clr test go system directoryservices create assembly system directoryservices from windows microsoft net framework v4 system directoryservices dll with permission set unsafe system directoryservices protocols create assembly system directoryservices protocols from windows microsoft net framework v4 system directoryservices protocols dll with permission set unsafe system directoryservices accountmanagement create assembly system directoryservices accountmanagement from windows microsoft net framework v4 system directoryservices accountmanagement dll with permission set unsafe nor can you create an assymetric key off system directoryservices accountmanagement dll create asymmetric key msft sda clr key from executable file windows microsoft net framework v4 system directoryservices accountmanagement dll results in msg level state line an error occurred during the generation of the asymmetric key
231228 we have situation where developers do not have any update permissions but they work with applications and see connection strings they know passwords from some sql accounts example sqllogin1 that have update permissions our operations currently are not perfect and sometimes production data needs to be modified no gui for that yet instead of contacting dba and asking him to modify the data developer would improperly use sql account sqllogin1 that has permission to modify the data and connect over sql server management studio to modify the data himself dba can not change password for sqllogin1 without developer seeing the new connection string and new password since the application connection string that uses sqllogin1 is maintained by developer question is there way to deny access to sqllogin1 sql login but only if it is connecting over ssms at the same time if sqllogin1 is connecting over net sqlclient data provider program name in the sys dm exec sessions it must be allowed to login this way we want to not let developer connect over ssms using sqllogin1 while the application that is using sqllogin1 would still be able to connect
231460 using the below example the predicates are the same however the top statement correctly returns rows the bottom statement returns even though the predicates do not match declare barcode nchar22 nrecb012zuki449m1vbjz declare tableid int null declare total decimal10 select from dbo transaction with index ix transaction transactionid paymentstatus deviceid datetime all where barcode barcode and statusid and tableid tableid and total total select from dbo transaction where barcode barcode and statusid and tableid tableid and total total why could this be happening further info the non clustered index in the top statement is not filtered checkdb returns issues server version microsoft sql azure rtm dec copyright microsoft corporation paste the plan link https www brentozar com pastetheplan id s1w ru68e further info have ran dbcc checktable transaction with all errormsgs extended logical checks data purity which indicates no issues can reliably reproduce the issue against this table when restoring backup of this database
231474 microsoft technet article recommends creating secondary file group as the default file group see reference below the secondary file group should have number of files say four that are each placed on separate disks as an added rule of thumb from colleague the number of files should be equal to the number of cpu cores my understanding is that this setup is ideal for mechanical spinning disk drives where there is performance boost by streaming data from multiple heads because spinning disk drive is much slower than solid state drive is this understanding correct if yes then my question is whether the cost based optimizer takes into account the newer solid state drives it seems like the performance bottlenecks from the spinning hard drives goes away when switching to the new solid state drive our it operations groups has informed me that while am currently allocated one virtual drive the data is really stored on an iscsi san which has multiple solid state drives this question is geared for trying to answer what is the most optimal setup for large database of this magnitude should have default secondary file group with just one file should have default secondary file group with number of files that equals the number of cores on the cpu is there performance boost by partitioning database table when using solid state drives the current project am working on requires scaled database that will be few terabytes in size that stores massive amounts of log data one week sample has approximately million records and we need to store rolling years of logs so am now looking at long running queries to find data have tuned the indexes to the point where nearly all of the work is attributed to non clustered index seeks the optimizer does not recommend adding missing index note the licensing on microsoft sql server is currently by cpu core so there is sensitivity to throwing more cores at the problem especially if that will not boost performance additionally am currently developing on sql server but will be migrating to sql server for both development and production update the project will be loading logs nightly where expect few perhaps none updates or deletes because the logs are not expected to change at all so they will not be re loaded everything else will be reads for analytic purposes the primary file group for the system tables where the secondary default file group is for everything else the reason for doing this is explained by the link that is referenced at the bottom of this question separate file groups will be created for the table partition there are other tables within the database that are small enough where they will reside within the secondary file group am only partitioning two tables where one exceeds million records partitioned by identity row number and the other will go into the billions of records partitioned by time monthly plan on partitioning by month over years so there will be partitions will create file groups for each year and then place files into the corresponding yearly file groups the partition strategy is to reduce read times as there will be lot of data scanning for analytic purposes the annual file group strategy is strictly for ease of maintenance for the dbas where they can remove years worth of data by removing single file group reference sql server best practices setting default file group
231628 our team has inherited an application and associated database the previous developers appear to have enforced rule where every index on every table has an include clause to always add every column that isnt otherwise part of the key these tables have on average anywhere from two to five indexes or unique constraints as well as foreign keys the intent looks to be to improve select performance regardless of what query is thrown at the database as access is via an orm that by default but not always retrieves all columns we expect that the side effects of this are increased storage requirements possibly significantly so and additional overhead time for insert update delete the question is is this sensible strategy our team has history with sql server but no members who would consider themselves experts on its internal behaviour though the question has been raised that if this strategy was optimal wouldnt it be the default by now what other side effects database server cpu memory tempdb usage etc should we be expecting or are some of our assumptions above incorrect additionally the application can be installed into both sql server on premise versions since as well as azure sql should we be prepared for any differences between the two or additional side effects on azure as result of this approach
231647 want to create the most efficient index for sparsely populated column only need equality operations so hash index should be beneficial now im wondering why partial hash index isnt smaller than full hash index create index full hash on mytable using hashmy id mb create index partial hash on mytable using hashmy id where my id is not null mb create index full btree on mytable my id mb create index partial btree on mytable my id where my id is not null mb both hash indices take exactly the same amount of space as shown in pghero however when using standard btree indices the partial index takes only of the space of the full index are partial hash indices not supported in postgresql
231682 in sql there is new execution metric log memory other than that it was added in am not finding anything about it execution metric sql cpu time duration execution count logical reads logical writes memory consumption physical reads clr time degree of parallelism dop row count log memory tempdb memory and wait times believe understand what all the other metrics are and why might care ran all the metrics for the top resource consuming queries during several specific periods recorded and now am examining the results know the very large values for log memory are in kbs what exactly is the metric log memory edit having received two answers checked the answer by lowlydba suggests it is combination of related fields from sys query store runtime stats using the code to validate provided by jadarnel27 in their answer created the database and ran the test query for the fields got results very similar summed used sum in excel my values and got bytes looked at query store for log memory used kb it shows value of kb this number is orders of magnitude larger it is also kb as compared to bytes in bytes it would be bytes summed the totals of all the fields and only got byte select from sys query store runtime stats qsrs where qsrs avg log bytes used suspect the answer to my question is that the log memory metric in sql is not any real value the value presented in this small experiment would be 354gb unrealistically high
231862 have been told that for data files backup operate at extent level and for log file backup operates at page level know that the file type for data file is always rows data and is stored in the form of extenteither mixed or uniform extent whereas logs are stored in the form of log vlfvirtual log files can please somebody shed some light on this concept in bit detailed level as am bit puzzled on how backup distinguishes between data and log if its full backup it would store all the committed change written to data file for differential all changes since last full backup from data file and for log backup all changes which is committed however not written to data file appreciate your valuable input on this
232048 if size my log files to evenly split the entire drive they reside on leaving no extra space available will log backups still be able to occur successfully have several databases with one log file per database is it good practice to not leave any space on the drive available allocate it all to the log files the drive is dedicated to the log files in this case data and the os live on their own partitions
232120 in production system on sql server all ids mostly pks in all tables are generated automatically and am informed that they are unique globally mean no ids are the same in the database even if tables are different want to know how this can be done if there are multiple ways please list them all thanks
232943 know this question has been asked number of times and also has answers to it but still need bit more guidance on this subject below is the details of my cpu from ssms below is cpu tab from task manager of the db server have kept the setting of maxdop at by following below formula declare hyperthreadingratio bit declare logicalcpus int declare htenabled int declare physicalcpu int declare socket int declare logicalcpupernuma int declare noofnuma int declare maxdop int select logicalcpus cpu count logical cpu count hyperthreadingratio hyperthread ratio hyperthread ratio physicalcpu cpu count hyperthread ratio physical cpu count htenabled case when cpu count hyperthread ratio then else end htenabled from sys dm os sys info option recompile select logicalcpupernuma countparent node id numberoflogicalprocessorspernuma from sys dm os schedulers where status visible online and parent node id group by parent node id option recompile select noofnuma countdistinct parent node id from sys dm os schedulers find no of numa nodes where status visible online and parent node id if noofnuma and htenabled set maxdop logicalcpupernuma else if noofnuma and htenabled set maxdop round noofnuma physicalcpu else if htenabled set maxdop logicalcpus else if htenabled set maxdop physicalcpu if maxdop set maxdop if maxdop set maxdop print logicalcpus convertvarchar logicalcpus print hyperthreadingratio convertvarchar hyperthreadingratio print physicalcpu convertvarchar physicalcpu print htenabled convertvarchar htenabled print logicalcpupernuma convertvarchar logicalcpupernuma print noofnuma convertvarchar noofnuma print print maxdop setting should be convertvarchar maxdop am still seeing high wait times related to cxpacket am using below query to get that with waits as select wait type wait time ms as waits wait time ms signal wait time ms as resources signal wait time ms as signals waiting tasks count as waitcount wait time ms sum wait time ms over as percentage row number overorder by wait time ms desc as rownum from sys dm os wait stats where wait type not in nbroker eventhandler nbroker receive waitfor nbroker task stop nbroker to flush nbroker transmitter ncheckpoint queue nchkpt nclr auto event nclr manual event nclr semaphore ndbmirror dbm event ndbmirror events queue ndbmirror worker queue ndbmirroring cmd ndirty page poll ndispatcher queue semaphore nexecsync nfsagent nft ifts scheduler idle wait nft iftshc mutex nhadr clusapi call nhadr filestream iomgr iocompletion nhadr logcapture wait nhadr notification dequeue nhadr timer task nhadr work queue nksource wakeup nlazywriter sleep nlogmgr queue nondemand task queue npwait all components initialized nqds persist task main loop sleep nqds cleanup stale queries task main loop sleep nrequest for deadlock search nresource queue nserver idle check nsleep bpool flush nsleep dbstartup nsleep dcomstartup nsleep masterdbready nsleep mastermdready nsleep masterupgraded nsleep msdbstartup nsleep systemtask nsleep task nsleep tempdbstartup nsni http accept nsp server diagnostics sleep nsqltrace buffer flush nsqltrace incremental flush sleep nsqltrace wait entries nwait for results nwaitfor nwaitfor taskshutdown nwait xtp host wait nwait xtp offline ckpt new log nwait xtp ckpt close nxe dispatcher join nxe dispatcher wait nxe timer event and waiting tasks count select max w1 wait type as waittype cast max w1 waits as decimal as wait cast max w1 resources as decimal as resource cast max w1 signals as decimal as signal max w1 waitcount as waitcount cast max w1 percentage as decimal as percentage cast max w1 waits max w1 waitcount as decimal as avgwait cast max w1 resources max w1 waitcount as decimal as avgres cast max w1 signals max w1 waitcount as decimal as avgsig from waits as w1 inner join waits as w2 on w2 rownum w1 rownum group by w1 rownum having sum w2 percentage max w1 percentage percentage threshold go currently cxpacket wait stands at for my server referred to multiple articles on the recommendation from experts and also looked at maxdop suggestions by microsoft however am not really sure what should be the optimum value for this one found one question on the same topic here however if go with that suggestion by kin then maxdop should be in the same question if we go with max vernon it should be kindly provide your valuable suggestion version microsoft sql server sp3 kb4022619 x64 sep enterprise edition core based licensing bit on windows nt build hypervisor cost threshold for parallelism is set at ctfp has been set to after testing the same for values ranging from default to and respectively when it was default5 and maxdop was wait time was close to for cxpacket executed sp blitzfirst for seconds in the expert mode and below is the output for findings and wait stats
233382 came across table like this create table table1 cd1 int cd2 varchar16 cd3 varchar21 cd4 decimal140 cd5 varchar4 cd6 decimal182 cd7 as left cd3 this table has more than billion rows totally unnecessary but this is another topic as you can see in the last column they use as left cd3 think this is pretty useless since we almost never have select on this table and here is just using space isnt it better to select that field during the select when needed
233610 have the following input id value null null null null null null expect the following result id value the trivial solution would be join the tables with relation and then selecting the max value in group by with tmp as select t2 id maxt1 id as lastknownid from t1 t2 where t1 value is not null and t2 id t1 id group by t2 id select tmp id value from tmp where id tmp lastknownid however the trivial execution of this code would create internally the square of the count of the rows of the input table on expected sql to optimize it out on block record level the task to do is very easy and linear essentially for loop on however on my experiments the latest ms sql cant optimize this query correctly making this query impossible to execute for large input table furthermore the query has to run quickly making similarly easy but very different cursor based solution infeasible using some memory backed temporary table could be good compromise but am not sure if it can be run significantly quicker considered that my example query using subqueries didnt work am also thinking on to dig out some windowing function from the sql docs what could be tricked to do what want for example cumulative sum is doing some very similar but couldnt trick it to give the latest non null element and not the sum of the elements before the ideal solution would be quick query without procedural code or temporary tables alternatively also solution with temporary tables is okay but iterating the table procedurally is not
233642 my postgresql column structure looks like this id from to now want an result which looks like this res where the first and the rows are permuted from to and to and the second column is omitted due to distinct operation
233674 is there way to find from the sql server backup file or msdb tables if the backup is encrypted with tde without trying to restore the backup file thanks
233716 im currently designing transaction table realized that calculating running totals for each row will be needed and this might be slow in performance so created table with million rows for testing purposes create table dbo table seq int identity11 not null value bigint not null constraint pk table primary key clustered seq asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on on primary on primary go and tried to get recent rows and its running totals but it took about seconds 1st attempt select top seq value sumvalue over order by seq total from table order by seq desc rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads row affected sql server execution times cpu time ms elapsed time ms suspected top for the reason of slow performance from the plan so changed query like this and it took about seconds but think this is still slow for production and wondering if this can be improved further 2nd attempt select select sumvalue from table where seq seq total from select top seq value from table order by seq desc order by seq desc rows affected table table scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads row affected sql server execution times cpu time ms elapsed time ms my questions are why the query from 1st attempt is slower than the 2nd one how can improve the performance further can also change schemas just to be clear both queries return the same result as below
233783 our application is running in sql enterprise cluster hosted at client we dont have administrative access they are saying that our database is generating deadlocks and sent us huge trace file the clip below shows deadlock encountered paragraph and it has some paragraphs relating to our database id but dont see any identifiers connecting that deadlock encountered paragraph to ours we use the read committed mode in with read committed snapshot isolation mode turned on in sql my simple question is from this log how to tell that this deadlock encountered is related to our database because elsewhere in that same log file they have other deadlocks more clearly identified with other databases in the same cluster weve asked for extended event tracing but dont know how long they will take thanks 58spid3sunknown 58spid3sunknowninput buf rpc event proc database id object id 58spid3sunknownspid ecid statement type select line 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2be20 value 0x5380c188 cost 58spid3sunknownport 0x00000002221a7400 xid slot wait slot task 0x000000015380c188 consumer exchange wait type waitpipegetrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x0000000141f060a0 value 0x72f3c188 cost 58spid3sunknownport 0x000000066ed0f160 xid slot wait slot task 0x0000000772f3c188 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2bac0 value 0x59403498 cost 58spid3sunknownport 0x00000001fc9157d0 xid slot wait slot task 0x0000000759403498 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x0000000141f06100 value 0x11dc1868 cost 58spid3sunknownport 0x000000018a587f20 xid slot wait slot task 0x0000000511dc1868 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2be80 value 0x72f3d868 cost 58spid3sunknownport 0x00000004a9a250e0 xid slot wait slot task 0x0000000772f3d868 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2bee0 value 0x11dc0cf8 cost 58spid3sunknownport 0x00000012f70769f0 xid slot wait slot task 0x0000000511dc0cf8 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownwait for graph 58spid3sunknowndeadlock encountered printing deadlock information 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x0000000141f06160 value 0xb8ebdc38 cost 58spid3sunknownport 0x00000001fc914160 xid slot wait slot task 0x00000001b8ebdc38 producer exchange wait type waitpipenewrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2bf40 value 0x59402188 cost 58spid3sunknownport 0x000000066ed0fcc0 xid slot wait slot task 0x0000000759402188 consumer exchange wait type waitpipegetrow merging 58spid3sunknownnode 58spid3sunknown 58spid3sunknowninput buf rpc event proc database id object id 58spid3sunknownspid ecid statement type select line 58spid3sunknownrestype exchangeid stype and spid batchid ecid taskproxy 0x00000003e0f2bdc0 value 0x59402558 cost 58spid3sunknownport 0x0000000225f57790 xid slot wait slot task 0x0000000759402558 consumer exchange wait type waitpipegetrow merging
233851 something very odd is happening here have query that looks like this select castft dop as smallint from tracking data where date mydate and identifier when run as raw query it returns data just fine when place it in stored procedure that alters the where clause it throws this error msg level state procedure myprocedure line batch start line the conversion of the varchar value overflowed an int2 column use larger integer column so here is the oddity go through all the possible data for that where clause with query like this select distinct dop from tracking data where identifier and select distinct castdop as smallint from tracking data where identifier and this is what get back nowhere does it have anything remotely too large for smallint so thought ok maybe its non printable ascii character but cant find any im little perplexed at the moment it runs find as raw query explodes as procedure and all possible data based on the where is valid my only suspicion is that the query plan is doing something odd with filtering or maybe runs with different validation when run as procedure
233861 am working with sql server database which includes lot of nulls to analyse my data want to extract all rows of the database table that include less than null marks my database is similar to this structure c1 c2 c3 c4 c5 null null null null null null null null null tried the query which doesnt return an error but no rows are selected select from test123 where isnullc11 isnullc21 isnullc31 isnullc41 isnullc51 expect this query to return the 1st and the fifth row but the result contains rows cant test the following code because dont have the rights to write on the database but here is pseudo code for creating table like mine create table test123 c1 float c2 float c3 float c4 float c5 float go insert test123c1c2c3c4c5 values 23null12 2nullnull12 23nullnull2 null3null1null 23null12
233884 id like to know what the general replacement is for cursor the general implementation of cursor see out and about is declare variable int sqlstr nvarcharmax declare cursor name cursor for select statement essentially to get an array for variable usually its subset of unique ids for accounts clients parts etc open cursor name fetch next from cursor name into variable while fetch status begin set sqlstr some query that uses str variable to do dirty work such as go through all our accounts if its some subset possible new cursor go through those accounts and connect this way map those fields and add it to our big uniform table exec sp executesql sqlstr fetch next from cursor name into variable end close cursor name deallocate cursor name since so many people are anti cursor with nod to so why do people hate cursors what is the general replacement for the general implementation preferably sql server
234086 sql server allows me to create multiple foreign keys on column and each time using just different name can create another key referencing to the same object basically all the keys are defining the same relationship want to know whats the use of having multiple foreign keys which are defined on the same column and reference to the same column in another table whats the benefit of it that sql server allows us to do thing like that
234144 if create stored procedure in the master database and want to execute it from any of my databases just follow this link making procedure available in all databases that give me this code example just by following the example above can call my procedure from any database what about if create table data type in master how can use it in any of my databases use master if not exists select from sys types where name thereplicatedtables create type thereplicatedtables as table obj id int not null primary key clustered obj id use apia repl sub go declare the tables dbo thereplicatedtables
234221 cant seem to figure out the answer ive seen multiple answers like this why does the transaction log keep growing or run out of space and everyone talks about running back ups on your log file so it shrinks down am doing that but it doesnt shrink anything also dont believe am running any super long transactions server sql server recovery mode full have maintenance plan to store days worth of backups task backups up the databases with backup type full task backs up transaction logs verify backup integrity is checked on both tasks my dbs normal ldf file is 22gb when run the above task the bak file is 435mb but the trn file is 22gb same as the ldf and after successfully running the ldf doesnt shrink at all despite everything ive read telling me it should what is going on here and why doesnt the log file ever shrink ive also tried running this command as mentioned in another answer select name log reuse wait desc from sys databases and it says log backup for the db with the huge log file based on an answer below am confusing allocated with used space these are my stats for for reasons have no clue why the initial size was set to 22gb
234303 am supporting vendor based application which is filled with blocking and deadlock deadlocks occur mostly involving two or three processes however noticed yesterday it was involving spids can somebody please help me in understanding this deadlock graph and solution on how to avoid this deadlock victim list victimprocess id process2ff017c28 victimprocess id process2f1538108 victimprocess id process2f618d088 victimprocess id process2f6d828c8 victimprocess id process2f6d83848 victimprocess id process2da9b5468 victimprocess id process2efac7468 victimprocess id process2efac7848 victim list process list process id process2ff017c28 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x2a785f800 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2f1538108 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0xdacf54e0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2f618d088 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x20df303b0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2f6d828c8 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x25090c6d0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2f6d83848 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0xb83c63b0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2da9b5468 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x20d4683b0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2efac7468 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x184e789f0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2efac7848 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0x2420aab80 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process id process2efac7c28 taskpriority logused waitresource object waittime ownerid transactionname implicit transaction lasttranstarted 08t15 xdes 0xab9103b0 lockmode ix schedulerid kpid status suspended spid sbid ecid priority trancount lastbatchstarted 08t15 lastbatchcompleted 08t15 lastattention 01t00 clientapp jtds hostname appname hostpid loginname irb app user isolationlevel repeatable read xactid currentdb currentdbname database name locktimeout clientoption1 clientoption2 executionstack frame procname adhoc line stmtstart stmtend sqlhandle 0x02000000b6b6e728e6bf289c908196a1f4e56b8892a5eab10000000000000000000000000000000000000000 unknown frame executionstack inputbuf p0 bigint p1 bigintupdate table name set status id p0 where id p1 inputbuf process process list resource list objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2f6d828c8 mode owner id process2da9b5468 mode owner id process2f6d828c8 mode ix requesttype convert owner id process2da9b5468 mode ix requesttype convert owner list waiter list waiter id process2ff017c28 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d828c8 mode owner id process2da9b5468 mode owner id process2ff017c28 mode ix requesttype convert owner id process2f6d828c8 mode ix requesttype convert owner id process2da9b5468 mode ix requesttype convert owner list waiter list waiter id process2f1538108 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d828c8 mode owner id process2da9b5468 mode owner id process2ff017c28 mode ix requesttype convert owner id process2f6d828c8 mode ix requesttype convert owner id process2da9b5468 mode ix requesttype convert owner list waiter list waiter id process2f618d088 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f618d088 mode owner id process2da9b5468 mode owner id process2f618d088 mode ix requesttype convert owner id process2ff017c28 mode ix requesttype convert owner id process2da9b5468 mode ix requesttype convert owner list waiter list waiter id process2f6d828c8 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f618d088 mode owner id process2da9b5468 mode owner id process2f618d088 mode ix requesttype convert owner id process2ff017c28 mode ix requesttype convert owner id process2da9b5468 mode ix requesttype convert owner list waiter list waiter id process2f6d83848 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d83848 mode owner id process2efac7848 mode owner id process2ff017c28 mode ix requesttype convert owner id process2efac7848 mode ix requesttype convert owner id process2f6d83848 mode ix requesttype convert owner list waiter list waiter id process2da9b5468 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d83848 mode owner id process2efac7848 mode owner id process2ff017c28 mode ix requesttype convert owner id process2efac7848 mode ix requesttype convert owner id process2f6d83848 mode ix requesttype convert owner list waiter list waiter id process2efac7468 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d83848 mode owner id process2efac7468 mode owner id process2ff017c28 mode ix requesttype convert owner id process2efac7468 mode ix requesttype convert owner id process2f6d83848 mode ix requesttype convert owner list waiter list waiter id process2efac7848 mode ix requesttype convert waiter list objectlock objectlock lockpartition objid subresource full dbid objectname database name schema name table name id lock2149d8800 mode associatedobjectid owner list owner id process2ff017c28 mode owner id process2f6d83848 mode owner id process2efac7468 mode owner id process2ff017c28 mode ix requesttype convert owner id process2efac7468 mode ix requesttype convert owner id process2f6d83848 mode ix requesttype convert owner list waiter list waiter id process2efac7c28 mode ix requesttype convert waiter list objectlock resource list deadlock isolation level is set as read committed snapshot when opened this deadlock graph in sentry one plan explorer it was scary version microsoft sql server sp3 kb4022619 x64 sep copyright microsoft corporation enterprise edition bit on windows nt build hypervisor
234368 would like to order this table but can not find way to make it work can you help me have this table need this have table where documents are listed the first field is the id of the document and the second field represents the father so must show list where you can see in an orderly manner that is the first document and and were generated from the the query select from table name order by col1 asc col2 asc doesnt work for me do this query select fr1 report id fr1 report parent from fara reports fr1 where fr1 report is delete and fr1 report is tmp order by fr1 report id asc fr1 report parent desc and this is my first image
235085 is there such thing as trigger for availability group failovers want certain action to happen when an ag fails over specifically want to turn on database setting turning on rcsi want to do this on failover in order to minimize disruption to workloads and scheduled maintenance windows are hard to come by know that sp procoption can be used to mark procedures as startup procedures this seems like it could work for failover clusters but not for availability groups did consider adding an alert sp add alert on message id in order to respond to failover actions with sql agent job but this seems less direct and in practice it seems slow
235153 need to recycle an apppool in iis on my server when the sql server service starts the route am taking is to have startup stored procedure which runs an agent job that has powershell job step have created grabbed from the internet powershell script which recycles the app pool load iis module import module webadministration set name of the site we want to recycle the pool for site default web site get pool name by the site name pool get item iis sites site select object applicationpool applicationpool recycle the application pool restart webapppool pool this works at the os level but only when powershell is run as an administrator even though the account am logged into windows as is in the admin group sure enough if create the job in agent with powershell step containing the above code upon execution get the error job step received an error at line in powershell script the corresponding line is pool get item iis sites site select object applicationpool applicationpool correct the script and reschedule the job the error information returned by powershell is cannot retrieve the dynamic parameters for the cmdlet filename redirection config error cannot read configuration file due to insufficient permissions process exit code the step failed so need way to run as administrator from sql server agent had success at the os level creating new script which runs the original script with elevated permissions if not security principal windowsprincipal security principal windowsidentity getcurrent isinrole security principal windowsbuiltinrole administrator relaunch as an elevated process start process powershell exe file myinvocation mycommand path verb runas exit now running elevated so launch the script path to my script ps1 which allows me to run the new script from either non elevated cmd prompt or non elevated powershell prompt however when try to run from agent as either powershell step or cmd step get the machine default permission settings do not grant local activation permission for the com server application with clsid longguidhere and appid longguidhere to the user nt service sqlserveragent sid longguidhere from address localhost using lrpc running in the application container unavailable sid unavailable this security permission can be modified using the component services administrative tool have also tried start process powershell verb runas filepath path to my script ps1 which again at the os level worked but when run from agent gives me the same error how can get either recycle the app pool without requiring elevated priveleges or run the script to recycle the app pool with elevated priveleges from sql server agent
235197 have written stored procedure which makes use of temporary table know that in sql server temporary tables are session scoped however have not been able to find definitive information on exactly what session is capable of in particular if it is possible for this stored procedure to execute twice concurrently in single session significantly higher isolation level is required for transaction within that procedure due to the two executions now sharing temporary table
235227 one of my query in was running in serial execution mode after release and noticed that two new functions were used in view which is referenced in the linq to sql query generated from the application so converted those scalar functions to tvf functionsbut still the query is running in serial mode earlier have did scalar to tvf conversion in some other queries and it solved the problem of forced serial execution here is the scalar function create function dbo findeventreviewduedate eventnumber varchar20 eventid varchar25 eventiddate bit returns datetime as begin declare currenteventstatus varchar20 declare eventdatetime datetime declare reviewduedate datetime select currenteventstatus select cis eventstatus from currenteventstatus cis inner join event1 with nolock on cis event1id id where eventnumber eventnumber and eventid eventid select eventdatetime select eventdatetime from event1 where eventnumber eventnumber and eventid eventid if currenteventstatus in and eventiddate begin set reviewduedate dateaddday eventdatetime while reviewduedate getdate set reviewduedate dateaddday reviewduedate declare eventdatejournaldate datetime select eventdatejournaldate select top ij date from eventpage eventjournal ij inner join eventjournalpages on ij pageid id inner join journal on formid id inner join event1 with nolock on event1id id where eventnumber eventnumber and eventid eventid and ij reviewtype supervisor monthly review order by ij date desc ifdateaddday eventdatetime getdate and eventdatejournaldate is null or dateaddday eventdatejournaldate getdate and dateaddday reviewduedate dateaddday getdate set reviewduedate dateaddday reviewduedate else if eventdatejournaldate is not null and dateaddday eventdatejournaldate reviewduedate set reviewduedate dateaddday reviewduedate end return reviewduedate end here is the converted tvf function create function dbo findeventreviewduedate test eventnumber varchar20 eventid varchar25 eventiddate bit returns functionresulttablevairable table currenteventstatus varchar20 event1datetime datetime reviewduedate datetime as begin declare currenteventstatus varchar20 declare eventdatetime datetime declare reviewduedate datetime select currenteventstatus select cis eventstatus from currenteventstatus cis inner join event1 with nolock on cis event1id id where eventnumber eventnumber and eventid eventid select eventdatetime select eventdatetime from event1 where eventnumber eventnumber and eventid eventid if currenteventstatus in and eventiddate begin set reviewduedate dateaddday eventdatetime while reviewduedate getdate set reviewduedate dateaddday reviewduedate declare eventdatejournaldate datetime select eventdatejournaldate select top ij date from eventpage eventjournal ij inner join eventjournalpages on ij pageid id inner join journal on formid id inner join event1 with nolock on event1id id where eventnumber eventnumber and eventid eventid and ij reviewtype supervisor monthly review order by ij date desc ifdateaddday eventdatetime getdate and eventdatejournaldate is null or dateaddday eventdatejournaldate getdate and dateaddday reviewduedate dateaddday getdate set reviewduedate dateaddday reviewduedate else if eventdatejournaldate is not null and dateaddday eventdatejournaldate reviewduedate set reviewduedate dateaddday reviewduedate insert into functionresulttablevairable select currenteventstatus eventdatetime reviewduedate end return end go is there anything wrong with my implementation of tvf function which is preventing the query to run in parallel mode use the tvf function in the query as below select reviewduedate from dbo functionresulttablevairableabc my actual query which uses the view is quite complex and if comment out the function part in the view and on executing the query runs in parallel so it is function which is forcing the query to run in parallel my actual query is in the below format select dv column1 dv column2 select reviewduedate from dbo functionresulttablevairableabc as columnx from demoview dv where condition1 conditon any help is appreciated
235412 am using in one of my reports to check for status of ifi being enabled on sql server it works well when using dmv sys dm server services for sql2016 however in sql2014 and sql2012 see difficulties in rendering this check via sql query for example if use below exec sys xp readerrorlog ndatabase instant file initialization there is no guarantee it will show status on file and sometimes keep querying the error log manually to find that out how can achieve the ifi check using better way so that it can show up on my ssrs report
235465 need to write stored procedure to receive numbers and insert them into table this is what ive developed so far declare int int int int create table tempnum int declare char1 while begin insert into temp select set end select from temp drop table temp know that can directly and statically insert the inputs into the table but just want to know is there any better way to do that wanted to use while statement but the problem is the numbers for variable are being inserted into the table mean the output is what want is
235469 in post on sqlservercentral com it is mentioned by several people that the files for the resource database mssqlsystemresource mdf and ldf should be placed in the same folder as the master database file this was for sql server is this still recommendation when it comes to sql server tried looking in bol but could not find any mention of this there
236675 suppose have stored procedure that is duplicated with some modifications in several databases and want to reference the database in which the stored procedure is stored even if it is executed in another database is there way to retrieve the full path or otherwise retrieve the database in which the stored procedure is stored rather than the current database
236768 have an application which creates millions of tables in sql server database non clustered am looking to upgrade to sql server clustered but am hitting an error message when under load there is already an object named pk tablenameprefix 179e2ed8f259c33b in the database this is system generated constraint name it looks like randomly generated bit number is it possible that am seeing collisions due to the large number of tables assuming have million tables calculate less than in trillion chance of collision when adding the next table but that assumes uniform distribution is it possible that sql server changed its name generation algorithm between version and to increase the odds of collision the other significant difference is that my instance is clustered pair but am struggling to form hypothesis for why that would generate the above error yes know creating millions of tables is insane this is black box 3rd party code over which have no control despite the insanity it worked in version and now doesn in version edit on closer inspection the generated suffix always seems to start with 179e2ed8 meaning the random part is actually only bit number and the odds of collisions are mere in every time new table is added which is much closer match to the error rate seeing
237128 im creating an sql server database with someone else one of the tables is small rows with data that will probably remain constant there is remote possibility that new row will be added the table looks something like this create table sometable id int primary key identity11 not null name varchar128 not null unique insert into sometable values alice bob something charles can dance dugan was here im looking at the char length of that name column and think that its values are probably never going to be larger than say characters maybe not even larger than is there any advantage to my changing this column to for example varchar32 also is there any advantage to keeping default column sizes to multiples of etc
237312 have job on an sql server server know this is not ideal that issue is being addressed the agent runs under nt authority network service want to add step that runs job that sits on another sql server server would use exec server msdb sp start job njobname ran from server manually it executes the job on server fine as expected if add the task to job on server it fails with message executed as user nt authority network service the execute permission was denied on the object sp start job database msdb schema dbo sqlstate error the step failed have assigned nt authority network service on server to the targetserverrole in msdb and then granted execute permission to concern user to sp start job and sp stop job server has server as linked server and tried setting local server login to remote server login mappings local login nt authority network service to impersonate the job still fails with the same error what do need to do thanks
237418 was wondering if there is way for select query to be made on each matched value found within in consider that the table below contains records for each of the values included referenced by the inoperator id like only to be returned for each match select top column from table where column in is there an efficient way can do this the only way could think of is to execute the query for each value within in which would be way too slow considering the actual query retrieves many column from various tables
237475 have an xml value like this want to concatenate all values and return them as single string abc now know that can shred the xml aggregate the results back as nodeless xml and apply valuestext to the result select select valuetext varchar50 as text from myxml nodes as for xml path type valuetext varchar50 however would like to do all that using xpath xquery methods only something like this select myxml is there such way the reason am looking for solution in this direction is because my actual xml contains other elements too for instance and would like to be able to extract both the values as single string and the values as single string without having to use an unwieldy script for each
237494 does the server cache get wiped similarly to when you restart the sql instance machine when you only restart the sql services themselves
237509 im running microsoft sql server sp2 cu6 on vcpu vm with max degree of parallelism set to and cost threshold for parallelism set to in the mornings when trying to display an estimated execution plan for select top query run into massive waits and the operation to render the estimated plan takes minutes often times in the minute range again this is not the actual execution of the query this is just the process to display an estimated execution plan sp whoisactive will show either pageiolatch sh waits or latch ex access methods dataset parent waits and when run paul randals waitingtasks sql script during the operation it shows cxpacket waits with the worker threads showing pageiolatch sh waits resource description field exchangeevent id port5f6069e600 waittype waitportopen waitertype coordinator nodeid tid owneractivity notyetopened waiteractivity waitforallownerstoopen the worker threads look to be bringing the entire stats table into memory as those page numbers as well as subsequent page numbers shown from paul randals query point back to clustered key for the stats table once the plan does come back its basically instantaneous for the remainder of the day even after see most of the stats table attrition from cache with only various records remaining that assume were pulled due to seek operations from similar queries would expect this initial behavior if the query was actually executing with plan that used scan operators but why is it doing this when evaluating execution plans only to arrive at seek operator as shown in the plan linked above what can do aside from running this statement before office hours so my data is appropriately cached to help improve performance here im assuming pair of covering indexes would be beneficial but would they really guarantee any changes in behavior have to work within some storage and maintenance window limitations here and the query itself is generated from vendor solution so any other suggestions besides better indexing would be welcome at this point
237671 ive got node ag setup as follows vm hardware configuration of all nodes microsoft sql server enterprise edition rtm cu14 kb4484710 vcpus gb ram long story to this one max degree of parallelism as required by app vendor cost threshold for parallelism max server memory mb gb ag configuration node primary or synchronous commit non readable secondary configured for automatic failover node primary or synchronous commit non readable secondary configured for automatic failover node readable secondary set with asynchronous commit configured for manual failover node readable secondary set with asynchronous commit configured for manual failover the query in question theres nothing terribly crazy about this query it provides summary of outstanding work items in various queues within the application you can see the code from one of the execution plan links below execution behavior on the primary node when executed on the primary node the execution time is generally around the second mark here is the execution plan and below are stats captured from statistics io and statistics time from the primary node rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table workitemlc scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table workfile scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulertask scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table wfschedulertask scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulerservice scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulerworkerpool scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table itemlc scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads row affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms execution behavior on the read only secondary node when executing on either read only secondary node node or node this query uses the same execution plan this is different plan link and roughly the same execution stats are shown there may be few more page scans as these results are always changing but with the exception of cpu time they look very similar here are stats captured from statistics io and statistics time from the read only secondary node rows affected table worktable scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table workitemlc scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table workfile scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulertask scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table wfschedulertask scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulerservice scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table schedulerworkerpool scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads table itemlc scan count logical reads physical reads read ahead reads lob logical reads lob physical reads lob read ahead reads row affected sql server execution times cpu time ms elapsed time ms sql server parse and compile time cpu time ms elapsed time ms other details ive also run both sp whoisactive and paul randals waitingtasks sql script on the secondary while this query is executing but am not see any waits occurring what so ever which is frankly frustrating this also doesnt look to be case of ag latency as the synchronization status is actually quite good https sqlperformance com monitoring availability group replica sync select ar replica server name adc database name ag name as ag name drs is local drs synchronization state desc drs synchronization health desc drs last hardened lsn drs last hardened time drs last redone time drs redo queue size drs redo rate drs redo queue size drs redo rate as est redo completion time min drs last commit lsn drs last commit time from sys dm hadr database replica states as drs inner join sys availability databases cluster as adc on drs group id adc group id and drs group database id adc group database id inner join sys availability groups as ag on ag group id drs group id inner join sys availability replicas as ar on drs group id ar group id and drs replica id ar replica id order by ag name ar replica server name adc database name this query seems to be the worst offender other queries that also take sub second times on the primary node may take seconds on the secondary node and while the behavior is not as severe it does look to be causing issues finally have also looked at the servers and checked for external processes such as scans external jobs generating unexpected etc and have come up empty handed dont think this is being caused by anything outside of the sql server process the question its only noon where im at and its already been long day so suspect im missing something obvious here either that or weve got something misconfigured which is possible as weve had number of calls into the vendor and ms related to this environment for all of my investigation just cant seem to find what is causing this difference in performance would expect to see some sort of wait occurring on the secondary nodes but nothing how can further troubleshoot this to identify the root cause has anyone seen this behavior before and found way to resolve it update after swapping states of the third node one of the read only replicas to non readable and then back to readable as test that replica is still being held up by an open transaction with any client queries displaying the hadr database wait for transition to versioning wait running dbcc opentran command yields the following results oldest active transaction spid server process id 420s uid user id name qds nested transaction lsn start time may 753pm sid 0x0 dbcc execution completed if dbcc printed error messages contact your system administrator when looking up this spid in sp who2 it shows it as background process with query store back listed as the command while we are able to take tlog backups suspect we are running into similar functionality of this resolved bug so plan on opening ticket with ms about this particular issue today depending on the outcome of that ticket will try to capture call stack trace per joes suggestion and see where we go final update issue self resolved after eclipsing the hour mark of the query store transaction being open as identified above the ag decided to automatically failover before this happened did pull some additional metrics per this link provided by sean the database in question had very large version store dedicated to this database specifically at one point had recorded pages in the reserved page count field and for the reserved space kb value per the errorlogs the failover occurred after minute deluge of transaction hardening failures related to qds base transaction and qds nested transaction transactions the failover did cause an outage of about minutes in my case the database is 6tb in size and is very active so that was actually pretty good in my opinion while the new primary node was online during this time no client queries could complete as they all were waiting on the qds loaddb wait type after the failover the version store numbers reduced to for reserved page count and for reserved space kb queries against the secondary read only replicas also began to execute as quickly as if they were run from the primary so it looks like the behavior disappeared entirely as result of the failover
237684 part of my workload uses clr function that implements the spooky hash algorithm to compare rows to see if any column values have changed the clr function takes binary string as an input so need fast way to convert rows to binary string expect to hash around billion rows during the full workload so id like this code to be as fast as possible have about tables with different schemas for the purposes of this question please assume simple table structure of nullable int columns ive provided sample data as well as way to benchmark results at the bottom of this question rows must be converted to the same binary string if all column values are the same rows must be converted to different binary strings if any column value is different for example code as simple as the following will not work castcol1 as binary4 castcol2 as binary4 it does not handle nulls correctly if col1 is null for row and col2 is null for row then both rows will be converted to null string believe that correct handling of nulls is the hardest part of converting the entire row correctly all allowed values for the int columns are possible to preempt some questions if it matters significant majority of the time the columns wont be null have to use the clr have to hash this many rows cannot persist the hashes believe that cannot use batch mode for the conversion due to the presence of the clr function what is the fastest way to convert nullable int columns to binaryx or varbinaryx string sample data and code as promised create sample data drop table if exists dbo table of ints create table dbo table of ints col1 int null col2 int null col3 int null col4 int null col5 int null col6 int null col7 int null col8 int null col9 int null col10 int null col11 int null col12 int null col13 int null col14 int null col15 int null col16 int null col17 int null col18 int null col19 int null col20 int null col21 int null col22 int null col23 int null col24 int null col25 int null col26 int null col27 int null col28 int null col29 int null col30 int null col31 int null col32 int null insert into dbo table of ints with tablock select null from select top row number over order by select null rn from master spt values t1 cross join master spt values t2 option maxdop go procedure to test performance create or alter procedure as begin set nocount on declare counter int dummy varbinary8000 while counter begin select dummy this code is clearly incomplete as it does not handle nulls castcol1 as binary4 castcol2 as binary4 castcol3 as binary4 castcol4 as binary4 castcol5 as binary4 castcol6 as binary4 castcol7 as binary4 castcol8 as binary4 castcol9 as binary4 castcol10 as binary4 castcol11 as binary4 castcol12 as binary4 castcol13 as binary4 castcol14 as binary4 castcol15 as binary4 castcol16 as binary4 castcol17 as binary4 castcol18 as binary4 castcol19 as binary4 castcol20 as binary4 castcol21 as binary4 castcol22 as binary4 castcol23 as binary4 castcol24 as binary4 castcol25 as binary4 castcol26 as binary4 castcol27 as binary4 castcol28 as binary4 castcol29 as binary4 castcol30 as binary4 castcol31 as binary4 castcol32 as binary4 from dbo table of ints option maxdop set counter counter end select cpu time from sys dm exec requests where session id spid end go run procedure exec will still be using the spooky hash on this binary result the workload uses hash joins and the hashed value is used for one of the hash builds dont want long binary value in the hash build because it requires too much memory
237935 in postgresql can create table with some test data and then in transaction migrate it to new column of different type resulting in one table rewrite upon commit create table foo int insert into foo values followed by begin alter table foo add column varchar update foo set casta as varchar alter table foo drop column commit however that same thing in microsofts sql server seems to generate an error compare this working db fiddle where the add column command is outside of the transaction txn1 begin transaction alter table foo add varchar commit txn2 begin transaction update foo set cast as varchar alter table foo drop column commit to this db fiddle which doesnt work txn1 begin transaction alter table foo add varchar update foo set cast as varchar alter table foo drop column commit but instead errors msg level state line invalid column name is there anyway to make this transaction visible with regard to ddl behave like postgresql
238263 when naming tables in sql try to stay away from sql reserved keywords but today colleague questioned the use of events as table name they said anything that turns green in ssms shouldnt be used as table name are there any conflicts or gotchas that should be concerned with using events as table name in ms sql server
238350 have this logon trigger to only allow certain users to log in to an oracle database even if they have the correct password to enter the database create or replace trigger sys logon trigger after logon on database declare this user varchar250 begin select osuser into this user from session where sid sys contextuserenvsid if this user not in list of users then raise login denied endif end it works for preventing users from entering most schemas but not all the sys or system schemas can still be entered regardless of the user this logon trigger is seemingly completely bypassed is there way to lock out these users even for these sys type schemas bit of context due to decisions made way before got involved with this all of the logins for this database have the same password additionally most users use the same login as many of our processes that read write to this database automatically we dont want to simply change the passwords because it would be very large effort to see what impact changing these passwords actually does to the system we would have to modify the code that the processes use to access the database and there are many of these an easier solution for us is to just lock out based on usernames if possible
238714 in case expression im trying to search inside text column to identify hyphen case when substringal alt address11 in157 and al new address contains then concatal alt addressal new address the hyphen can be located anywhere in the column and so just need to know if it exists regardless of where it actually is in the column im currently using the exact same code that josh provided like but it doesnt work because it doesnt return the correct data for several specific instances where know that it should be the exact text in alt address is churchill circle the exact text in new address is however the results that are returned does not include the new address have confirmed that dash in new address really matches the dash im using the search ascii
238783 created sample table as below create table dbo statisticsdemo id int identity11 not null name nvarchar null on primary then inserted below data as below select namecount as count from statisticsdemo group by name name count aabbcc xxyyzz then created below non clustered index create nonclustered index nci statisticsdemo name on dbo statisticsdemo name asc now ran the below query select name from dbo statisticsdemo where name aabbcc as expected it is returning rows but it is doing index seek on non clustered index but as per my knowledge it should do index scan as of data satisfied the filter criteria mentioned in the select query can some please tell me why it is doing index seek instead of index scan the purpose of the entire activity was to proveas am about to give presentation on statistics that sql server looks into statistics to identify the number of records which matches the filter criteria of query before preparing the execution plan and based on the of records matches out of total records in table it will either decide to do scan or seek if of records matches is approximately equal to total number of records in table it should do scan but that is not happening same is the case when am using adventureworks2016 database and running below query select from sales salesorderheader where salesorderid and salesorderid the above query returning records out of but it is still doing clustered index seek am getting terribly confused and it is shaking my concepts can some please help ps cleaned plan cache as well but no luck sql server version is
238873 have created maintenance plans on my sql server say which generate databse backup times daily as read in this the full recovery is good when need point in time recovery my exact question is generating multiple daily backup with simple recovery model similar to full recovery backup with longer period
239314 it seems like ive misunderstood the concept of order by have table with this structure and data create table testtest value1 int value2 int insert into testtest values with the query below select value1 value2 from testtest order by value1 descvalue2 desc expect both columns value1 and value2 from to because use desc for both columns but see this output why isnt value2 also in descending order
239400 cannot work this out how do get the subquery to return the lowest course weight for each lecturer it currently only returns the lowest and is my subquery wrong or outer query question show each lecturer lowest coursework weighting displaying the staff id the module id and the weighting selected select m1 moduleid m1 cwweight staffid from dbo module as m1 inner join dbo lecturer as on m1 moduleconvenor staffid where m1 cwweight select minm2 cwweight from dbo module as m2
239646 im trying to write function to receive number and return its binary format this is what ive developed so far and its ok for inputs but it does not return correct binary format for other numbers could not find the problem and was wondering if you could find the problem create or replace function show binaryi number in number return varchar2 as binary varchar250 number2 number number begin while number2 loop binary binary remainderi number2 number2 number2 end loop binary binary to chari number2 select reverse binary into binary from dual return binary end
239788 have table with two columns want to count the distinct values on col over conditioned by col mytable col col expected result col col result tried the following code select count distinct col over partition by col as result from mytable count distinct col is not working how can rewrite the count function to count distinct values
240469 on production sql server we have following config dell poweredge r630 servers combined into availability group all are connected to single dell san storage unit which is raid array from time to time on primary we are seeing messages similar to below sql server has encountered occurrences of requests taking longer than seconds to complete on file data mydatabase mdf in database id the os file handle is 0x0000000000001fbc the offset of the latest long is 0x000004295d0000 the duration of the long is ms we are novice in performance troubleshooting what are the most common ways or best practices in troubleshooting this particular issue related to storage what performance counters tools monitors apps etc must be used to narrow down to the root cause of such messages might be there is extended events that can help or some kind of audit logging
240503 im trying to understand the performance impact of selecting data from view where one of the columns in view is function of other data in the original table does the computation get performed irrespective of whether or not the computed column is in the list of selected columns if had table and the view declared like so create table price data ticker text ticker of the stock ddate date date for this price price float8 closing price on this date factor float8 factor to convert this price to usd create view prices as select ticker ddate price factor price factor as price usd from price data would that multiplication be performed in query like the one below select ticker ddate price factor from prices is there reference that guarantees this one way or the other was reading the documentation on the rule system in postgres but think the answer really lies with the optimiser since nothing in the rule system documentation indicated that it wouldnt be selected suspect in the above case the computation isnt performed changed the view to use division instead of multiplication and inserted for factor into price data the query above didnt fail but if the query was modified to select the computed column the modified query failed is there any way to understand what computations are being done when select is carried out guess im looking for something like explain but which also tells me about the computations that are being performed
240540 the documentation for sys dm exec query stats states the following an initial query of sys dm exec query stats might produce inaccurate results if there is workload currently executing on the server more accurate results may be determined by rerunning the query sometimes query that dmv during an active workload and would prefer accurate results do not know how to apply the above warning in practice should always query the dmv twice and use the second result set because that will be more accurate that feels bit far fetched do need to be aware of the ways in which the dmv can be inaccurate so can factor that into my analysis if so what kind of inaccuracies can appear missing rows outdated values inconsistent rows or something else what are best practices when using sys dm exec query stats during an active workload
240742 have the following query using mariadb innodb select id sender id receiver id thread id date created content from user message where thread id and placeholder false order by date created desc limit this query fetches messages according to the given conditions and sorts by date created have covering index over thread id date created when running explain the correct index is used and get the output using where although the query is using column in the middle of the statement that is not in the index can use any value for placeholder and the result is the same if change the sorting to use another column the explain correctly indicates using where using filesort im having head scratching moment could anyone please shed light on this what would expect to see is that an additional filesort would be needed as the covering index could not be completely used due to the additional column
241302 is there way to find the events when system health extended event files are rolling over rather than manually monitoring for the events for my medium load server they stay upto days but for heavy loaded servers these files are rolling every mins or so but no fixed pattern or timings we know the reason why and working to filter out unwanted events or ones reported as issues am curious if there is way we can query at what time would the roll over of files is happening do not see much documentation on ms docs as well but cant find this info please suggest if its possible and how
241607 consider declare stringsvar varchar1000 declare emp id int declare strvar sysname employee test set stringsvar select from strvar where emp id emp id print stringsvar the above query is giving an error as mentioned below msg level state line conversion failed when converting the nvarchar value select from employee test where emp id to data type int what needs to be done in this case
241656 have table like this tb1 cod a001 a002 a003 cars baby nasa and then second table tb2 cod no col tb1 description a001 something a002 lasagna what im trying to do is something like this obviously this doesnt work select a001 as select description from tb2 where no col tb1 a001 a002 as select description from tb2 where no col tb1 a002 from tb1 tried with inner joins and some dynamic sql but just cant think in logic for this theres some questions about this but none of them could help me think this is impossible to do with single statement and only possible with dynamic sql edit the correct result would be select cod tb1 a001 as something tb1 a002 as lasagna from tb1 and the result would be cod something lasagna cars baby
242187 have an application written in php with laravel that regularly prepares and executes statements like this all parameters are varchar10 select c1 c2 c3 c4 from mybigtable where is active and c1 in p1 p2 p3 p4 p250 and c2 is not null users have big data grid and they can select many rows there is even button to select all if they select rows this statement is what happens but it takes more than one minute to run which is unacceptable table mybigtable has about millions rows the estimated execution plan shows that of time is spent on an index seek non clustered from this deduce that the situation can not be improved using indexes and that the only issue is in the use of prepared statements if you think am wrong just let me know moreover understand that these prepared statements are prepared used once and discarded so dont think they are really beneficial what recommendation should give to the developers should just tell them to stop using prepared statements and hardcode the values in the query or should give them some workaround like the use of temporary tables make temporary table insert values then make query on mybigtable joined with temp or any other idea edit execution plan https www brentozar com pastetheplan id rj b2xalh
242649 have table as such where ids are repeating id num null null null null null null want to get table where only have the ids which have both null and non null values in the column labelled num like so id num null null
242813 say you have two relations and where has tuples and page accesses and has tuples and page accesses assuming is the outer relation then how many tuple comparisons and page accesses are done and how many page accesses if is the inner relation for each tuple in do for each tuple in do if and satisfy the join condition then output the tuple rs so in order to find out how many tuple comparisons are done need to do because the algorithm is doing this for each tuple and we have in total tuples for and tuples for thus comparisons in total but how to know the page accesses now if is outside we have tuples page accesses for page accesses for page accesses and if is inside then page accesses im not sure if it is correct like that or how is this done correctly pleaseee
242908 ive been having an ongoing debate with various developers in my office on the cost of an index and whether or not uniqueness is beneficial or costly probably both the crux of the issue is our competing resources background have previously read discussion that stated unique index is no additional cost to maintain since an insert operation implicitly checks for where it fits into the tree and if duplicate is found in non unique index appends uniquifier to the end of the key but otherwise inserts directly in this sequence of events unique index has no additional cost my coworker combats this statement by saying that unique is enforced as second operation after the seek to the new position in the tree and thus is more costly to maintain than non unique index at worst have seen tables with an identity column inherently unique that is the clustering key of the table but explicitly stated as non unique on the other side of worst is my obsession with uniqueness and all indexes are created as unique and when not possible to define an explicitly unique relation to an index append the pk of the table to the end of the index to ensure the uniqueness is guaranteed im frequently involved in code reviews for the dev team and need to be able to give general guidelines for them to follow yes every index should be evaluated but when you have five servers with thousands of tables each and as many as twenty indexes on table you need to be able to apply some simple rules to ensure certain level of quality question does uniqueness have an additional cost on the back end of an insert compared to the cost of maintaining non unique index secondly what is wrong with appending the primary key of table to the end of an index to ensure uniqueness example table definition create table test index id int not null identity1 dt datetime not null defaultcurrent timestamp val varchar100 not null is deleted bit not null default0 primary key nonclusteredid desc unique clustereddt desc id desc create index nonunique nonclustered example on test index is deleted include val create unique index unique nonclustered example on test index is deleted dt desc id desc include val example an example of why would add the unique key to the end of an index is in one of our fact tables there is primary key that is an identity column however the clustered index is instead the partitioning scheme column followed by three foreign key dimensions with no uniqueness select performance on this table is abysmal and frequently get better seek times using the primary key with key lookup rather than leveraging the clustered index other tables that follow similar design but have the primary key appended to the end have considerably better performance date int is equivalent to convertint convertvarchar current timestamp if not existsselect from sys partition functions where name npf date int create partition function pf date int int as range right for values go if not existsselect from sys partition schemes where name nps date int create partition scheme ps date int as partition pf date int all to primary go if not existsselect from sys objects where object id object idndbo bad fact table create table dbo bad fact table id int not null identity implemented elsewhere and cdc populates date int int not null dt date not null group id int not null group entity id int not null member of group fk id int not null tons of other columns primary key nonclusteredid date int index ci bad fact table clustered date int group id group entity id fk id on ps date intdate int go if not existsselect from sys objects where object id object idndbo better fact table create table dbo better fact table id int not null identity implemented elsewhere and cdc populates date int int not null dt date not null group id int not null group entity id int not null member of group tons of other columns primary key nonclusteredid date int index ci better fact table clustereddate int group id group entity id id on ps date intdate int go
243115 transactional database used for booking things our vendor was asked to replace temptables with tablevariables because of heavy compile locks but instead they replaced with an actual table that adds spid as column to ensure the stored procedure only acts on the applicable rows do you see any risk in this method of operation before all transactions were isolated within their own transaction worried we may end up locking this table bunch but their opinion is that sql uses row level locking and this wont create more locks sql server version enterprise create table dbo qrytransactions id int identity not null constraint pk qrytransactions primary key clustered spid int not null orderid int itemid int timetransactionstart datetime timetransactionend datetime other fields create index idx qrytransactions spidid on qrytransactions spid id include itemid orderid timetransactionstart timetransactionend
243484 is it possible to capture all queries sent to an ms sql server without third party tooling and without using deprecated features im looking for something similar to the general query log in mysql heres an example using third party tool https blog devart com capturing sql server trace data html heres an alternative using deprecated features https docs microsoft com en us sql tools sql server profiler sql server profilerview sql server is there non deprecated native solution
244000 so database administration is not strong point of mine so have few questions this all relates to the performance of website im looking at and have spent fair bit of time researching diagnosing there are two sites test site and live site have noticed page load times are much longer on the live site as opposed to the test site the specs of the test site are really low vps virtual processors with 14gb ram and sql website running on the same vps the specs of the live website are dedicated servers all with virtual processors and 16gb ram the live site uses separate dedicated server for sql with virtual processors and 112gb ram the website averages concurrent users average at any one time but can fluctuate from to and when we send out an email campaign it can hit when we hit the cpus as you would expect can hit depending on what actions users are doing on the website but they hold up fairly well same with the ram would expect the live website to actually perform faster than the test website but this is not the case have looked through the code but strongly believe its due to the database the database size of the live website is over 50gb when backed up and the test site is 4gb both databases are quite fragmented and im wondering how likely the fragmentation will affect performance also because the live website is many times larger than the test site will high fragmentation affect the performance of the live site much more than the test site here is capture of one of the indexes in the products table because have never dealt with this before what are some tips around rebuilding refreshing indexes should put the website into maintenance mode so no one can interact with tables while im rebuilding refreshing indexes how long would it take on table with indexes and products thanks
244345 have query for producing some text for creating some values text to put in sql file for inserting some rows get blank line in the results postgres select obj id obj type from il2 objects where obj id order by obj id column sciencedomain pis instrument rows doing select its pretty clear its being caused by the obj type being null for obj id postgres select from il2 objects where obj id obj id obj type instrument sciencedomain pis rows confirming its null postgres select from il2 objects where obj type is null obj id obj type why is the result of the first select giving me blank row even casting obj type text still gave me blank row additional info the schema for what its worth postgres il2 objects table il2 objects column type collation nullable default obj id integer not null generated by default as identity obj type character varying indexes objects pkey primary key btree obj id
244756 have field that may start and end with quotes only want to remove the quotes if the string starts and ends with quotes if quotes exists inside the string then wish to retain them have used reverse and stuff which works if the field does start and end with quotes but if it does not it just returns null how can ensure still get the field result if it did not start with quotes what have reversestuffreversestufffieldname charindex fieldname len charindex reversestufffieldname charindex fieldname len
245148 every query run in ssms append the annoying message completion time how can disable that text
245178 have lot of large tables around million wide rows which need to be regularly loaded into sql server for read only reporting would like these tables to be as small as possible on disk and this matters more than performance improvements in either loading or querying here is what have been doing for the tables which require no further indexing create the table with data compression page use bcp to bulk insert the data from flat file into the new table column types in the tables are varchar never more than not max float tinyint or date not datetime all columns are created as nullable and no primary or foreign keys are defined they dont matter for the querying and the tables are never updated directly default collation on everything is sql latin1 general cp1 ci as when do this can see in sys allocation units that page data compression has been applied to the heap and can see in sys partitions that the fill factor is correctly since the tables are much smaller than uncompressed tables would be thought the compression was accomplished however if then rebuild with the same option data compression page the supposedly already compressed table gets about smaller it looks like its going from about rows per data page to rows per page only once though rebuilding again after that doesnt make it any smaller than the first rebuild did the questions so my questions are what is going on here and is there way to get this extra small compressed size directly as load the table without having to rebuild after the data is loaded
245699 im trying to use cursor to clean up temp tables when they are no longer needed have small table which has the names of the temp tables along with an identifier the cursor is stuck in an infinite loop but only if execute certain statements in it if just print out the values from the fetch it works perfectly here is the code declare id bigint declare table name varcharmax declare st cursor local fast forward for select id tablename from searchtables where customerid is null open st fetch next from st into id table name while fetch status begin ifobject id table name is not null execdrop table table name update searchtables set deleted where id id print cast id as varcharmax table name fetch next from st into id table name end close st deallocate st if comment out these lines ifobject id table name is not null execdrop table table name update searchtables set deleted where id id print outputs all of the ids and table names if dont comment them all get is the first row over and over until cancel the query also tried changing the if line to execdrop table if exists table name but that didnt work either
245781 want to run sql server vulnerability assessment from sql server agent job currently am attempting job with powershell script and am running command like the one below invoke sqlvulnerabilityassessmentscan serverinstance escape dquotesrvr database adventureworks have confirmed that invoke sqlvulnerabilityassessmentscan is available on the sql server can run it from the powershell command prompt there but when run my job receive an error stating that the term invoke sqlvulnerabilityassessmentscan is not recognized as the name of cmdlet after looking at this microsoft article am wondering if sql agent only has subset of powershell cmdlets that it can access how can run the vulnerability assessment scan from sql job
245983 is there any way of seeing details of sessions from the past fully expect the answer to be no am trying to find out what server certain application is running on that application accessing certain database know the application runs at some point on friday evening so was hoping that sql server maintains session history so could just look up sp who like details for the hours in question like say dont expect this to be possible but right now am absolutely clutching at straws thanks in advance edit found the server in question by other means in the end the job executed sql statement to create file which was then ftpd to another site that site has ftp logging that could give me the name and ip address of the source server thanks for the answers only wish could accept them both as they both make very good points and give very good suggestions
246015 im trying to set up test environment here to learn more and apply this on our production environment ive created and installed the windows cluster with cluster manager now im going to install sql server enterprise on both machines and create ha with then with availability groups the failover will be manual they will be in synchronized data transfer my question is can normally install stand alone installarion or add the sql server or need to use new sql server failover cluster installation cant find anything about this on the internet im going to install sql server on both nodes that will be configured in the cluster
246372 in our environment we have some servers that are in an always on availability group and some that are standalone we normally backup to network share but we have recently observed that as the databases are growing bigger the time taken is getting longer which slows down the whole network ola hallengrens script is being used with compression and also splitting the backup files am only performing daily full backups the backups are going to the network share emc isilon drive am never comfortable with emc dd boost the only alternative is to do local backup and then copy to the same network share is there an efficient way other than the above
246448 was talking with the bossman and he prefers subqueries over ctes personally loathe subqueries he mentioned that subqueries can be faster but am not convinced ran this short test with classes as select top classkey from dimclass group by classkey order by count1 desc policies as select carrierkey policykey periodeffectivedate from dimpolicy exposure as select policykey classkey from dimexposure select from policies inner join exposure on policykey policykey inner join classes on classkey classkey has an excution plan of https www brentozar com pastetheplan id sjyjuznhs select carrierkey policykey periodeffectivedate from dimpolicy inner join select policykey classkey from dimexposure on policykey policykey inner join select top classkey from dimclass group by classkey order by count1 desc on classkey classkey has the same execution plan https www brentozar com pastetheplan id rjs lbvsr question is this always the case in the weird and wonderful world of sql server the answer always seems to be it depends
246495 have table of names table john jim jason and table of xml strings table example show title showtitle false showline false showdescription false showexpandcollapse false isvisible true description pagebreakafter false pagecaption applink infolink imagelink pause false pausemessage pausemessagestyle pausetitle screenstyle showoption sequence name jim caption test selectoptionsimagelinkfieldexpression imagelink show example vars var name matrixname value lockexitpaircompatability value var var name jason value ifexistsminute value min tonumberminute value min value var var name minnum value ifexistsagency and agency cert minnum value var var name where value site root site and hwtype in tosqlarrayroot components activedoor hardwaretype value var var name where2 value and hwsubtype in tosqlarrayroot components activedoor locksubtype value var var name compatiblelist value usr fetchmatrix value var vars example rule property name collectionvariable displayname collection variable valuetype rvalueexpression john property property name keyvariable displayname key variable valuetype lvalueexpression nextlock property rule how can search to find which names in table arent in any of the xml strings the data shown are just examples the xml in the database is of all different formats in the xml data it might be root name would still want it to find it even with the root attached to it
246649 have the following query declare linq uniqueidentifier set linq some guid select top eventid as eventid datecreated as datecreated locationid as locationid sourcename as sourcename sourcestate as sourcestate priority as priority eventdescription as eventdescription firsttrigger as firsttrigger from dbo watchdog where locationid linq and firsttrigger order by datecreated desc watchdog table defines indecies clustered index on eventid primary key column unclustered index on datecreated column this is actual execution plan for the query reading other posts on how to eliminate key lookup added another non clustered index which includes all columns from select create nonclustered index locationid firsttrigger on dbo watchdog locationid asc firsttrigger asc include eventid datecreated sourcename sourcestate priority eventdescription with statistics norecompute off drop existing off online off on primary go however this didnt help and actual execution plan is the same if look at key lookup the output is actually included in newly added non clustered index my question is why its still doing key lookup instead of index scan seek update following some suggestions in the comments dropped newly created non clustered index instead recreated non clustered index on datecreated column including columns from select now execution plan is the following also query execution time dropped from minute to few seconds this table has million rows does this mean key lookup was done due to order by on non clustered index
246734 am using recursive cte on tree structure to list all the descendants of particular node in the tree if write literal node value in my where clause sql server seems to actually apply the cte just to that value giving query plan with low actual rows counts et cetera however if pass the value as parameter it seems to realize spool the cte and then filter it after the fact could be reading the plans wrong haven noticed performance issue but am worried that realization of the cte could cause issues with larger data sets especially in busier system also normally compound this traversal on itself traverse up to ancestors and back down to descendants to ensure that gather all related nodes due to how my data is each set of related nodes is rather small so realization of the cte doesn make sense and when sql server seems to realize the cte it is giving me some quite large numbers in its actual counts is there way to get the parameterized version of the query to act like the literal version want to put the cte in reusable view query with literal create procedure as begin with descendants as select parentid id id descendantid from tree where parentid is not null union all select id id descendantid from descendants join tree on descendantid parentid select from descendants where id order by id descendantid end go exec query with parameter create procedure id bigint as begin with descendants as select parentid id id descendantid from tree where parentid is not null union all select id id descendantid from descendants join tree on descendantid parentid select from descendants where id id order by id descendantid end go exec setup code declare count bigint create table tree id bigint not null primary key parentid bigint create index tree 23lk4j23lk4j on tree parentid with number as select cast1 as bigint value union all select value from number where value count union all select value from number where value count insert tree id parentid select value case when value then value end from number
246804 have noticed this error occasionally in the sql error log spid20sunknownappdomain master sys runtime is marked for unload due to memory pressure am using sql server sp1 cu5 am pushing for patching but the company is resistant everything have read points to non clr specific memory pressure there are suggestions around changing the memtoleave setting in the start up parameters is this still the case for newer versions of sql server or are there other recommendations
246832 database sql server enterprise cu16 we recently tried switching from the default index rebuild maintenance jobs to the ola hallengren indexoptimize the default index rebuild jobs had been running for couple of months without any issues and the queries and updates were working with acceptable execution times after running indexoptimize on the database execute dbo indexoptimize databases user databases fragmentationlow null fragmentationmedium index reorganizeindex rebuild onlineindex rebuild offline fragmentationhigh index rebuild onlineindex rebuild offline fragmentationlevel1 fragmentationlevel2 updatestatistics all onlymodifiedstatistics performance was extremely degraded an update statement that took 100ms before indexoptimize took 000ms afterwards using an identical plan and queries were also performing several orders of magnitude worse since this still is test database were migrating production system from oracle we reverted to backup and disabled indexoptimize and everything returned to normal however we would like to understand what indexoptimize does differently from the normal index rebuild that could have caused this extreme performance degradation in order to make sure we avoid it once we go to production any suggestions on what to look for would be greatly appreciated execution plan for the update statement when it is slow after indexoptimize actual execution plan coming asap havent been able to spot difference plan for the same query when it is fast actual execution plan
246888 want to atomically reserve inventory for objects for users have two inventory tracking tables one for global inventory and one for personal inventory global inventory table columns objectid uniqueidentifier count int personal inventory table columns userid nvarchar64 objectid uniqueidentifier count int there are two tables because have to enforce maximum allowed reservations for each object in general as well as per user for example an object may be restricted to have reserved overall with maximum of of that object per user the global inventory table is uniquely keyed on objectid and the personal inventory table is uniquely keyed on userid objectid the primary key is the only index on each table so locks are only taken on the rows and never some other index key the global inventory table will always reside on single database the personal inventory table may be sharded across multiple databases so the transaction that updates the global and one or more personal tables may be distributed this reserve inventory transaction is the only transaction that will ever be performed on these tables sample request to reserve inventory for objects looks like this some objects have restrictions while others do not objectid quantitytoreserve objectid quantitytoreserve globalmax personalmax objectid quantitytoreserve such request to reserve inventory for multiple objects has to be completed atomically and must succeed for all objects complete set of locks is always taken out on the global table before attempting to take any on the personal table atomic reservation is achieved by starting transaction and then updating rows in the global inventory table first for all object ids in ascending order an update statement for an object without restrictions looks like this update globalinventory with rowlock xlock holdlock set count count quantitytoreserve where objectid objectid an update statement for an object with restriction looks like this update globalinventory with rowlock xlock holdlock set count count quantitytoreserve where objectid objectid and count quantitytoreserve globalmax similar update statements are later used to update and lock rows in the personal inventory table with the addition of userid in the predicate specifically join with userids table updating the rows in order by object id effectively takes out exclusive row locks on the records in that order which are then held for the remainder of the transaction believe this happens even without the holdlock hint for update statements other concurrent transactions will block waiting for the transaction to commit the updated rows before the others can obtain the locks required to update those rows because the row locks are taken out in ascending order once all locks in the set have been acquired that guarantees that no other transaction holds any of those exclusive locks this is well established fact locking order matters and unlocking order does not just ask linus https yarchive net comp linux lock ordering html my first question is will these update statements work as intended this question has multiple parts such as will the predicate identify the rows to update and will the predicate hold true by the time the exclusive row lock is held just before the rows are updated do have the correct lock hints since weve established that no other transaction holds any of the locks the current one holds it logically follows that no other transaction would be attempting to update or lock any of the records with those same object ids in the personal inventory table at this point think need to force the database engine to use row level locks when updating the personal inventory rows as well the reason is if it escalates to page lock it could inadvertently lock records that happen to be on that page but dont belong to the set of object ids the transaction is working with for example suppose concurrent transaction locks global record so it seems totally unrelated to the first transaction working with records for objects and none of the global or personal inventory records should overlap so there should be no lock contention in the personal inventory records each is working with either however if this concurrent transaction takes out page level locks in the personal inventory table and some of those pages for happen to contain records for object transaction could inadvertently hold page locks for records belonging to both and likewise the first transaction may also hold page level locks that contain some records for and and neither transaction can proceed because each one has locked pages that the other is waiting for in other words page level locks destroy the established locking order by locking unrelated records in an arbitrary order updating records in the personal inventory table is bit more complex because it has to update multiple rows the update statements will still run for one object id at time but it will be joined with temporary table that establishes the set of user ids update pi with rowlock xlock holdlock set count count quantitytoreserve from personalinventory pi inner join userids uids on pi userid uids userid where objectid objectid and count quantitytoreserve personalmax my second question is will this update statement with join to userids table take the right exclusive row locks only the records actually updated as result of satisfying the predicate its critical to the correct functioning of this system that this is the case so if its not id like to know and id like to know why if the expected locks arent held what locks are held please assume that have disabled lock escalation on the table other notes was concerned with whether forcing such row level locks was even possible but then discovered theres table option to disable lock escalation im concerned more about correctness than performance here using database row locks is going to be many orders of magnitude faster than any other locking solution that involves multiple round trips to the server using sp getapplock also will not work because it would redundantly perform the same function that the locks on the global inventory table achieve while simultaneously doing nothing to prevent the page lock creep just mentioned by using database row level locks multiple concurrent transactions can complete quickly simultaneously with minimal lock contention this will result in atomic high throughput inventory reservations without having to worry about managing transactions at the application level which ultimately would be more complex and less reliable page locks would be acceptable if there was way to force one part of composite key to reside on different data pages but dont think thats possible for example if keyed the personal table on objectid userid id have to ensure each page contains records for at most single object id and many users
247090 someone said its preferable to craft your queries to avoid duplicate key exceptions but im not convinced thats more performant that just setting ignore dup key on for the index my goal is to ensure row or set of rows exists for one or more users before attempting to update those rows do this so that when attempt to update the row with an update statement like the one below and no rows are affected its because the count portion of predicate wasnt satisfied as opposed to the row not existing at all the id portion of the predicate not being satisfied update inventory set count count where id and count maxinventory could run existsselect from inventory where id to check for that single row and only insert the row if it does not exist that simply avoids unnecessary inserts the insert if necessary would still have to contend with concurrent transactions so duplicate key exceptions can still occur im curious whether its more performant to just turn on ignore dup key in this scenario rather than allowing the error to be thrown and caught specifically im curious if its as fast or possibly even faster than running an exists check to just attempt to insert the record and let it ignore duplicate keys this becomes even more important when im checking for and initializing multiple records at once for example if need to ensure records for thousands of users exist in single update statement the logic would be much simpler if just ran that insert statement up front letting it ignore duplicate keys avoiding duplicates would be more complex because id have to first query the table for which records dont exist then attempt to add just those records again ignoring duplicate keys just inserting may be faster even if all the records exist could meet it halfway and check whether any of the records are missing such as with left join or count comparison but why bother if the insert ignoring duplicate keys is just faster is is good idea to use ignore dup key and just attempt inserts instead of bothering with checking for row existence ahead of time if not why
247411 is it possible to group by elements as in column like value in pivot table have table dbt status which contains various statuses of databases instances etc and dont want to pivot query all the prod and test values as single values but group them instead of having columns for the statuses prod prod acc prod app etc would have only one column containing the values for name like prod and name like test what have so far table definition create table dbt status id int identity11 not null name nvarchar not null constraint pk status primary key clustered id asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor on primary constraint ix status unique nonclustered name asc with pad index off statistics norecompute off ignore dup key off allow row locks on allow page locks on fillfactor on primary on primary go table values insert into dbt status id this column value is auto generated name values test acc test app test dba prod acc prod app prod dba prod test migrated offline reserved the pivoted status table select database status as db status as test acc as test app as test dba as prod acc as prod app as prod dba as prod as test as migrated as offline as reserved from select id name from dbt status as source pivot countname for id in as pivottable output so far db status test acc test app test dba prod acc prod app prod dba prod test migrated offline reserved database status db fiddle the dbfiddle so far question instead of having multiple rows for the various test and prod values would prefer to have them grouped similar to the following db status test prod migrated offline reserved database status dont have any clue how to go about solving my question to be honest only just grasped pivot yesterday after extensive trial and errors this question is loosely related to the question how to create sums counts of grouped items over multiple tables have already asked the tables dbt instance and dbt database contain column with the statusid which corresponds to the table we are looking at now
248503 below are my two tables am trying to write query that joins these tables and returns the source title and destination title for the given source id of result map table have tried written the query but can only solve one part of the puzzle not the whole thing select title as sourcetitle from result map as rm inner join table result tr on rm sourceid tr id the above query gives me the title for the source but not the destination title
248587 have an availability group ag with multiple databases db db db and multiple secondaries sec sec and one of the databases will not resume synchronization on just one of the secondaries for this example db is not synchronizing on sec and no amount of restarting sql server or resuming hadr will get it to start again dont want to remove the replica secondary sec from the ag because would then have to resync all of the databases db db and db and that would take more time than is necessary also dont want to completely remove the database db from the ag because there are other secondaries sec where there is no problem and dont want to have to resync it or temporarily lose my hadr on the secondary where it is working how can remove just this one secondary database from the ag resynchronize it and add it back to the ag
248610 need to use sql server database mail feature in my project however database mail seems not available on my development instance there should be database mail node in the object explorer in this screenshot cant seem to find any also shown is my sql server version is database mail not supported in this sql server version which sql server version or edition required to use database mail all documentation can find online always assume the feature is already installed and just go on explaining how to configure it if my sql server version supports database mail how to install enable the feature
249465 have the scenario where have to generate backup of the database sql server and restore into the new server sql server while taking backup data should not be changed in any case so have two options to do that but am not sure how it will work everything is using sql job only set read only database and restore into the new db server is it possible to restore read only db on the new server the destination server is already having read write online database by the same name set an offline database and restore onto the new db server is it possible to restore an offline db in the new server the destination server already has an online read write database by the same name
249520 have requirement to come up with reporting for indexes for our whole production environment found the below script online and modified it according to my requirement and trying to execute it using cursor have also tried sp msforeachdb to get result from all databases in the instance the script should show all exact duplicate indexes in particular database although have wrapped my query in double quotes but keep getting too many errors if run the script without loop it returns the result correctly please see the script and errors im getting below the script have been struggling with this for the past few days and have looked exhaustively online here and many other posts but can figure this out will need to send the result in email body to our distribution list for all servers so will appreciate if someone has better idea on how to establish this script declare db name as nvarcharmax declare db names cursor for select name from sys databases where name not inmaster model msdb tempdb and state open db names fetch db names into db name while fetch status begin if object idtempdb indextemp is not null drop table indextemp if exist drop the temp table exec begin use db name with cte index data as select schema data name as schema name table data name as table name index data name as index name stuffselect column data key cols name case when index column data key cols is descending key then desc else asc end include column order asc desc from sys tables as inner join sys indexes index data key cols on object id index data key cols object id inner join sys index columns index column data key cols on index data key cols object id index column data key cols object id and index data key cols index id index column data key cols index id inner join sys columns column data key cols on object id column data key cols object id and index column data key cols column id column data key cols column id where index data object id index data key cols object id and index data index id index data key cols index id and index column data key cols is included column order by index column data key cols key ordinal for xml path as key column list stuff select column data inc cols name from sys tables as inner join sys indexes index data inc cols on object id index data inc cols object id inner join sys index columns index column data inc cols on index data inc cols object id index column data inc cols object id and index data inc cols index id index column data inc cols index id inner join sys columns column data inc cols on object id column data inc cols object id and index column data inc cols column id column data inc cols column id where index data object id index data inc cols object id and index data index id index data inc cols index id and index column data inc cols is included column order by index column data inc cols key ordinal for xml path as include column list index data is disabled check if index is disabled before determining which dupe to drop if applicable from sys indexes index data inner join sys tables table data on table data object id index data object id inner join sys schemas schema data on schema data schema id table data schema id where table data is ms shipped and index data type desc in nonclustered clustered insert all records into temp table indextemp with appropriate filters select into indextemp from cte index data dupe1 where exists select from cte index data dupe2 where dupe1 schema name dupe2 schema name and dupe1 table name dupe2 table name and dupe1 key column list dupe2 key column list and isnulldupe1 include column list isnulldupe2 include column list and dupe1 index name dupe2 index name and index name not like pk return duplicate tbale names only select from indextemp where table name in select table name from indextemp group by table name having count order by table name end fetch db names into db name end close db names deallocate db names for each database get these errors msg level state line incorrect syntax near the keyword as msg level state line incorrect syntax near the keyword order msg level state line an expression of non boolean type specified in context where condition is expected near and
249593 im accessing and creating reports from vendor via replicated sql server database theyve done some absolutely insane things that ive been trying to solve for but this one takes the cake they have table that has many standard columns but this table also has column called data the column is legacy text data type and it contains giant hundreds list of key value pairs each pair is separated by crlf and the key and value are separated by an equal sign example select mytable data from mytable where tblkey result key value key value key value key value im trying to determine the most efficient way to break that column out into usable table of data the end goal would be to be able to query the table in way that returns the table key along with specified key values as column fields as such tblkey key key key value value value value value value value value value is there way to mold that column into view cant imagine that function would be particularly efficient but im sure could parse things out that way using string split or something of that sort has anyone run into this type of atrocity before and found good way to manipulate it into usable data edit to add dbfiddle sample data the data is replicated from vendors source so cant create new tables can create views procedures and functions thats what im looking for advice for decent way to accomplish
249617 we have postgresql table with billion rows that has developed nasty habit of missing the proper indices and doing primary key scan on certain limit operations the problem generally manifests on an order by limit clause common pattern in django pagination where the limit is some relatively small subset of the results matched by the index an extreme example is this select from mcqueen base imagemeta2 where image id in order by id desc limit where the items in that in clause are and total rows matched by the index on image id is the explain shows that it misses the image id index and instead does pk scan of 5b rows limit cost rows width index scan backward using mcqueen base imagemeta2 pkey on mcqueen base imagemeta2 cost rows width filter image id any bigint if the limit is increased to it works as expected limit cost rows width sort cost rows width sort key id desc index scan using mcqueen base imagemeta2 image id 616fe89c on mcqueen base imagemeta2 cost rows width index cond image id any bigint this also happens on queries where the index matches rows and the limit is set to so something that easily happens in real world rest api pagination the table definition is mcqueen mcqueen base imagemeta2 table public mcqueen base imagemeta2 column type modifiers id bigint not null default nextvalmcqueen base imagemeta2 id seq regclass created at timestamp with time zone not null image id bigint not null key id smallint not null source version id smallint not null indexes mcqueen base imagemeta2 pkey primary key btree id mcqueen base imagemeta2 image id 616fe89c btree image id mcqueen base imagemeta2 key id a4854581 btree key id mcqueen base imagemeta2 source version id f9b0513e btree source version id foreign key constraints mcqueen base imageme image id 616fe89c fk mcqueen foreign key image id references mcqueen base imageid deferrable initially deferred mcqueen base imageme key id a4854581 fk mcqueen foreign key key id references mcqueen base metakeyid deferrable initially deferred mcqueen base imageme source version id f9b0513e fk mcqueen foreign key source version id references mcqueen base metasourceversionid deferrable initially deferred im novice at best when it comes to tuning but figure that the defaults for statistics are not up to that tables size and so it naively thinks that pk scan is faster than an index scan
249715 default collation type in sql server allows for indexing against case insensitive strings yet the case of the data is persisted how does this actually work im looking for the actual nuts and bolts bits and bytes or good resource that explains it in detail create table casetest fruitnames nvarchar50 not null create unique index ix fruitnames on casetestfruitnames insert into casetest values apples insert into casetest values pears this insert fails insert into casetest values pears this yields pears as result select from casetest forceseek where fruitnames pears update casetest set fruitnames pears where fruitnames pears this yields pears as result select from casetest forceseek where fruitnames pears questions about sql server collations you were too shy to ask by robert sheldon covers how to use collation it does not cover how collation works im interested in how an index can be efficiently created queried not caring about case while simultaneously storing case data
249838 me and colleague of mine discussed the implications of use of the serializable isolation level he said it locked the entire table but disagreed to that telling him it potentially could but it tries to apply range locks and it doesnt apply true serialization as explained here the serializable isolation level cant find anything in the docs either for the locks entire table set transaction isolation level the doc states bunch of things regarding range locks so in theory you could lock the entire table by simply having range lock that locks the entire range of possible values in the table but it doesnt lock the table am completely wrong here does it in fact lock the entire table or tables
250166 could you please give little clarification about sql server backup message for example backup database successfully processed pages in seconds mb sec for example we have disk with database and we have separate disk for backup does it mean that mb sec is that one disk reads data for backup with speed of mb sec and another disk writes data with speed mb and as sum it gives us speed of mb sec is it correct
250395 here is an example of some tables that exist in database where work the data isnt actually around schools but the structure is identical there are four tables school school id school name clubtype clubtype id clubtype name club club id school id clubtype id student student id name club id knowing that the club table will never have additional columns because the real data isnt actually about school clubs believe clearly better design eliminating the club table to avoid joins would be school school id school name clubtype clubtype id clubtype name student student id name school id clubtype id edit we also know that each club id may only have one type the relationship from club to clubtype is to my question is does the first example violate some known rule of database normalization or some other mathematical principle or is it just case of poor design
250490 recently we have an audit database that went over one terabyte and since we have storage problems management is looking for options my proposal is at the end of each year we take backup and truncate all the tables which will keep the database manageable it will not be beneficial to have an archive database as it will again consume the same space would like to have an expert opinion about the options that can propose to the management that is either allocate more space or truncate the whole database every year
250573 have table like this in sql server id start mile end mile what want to do is to split the miles into thousandths per id like this id start mile end mile any ideas as to how to go about doing this am trying to stay away from cursors unless thats the only way to do this have been able to get this query together but not sure how to incorporate it with the id and mile limits have so that it runs for the whole table without the declared from and to variables declare from decimal15 declare to decimal15 with cte as select from as value union all select convertdecimal15 value from cte where value to select from cte option maxrecursion
251034 recently accidentally shrunk tempdb log to almost after getting alerted that log drive filling up am told that it will lead to slowness can someone please explain why it will lead to slowness
251805 using windows authentication is there way to prohibit users from connecting via odbc to the database
251988 know the logical order of execution of the sql query which is from on join where group by with cube or with rollup having select distinct order by top what will happen if there are more than one join in the query for instance if we have query like this select from user branch t1 inner join dimcustomer2 t2 on t1 branch code t2 branch code inner join customer guarantee t3 on t3 customer num t2 customer num some example data customer guarantee customer num branch code user branch user id branch code u1 dimcustomer2 customer num branch code how will this execute which join will be executed first and what if there are different kind of joins in the query what would be the order of executing joins in that case thanks in advance
252191 say am running log backup and that log backup takes minutes to complete during that minute window further transactions are run given the below example which transactions does the log backup actually contain transaction commits transaction opens log backup begins transaction opens transaction commits log backup completes transaction commits
252661 take the following example select calculationa as cola calculationb as colb calculationa calculationb as colc from tablea would calculationa and calculationb each be calculated twice or would the optimizer be clever enough to calculate them once and use the result twice would like to perform test to see the result for myself however am not sure how could check something like this my assumption is that it would perform the calculation twice in which case depending upon the calculations involved might it be better to use derived table or nested view consider the following select tableb cola tableb colb tableb cola tableb colb as colc from select calculationa as cola calculationb as colb from tablea as tableb in this case would hope that the calculations would only be performed once please can someone confirm or refute my assumptions or instruct me on how to test something like this for myself thanks
253166 im reading about sql server high availability solutions and disaster recovery and among the available resources the sql server have snapshot feature in theory all seems like beautiful also read that snapshot will copy database at point in time and you can use this to restore database in this answer there is comment by peter schofield about sql server snapshots not having support and is useful in development environment for quick rollbacks perhaps the biggest hindrance to adoption is that management studio didnt offer support it sounds like an ideal use of snapshots in dev environment just for quick script deployments and quick roll backs would like to know if snapshots are really useful in production environments what are some examples of usage in production and please include personal examples about when youve used snapshots to provide solution on production systems the principal objective is provide some examples of real usage and through these examples get some useful ideas for me and for everyone who will be reading this post in my case use sql server enterprise edition in production environment
253873 out of habit never use select in production code only use it with ad hoc scrap queries typically when learning the schema of an object but ran across case now where im tempted to use it but would feel cheap if did my use case is inside stored procedure where local temp table is created that should always match the underlying table used to create it whenever the stored procedure runs the temp table is populated much later on so quick hack to create the temp table without being verbose would be select into temptable from realtable where especially for table with hundreds of columns if the consumer of my stored procedure is agnostic to dynamic result sets then are there any issues with me selling my services to select
253924 have stored procedure that returns multiple recordsets for use in an application sometimes some of those recordsets are empty id like to reduce overhead and only return those which have or more rows my question is how can only return those recordsets which have rows the application simply expects or more recordsets and loops through each and prints them out know can skip them in the application code but am trying to prevent them from being returned at all if empty the procedure is as simple as this create procedure bfsp proc nm as begin select from table select from table select from table return end go in the actual procedure some of the queries are expensive so dont want to have to test the query then if it returns row or more execute it again as it would be too expensive
254252 today wanted to define uuid of value being sql server man id usually select cast0x0 as uniqueidentifier but im in postgres world now so whipped out sensible select cast x00 bytea as uuid error cannot cast type bytea to uuid line select cast x00 bytea as uuid damn so head over to the postgres docs on type conversion hoping to see doc like this one to review which datatypes can cast between by default without needing to create an explicit cast if such chart does exist in the docs its well hidden google is likewise not super helpful in this regard is there good documentation reference for default permissible type casts in postgresql to be clear dont actually care about the uuid
